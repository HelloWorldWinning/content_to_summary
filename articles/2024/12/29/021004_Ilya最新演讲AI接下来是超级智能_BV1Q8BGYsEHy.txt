Timestamp: 2024-12-29T02:10:04.060103
Title: Ilya最新演讲AI接下来是超级智能 BV1Q8BGYsEHy
URL: https://b23.tv/EYMsoqN
Status: success

Description:
*   **Core Ideas & Takeaways:**

    *   **Historical Context:** The talk revisits a 2014 presentation on autoregressive models, emphasizing the core concepts: large neural networks trained on large datasets.
    *   **Scaling Hypothesis:** The speaker reflects on the "scaling hypothesis" – the idea that increasing data and network size leads to guaranteed success – which proved generally correct.
    *  **Connectionism:** The idea that artificial neurons can emulate biological neurons, driving development of large neural nets, despite the difference in learning capacity.
    *   **Pre-training's End:**  Pre-training (as currently practiced) will end because dataset growth is limited while compute power continues to grow.
    *   **Post-Pre-training:** The future lies in areas like agents, synthetic data and inference-time compute, to overcome the limit on data.
    *   **Reasoning as a Differentiator:** Reasoning will be a core feature of future models, making them less predictable than current systems focused on replicating human intuition.
    *   **Superintelligence:**  The field is headed towards superintelligence which will have agentic, reasoning, self aware abilities and be qualitatively different and unpredictable.
    *   **Bio-inspired AI:** While neural nets are inspired by biology, more specific inspirations could still be explored.
    * **Auto Correction:** Future reasoning based models could auto correct themselves and avoid hallucinations.

*   **Core Conclusion:**  The field of AI is transitioning from an era of scaling to a new phase where reasoning, and different architectures and abilities (beyond those found in the current scaling paradigms), will shape the future, ultimately leading to the development of systems with radically different properties.

*   **Fundamental Conclusion:**  The core of progress will be how to reason with and build systems using limited and existing data, which is the major challenge, and will result in systems qualitatively different from today.


Content:
 I want to thank the organizers for choosing a paper for this award. It was very nice. And I also want to thank my incredible co-authors and collaborators, Oriol Vinyel's and Kwakli, who stood right before you a moment ago. And what you have here is an image, a screenshot, from a similar talk ten years ago at Newrips in 2014 in Montreal. And it was a much more innocent time. Here we are shown in the photos. This is the before. Here's the after, by the way. And now we've got my experienced, hopefully visor. But here I'd like to talk a little bit about the work itself and maybe a ten-year retrospective on it. Because a lot of the things in this work were correct, but some not so much. And we can review them and we can see what happened and how it gently flowed to where we are today. So let's begin by talking about what we did. And the way we'll do it is by showing slides from the same talk ten years ago. But the summary of what we did is the following three bullet points. It's an author-regressive model trained on text. It's a large neural network and it's a large data set. And that's it. Now let's dive in into the details a little bit more. So this was a slide ten years ago, not too bad. The deep learning hypothesis. And what we said here is that if you have a large neural network with ten layers, then it can do anything that a human being can do in a fraction of a second. Like why did we have this emphasis on things that human beings can do in a fraction of a second? Why this thing specifically? Well, if you believe the deep learning dogma, so to say, that artificial neurons and biological neurons are similar, or at least not too different, and you believe that real neurons are slow than anything that we can do quickly by V, I mean human beings. I even mean just one human in the entire world. If there is one human in the entire world that can do some task in a fraction of a second, then a ten-layer neural network can do it too, right? It follows. You just take their connections and you embed them inside your neural net. They're the official one. So this was the motivation. Anything that a human being can do in a fraction of a second, a big ten-layer neural network can do too. We focused on ten-layer neural networks because this was the neural networks we knew how to train back in the day. If you could go beyond in your layers somehow, then you could do more. But back then we could only do ten layers, which is why we emphasized whatever human beings can do in a fraction of a second. A different slide from the talk, a slide which says, are main idea. And you may be able to recognize two things, or at least one thing. You might be able to recognize that something autoregressive is going on here. What is it saying really? What does this slide really say? This slide says that if you have an autoregressive model, and it predicts the next token well enough, then it will in fact grab and capture and grasp the correct distribution over whatever cover sequences that come next. And this was a relatively new thing. It wasn't literally the first ever autoregressive neural network. But I would argue it was the first autoregressive neural network where we really believed that if you train it really well, then you will get whatever you want. In our case, back then was the humble today humble, then incredibly audacious task of translation. Now I'm going to show you some ancient history that many of you may have never seen before. It's called the LSTM. To those unfamiliar, an LSTM is the things that poor deep learning researchers did before Transformers. And it's basically a ResNet, but it's rotated at 90 degrees. So that's an LSTM. And it came before, it's like kind of like a slightly more complicated ResNet. You can see the RIS your integrator, which is now called the residual stream, but you've got some multiplication going on. It's a little bit more complicated, but that's what we did. It was a ResNet, a rotated 90 degrees. Another cool feature from that old talk that I want to highlight is the Pius parallelization. But not just any parallelization, we used Pipeline in it, as witnessed by this one layer per GPU. Was it vise to Pipeline? As we now know, Pipeline in is not vise. But we were not as vise back then. So we used that and we got a 3.5x speed up using 8 GPUs. And the conclusion slide in some sense, the conclusion slide from the talk from back then is the most important slide, because it's spelled out what could arguably be the beginning of the scaling hypothesis, right? That if you have a very big data set and you train a very big neural network, then success is guaranteed. And one can argue, if one is charitable, that this indeed has been what's been happening. I want to mention one other idea. And this is, I claim, the idea that truly stood the test of time. It's the core idea of deep learning itself. It's the idea of connectionism. It's the idea that if you allow yourself to believe that an artificial neuron is kind of sorta like a biological neuron. Right? If you believe that one is kinda sorta like the other, then it gives you the confidence to believe that very large neural networks. They don't need to be literally human brain scale, they might be a little bit smaller. But you could configure them to do pretty much all the things that we do, human beings. There's still a difference, or I forgot to say then. There's still a difference because the human brain also figures out how to reconfigure itself. Whereas we are using the best learning algorithms that we have which require as many data points as their parameters. Human beings are still better in this regard. But all this led, so I claim, arguably, is to the age of pre-training. And the age of pre-training is what we might say, the GP2 model, the GP3 model, the scaling laws. And I wanna specifically call out my former collaborators, Alacradford, also Jared Kaplan, Daria Modae, for really making this work. But that led to the age of pre-training. And this is what's been the driver of all of progress, all the progress that we see today. Extra-large neural networks. Extra-ordinary neural networks. Trained on huge data sets. But pre-training, as we know it, will unquestionably end. Pre-training will end. Why will it end? Because while compute is growing through better hardware, better algorithms, and larger clusters, right, all those things keep increasing your compute. All these things keep increasing your compute. The data is not growing because we have but one internet. We have but one internet. You could even say, you can even go as far as to say the data is the fossil fuel of AI. It was like created somehow. And now we use it. And we've achieved peak data. And there'll be no more. We have to deal with the data that we have. I would still still let us go quite far. But this is, is only one internet. So here I'll take a bit of liberty to speculate about what comes next. Actually, I don't need to speculate because many people are speculating too. And I'll mention their speculations. You may have heard the phrase agents. It's common. And I'm sure that eventually something will happen. But people feel like something agents is the future. More concretely, but also a little bit vaguely, synthetic data. But what does synthetic data mean? Figuring this out is a big challenge. And I'm sure that different people have all kinds of interesting progress there. And an inference time compute, or maybe what's been most recently most vividly seen in 01, the 01 model. These are all examples of things of people trying to figure out what to do after pre-training. And those are all very good things to do. I want to mention one other example from biology, which I think is really cool. And the example is this. So about many, many years ago at this conference also, I saw a talk, where someone presented this graph. But the graph showed the relationship between the size of the body of a mammal and the size of their brain. In this case, it's in mass. And that talk, I remember vividly, they were saying, look, it's in biology, everything is so messy. But here you have one rare example where there is a very tight relationship between the size of the body of the animal and their brain. And totally randomly, I became curious at this graph. And one of the early, so I went to Google to look for this graph. And one of the images and Google images was this. And the interesting thing in this image is you see like, I don't know, is the mouse working? Oh yeah, the mouse is working great. So you've got this mammals, right? All the different mammals. Then you've got non-human primates. It's basically the same thing. But then you've got the hominids. And to my knowledge, hominids are like close relatives to the humans in evolution. Like the Neanderthals. There's a bunch of them. Like it's called homo-hubbilys, maybe. There's a whole bunch. And they're all here. And what's interesting is that they have a different slope on their brain to body scaling exponent. So that's pretty cool. What that means is that there is a precedent. There is an example of biology figuring out some kind of different scaling. Something clearly is different. So I think that is cool. And by the way, I want to highlight this x-axis is log scale. You see this is a hundred. This is a thousand, 10,000, a hundred thousand. And likewise in grams, one gram, 10 gram, 100 grams, 1000 grams. So it is possible for things to be different. The things that we are doing, the things that we've been scaling so far, is actually the first thing that we figured out how to scale. And without doubt, the field, everyone who is working here will figure out what to do. But I want to talk here. I want to take a few minutes and speculate about the longer term. Where are we all headed? Right? We're making all this progress. It's a stounding progress. It's really, I mean, those of you who have been in the field 10 years ago and you remember just how incapable everything has been. Like, yes, you can say, even if you kind of say, of course, deep learning, still to see it is just unbelievable. It's completely, I can't convey that feeling to you. You know, if you joined the field in the last two years, then of course you speak to computers and they talk back to you and they disagree and that's what computers are. But I doesn't always been the case. But I want to talk to you a little bit about super intelligence, just a bit. Because that is obviously where this field is headed. This is obviously what's being built here. And the thing about super intelligence is that it will be different qualitatively from what we have. And my goal in the next minute to try to give you some concrete intuition of how it will be different. So, you do yourself good reason about it. So, right now we have our incredible language models and their unbelievable chat box and they can even do things. But they also kind of strangely unreliable and they get confused while also having them, thematically superhuman performance on e-vals. So, it's really unclear how to reconcile this. But eventually, sooner or later, the following will be achieved. Those systems are actually going to be agentic in a real ways. Whereas right now, the systems are not agents in any meaningful sense. Just very, that might be too strong. They are very, very slightly agentic. Just beginning. It will actually reason. And by the way, I want to mention something about reasoning. The most reasoning is that a system that reasons, the more it reasons, the more unpredictable it becomes. The more it reasons, the more unpredictable it becomes. All the deep learning that we've been used to is very predictable because if we've been working on replicating human intuition essentially, it's like the gut feel. So, after the 0.1 second reaction time, what kind of processing we do in our brains? Well, it's our intuition. So, we've endowed our eyes with some of that intuition. But reasoning, and you're seeing some early signs of that, reasoning is unpredictable. And one reason to see that is because the Chess AI's the really good ones are unpredictable to the best human chess players. So, we will have to be dealing with AI systems that are incredibly unpredictable. They will understand things from limited data. They will not get confused all the things which are really big limitations. I'm not saying how, by the way, and I'm not saying when, I'm saying that it will. And when all those things will happen together with self-awareness, because why not? Self-awareness is useful. It is part, you are ourselves, are parts of our own world models. When all those things come together, we will have systems of radically different qualities and properties that exist today. And of course, they will have incredible and amazing capabilities, but the kind of issues that come up with systems like this, and I'll just leave it as an exercise to imagine. It's very different from what we are used to. And I would say that it's definitely also impossible to predict the future. Really, all kinds of stuff is possible. But on this uplifting note, I will conclude. Thank you so much. Thank you. Thank you. Now in 2024, are there other biological structures that are part of human cognition that you think are worth exploring in a similar way or that you're interested in anyway? So, the way I'd answer this question is that if you are or someone is a person who has a specific insight about, hey, we are all being extremely silly because clearly the brain does something and we are not. And that's something that can be done, they should pursue it. And I personally don't. Well, depends on the level of obstruction you're looking at. Maybe I'll answer it this way. Like there's been a lot of desire to make biologically inspired AI. And you could argue on some level that biologically inspired AI is incredibly successful, which is all of the planings biologically inspired AI. But on the other hand, the biological inspiration was very, very, very modest. It's like, let's use neurons. This is the full extent of the biological inspiration. Let's use neurons. And more detailed biological inspiration has been very hard to come by. But I wouldn't rule it out. I think if someone has a special insight, they might be able to see something and that would be useful. I have a question for you about sort of auto correct. So here is the question. You mentioned reasoning as being one of the core aspects of maybe the modeling in the future and maybe a differentiator. What we saw in some of the poster sessions is that hallucinations in today's models, the way we're analyzing, maybe you correct me, you're the expert on this. But the way we're analyzing whether a model is hallucinating today without, because we know of the dangers of models not being able to reason, that we're using a statistical analysis, let's say some amount of standard deviations or whatever away from the mean. In the future, do you think that a model given reasoning will be able to correct itself, sort of auto correct itself? And that will be a core feature of future models so that there won't be as many hallucinations because the model will recognize when, maybe that's too esoteric of a question. But the model will be able to reason and understand when a hallucination is occurring. Does the question make sense? Yes, and the answer is also yes. I think what you described is extremely highly plausible. I mean, you should check. I wouldn't throw out that it might already be happening to some of the early reasoning models of today, I don't know. But longer term, why not? I mean, it's part of Microsoft Word, auto correct. It's a core feature. Yeah, I just, I mean, I think calling it auto correct is really doing the service. I think you are. And then you say auto correct, you evoke like it's far grander than auto correct, but other than, but, you know, this point aside, the answer is yes. Thank you. Hi, Alia. I loved the ending mysteriously leaving out. Do they replace us or are they, you know, superior? Do they need rights? You know, it's a new species of homo sapien spawned intelligence. So maybe they need, I mean, I think the RL guy thinks they think that, you know, we need rights for these things. I have an unrelated question to that. How do you, how do you create the right incentive mechanisms for humanity to actually create it in a way that gives it the freedoms that we have as homo sapiens? You know, I feel like this in some, in some sense, those are, those are the kind of questions that people should be reflecting on more. But to your question about what incentive structure should we create? I, I don't feel that I know, I don't feel confident answering questions like this because. It's like you're talking about creating some kind of a top down structure government thing, I don't know, you could be a cryptocurrency to. Yeah, I mean, there's a bit tensor, you know, there's things. I don't feel like I am the right person to comment on cryptocurrency, but, but, you know, the reason chance, by the way, what you describing will happen. And, that indeed we will have, you know, in some sense, it's, it's, it's not a bad. And the result, if you have a eyes and all they want is to coexist with us. And also just to have rights, maybe that will be fine. It's. But I don't know, I mean, I think things are so incredibly unpredictable. I, I hesitate to comment, but I encourage the speculation. Thank you. And, yeah, thank you for the talk. It's really awesome. Hi, yeah, thank you for the great talk. My name is Shalev Lipschitz from University of Toronto. Working with Sheila. Thanks for all the work you've done. I wanted to ask, do you think LLM's generalized multi hop reasoning out of distribution? So, okay, the question assumes that the answer is yes or no, but the question should not be answered to the yes or no. Because what does it mean auto distribution generalization? What does it mean? What does it mean in distribution? And what does it mean auto distribution? Because it's a test of time talk. I'll say that long, long ago, before people were using deep learning, they were using things like string matchin. And grams. For machine translation, people were using statistical phrase tables. Can you imagine? They had tens of thousands of code of complexity, which was, I mean, it was truly unfathomable. And back then, generalization meant is it literally not in this, is the same phrase in using the data set. Now, we may say, well, sure, my model achieves this high score on, I don't know, math competitions. But maybe the math, maybe some discussion in some forum on the internet was about the same ideas. And therefore, it's memorized. Well, okay, you could say maybe it's in distribution, maybe it's memorization. But I also think that our standards for what counts as generalization have increased really quite substantially, dramatically, unimaginably, if you keep track. And so I think the answer is to some degree, probably not as well as human beings. I think it is true that human beings generalize much better. But at the same time, they definitely generalize out of distribution to some degree. I think it's a useful, topological answer. Thank you. Unfortunately, we're out of time for this session. I have a feeling we could go on for the next six hours. But thank you so much, Julia, for the talk. Thank you.
