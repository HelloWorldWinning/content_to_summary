Timestamp: 2024-12-22T01:30:24.618193
Title: The Kolmogorov-Arnold Theorem
URL: https://youtube.com/watch?v=nS2hnm0JRBk&si=G5vQkYf9icm9173u
Status: success

Description:
Okay, here's a comprehensive summary of the video in English, focusing on the key points and maintaining the original context:

**Summary:**

This video by Lou Sano from Sano Academy explains the Kolmogorov-Arnold theorem and its connection to neural networks. The video begins by comparing simple and complex functions, arguing that functions where variables are added are simpler than those where variables are multiplied or entangled. This separability is important for several reasons, including ease of differentiation and simplicity in machine learning algorithms.

The core of the video revolves around the Kolmogorov-Arnold theorem, which states that any continuous, multi-variable function, no matter how complex, can be expressed as a sum of simpler, single-variable functions. This seemingly counterintuitive result is explained through a series of examples and illustrations.

Key concepts:

*   **Separable vs. Non-Separable Functions:**  Functions where variables are added are considered separable and simpler, while those with multiplication or other entanglements are non-separable and more complex. Separable functions can be thought of as strands that do not intertwine, while non-separable functions have variables entangled.
*   **Derivatives and Complexity:** Derivatives of sums are simpler to calculate than derivatives of products because they involve sums of individual derivatives, while products generate much more complex expressions.
*   **Machine Learning:** Separable functions are preferred in machine learning. Linear regression uses sums of features, while polynomial regression adds products, significantly increasing complexity.
*   **The Core Idea:** While an entangled function like  `x * y` cannot be directly rewritten as `f(x) + g(y)`, it *can* be rewritten by introducing an additional variable (e.g., `z = x + y`) and expressing the original function as a sum of single-variable functions that operate on x, y, and this new, combined variable.  A specific example is provided: `xy = (x^2 / 2) - (y^2 / 2) + ((x+y)^2 / 2)`.
*   **Formalizing the Theorem:**  For a function `f(x1, x2)`, the theorem posits that `f(x1, x2)` can be expressed as a sum of five terms. Each term involves applying a single-variable function to a new variable, which is itself a sum of two functions (one of x1 and one of x2).  In general, a function of 'n' variables can be expressed as a sum of `2n + 1` terms.
*   **Practical Application:**  While these functions within the sum can be complicated, they are simpler in the sense that they act on single variables.  A key condition is that this equivalence or approximation holds within a bounded closed region, which is sufficient for machine learning, as datasets fit within closed regions.
*   **Approximation vs. Expression:**  While the theorem states the existence of such functions, they can be extremely complex. In practice, we approximate the target function with simpler and well behaved functions.

**Kolmogorov-Arnold Neural Networks:**

*   **Neural Network Interpretation:** The theorem is practically realized through Kolmogorov-Arnold neural networks. These networks differ from standard networks by using functions on edges rather than weights.
*   **Network Structure:**  Each node adds the outputs from incoming edges. Edges contain functions (e.g., identity, squaring, sine) that are applied to the inputs.
*   **Example Construction:** A demonstration shows how to construct a network to express `x1 * x2` using layers of nodes, edges with functions, and adding operations. Another example show how to implement a more complex function `e^(sin(x1^2+x2^2)+sin(x3^2+x4^2))`.
*   **Two-Layer Architecture:** The theorem guarantees that a two-layer Kolmogorov-Arnold network is sufficient to express any multi-variable function. The first layer (lowercase `f`) separates functions of each input variable before combining them in a sum. The second layer (uppercase `F`) operates on this sum.

**Conclusion:**

The Kolmogorov-Arnold theorem shows how complicated multi-variable functions can be constructed from sums of simpler, single-variable functions, and it is this core idea that is implemented by Kolmogorov-Arnold neural networks. Although the functions on the first layer can be very complicated, they operate on separated variables. These networks provide a powerful and mathematically grounded approach to approximating and representing complex relationships in data. The video also promotes the creator's book and encourages viewers to subscribe to the channel and/or support him on Patreon.


Content:
hello I'm Lou Sano and this is Sano Academy and in this video I'm going to tell you about the kogoro Arnold theorem the kogoro Arnold theorem is a beautiful theorem which says roughly that any function that is continuous no matter how ugly it is can be expressed as a sum of relatively nice things the theorem is this but don't worry about the complicated formula we're going to see it in a very simple way now one of the great things about the kogoro Arnold networks is that they give rise to a new beautiful and very well performing architecture neural networks called the korov Arnold neural networks if you want to learn more about these networks you can check out this video on my channel the two videos complement each other but they can be seen in any order so are you ready to learn more about the beautiful kagor of Arnold theorem let's get into it so let's begin by playing a game called which function is simpler so out of these two functions x + y and x * Y which one looks simpler to you and I know I'm being vague but I will be more specific later so if you're like me you probably prefer the sum to the pro because the sum is just a simpler thing than the product so let's say that x + y is simpler than x * y now let me give you two new functions x 2 + sin Y and still x * y so which one do you think is simpler so x * y looks simpler right because it doesn't have a square or a sign or complicated things but I'm going to argue that this one on the left is simpler and the reason is because we're still adding but I'll be more specific about it let's go back to x + y and x * y y is X X Plus y simpler well I like to see every variable X and Y as a strand for example these two and when they're added they're separate but when they're multiplied they are somehow entangled like this now this is just the mental picture that I have now what happens with x^2 + sin y well the strand for x square is a little more complicated it complicates the X and the strand for sin Y is also a little complicated because sin Y is a more complicated function than simply y however no matter how complicated the strand for x and the strand for y are they are still separated by the sum in other words we have a function of x no matter how complicated plus a function of Y no matter how complicated and we separated X and Y on the other hand x * Y is a function of both there's no way to write it as a function of X plus a function of Y so in here the X and Y are way more intertwined than on the left which they're complicated but separated so we're going to just call them separable and non-separable and this is not standard terminology this is just how I call them so on separable we're going to have simple functions like x + y and then more complicated things like x^2 + sin Y and on the nonseparable side we're going to have things where the X and Y are not separable where they're entangled x * y for example x to the Y or things like cosine of x plus Y which if you look at it there's no way to separate us a function of X and a function of Y this also happens in more variables right in three variables we have things like x + y + z or complicated things like 2x^2 + y y y + logarithm of Z no no matter how complicated each function is it's on one single variable on the other hand the on separes we have things like the product of X Y and Z or things like xÂ² y plus Z which even though the Z is separated the X and the Y are still entangled so on the left everything's a function of X plus a function of Y plus function of Z and on the right things are all entangled and it's a function of all things now why is it that I prefer separable functions than non-para bles so for example for four variables why do I prefer something like this where the strands are separated then something like this where they're all entangled well there are many reasons one of them is derivatives think about this if I take the derivative of a sum that is the sum of the derivatives I only have to calculate each derivative and add them together on the other hand if I take the derivative of a product I get something horrible where each variable appears in every single summoned underneath imagine doing this for a million variables on the left you have a sum of a million derivatives and on the right you have a sum of a million terms each one of them with a million products which is much much uglier because for for example if I take the variable Y and change it a little bit well only one term changes on the left and on the right every single term changes now on the left things can be ugly I could have something like sine of Z square instead of set and the derivative is still something I can calculate because calculating derivatives of one variable is just a lot easier that's why we learn single variable calculus way before we learn multivariable calculus because when you start joining all these variables multiplying them Etc the derivatives just get a lot more complicated now another reason that we want to separate the variables is machine learning let's say for example that I have a data set of house prices and so y the target is the price and the features X1 all the way to X6 are the size the number of rooms the location etc etc etc I could do something simple like linear regression where the estimate of the price is a sum of some factors that are scalers times the values of the features that's simple easy and it's called regression the moment I start adding products of variables things get complicated and I no longer have linear regression I have things like polinomial regression or even more complicated regressions now here's the thing I'm allowed to mess with the columns a little bit I could change X2 to things like X2 squ I could change X5 to the logarithm of X5 this is okay but the moment I combine two columns for example X3 and X4 I take the product of the two then the algorithm just gets a lot harder and so I hope I've managed to convince you that if you have a sum of things it's just much better than if you have things like product or exponents or anything like that now here's the question if I have something like XY a function of two things could it be that I can manipulate this and express it as a function of X plus a function of Y this would be lovely but perhaps too lovely because it's not true if I have the product of two things I can't just turn it into the sum of two things however something very close is true I could introduce a new variable zet and I could express XY as a function of X plus a function of Y plus a function of Z and now let me tell you what set is I warn you it's going to look like I'm cheating but I promise I'm not check this out I can write XY ASX 2 / 2 - y^ 2 / 2 + x + y^ 2 / 2 that is z z is x + y my new variable Z I've introduced is the sum of X and Y now why is this expression correct well I encourage you to pause the video and actually work it out but I've actually worked it out for you here check it out X2 y^2 + x + y^ 2 can be expanded like this I'm expanding the square on the right and then I cancel the x square and the Y sare and I get 2x y so if I divide everything by two I get the expression below now this doesn't look that much simpler but believe it or not this is simpler because I don't have my variables entangled - xk2 is something complicated - yun2 is something equally complicated and this one on the right is equally complicated because if I look at set being x + y then it is simply z^ 2 / 2 however you could say well x + y^ 2 still contains an XY there's still some entanglement here because when I expand x + y^2 there's still an X Y there so there's still a product yes it is true that there is still a product but that's not so bad and that's what the kagor of Arnold theorem says if you have a very complicated function like the one you see on the left where X and Y are super entangled well I could write it as a sum of several things for example five and what are these things well these are new variables T1 T2 T3 T4 T5 they're completely separated but now here's the catch each one of the t's can be expressed as a sum of a function of X and a function of Y now this may look confusing I will elaborate on this but the essence of this theorem says that if I have X and Y really int I can write this as a sum of five things each one on a single variable and a single variable is a sum of a function of X and a function of Y completely separated let me be more specific now let's change the variables to X1 and X2 because we're going to need to add more variables and so some function that really entangles X1 and X2 can be expressed in the following way I'm going to take just X1 and X2 several times I'll tell you later how many times now I'm going to operate on X1 and on x2 separately and these functions are called 511 512 all the way to F N1 F N2 notice that on the right everything is separated the F could be horrendous but that doesn't matter because the X1 and the X2 do not entangle now this would be lovely but it's too nice to be true this is not the case but I just need one more step I'm going to introduce some functions called Capital fi and what they do is they operate on the sums so Capital F1 operates on the first sum Capital 5 2 on the second sum all the way to capital f n which operates on the last sum now this is very likely to entangle the X1 and the X2 they're no longer separated but at least the entanglement is controlled to the capital five functions now this can be seen in another way I am going to call each one of these sums of a function of X1 plus a function of X2 I'm going to call them let's say T1 and T2 and all the way to T n these are my new variables let's not forget that they are a sum of something of X1 plus something of X2 but we can still treat them as individual variables and now the F's operate on the t's so F1 of T1 can be really ugly but it's only a function of T1 and the same thing for 5 2 of T2 and all the way to f n of TN and these are the capital F's I'm talking about now the capital F could be complicated and they could really complicate our TN but that doesn't matter each TI is the sum of some function of X1 and some function of X2 now here's a question how many of these terms do I need well it's actually a really nice number it's five you only need five summons to express any continuous function of two variables and where does the five come from well it's the number of variables which is 2 multipli by 2 + 1 that is the five and that is the colore of Arnold theorem it says that if I have two variables I can express that as a sum of five terms if I were to have three variables well I can express that as a sum of seven terms because it's 2 * 3 + 1 and every time it's going to be two more so if I have a function of n variables I can express it as a sum of 2 n + 1 terms so I'm going to express it as a summation formula like this and now we're almost done with the statement of the theorem it says that every multivariable continuous function can be expressed by a finite number of single variable functions and if you remember what each TQ was it's a sum of functions of the XP so I can think of TQ as a single Strand and over here is where you have functions on all the separate strands so we can write the sum like this and this looks like a complicated thing but no matter how ugly the function f is this is the important thing no matter how complicated f is the part that's inside the bracket is a bunch of functions of the XI separated now if we take the sum as a new variable then the capital F Q are going to operate on this variable so it's a really nice way to express the function f as a sum of things now notice there's an asterisk is because certain conditions apply what are the conditions that apply well it's one that actually works well for machine learning it says it works inside a nice close bounded region what does this mean that if my function f is this thing over here then I can get it to be equal to that sum but not everywhere only inside some closed bounded region outside of the region it can go in any direction and they don't have to be the same they can be really different but inside the region the f is equal to that summation on the right so it seems like a huge restriction but this is a restriction that always works in machine learning why does it always work in machine learning because machine learning is based in data sets and no matter how big a data set is it always fits inside a close bounded region so any theorem in mathematics that said something something something I can approximate a function inside a close bounded region works for machine learning because all data sets are inside a close Bounder region and it doesn't really matter to us what the model does outside of the data set we only care about how the model behaves close to the data set now here is a problem that may arise as you can imagine if the part on the left which is f can be really complicated well something in the right has to be really complicated because you can't just get rid of complications that easily and the problem is this that these small five functions could be really ugly they exist but they could be so complicated that we can't really work for them but there's good news if you change the word express to the word approximated then this really ugly can be changed to nice I know I'm being very big with these definitions but this is going to be more clear when I introduce the color of Arnold networks which are a nice way to express this theorem and again this only happens in a close- bounded region so if your function is like this then you can approximate it using the sum in the right but only inside a close- bounded region that means that these two functions are really close but inside the close bounded region outside I don't know what happens so that's it that's the kagor of Arnold theorem now that you know about the theorem let me tell you more about the kagor of Arnold networks if you want to know more about the details and the whole architecture and how to get trained check out this video over here in my channel but what I'm going to tell you here is how the networks connect to the theorem so let's build a small neural network just like before we have a node with a variable X1 that connects to a higher node now normally in neural networks you would have a weight on that H that's a number on the edge but in color Arnold networks instead you have a function on the edge so the function could be something like X squ and what's the output well for the output I simply take the function and apply it to the variable so I get f ofx which is x^2 applied to X1 which gives me X1 2 and if I have more variables that connect to this node for example X2 with an edge containing let's say the function G of X which is s of X then on top I'm going to add the S of X2 so it's the function applied to X2 so as you can see the node only adds things that's why it has a plus so it's simpler there but the edges don't contain weights they contain functions and so if you imagine a large neural network with functions on all the edges and they connect with sums then you have a color of arnal network now let me show you how a color neural network looks like for an expression for example for the function X1 X2 if you remember from before we can express the product of two things X1 X2 as 1 12 X1 + X2 2 - a/ X1 2 - a/ X2 2 before we did it with X and Y now we're doing it with X1 X2 but it's the same thing so how do we build a network with this well we have the inputs X1 and X2 and the output which is going to go over here now let's build this first summon 1 12 * X1 + X2 2 actually I want to build this in pieces so I actually just want to build the inner part X1 + X2 that's going to go on this node over here and to build this node with the sunx 1x2 I simply need to combine these two so I'm going to have a function here yal x the identity function that outputs the same thing that it inputs and when that gets applied to X 1 then we get an X1 now how do I get the X2 well the same identity function y = x applied to X2 gives me S2 and now I just add them so now we have a Noe for X1 + X2 now how do we make a Noe for 1 12 X1 s well remember I only want the X1 so we're going to have a Noe for X1 how do we get the X1 in the same way as before with the identity function that applied to X1 gives me X1 now I don't want to have an X2 there so how do I not have an X2 well well I take a different function the function y = 0 the function that sends everything into a zero and that way when I apply it to X2 I get zero and I add zero to X1 so I get what I wanted now let's quickly do the third one which is 1 12 X2 s i do something very similar I'm going to have it on this note over here I have my first function y = 0 because I don't want the X1 so I turn it into zero using this function and I add it here and for the other one I use the identity function that turns X2 into X2 and I add to the zero so I get for the first note X1 plus X2 for the second one X1 and for the third one X2 that's my first layer and now my second layer is going to take care of the one halfes and the squares so how does it work well I'm going to have three edges each one with a function and the function is very simple it's simply the function 12 x^2 for the first one and -/ 12 X2 for the second and third so now when I take this function and apply it to X1 + X2 then I am squaring it and dividing by two so so I get 12 * X1 + X2 2 similarly when I apply the function - 1 12 X2 to this one over here to X1 then I get -2 X1 2 and for the third one same thing happens when I apply this function -2 x^2 to X2 then I get -2 X2 2 and then the node on top simply adds the three things and when I add these three things I get X1 X2 because of the expression on the left and so that is the color rnl Network that breaks down the function X1 * X2 now you can have more layers for example let's take this more complicated function that is actually in the paper and in the talks given by the authors of the paper e to the sin X1 2 + X2 2 plus sin of x3 2 + X4 squ so we have four inputs now and we're going to build the network the first layer takes care of X1 1^ 2 + X2 2 this one is over here where the functions are simply y = x^2 this one over here gets built like this where the functions are y = x^2 now notice that I haven't drawn all the edges between the first and the second layer so you can assume that every Edge that I haven't drawn is simply an edge with the function yal 0 like before now the next layer is going to take care of the signs so it's simply this node over here with functions sin x attached to it so now the stuff on the first layer gets applied the function sign and it gets added on the top and we need one more layer the one for the function e so it's simply a node with the function y = e to the X and then we get e to the whatevers on the bottom layer and that is a color of Arnold Network for this complicated function in the left now here's something interesting the kogoro Arnold theorem as I'm going to show you soon tells you that you only need two layers so we could find the network to express this function on the left with only two layers however it may be more complicated and it may be a very big layer but just keep in mind that the kogoro Arnold theorem tells you that you can express any function with a network of only two layers and why is this the case well check out the formula there you can see the two layers the first layer is given by the lowercase five and the second layer is given by the uppercase F let me be more specific the X's are the inputs and they are over here the second layer is given by these functions the lowercase f and it looks like this every single one of these edges is going to have one of the functions F and the theorem says that if we have n inputs then we need at the most 2 n + 1 noes in that middle layer now the final layer is given by the capital F what these do is they apply a function to each one of the Expressions coming out of the previous layer these functions are the capital F from 51 to 5 2 n + 1 and then this network gives you the expression of f let me be more specific no matter how ugly and entangle the function f is we're going to express it using this network now what are the inputs the inputs are the variables X1 all the way to xn which you can think of them as really nice straight strands and they go over here at the input of the neural network now the first layer is going to complicate this but without entangle them it's going to apply something to X1 something tox2 all the way to something to xn and that's going to be the output of the second layer for each one of these notes I don't have space to draw them all but imagine that out of these nodes Come N complicated strands that are still separated and then before we get to Layer Two let's take each one of those sums of separated complicated functions of XP and think of them as one variable we're going to think of them as one thing and then we're going to apply capital F to them and that's going to be the second layer of the neural network so that capital F is going to complicate this variable a bit and then all we do is we add all of this in the last node and then we get an expression of our function f and so that's it that is the color of Arnold theorem it says that no matter how ugly your function is you only need a TW layer komogorov Arnold Network to express it or in a simpler way to approximate it using nice functions Capital 5 and lowercase 5 and That's all folks I hope you enjoyed this video which had a bit more math than the usual video but I hope you appreciate the beauty of how this theorem can actually simplify a complicated function if you like this video please subscribe to the channel and you'll get to see a lot more of this material my videos will always be free but if you want to support me even more I encourage you to join the channel or to support me on patreon any of these two will give you some really cool perks like early access to videos q&as with me and even your name on the videos so I encourage you to check out this option if you want to follow me on Twitter you can also follow me Sano Academy or you can check out my page san. Academy where I have a lot more material courses blog posts videos Etc you can also check out my book grocking machine learning here is a 40% discount code that also appears on the comments of the video so thank you very much and see you in the next video
