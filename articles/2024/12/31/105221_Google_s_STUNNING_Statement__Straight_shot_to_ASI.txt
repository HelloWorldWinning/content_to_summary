Timestamp: 2024-12-31T10:52:21.150031
Title: Google's STUNNING Statement "Straight shot to ASI"...
URL: https://youtube.com/watch?v=Cz7Snh8rYAw&si=N94WF_HVSqMQAR5G
Status: success

Description:
Okay, here's a concise, structured summary of the provided content, focusing on core ideas:

**Summary:**

1.  **Straight Shot to ASI:**  Several prominent figures (Logan Kilpatrick, Ilia Sutskever, Sam Altman) are suggesting a direct, rapid path to Artificial Super Intelligence (ASI) is becoming increasingly probable, rather than a gradual, incremental process.

2.  **Key Enabler: Test Time Compute:** The success of scaling "test time compute" (allowing AI models more processing time for reasoning *during* inference, not just during training) is seen as a crucial driver of this rapid progress. This is achieved through an approach of synthetic data generation by models themselves.

3. **Self-Improving AI (Star):** The self-taught reasoner (Star) model, developed in 2022 at Stanford, enabled AI models to improve themselves via iteratively creating their own training data, allowing for a rapid improvement in reasoning capabilities.

4. **Reasoning Models:** New models (e.g., Open AI’s 01, 03) emphasize reasoning ability and "thinking" before answering, using the Star approach, which is a significant shift from prior “non-reasoning” models. These models perform significantly better on complex reasoning tasks with more test time compute. This was also called "Quiet Star" by others and “strawberry” internally.

5. **Rapid Performance Increase:** Reasoning-focused models show a dramatic performance leap in benchmarks like the ARC-AGI test, demonstrating a near vertical improvement, making previously thought "impossible" AI capabilities a reality.

6. **Narrow vs. General ASI:** Current examples of super intelligence exist only in narrow domains (e.g. AlphaGo, AlphaFold). ASI refers to general super intelligence, which is a broad ability to exceed human capabilities across many areas of endeavor.
7. **Emerging/Competent/Expert AGI** Different classifications of AGI are being defined; the models discussed here are in the range of emerging or competent with potential to become Expert AGI.

**Core Point:**
Rapid advancements in AI, particularly scaling test time compute and reasoning capabilities are making a direct path to Artificial Super Intelligence seem increasingly viable in the near term.

**Fundamental Point:** The traditional understanding of incremental AI progress is being challenged by a new paradigm of self-improving reasoning models that, with increased computational power at inference time, could achieve superintelligence much faster than previously thought.


Content:
here's Logan Kilpatrick a lead at Google AI saying straight shot to ASI artificial super intelligence is looking more and more probable by the month this is what ilas saw so again he's the lead product for Google AI Studio working on the Gemini API and AGI so as you recall open AI co-founder ilas sover writ so he created his new startup SSI safe super intelligence quickly raised 1 billion now I think it's valued at over 5 billion probably more at this point it was just the latest update it was 5 billion so Logan here continues Ilia found SSI with the plan to do a straight shot to artificial super intelligence no intermediate products no intermediate model releases now this stirred up quite a few people it seemed strange like really there's nothing between where we are now and super intelligence there's just a straight shot there how's that possible AI rolls around only once subscribe this is from their website ssi. in we started the world's first straight shot SSI lab with one goal and one product a safe super intelligence so here Logan continues many people himself included saw this as unlikely to work since if you get the flyws spinning on models products you can build a real moat and certainly I think most people just kind of intuitively thought that yeah you need some sort of incremental approach an iterative approach you improve here there how's it a straight shot to artificial super intelligence if we're still debating if we have artificial general intelligence or not like we haven't even really crossed that bar I think we're really getting to AGI like we're right around AGI now but still it's kind of strange to think of artificial super intelligence but Logan continues and actually explains why he thinks this is the case he's saying however the success of scaling test time compute which Ilia likely saw early signs of is a good indication that this direct path to just continuing to scale up might actually work he's saying we are going to get to AGI but unlike the consensus from 4 years ago that it would be this inflection point moment in history it's just going to look a lot like a product release with many iterations and similar options in the Market within a short period of time which for what it's worth is likely the best outcome for Humanity so he's personally happy about this all right so let's unpack that a little bit what are they talking about we he talking about but keep in mind he's not the only one there's a number of people talking about this idea of super Intelligence coming sooner rather than later of course Ilia believes it's a straight shot there here's an article out of Forbes Sam Alton AI is integrated super intelligence is coming so this article in part talks about this article this blog post by Sam Alman called the intelligence age so when they talk about the September Manifesto in which he wrote about the future change that's the article they're talking about the blog post they're talking about so the question is how does Super intelligence emerge so Som I'm saying you have to look at the rate of scientific progress and how things will compound these advances over the next few years and this of course is he made that big statement that I think got a lot of people uh Curious so he's saying it is possible that we're going to have super intelligence in a few thousand days all right so that's Sam Alman saying it we also have elas sover believing it betting a lot of money on it and now somebody you know a lead from Google working on the Gemini API saying yep straight shot to Super intelligence from here let's see where this whole thing kind of originated okay so really fast this is Reuters July 15 2024 this is where we kind of find out about it that open the ey is working on uh a new reasoning technology under the code named Strawberry now this thing goes by a lot of different names so it's I know it's kind of hard to keep track of it all but you must have heard things like strawberry before that there was qar and now we're kind of seeing like the 01 model the 03 model they're all kind of the same thing they're part of like the the same line of AI models but before we just had you know rumors and speculations and and leaks and now we're beginning to see it kind of emerge and we're seeing it just completely obliterate a lot of these tests uh in mathematics and coding Etc that we thought were kind of you know maybe impossible for REI to do before certainly its success on the arc AGI test was very noteworthy got 8% when it was allowed to think about it at length that was not the official score because it took more compute than you could take under the rules of the archyi test but the point is you know this was what used to be rumors and I remember like posting about the leak of qstar and people calling me crazy in the comments saying like how could you believe this nonsense but it's here right a year and one month later it's true we we're seeing it we're we're we're messing around with the 01 model and we can see how good it is at you know reasoning this thing that we only heard rumors about last year now I've said this many times in this Channel and this is one of the reasons that we look a lot of the scientific papers that get published is because very often you see a paper that gets published and then 6 to 12 to 18 months down the road you see all the startups and all the kind of like the financially German companies explode of various products and applications from that paper so a lot of this progress is driven by you know the researchers the academics the universities and what's interesting is a lot of the stuff that we're seeing now can be attributed to in part at least to this Stanford paper that was released in 2022 now obviously there's a lot that went into it this paper just like one many contributions to this thing but this was kind of like the first clue that we see to what's about to happen right many well not that many years down the road a few years down the road all right so this method that was developed at Stanford in 2022 also keep in mind that the Google you know announcing the transformer in 2017 you know because a lot of these people publish their work is what allows everyone else to kind of build on top of it so open didn't invent all of these things they were just incredibly good at seeing kind of where everything was going at reading the published papers and kind of combining that into actual tangible progress in in the AI field all right so self-taught Reasoner so here's that paper May 2022 early on right star self-taught Reasoner bootstrapping reasoning with reasoning kind of a weird way of putting that right bootstrapping reasoning with reasoning right so notice Noah Goodman is uh one of the sort of authors on this paper it's Google research Stanford it's Etc and we've covered this in detail before but the big kind of Point here was this okay so one of the creators Professor Noah Goodman so this is kind of his quote this is what he was saying I'm not sure if it was at the time 2022 or when the whole thing about qar was coming out in 2023 but he's saying that star enables AI models to kind of bootstrap themselves into higher intelligence levels via iteratively creating their own training data so meaning synthetic data so these models create kind of their own reasoning their own training model and then they use that sort of synthetic data to improve themselves so whereas sort of like the AI 1.0 it will get trained on human data it will get pretty good uh just on human training data when sort of pattern that we've been seeing is that it gets a whole lot better when it's allowed to kind of train itself we saw that with the alpha go model Google's uh chess AI all right so when an exclusively looked at human played games it got pretty good it got as good as the best human but when it was allowed to play billions of games against itself and keep improving over time it became much better than any human could be that's that's kind of the idea of self-play or creating synthetic data or creating its own training data right so and no Goodman continues in theory this could be used to get language models to transcend human level intelligence and just to you idea how this whole thing kind of worked and it's kind of summarized in this little picture I think pretty pretty well basically let's say you have like a very simple question right what can be used to carry small dog and you have a couple answers that you can choose from the model has to generate reasoning for how it arrives at the answer so for example the answer must be something that can be used to carry a small dog baskets are designed to hold things before the answerers basket B right so this is the answer generated by the model it's also its reasoning right so if it's reasoning leads to the correct answer where as you can see here produces a rationale and the answer this is this right that gets fed into the data set that gets fine tuned on right so version 1.0 comes up with this rationale and answer and it does that a bunch of times all that data is fed into the model to training data to create version 2.0 right and 2.0 tries again but guess what now it's a lot more accurate and a lot better at answering these questions so that's what they mean by self-taught Reasoner or or bootstrapping reasoning with reasoning it's just this kind of like Loop that builds on itself it's iterative it's recursive like it's kind of like compounding right now by the way that same team also uh plus a few other people they they published uh another paper in March 2024 quiet star language models can teach themselves to think before speaking right so this is kind of building on Star right the self-taught Reasoner and now it's quiet star or Q star so you can see where maybe that kind of nickname came from from right so this idea that we can separate its outputs and its final answer and it's kind of reasoning steps into two things so you can think of what it was like thinking before speaking and that's more or less exactly what the 01 system is and in fact this whole new approach to AGI the the reasoning AI models and here's the open AI kind of introducing this model the 01 preview right so how it works we train these models to spend more time thinking through problems before they respond much like a person would through training they learn to refine their thinking process try different strategies and recognize their mistakes and again so all these things are connected right so star quiet star the leaks from opening eye that people were calling qar then the strawberry model right the idea of thinking and having quotequote thoughts and then talking the actual outputs and then when certain chains of thoughts sort of so to speak lead to the right answer like it gets rewarded on that and the ones that don't kind of gets discarded and over time that kind of iter ative bootstrapping process leads to better and better reasoning so that's kind of the important thing to understand that there's a lot of these sort of names that we use for it they're they're slightly different but they're all similar so when we say this model thinks through problems before they're responding right that's kind of like the same thing as saying you know test time compute right so when Logan's talking about scaling test time compute which Ilia likely saw the early signs of right cuz that's when you know that whole thing with the board coup right when SE was fire there's this whole thing that was happening this is what they saw they saw that this idea of scaling test time compute AKA letting the models think AKA quiet star a quiet reasoning behind the scenes that it's a good indication that this direct path just continue to scaling up might actually work it's interesting because at the time I remember in the video I was saying like the stuff that we see coming out of open AI is kind of looking at a very distant galaxies right we're seeing something that happened you know billions of years ago so it's kind of similar here cuz we you know when the 01 model got introduced to us and we saw what it was able to do that was more or less a year after the people the kind of inside people at open eye after they saw it and they kind of realized what was coming and it was simply this so this is the Aime a very complex complicated very hard very challenging math competition so here's uh how well the 01 does so pass at one accuracy so you know pass at one just means it gets one chance to answer it right not a bunch of different guesses so does it guess it right yes or no or I shouldn't say guess does it does it know the answer right and then you see how the sort of the accuracy keeps improving from what is that let's say 32% or something like that to uh let's call that I don't know 68% I'm just kind of estimating here but when we increase the train time compute so meaning we give it more and more resource during training time right as we're sort of building this model we add more and more you know Nvidia cards to the mix Etc so basically we train it with more resources right so it's accuracy it's quote unquote intelligence its reason abilities they improve and we knew this for a while right we kind of understood this this is the scaling laws that people refer to but this is the new thing that spooped everybody this is what Ilia saw quot unquote that with test time comput so when you ask it a question and you give it time to think so that that sort of like thinking process giving it more resources to think that's the test time compute that's the quiet reasoning behind the scenes that's whatever you want to call it that's all the same thing right so as we give it more time to think test on compute the accuracy you notice it it really kind of improves rapidly in fact these were some of the scores that the 01 preview the 01 model received and at the time it was kind of mind-blowing people were not ready for it that was of course nothing compared to when they announced 03 so the 03 isn't ready for prime time yet we don't have access to it yet but I do think that there's some red teamer or some some close people that are close to it that are trusted testers that are testing it out I mean here's how well it scored on the arc AGI something that was considered to be a very difficult test for these AI models right so notice the 0 and mini scored like 7% right the 01 preview again the o1 preview was this thing that was blowing people's minds right that scored 133% the 01 depending on how much sort of computer we gave it right so we had 25 31 30 32% right but that was still very very low compared to to the human Baseline which I believe they said at at 75% sorry I had to look it up 85% that's the sort of the The Benchmark that rgi said like once this is sort of overcome and an open-source thing that is under the sort of the computer requirements there's only so much you can spend on these um on running the the machine I think 10,000 is the maximum that you can spend on compute to answer these questions so the 03 model gets 76% but that's because it was limited by compute when they were able to use you know unlimited compute and I think we estimated something like $300,000 that uh we we would have have spent on that model to run it at that sort of capacity but that's kind of like the retail price of course you know with opening ey they probably are able to run it much cheaper but the point is it got 88% right so if you're thinking in terms of this test at least which was meant to be you know easy for humans but very difficult for these models right for these large language models for these neural Nets it certainly seems that we've hit some insane inflection point we're just adding more test and computes improving these uh architectures it seems to really really improve its abilities to just crack these tests and so here's kind of a chart showing the gpt's ability the opening eyes models their abilities to do well in this test so as you can see here you know 2019 gpt2 gpt3 I mean close to zero just really really bad bad GPT 4 still horrible GPT 40 you know what was that 5 7% whatever it was I mean just absolutely terrible one tenth of that what what a human can do right but this is kind of around the time when you know ilio was seeing the thing whatever he saw at this is the point where they might have seen how well this test time compute scales right then in about a year we see the 01 previews so again that's that whole idea of looking at the farway Galaxy you know what we're seeing is way in the past right so we're seeing this model emerge and and do pretty well it does you know just a little bit over 20% accuracy 20% score on the rcgi but again it seems like they were kind of disturbed over its abilities back in November 2023 but notice what happens to that chart from all of this the sort of non reasoning models if you will so those models you can tell them oh think through before answering blah blah blah but here that whole process is is baked into it and we're able to sort of like improve and increase how long it's allowed to think about it so I can sit there for a long long time and just meticulously think through everything create that sort of like the synthetic data if you will it's not really using that for training itself but it is using it to answer the question but notice what happens to the Chart here so literally GPT 40 that's the last sort of non reasoning model oh one previews the first sort of example we see of the reasoning model right so then new kind of protocol and just notice that this insane Improvement this happened fast this happened months apart from 20% 1 of of what a person could do or something like like much much worse than a human being to better than an average human in a near vertical line so again when we're talking about this idea of straight shot to ASI artificial super intelligence it might seem a little bit I don't know unintuitive kind of maybe laughable at first like oh yeah you know really it's just around the corner it's a straight shot there when you look at a chart like this you know it's pretty obvious that something happened here between the non- reasoning models and the reasoning models and the difference between these two is all the stuff that we're talking about test time compute the ad behind star self-taught Reasoner right so bootstrapping reasoning with reasoning right so that idea have like Chain of Thought thinking head behind the scenes all of that happened between here and here and then rapidly it shoots up almost straight up as we've seen with the 03 we have to create new mathematical test for it new benchmarks because it now absolutely exceeds any human-made benchmarks like human math problems are now just child play for it we have to create like some Frontier future math to test it on just to have an understanding of how smart it is so with that in mind you know what I mean so we're seeing this rapid Improvement this almost another scaling Paradigm that is the test time compute now we might find that there's some limit to it right we might keep scaling it for a little bit and then there's some ceiling something we didn't anticipate something that's going to stop that from just continuing but for a second you know how you're doing a math proof you can like let's assume this and then based on this let's think through that well for a second let's assume that there isn't uh a ceiling coming to that soon let's assume that it's going to keep running for a little bit if that's the case then do you agree with what Logan's saying do you agree with what Ilia is saying do you agree with what Sam alman's saying that artificial super intelligence and specifically we're talking about it's a little bit difficult because you know we we I would say we have artificial superintelligence already which is kind of shown in some of the um Alpha models and Google deep m models but they're usually narrow super intelligence right so they're super good better than humans that I mean they can beat any human at go they can predict the protein folding better than any computer or any human could Alpha Cubit you know is able to predict the quantum error rates when we're talking about those Quantum chips that Google uh has announced so we already have sort of super intelligence but in the same way that a calculators like super intelligent at doing these calculations better than human being is it's narrow right we sort of have narrow super intelligence by way that's not kind of like my idea that made up so this is Google Deep Mind and their kind of AGI list right so they they have this idea of breaking into narrow and general so if we talk about narrow right so something that's scoped for a specific task or a set of tasks right we already have examples of superhuman AI that's made for narrow tasks Alpha fold Alpha zero stockfish Etc right pretty obvious we already have that but when we're talking about General right so certain wide range of various tasks some metacognitive abilities like learning new skills well ASI is not yet achieved in fact as of when they put this out they said the competent AGI is not yet achieved expert AGI is not yet achieved virtuoso AGI is not yet achieved right we might be seeing emerging AGI which they uh say is chbt Bard llama Etc right so this is sort of as Microsoft put it uh Sparks of AGI right so Proto AGI emergent AGI so this is kind of where we were in 2023 let's say soort of The Next Step that we're trying to determine is like competent AGI right so it's at least 50th percentile of skilled adults then expert AGI where it's better than 90th percentile of skilled adults and this is the sort of I know a lot of you disagree with me when I say we're kind of entering we're aiish perhaps the 01 03 could be sort of thought of as AGI but I think if we sort of break it down these categories it might be easier to talk about right so Google deep again they're saying we've had examples of emerging AGI now do we have competent AGI do we have expert AGI virtuos AGI where it's better than 99% of skilled adults this is where we kind of get into a conversation where we don't all agree I would argue that the 03 model is an example of we're entering somewhere here competent or expert keep in mind that if you think of this sort of intelligence as almost like electricity or infrastructure right so in of itself maybe yeah I can't do all the things right off the bat but we give it certain scaffolding this is what they talk about with AI agents for example right so it knows how to reason through things to do things it doesn't mean that it can like go and click on buttons and interact with the website Etc but if you give us certain scaffolding all of a sudden we're seeing that okay it's able to do those things but the point is that the sort of the intelligence the electricity it was there enough to power those abilities we just didn't have the scaffolding yet so I believe we have the intelligence already for it to nail these categories expert AGI competent AGI Etc so meaning that if we hit pause on AI progress now so these models didn't get any smarter we just had more time to build various tools and Scaffolding all this stuff for it I think we would be at expert egi with no like if 03 was the best we can do and that got released and we had 5 10 years of software engineers and everything else kind of like figuring out how to best apply it I think we would say yeah we're expert e GI that's that's that's my opinion I'm not pushing it on anybody uh I know some of you disagree with that but I feel like we're entering these two sort of categories right now with these reasoning models sometimes when I'm editing my own videos later I realized I went on this like 20 minute like some rabbit hole some other like side quest that that had nothing to do with my original point my original point was simply this if we assume that these the idea of scaling test on compute right if that continues for a little while longer if we don't hit some ceiling there right again kind of keeping this chart in mind right this is where we went from no reasoning to reasoning right this is this is the test time compute curve so this is this curve here is is before test time compute and this curve is after test time compute right so what I'm saying is assume this will continue for a little while longer if we assume that's the case do you agree with Logan and IL and Sam opman every everyone else that it's a good indication that this direct path to just continue to scaling up might actually work to get us to artificial super intelligence let me know in the comments does this curve if it continues does that hit artificial super intelligence specifically kind of broad or general artificial super intelligence let me know in the comments if you feeling extra generous hit the Thumbs Up Button make sure you subscribe got a lot of good stuff coming very soon if you made it this far thank you for watching my name is R Roth and I'll see you next time
