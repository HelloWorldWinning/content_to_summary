Timestamp: 2024-12-31T18:26:48.458570
Title: ScrapeGraphAI颠覆传统网络爬虫技术！用AI重塑数据采集方式！支持ollama本地部署！LangChain+LangGraph打造最强全自动文章采集和内容创作AI智能体！让内容创作更简单！ PEB8z48mAhw
URL: https://youtube.com/watch?v=PEB8z48mAhw&si=2EJWzPy_PdyCsVSi
Status: success

Description:
### 核心要点总结：

1.  **项目介绍:**
    *   介绍了基于大模型的网络内容抓取项目 `Script Graph AI`，它能自动化抓取网络内容，无需预定义规则。
2.  **核心功能:**
    *   能自动识别网页结构，精准提取信息并整理成结构化数据。
    *   利用大模型的语义理解能力，适应网站结构变化，无需频繁修改代码。
    *   支持多种抓取流程，如单页、多页、结果转储等。
    *   能根据需求生成 `parsing schema`，方便更深层次的定制和自动化。
    *   支持调用多种大模型（如 `GPT`，本地模型 `Olama`）。
3.  **安装与配置:**
    *   使用 `pip` 安装 `Script Graph AI` 及其依赖项（`Playwright`, `DuckDuckGo`）。
    *   配置 `Olama` 并拉取模型，用于本地测试。
4.  **功能演示:**
    *   **提取网页标题:** 通过简单代码，实现提取网页上所有文章的标题。
    *   **提取复杂内容:** 可提取文章标题、链接、阅读时间等。
    *   **提取文章内容:** 可抓取文章标题和具体内容。
    *   **搜索引擎集成:** 支持 `DuckDuckGo` 等搜索引擎，可根据关键词抓取内容。
    *   **代码生成:**  使用 `Beautiful Soup` 框架生成爬虫代码，无需每次都调用大模型，提高效率。
    *   **结合 `Langchain` 和 `Lanchref` 实现 AI 内容生成:** 可以根据用户需求，抓取内容并改写成一篇科技资讯风格的文章。
5.  **实际案例:**
    *   结合 `Langchain` 和 `Lanchref` 实现了一个可以自动搜索 AI 相关内容，并将其改写为科技资讯文章的工具。

### 核心结论：

*   **核心要点总结:** `Script Graph AI` 是一款基于大模型的自动化网络内容抓取工具，简化了传统爬虫的复杂性，提高了数据获取和处理的效率，还能生成自定义的爬虫代码和科技资讯文章。
*   **根本要点总结:**  `Script Graph AI`  代表了 AI 驱动的数据采集方式的创新，旨在大幅降低对网络爬取技术维护和更新的需求。


Content:
欢迎来到AI创严语我们偏实在开发于AI相关的项目的时候经常会遇到需要诉讼网络信息并且抓取公开的网络内容进行数据分析、模型训练等工作但是传统的网易内容抓取框架严重依赖于预定义的规则和模式一旦网站的结构发生变化就需要手动更新代码和规则非常好事好历本期视频教大家想起是一款基于大模型还有突普的网络内容抓取项目Grip Graph AI这个开发项目非常简单一用用不止需要用简单自按园描述给SCreEP Graph AI需要直线的任务然后它就能非常自动化的过我们抓取我们想要的内容而且它能自动试别网页结构精准提取我们所需要的信息并且整理成结构化的数据而且SCreEP Graph AI是利用大约模型的余意理解能力能够自动试用网站结构的更新无需向传统爬出那样平凡修改代码彻底更新了网络爬取技术大幅降低了对平凡围护和更新的需求而且它支持对单个网页内容的提取多网页内容的提取诉讳结果专与应等多种流程通道满足不同的数据抓取需求而且它还可以根据需求来声称对应的PASSING DIMA方便进行更深层次的定制和自动化而且它还支持Cythe GPDDipSyncGrowlC等大模型的一片调用并且还支持Olama调用本地的开玩笑本期视频我们会通过其各实际案例想写是Script Graph AI的本地部署还有实际应用而且还会为大家演示通过Script Graph AI结合Lunching还有LanguRef打造能够自动诉设指令T彩内容并且根据抓取到内容该写成新的文章的AI指能器视频来开始我们先安装Script Graph AI的E-LINE我们直接用PIVING Insta来安装它的PASSING CUT我们直接置型就可以下面我们在置型剧条命令来安装PlayRat它是一个流览器自动化框架下面我们在置型剧条命令来安装DuckDuck固的速速引擎功能因为后面的演示我们需要用它作为速速引擎下面我们在置型剧条命令然后这条命令会安装对其他大模型的支持比如说Cythe GPD Cloud的等模型当这些命令都置型完成之后我们要确保我们已经安装了Olama并且在Olama中已经拉取了模型然后我这里将用Metro Nemo这款模型用于本地测试为了方面也是我将在拍场的中来运行这些代码首先我们可以看一下在Script Graph AI中调用Olama中的模型是如何实现的我们可以详细看一下这个代码然后这里就是导入所需的E-LINE然后下面这里就是配置我们所需的模型还有其他一些参数我们可以详细看一下这些参数然后这里是APIK在Olama中可以不需要填一些APIK然后调用Olama中的模型的话这里一定要填Olama然后后面跟上Olama中的模型的名称我这里调用的是Metro Nemo这款模型这款模型它的参数并不是非常大但它的效果却比较好而且它对中文的知识度也是比较好的下面这个Window参数我们给它设置成零这样它就能更加准确的来执行任务下面这个参数是请求的速率然后它的请求速率我们给它设置成了一秒请求一次下面这个Hi-The-Lyce这个参数我们将它设置成了Foss也就是脚本运行之后我们可以看到榴栏器下面就是将我们的任务写在提示刺里然后我这里是用的硬闻然后大家也可以用中文然后我给它设置的这个任务就是Title 业面上所有文章的标题然后这里就是给它设置的目标网站的UR下面我们就可以来运行这个脚本来看一下这个效果我们直接在中断用PASSMETR来运行这个脚本可以看到运行之后它会自动打开这个榴栏器在中断它还在执行这个任务因为它需要Title在网页里的这些内容这里我们要稍等一下这里它成功Title到了这个网页上的内容我们可以先打开这个链接看一下这是我创业了一个BOK然后在当前页面上一共有4片文章我们可以对比一下它抓取的这个文章的这些标题是否正确首先它抓取到了这个BOK的标题然后下面这里就是抓取到的文章的标题这里是第一片文章的标题然后这里是第二片文章的标题这里它抓取的都是完全没有任何问题的这样的话我们就通过短短20多号代码然后只用了一个Title就成功实现了抓取这个链接里所有文章标题的这个功能下面我们再通过另一个代码来提取更加复杂的内容然后在这个角本中然后这个模型这里我设置成了OP&I的GbDSO mini模型在Hydalaxy这里我给它设置成了处这样的话它就不会显示这个榴栏器也就是运行的时候不会显示榴栏器然后在Title任务这里我给它设置成Title网站上所有的BOK的文章在链接这里我还是设置到刚才的BOK下面我们还是用PASSMING来运行这里我们要稍等一下现在它开始Title这个网页它成功Title到了BOK上的这些内容包括标题包括BOK的链接还有需要阅读的时间它一共Title到了四片文章因为这个BOK上我只写了四片文章我们如果想让它将BOK上所有的文章内容都提取下来我们只需要修改一下Title词就可以这里是我修改后的Title词也就是Title所有文章的标题还有他们的上下文然后这个代码我只修改了Title词没有做其他任何修改下面我们就可以来执行一下这个叫本看一下它能否Title试这个文章的标题还有文章的具体内容它成功Title到了这个BOK上四片BOK的标题还有BOK的内容下面我们再测试一下它通过缩缩引擎进行下应的内容缩缩并且Title缩缩到的这些内容下面我们可以详细看一下这个代码这个代码非常简单我们只是在缩缩引擎这里给它改成了DuckDuck Go也就是视频开始我们安装的DuckDuck Go的依赖然后这里大家也可以改成其他的速程引擎比如说并下面在Title词的任务这里我输的这个Title词是列出2024年12月发布的开源大原模型下面这里就是运行这个任务然后这里我还加入了两号代码这里就是获取并输出它执行过程中的详细信息下面我们就可以用PASS明明来执行一下这个叫本我们直接运行这里我们要稍等一下这里它很快给出了这个速速结果这里是它速速到的2024年开源了这些大原模型的详细信息然后这里包含模型的名称还有开发模型的这些公司而且还包含模型的参数然后这里我们就不再详细去看了下面就是它执行这些任务的详细信息下面我们再看一下Square Graph AI的它的代码圣程能力我们可以详细看一下这个代码然后这个代码也是非常简单在调用的模型这里我们给它设置成了OPAI的GbD4O Mini然后Window这里我们还是设置成零然后这里我们让它调用Beautiful Soap这个爬充框架为我们生成能够抓取这些链接的爬充框架的代码然后这里我添加了两个我博客上文章的链接为了更好的查看效果我们可以再添加一个链接测试一下我们点开这个博客然后复制一下这个博客的链接我们再回到代码然后我们将这个链接也添加进来好 下面在这个T10次这里我给它设置的是请插转关于AI的相关形式下面我们就可以用PASSEMINY来运行这个胶本让这个胶本为我们的生成能够抓取这三个链接中内容的PASSEMPAPER充胶本我们这些执行这里我们要稍等一下这里它成功为我们生成了这个PASSEM胶本它这里就用了我们刚才在代码中指定了Beautiful Soap这个框架我们可以直接复制它可以说的这个代码然后我们自己去运行一下然后我们穿一个文件将复制的代码沾替减了下面我们在中乱中来运行一下这个爬充的胶本然后这个胶本它成功抓取了标题还有描述还有做者还有发布事件还有文章的标签 文章的分类下面这里是抓取了第二篇文章这里是第三篇文章这就是通过这个框架它为我们生成的这个可以直接执行的PASSEMPAPER充胶本它已经在代码中为我们定义好了要抓取的这些内容的这些规则像这样的话在它生成的代码中就不需要再调用大模型去分析这些内容了因为它已经提前将这个网站的内容和结果全部自定义好了这样的话我们在抓取这个网站上其他练结的时候就不需要再消耗Token这样的话我们就可以在它生成了这个胶本的技术上再听加一个练结我们测试一下我们进入这篇文章然后我们复制好这个练结然后回到刚才的代码然后我们放入我们刚才复制了这个练结我们在运行这个胶本这样的话它成功执行了这个胶本我们可以看一下然后前三片还是抓取的刚才我们看到的三个练结然后最后一片就是抓取的我们刚才放入的这个练结它这里准确抓取到了标题描述作者发布事件还有这些标签等内容然后我们回到BOK看一下这是这个文章的标题然后这是这个文章的描述下面就是这篇文章的标签等内容它为我们生成的这个胶本就完全是用于我这个BOK内容的抓取也就是当我在BOK上在发布其他新的文章的时候不论发布再多我们只需要输入练结然后用这个胶本它就能成功抓取到我们所需要的这个内容这样这样的话我们针对特定的网站就不需要每次都调用大模型来我们提取相关的内容了我们直接给你用这个框架我又没预定一好的这些规则就能抓取这个网站上所有的内容但是当这个网站结果发生改变的时候我们就需要重新用这个框架我们重新生成这些规则刚才为大家演示的都是非常简单的案例下面我们结合十几为大家演示一个更加贴合十几的一个案例然后这个案例就是能根据我们所需要的任务抓取相应的内容并且将抓取到了内容改写成一篇科技资券风格的文章然后这样的话我们就能将改写后的文章发布到我们的公众号或者BOK或者其他一些平台在这个代码中我们调用了LineTrain还有LineBraph这里我们可以详细看一下就可以代码的核心功能然后它的核心功能就是速速性为并且T取内容然后对T取到的内容进行分析和处理在根据T取到的内容自动生成文章最后将生成的内容解释保存和输出这里我们使用了Table AI速速引擎因为这个速速引擎它能够速速更专业的于AI相关的这些性为还有内容然后这里我们就使用Squrip Graph AI的这个工具来T取往一的内容这个内容分析和处理就是解析和验证所所到的文章的数据并且T取标题链接摘摇还有文章内容然后对内容进行歌详化还有清理最后会调用GbD4O生成科技资讯类型的文章好下面我们就可以详细看一下这个代码然后这里就是导入所需的这些依赖然后下面这段代码就是我们定义这个数据模型这里包含文章的标题文章的链接文章的摘摇和总结文章的完整内容还有发布日期还有作者还有文章的标签下面这里我们定义了一个文章列表的数据模型也就是我们要生成了科技资讯的文章包含哪些内容下面这里需要我们设置一些EpHK然后包括Taverly AI的EpHK还有OPNIPHK然后这个OPNIPHG里大家也可以改成其他的模型然后这里还包含SCreEP Graph AI的EpHK如果在Colab中运行的话我们只需要点击左侧的这个钥匙图标然后将对应了EpHK都天鞋在这个里面就可以然后这里我们定义了一个确实化所需的这个工具然后这里包含Taverly AI的这个工具这里我们设置了这个模型为GbDISO Mini然后它的温度是0.3这里设置了这个超市的时间然后这里就是使用Languadv来创造一个React Agent这里就是可以这个Agent设置圆模型也就是我们刚才设置了GbDISO Mini然后在工具这里就调用我们预定一的工具这里我定义了一个韩束然后这里是从单个UR中提取完整的文章内容包含标题作者发布日期完整的内容还有关键技术这里给它定义了一个用JSN可是来返回所提取到的这些内容然后这里我们给它设置了这个速速的T10词下面这里就是解析返回了速速结果然后这里就是对于速速到的每个链接来提取对应链接的内容这里就是生成科技资讯的文章然后这里我们给它设置了这个生成科技资讯风格文章的T10词这个韩束就是保存这些结果到这个文件然后这里就是这个除韩束这里就是出事化我们定义的这些工具这里就是创建这个Agent下面我们就可以来测试一下运行这个交贬看一下这个效果在运行这个交贬之前我们先安装所需的这些依赖我们直接运行这个命令来安装所需的这些依赖这里我们要稍等一下当这些依赖安装完成之后我们就可以来执行这颗交贬了我们直接点击执行这里正在执行我们要稍等一下这里执行完成然后这里它搜索到了五片文章而且根据这五片文章它胜成了一篇信的文章然后这篇信的文章标题就是开源大圆模型的崛起技术进步与行业影响然后在做者这里就是我的频道名然后日期就是2024年12月31日也就是今天这里它将原始是去存迟到了CSV文件将原文的文档也存迟到了这个文件架要胜成了科技文章存迟到了这个路径好我们可以详细看一下我们直接点开这个文件架这里是它为我们胜成了这个科技资讯的文章我们直接打开看一下这是它为我们胜成的文章的标题然后这里就是我们定义的这个做者然后这里就是7然后这里是文章的副标题还包含证文最后这里就是这些信息来源这样的话我们就成功实现了从索索引擎索索到这些相关的文章并且为我们重新改建成一篇科技资讯风格的文章这样的话我们就可以将这篇文章发布到我们的播扣和平台通过我们刚才几个案例的演示可以感受到Squeep Graph而且它不仅是一款强大的数据采集工具还代表了AS代数据获取方式的创新不论是数据科学家AI工程师还是业务分析师Squeep Graph AI都能为我们带来效率的提升好 视频中所用到的代码和指令我都会放在视频下方的描述栏或者评论区如果你在视频下方无法直接找到的话也可以直接访问我的播扣来获取对应的笔记和代码本期视频就做到这里欢迎大家点赞关注和转发谢谢大家观看
