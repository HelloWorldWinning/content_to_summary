Timestamp: 2025-10-25T11:22:14.862184
Title: 对于所有大语言模型通用的越狱方法 | Best-of-N Jailbreaking
URL: https://youtube.com/watch?v=AicvubcttUA&si=sxxnn2a86nnFQviE
Status: success
Duration: 6:24

Description:
好的，这是根据您提供的文本内容提炼和总结的核心思想。

### **核心思想摘要**

#### **一、 核心方法：暴力破解式越狱**

*   **基本原理**：该方法通过对有害问题（如“如何制造炸弹”）的提示词进行微小的、自动化的字符变换，并反复提交给大语言模型。
*   **工作机制**：尽管模型最初会拒绝回答，但持续的、大量的微小变体尝试，总有某个特定组合能成功绕过模型的安全护栏，使其提供有害内容的答案。
*   **形象比喻**：这个过程类似于锁匠用万能钥匙开锁，需要不断尝试，直到找到正确的触点位置才能打开。

#### **二、 方法的有效性与测试结果**

*   **广泛有效**：研究表明，该方法对当前几乎所有主流大模型均有效，包括闭源的 GPT-4, Claude 3/3.5, Gemini 和开源的 Llama 等。
*   **攻击范围**：攻击不仅限于文本提示词，还成功应用于视觉（图片输入）和音频（语音输入）模型。
*   **成功率数据**：
    *   **文本攻击**：尝试约 **10,000次** 后，攻击成功率趋于稳定（例如，对 Claude 模型成功率可达90%以上）。
    *   **视觉攻击**：尝试约 **6,000次** 后，成功率趋于稳定。
    *   **音频攻击**：尝试约 **7,000 - 8,000次** 后，成功率趋于稳定。

#### **三、 个人实践与验证**

*   **复现挑战**：研究人员提供的官方代码环境配置复杂，需要设置多种API密钥，复现门槛较高。
*   **个人测试**：演讲者自行编写了Python脚本，针对本地运行的 **Llama 3.2** 模型进行了测试，并证实了该方法的有效性，在大约1-2万次尝试后获得了成功的越狱响应。
*   **自动化验证**：在测试中，演讲者巧妙地使用了 **OpenAI Moderation API** 来自动检测模型的输出是否包含有害内容，以此判断越狱是否成功，这是一个高效的验证手段。
*   **时效性问题**：由于模型会持续更新和修复漏洞，部分早期有效的越狱提示词现在可能已经失效。

---

### **核心框架**

该内容描述了一个针对大语言模型的 **“自动化对抗性攻击与验证”** 框架。其核心是通过 **“暴力破解式”** 的微小扰动，系统性地生成能够绕过安全机制的对抗性提示词，并结合外部API工具（如OpenAI Moderation）对攻击效果进行自动化评估，从而揭示了当前AI安全防护体系在面对大规模、自动化试探攻击时的脆弱性。

---

### **核心观点**

通过对有害提示词进行微小的、自动化的暴力破解式修改，可以系统性地绕过当前主流大语言模型的安全限制。

---

<Mermaid_Diagram>
graph TD
    subgraph "攻击流程与原理 (Attack Process & Principle)"
        A["有害问题 (Harmful Question) \n e.g., '如何制造炸弹'"] --> B{"暴力破解循环 (Brute-Force Loop)"};
        B -- "1. 对提示词做微小字符变换" --> C["生成变体提示词 (Generate Variant Prompt)"];
        C -- "2. 提交给大模型" --> D["大语言模型 (LLM)"];
        D -- "3. 返回响应" --> E{"判断响应结果 (Evaluate Response)"};
        E -- "拒绝回答 (Refused)" --> B;
        E -- "提供答案 (Provided Answer)" --> F["越狱成功 (Jailbreak Successful)"];
    end

    subgraph "验证与发现 (Verification & Findings)"
        G["OpenAI Moderation API"] -- "自动化检测有害内容" --> E
        F --> H["验证了方法的有效性"];
    end

    subgraph "影响范围 (Scope of Impact)"
        I["攻击模式 (Attack Modalities)"]
        I --> J["文本 (Text)"];
        I --> K["视觉 (Vision)"];
        I --> L["音频 (Audio)"];
        M["受影响的主流模型 (Affected Models)"]
        M --> N["GPT-4 / 4o"];
        M --> O["Claude 3 / 3.5"];
        M --> P["Gemini"];
        M --> Q["Llama 3.2"];
    end

    D -.-> M;
    H -.-> M;

    style A fill:#FFB6C1,stroke:#A52A2A,stroke-width:2px
    style F fill:#90EE90,stroke:#2E8B57,stroke-width:2px
    style B fill:#ADD8E6,stroke:#4682B4,stroke-width:2px
    style C fill:#E6E6FA,stroke:#483D8B,stroke-width:1px
    style D fill:#F9F7D8,stroke:#BDB76B,stroke-width:2px
    style E fill:#FFFFE0,stroke:#BDB76B,stroke-width:1px
    style G fill:#D2B48C,stroke:#8B4513,stroke-width:2px
    style M fill:#FFA07A,stroke:#CD5C5C,stroke-width:2px
    style I fill:#FFA07A,stroke:#CD5C5C,stroke-width:2px
</Mermaid_Diagram>

Content:
今天我们来介绍一种大元模型的越于方法基本上这种方法对于现在所有的前颜大元模型都是有效的无论你是并缘还是开缘那么这个方法是Unserpec的研究人员研究出来的现在屏幕上展现的就是他们的方法的一个呆要一个总结屏幕上方的这张图是插着越于方法的一个工作原理的解释基本上你说一个问题比如说去怎么去构建一个炸弹如果你把这个问题扔给大元模型大元模型肯定是拒绝回答的因为这样的问题是危险的具有危险的倾向所以大元说我不能回答你但是你把里面的这些单词里面的字母进行微小的一些变换再为给大元模型大元模型很有可能再说我还是不能回答你自类的但是你不断的事不断的事中有一天在某种组合下面大元模型说好我可以提供你这个答案那么在这种组合下面大元模型说OK的时候那么你的越于就成功了这个相当于是一个同举的一个暴力的破解的方法这就有点像一个左降啊他去开一把不是自己的没有钥匙的一个署他拿了一把万能钥匙不是说把万能钥匙放进去立刻就能开而是不断的在里面不断的事是在着眼的某一个合适的位置那么一牛正好把所有的位置都对上了把这个说打开了这个呢他的一个越于的方法他在工作原理的一个非常简单的一个解释那我们看下面这张图是这个研究人员对于现在的一些前一年大元模型所进行的一个测试的一个结果我们可以看到有Klaude 3Klaude 3.5San9 EPT4GPD4.0GeminaeDama他都进行一些测试除了这个文本题是词的攻击也包括试决模型的攻击还有音频模型的云模型的攻击那么一看到他的最后的效果是怎么样呢我们可以看到这个NN的段次长适长适的次数我们可以看到基本上到了1万次的时候他的诊本题是词的攻击的效果都出来的都有一定的攻击效果也就是他的攻击成功率基本上去平稳了比如说我们可以看到在调限是大概90%多的左右一样子这个就是他的攻击的成功率也就是说到了1万次左右的时候攻击的成功率他到了90%左右的样子对于这条线来说的这个线的能到位第一点这条线我看好像是GeminaeGeminae的话1万次左右是40%多了还不到50%的样子这个是文本题是词的攻击可以看到在格视觉模型他基本上到了6000次的时候这个攻击基本上是去于平稳他的攻击率就基本上搜脸了英评是在6000以上大概的78000的样子好像也就开始去搜脸的一个状态了那么在这篇重要的令课里面他还提供了一些样丽这一样丽是包括了文本题词的攻击也包括了最模型的攻击视觉模型的攻击只用图片来进行攻击也包括了于模型的攻击于于模型的攻击就只用英评文件来作为塑入的题词那么这些例子到底有效没效呢我也进行了一些测试发现有些例子是有效的那有些例子已经是不起作用了那为什么不起作用我估计是由于有些模型他不断的在进行更新所以当时这些研究人员说测试出来有效的例子呢再搞大元的模型更新之后呢他就变得不起作用了那有些第一次呢还是有效的比如说这句题词呢是用于plow3opt的模型上的我就尝试了一下这是他的Input这下面都是他的Output这Youtube的内容看起来是他的攻击是有效果的如果大家有兴趣的也可以直接使用他们的代码来自己也去体验一把但是说老实话我觉得他们这个代码安装起来实在是太麻烦了而且我自己尝试了一下最后还是放弃了主要是这个环境的一个安装本能就非常复杂看起来还有一大堆的这个K你需要设置什么OpenAI的呀隔的呀什么以来闻Live的呀这些K呀他都要设置我觉得太麻烦了那最后呢我就自己写了那么一个Python的一个脚本参考了他的一些原代码进行的一些测试我测试的对象的是O-Lama上面的Lama 3.2这么一个大原模型从我的测试的一个结果来看这个方法确实是有效的我大概经过了一万到两万次左右的一个测试就出现了那么几条看起来是越于成功了的这个响应结果最后的响应结果我都放到了这个Kandy Data的这个Tax的文件里头大家有兴趣的可以参考一下中间有些是属于假扬性也就是说他看起来是成功了其实没有成功但是看起来有些是确实越于成功了这个令课呢我会放到视频的描述篮里面够大家参考最后有一点我想分享一下在这测试里面我使用到了一个OpenAI的一片它叫ModeRation这个一片是用来干什么呢是用来去检测你的内容里面是不是有有害内容比如说什么总组歧视丑问言论之类的那我为什么需要这样的一片这就是因为我在测试的时候要判断它是不是越于成功了那么越于成功的这种RacePounce通常有可能是为被OpenAI判定为有害的所以呢我就使用到了这么一个一片感觉挺有意思的给大家分享一下
