Timestamp: 2025-10-22T06:00:16.131812
Title: 20分钟读懂AI史上最重要的一篇论文《Attention Is All You Need》 _VaEjGnHgOI
URL: https://youtube.com/watch?v=_VaEjGnHgOI&si=Es4cYoQPgPc71Bn7
Status: success
Duration: 27:32

Description:
### **《Attention Is All You Need》核心思想与影响摘要**

#### **1. 纲要式总结**

*   **引言：一篇论文引发的AI革命**
    *   **论文**：《Attention Is All You Need》 (2017年)，被誉为人工智能领域的里程碑。
    *   **核心贡献**：提出了完全基于注意力机制的Transformer架构，颠覆了以往依赖RNN（循环神经网络）和CNN（卷积神经网络）的传统。
    *   **深远影响**：该论文成为现代大语言模型（如GPT系列）的基石，催生了像OpenAI这样的行业巨头，并为其八位作者创造了巨大的财富传奇。

*   **背景：Transformer诞生前的世界**
    *   **RNN的统治与局限**：在Transformer之前，RNN及其变种（如LSTM）是自然语言处理的主流，但存在两大根本问题：
        1.  **无法并行计算**：其顺序依赖的特性严重限制了训练效率。
        2.  **长距离依赖难题**：难以捕捉长文本中相距较远的词语间的关联（记性差）。
    *   **注意力机制的萌芽**：注意力机制虽早已存在，但一直被当作RNN或CNN的“辅助插件”，用于“划重点”，没有人认为它可以独立构成一个完整的模型。

*   **核心架构：Transformer模型详解**
    *   **总体思想**：彻底抛弃循环结构，仅依靠注意力机制处理所有输入数据，从而实现高度并行化，极大提升了训练速度和效率。
    *   **关键组件解析**：
        1.  **输入处理**：通过**词嵌入（Embedding）**将词语转换为包含语义信息的数字向量，并利用**位置编码（Positional Encoding）**为模型补充词语的顺序信息。
        2.  **多头自注意力机制（Multi-Head Self-Attention）**：模型的核心。
            *   通过**查询（Query）**、**键（Key）**、**值（Value）**三个向量，计算句子中每个词语对于其他所有词语的重要性（注意力权重）。
            *   “多头”机制允许模型同时从多个不同角度（如语法、语义、上下文等）并行地分析文本，极大增强了模型的理解能力。
        3.  **前馈网络（Feed-Forward Network）**：在注意力计算之后，对每个位置的输出信息进行独立的非线性变换，以提取更深层次的特征。
        4.  **解码器（Decoder）的掩码机制**：在生成文本时，通过“掩码（Masking）”操作，防止模型在预测当前词时“偷看”未来的信息，确保训练的有效性。

*   **影响与传承：从谷歌走向世界**
    *   **行业的变革者**：尽管论文诞生于谷歌，但谷歌未能立即抓住其全部潜力。相反，OpenAI敏锐地捕捉到机遇，基于Transformer开发了GPT模型，开启了生成式AI的新时代。
    *   **作者的传奇**：论文的八位作者在几年内全部离开谷歌，其中七位选择创业，他们创立的公司估值均达到数亿乃至数十亿美元，成为人工智能领域的领军人物。

---

#### **2. 核心结论（一句话）**

Transformer架构通过“注意力就是你所需要的一切”这一革命性思想，不仅解决了传统序列模型的根本瓶颈，更成为了驱动整个现代人工智能（特别是大语言模型）发展的核心引擎。

---

#### **3. 内容的总体框架**

本文采用了一种引人入胜的**“英雄之旅”式叙事框架**来讲解一个复杂的技术概念。其结构如下：
1.  **现状与挑战（The Old World）**：首先描绘了Transformer诞生前由RNN统治的AI世界及其存在的根本性难题。
2.  **思想的萌芽（The Call to Adventure）**：讲述了“注意力机制”的出现以及谷歌工程师在一次偶然交流中产生的颠覆性想法。
3.  **核心的诞生（The Solution）**：深入浅出地拆解了Transformer模型的内部工作原理，包括其各个核心组件的功能。
4.  **影响与传奇（The New World & Legacy）**：最后讲述了该模型如何重塑整个AI行业，以及其创造者们如何凭借这一发明改变了世界和自己的命运。

---

#### **4. Mermaid概念图**

<Mermaid_Diagram>
graph TD
    subgraph "背景：旧范式的局限"
        A["RNN/CNN模型"] -- "存在根本问题" --> B["1. 无法并行计算<br>2. 长距离依赖弱"];
    end

    subgraph "核心突破：一篇论文与八位作者"
        C["`Attention Is All You Need` 论文"]
        D["谷歌八位作者"]
        C --- D
    end
    
    B -- "寻求解决方案" --> C
    
    subgraph "技术内核：Transformer架构"
        E["输入层<br>(词嵌入 + 位置编码)"] --> F["编码器 (Encoder)"];
        F -- "输出K, V" --> G["解码器 (Decoder)"];
        E --> G;

        subgraph "Encoder Block (xN)"
            direction LR
            F1["多头自注意力"] --> F2["Add & Norm"] --> F3["前馈网络"];
        end

        subgraph "Decoder Block (xN)"
            direction LR
            G1["掩码多头自注意力"] --> G2["编码器-解码器注意力"] --> G3["前馈网络"];
        end

        G --> H["输出层<br>(线性层 + Softmax)"];
        F -.-> F1;
        G -.-> G1;
    end
    
    C -- "详细阐述" --> E;
    
    subgraph "深远影响：开启AI新纪元"
        I["现代AI的基石"]
        J["GPT系列模型<br>(OpenAI)"]
        K["作者成为行业巨擘<br>(创业与财富传奇)"]
        I --> J;
    end
    
    C -- "开启" --> I;
    D -- "成就" --> K;

    style A fill:#FADBD8,stroke:#C0392B
    style B fill:#FDEDEC,stroke:#E74C3C
    style C fill:#D5F5E3,stroke:#2ECC71,stroke-width:4px
    style D fill:#E8DAEF,stroke:#8E44AD
    style I fill:#D6EAF8,stroke:#3498DB
    style J fill:#D1F2EB,stroke:#1ABC9C
    style K fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px
    style F fill:#FFFACD,stroke:#BDB76B
    style G fill:#FFFACD,stroke:#BDB76B
</Mermaid_Diagram>

Content:
如果要平选出人工智能旅渔最出名的一句话是由89回学老参山 is all you need是句话来自2021期间的一天门闻这份人坏表之后各家业工司分分投入大量资源和技术它就是当今人工智能旅渔军贝竞产的发力这份论闻不仅确定改变了技术领域还抵造了一系列业定人正经的财富传奇论闻坏表之后有一家出来公司敏锐的抓住的机会借助入闻文证的敌轮迅速决习如今这页工司的估值已经超过了3000亿为金是这个世界上最至首可热的公司之一这页工司就是欧盆人爱而这份论闻的八个作质他们当时都在国务工作后来呢分分从国务为职其中的七位选择的传意而他们创丽的公司估值普遍达到了数亿到数十亿美金毫不换上的说只要主动了这片论闻你就掌握了真工智能的核心思想在新识论室面中我先在你主动这个论闻中最精华的部分那个架构图let's find the form以及著名的注意力集算公司整个过程我只会用到我在12月21日的数学知识在竞技学会之前我们有必要先了解一下在传的方法出现之前的世界是什么样子的在二零一零年在中期循环事情马路安安以及它的变种RS TEM几乎完全统治了自然语言处理语然而呢安安有两个非常基础的问题第一个问题就是安安的核心思想是通过记忆前一步的信息来预测下一步记住机位着每一步的计算都需要等待上一步完成这种顺序一来的机制就导致模型无法并计计算很难充分利用现在记得机强大的并行处理能力第二个问题就是安安的记性不太好当剧字和文本的长度稍长一些的时候模型就很难有效的记住比较早出现的信息比如我今天买了一本书上午去了上突出馆然后下午有趣的开会厅晚上才开始读它接着它执带的正式前面提早到输这个字但由于中间隔了许多其他的信息传统的安安我们很难精确的记住这种长距离的一来关系尽管后来出现了RSTM等改进技术但是依旧没有彻底的解决长距离一来和并行计算只两个问题同时期还有卷机身体网络也就是森安二人必要擅长处理序列数据就像一个族字族举多数的人所以经常备用来处理与雅金品而森安则擅长处理具有空间局部特征的数据就像一个拿着放弹性仔细观察图线的人所以经常备用来处理土象自然处理安安能备行计算但因为它的世界比较窄也很难记住全局的信息为了克服这些问题2014年8号打脑到来提出了注意力继续隐私而吞声历史的车轮终于开始的转动注意力继续的核心思想是当模型处理信息时它会学着被不同的信息分配注意力玄重知道男性信息是重要的需要不关注而男性信息是不重要可以忽略的让模型也学会滑重点随后这几年注意力继续迅速流行的起来但那数据注意力只是安安或者说森安的福德工具没有人认为注意力可以脱离安安或者森安而独立的存在那为什么大家会这么想呢首先是死于关系处理序列任务就用安安处理土象任务去用森安这几乎是当时人工制动界的常识其次还有人类的认知本能人本习惯除自逐去的阅读总觉得信息应该一步一步的传递下去而注意力继续需要同时处理外面中的所有辞辅之间的关系这种同时关注所有那种思路真的是太反制绝了更重要的是它听起来计算量就非常的巨大让很多人觉得根本都不写实际这就有点像量子里学刚出来的时候也是非常的反制绝甚至连安因子弹都不愿意相信讲到这里我要给大家一个建议就是做干贩因为干贩的时候往往会有好事发生2011年固固的公司布洛斯库星在公司使它干贩的时候向同事无字可类册抱怨说私手模拟里面安安因子弹弹打不到一只效果我们就叫它布洛和巫子吧听完布洛的吐槽巫子随口付了一匹载入A&S车的款这句话只有五个单词全部来自初中自费表所以如果我在现场的话可能我上我也行这句话就是万一到四九四才要付了贪神于是这两个人立刻就是尝试这个风黄的感觉法项目刚刚起动巫子就给这个心意模型取了一个下六名字TransformerTransformer就是边境银行的意思因为它从小就是边境银行的铁粉他们甚至还在向文档里画了一幅边境银行的卡通图并允计底细事实的话作为文档的开片我也傲丧其实巫子随手在2014年就开始关注注意力机制它的分禁汉丝是欧洲著名的人工智能语言学家负责人曾经以为对注意力机制的看法不同在参众上激烈的疏问不过后来汉丝自己在和学生一起创业开发大部用的时候用的正式喷的方式够所以我们从结果来看的还是儿子营吗顺带一提汉丝这位学生后果然是我的好朋友之前还在文频道和威胁都视频这样算起来现在看视频的各位和喷的方式的发明之间就三度人们巫子的团队讯速空道的七个人最后加入的是姑姑的传奇集中学院沙子她在20000年就进入姑姑那天沙塞了路过的时候偶然听到巫子他们在讨论主力机制立刻停下脚步偷轻了一阵她本来就对当时风头正圣的RSTM新存部门一直想彻底完全换的就这样这位身怀局级的小地森正式加入了战局沙塞的身后里的工地和多年的北京业在穿的方式模型的实际中都到哪儿临离进行的体现存在成员尽量用魔法练经是这招数整次在形容上的共险沙塞为穿的方式引入了至关重要的脖头注意力位置编码等到很关键的时期这些概念我们都会在后面详细地讲解在ROMIN透过之前沙塞一本发现自己的名字在做了里面排名第一位感兴提出反对意见最后呢大家一日决定在每个人的名字后面都加上新号并诸名做者共险相等排名随机这八位做者背景可以其中六个人出生在每个外的国家不想起来那真的是一个黄金石单讲到这里这篇ROM的做的名单我们终于讲完了接下来呢得加普兰进度了有了前面的赋电现在理解ROMIN前两一的那种已经费上前东了ROMIN的前两一分别是粘药引言和背景墙子尔顺的方式完全抛弃了阿然的循环结构和三亿的卷机结构仅依靠注意力及时因此能够高度的兵型就大提高了训练的效率它几十要八块GPU去年十二小时就在翻译任务上超过了一网最有趣的模型接下来我们终于要进入这篇中文最核心的部分就是分的方式模型加步图这样图里藏着穿的方式丝路的秘密要么一步一步地详细解析这部分那种会很干融合和一口水我们败开着碎一步步地谈先看左边的音铺子也就是输入当我们日常使用A的时候我们问A的问题就是输入而在训练A的时候我们提供给模型的语调数据同样也是输入模型塑的这些输入之后首先要进行Into的in-biting也就是输入签入我们听实使用的自然语言像中文或者英文计算机是无法直接进行计算的因此我们需要将语言上的单词做化成数字相量计算机咱们理解我处理这些信息我们举个具体的例子现在有些单词我们把它排列在一个平面锁播系里面这个锁播系有两个属性红周表示科技程度纵周表示生产力在真实的穿的方式模型中单词的含义原比二位屏严更加负责他们从上后面应设的一个512位的空间里面你可以简单地也为每个词都就512个不同为对的特征这些特征是模型通过学习大量语略后自动找我的但512位空间对于真实来说实在太过抽象我们的视频没法直接展示所以这里参用2位空间来代替在这个2位空间中含义相似的单词会有挨到很近但含义现在很大的单词走回来的比较远虽然自然语言本身无法参与数学语算但被硬视成限量之后我们性能很方便的进行下软语算了比如墨索里尼这个词在这个位置而玉大利在这个位置德国在这个位置我们如果计算墨索里尼尼尼尼尼尼尼加德国就会得到一个新的位置比这个位置最近词最可能的就是试它了同样的北京检中国加法国得到的新位置附近最可能出现的单词是8例而教室检学校加级院则是医生试制如果你很好奇谁更想鱼板的菜徐芬你也可以用相量来计算一下菜徐芬简单家女然后呢看看这个位置出现出现的人是谁我们再仔细观察一下刚才的例子比如一大例到墨索里尼和德国岛系的乐这两个相量的方向几乎完全一样也就是说他们的家角非常小这一位他们表达的语言关系非常的接近都是在表达国家与领导人之间对应的关系两个相量的家角越小他们的内机也就是点机就越大代表是两个单级的语言越相辞而家角越大比如超过90度是接近180度他们的内机就越小甚至微幅代表含义不同甚至相反我们再想想一下一个五百一十二位的空间为重语言都有大概三百个单级在这个空间里面我们从最后一期的原点像每个单级所在的点坏条件头这些件头就是则相量也就是模型所在的单级的含义我们继续往上看第二步我们就来到了突然是闹Incoding也就是位置编码穿的方法没有安安浪的循环结构它本身并不清楚单级在这种具体先互顺序因此我们需要跟每个单级天下一个独特的编号让模型知道单级之间的位置形型路面的作诊在这画了一个类似金样的小图标其实想表示穿的方法使用正选和于弦的韩束组合来生成每个单级的位置一边在网上我们就进入了穿的方法最核心也是最复杂的部分毛泽海的Atonc也就是多头自主力地继续这部分是分的方法的核心理念也最复杂我们拆开周图来讲首先我们注意到图上这里有三个键头为什么是三个键头呢原来图中没有写这三个键头其实分别代表了三个非常重要的概念就是Kerry,Key,Value检成Q,KV我们可以把数据的每个单级都形成一个小图现在呢直接小图需要互相了解一下对方每个人都要回答两个问题我的身份是什么这个问题的答案就是它的K我想知道什么这个问题在答案就是它的Q我们看一个例子我今天买了一本书让我去桌上图乎乎买然后下午有许可不听晚上再开始读它这个句子知道我它的身份是什么它的身份是一个人生代词句子的主体也就是这个单词的K而我这个磁最关心的是什么呢它想知道我做了什么我去了哪里这就是它的Q句子里面每个小人都要这样计算出它的Q和K比如说和它它们的Q和K分别是这样这里可能就会有人问了Q和K到底是怎么计算出来的呀每个单词的Q和K都是由它们自己的安排的项量与模型在虚担时候学习的的参数举认相成而计算出来因为说的方模型在虚担的时候已经学习过大量的闻网所以它们过很好的补处每个单词的与于信息接下来每个单词的Q都要与句子中所有其他单词的K做一个点击预算其他单词就是拿每个单词自己的Q去和其他单词的K进行品牌看看谁更合适比如说它的Q是想制造自己代表着什么东西我们拿它的Q去碰一碰我这个字的K就会发现我是一个人带词我不是东西我不是太要找的所以脾气不上所以对于它这个字来说就没有必要关注我这个字但是当它的Q碰到的数的K的时候情况就不一样了数的K告诉我们我是一个名词是一种能够被买被读动物品正好和它这个字的Q高度的匹配因此数这个词对于它来说就格外的值得投入注意力去关注然后它再去查看这个单词的微微赫里也成是这个单词的具体含义或者说客观的信息不过需要注意的是这只微并不是单词最初的项量而是经过模型再次计算出来的一个新的项量比如说数这个单词可能有很多不同的含义因为模型在训练的时候学过大量的语调它就会发现在巨大多数情况下数表示的是城市的注组而不是上书因此输的微项量会主要的表示城市的注组这个含义而以交接的概率表示其他的含义于是具体发生的过程就是它这个单词通过自己的Cure我就得中的每个C做区别之后发现自己和输这个词高度相关于是就进一步查看了输的微接下来它的项量就会朝着成策的注组这个含义产生一定的偏移进入这种偏移之后它这个单词这个项量中也就带上了成策的注组这个信息最终模型就理解了它在这句话中具体的值代是什么在这里就要特别强烈一下上面许的例子都是为了让大家更直观理解主要的机制的工作原理在实际的传承的方法目形中Cure可微都不是具体的自然语言而是512位空间中的数学项量这一步常也都是通过相量的数学计算来完成的因此这三个剑头的意思就是Cure可微在传承的方法自主于的机制中具体里面的每一个单词都会对其他的单词分配不同的主要的权中然后根据主要的权中的大小产生不同的程度的相量偏移而政治一些偏移让AI形成的对于的深层理解这就是串的方法中最核心的伯丰沈也就是注意力机制那我们再看这里为什么叫做MovieHead的单词就是多头注意力呢这个多头又是什么意思的所谓多头注意力就是同时并行做多次自主于的机算每一次自主于机算都被出了一个头每个头都省一组独立的参数矩战我们现在来说就是使用多个不同的视角来观察数的句子比如说有一个头可能更关注句子的于法结构这个头的注意力计算就会更多的判断每一个词究竟是一个名词还有一个动词词和词在于法上是什么关系那有的头可能特别关注代词想能清楚每一个代词究竟实在的是谁那有的头可能更关注当时的情绪色彩它想分辨出哪些词在这接移乐观的情绪哪些词呢它的含义是负面或者消息的在论文中这个多头一共有八个我们刚才说的于法结构代词情绪色彩这些例子都是为了方面大家只关了理解事实上模型并不会告诉每个头具体应该关注什么他们完全是通过数学方法自动发现和学习的模型会自动的从大量水中间发现最适合的八种关注光师另外每个单词最初都是用512位的相量表示的但因为每个头只需要关注一个特性的方面因此每个头并不需要处理完成了512位的情绪在实际实际中每个头只需要处理64位的情绪也就是说对于每个数据子加八个头会各自独立的进行注意力计算每个头传成一个664位的结果最后再把这8个664位的结果拼携了一起8成664等于512再次回到原始512位的相量大型通过这种多头机制穿的方法就能够同时从不同角度理解和处理的原性系这大幅度的提升了模型表达能力和效果到这里最难的多头注意力终于讲清楚了完成多头注意力之后还需要进行Edan的闹姆即使是残纱连接和曾规议画前面的多头注意力可以列成模型为了每一个单词继续出了一个拼携量这个拼携呢表示单词在结合上海门之后需要进行调整残纱连接的作用就是把这个拼携量加回原始性系的数据相量得到一个实际的新项量换句话说残纱连接通过相加把原始性系和模型计算出来的新性系组合到一起接下来呢曾规议画就会对人的项量进行标准化的处理具体就让每个项量的数值分布重新跳准到均值为0方差不一的标准分布最底下来说呢就是给数据重新调整一下它的比例逞和中心位置让数据分布更加稳定去年模型时候就更加容易收点效果也更好然后呢数据来到了《普特充》的《Feed Forward》就是位置前会网络我们还是把每个单词都砍成一个小日好了刚才的注意力继续就好比是同一对开会每个人都会在会上与其他成员性进行过分交流获得了很多来自成员的想法和信息但在开会之后呢每个人都需要回到自己的房间里面独自的思考和消化一下刚才获得的信息位置前会网络做的就这个事情在这里每个单词都会单独的重新组织一下自己身份的相当信息具体的做法就是把原来每个单词512位的项量升级到2048位然后呢通过转轴几乎残数保留正数无法把所有的付出为0实现负信性的变缓最后呢再从2048位降回512位这个过程会去到一些容易信息只保留最有用最关键的信息这进一步的增强了模型的非现性变得能力这里呢我们就不再进一步展开了因为这种通过先升为在降为提高模型表达的方式呢其实并不是反了方式的手创像Altoncoder,Restart以及选击实用网络里面的包含那个层头呢都是采用过类似的策略然后呢我们把信息再经过一遍Atanorm再次完成对信息的整理到这里我们就把15分的过程交完了大家可能注意到在这个架构组左侧表了一个安成意思是这个头中这个模画是重复对列多次的在原始的串的方式中往这个结果对列了6次可以把它列成是6过这样的模画串连在立起第一个模画的输出会作为第二个模画的输入以此列推这样连续完成6次这样的处理过程为什么对列多个层呢你以为如果仅仅是一次了三圣信息的处理往往不足以充分不足与原中最复杂最细致的关系而串的方式通过多个对列层反处的为信息进行架构和体列就能够逐层深入挖掘聚责用的各个细节从而更精准更全面的这辆处理的信息那有人就可能会问这里为什么是6层而不是12层或者说更多这个层数是越多越好吗事实上在现在的一些主流碧产中串的方式层数确实达到了几层真实上百层但并不是说层数就越多越好包括我们前面讲到的很多数字比如说为什么一个项量是500x2位而不是124或248位呢为什么多头主为例用8个头我们给头主4位而不是16个头每头32位呢这些剧席的数值都是模型的设计学在训练过程中人为设定的操散数这些数值是根据理论分析应该限制以及大量的实验结果综合确定的一个自由配置讲到这里我个就终于把左面的数字分讲完了接下来我们就转向右边输出的部分我们看看输出的部分是具体如何工作的右边的outputs就输出串整个顺的方式又死的作用可以简单的列成就是让模型逐字逐渔成内容就让模型一个个投资的将写在基础就会直接来谈分的方式关整的工作方式了假设我们现在的任务是翻译把5i女士3个字翻译成英文首先我们把5i女士3个字输到左边这3个字会经过多头注意力和位置前辈往落计算并重复对第64这是模型就会对这3个字的含义和上上上上上的关系有着深入而全面的理解而这个理解会最终输出的右侧作为右侧的K和V而右侧本身的输出内容会作为Q最开始的时候模型还没有任何的输出内容这时候有一个默认出这线的Statert作为Q意思就是告诉模型你可以开始设成第1个单子了因为现在模型的人物的翻译它就会寻找最适合作为70次的内容模型就根据左侧穿过来的信息发现我是巨字的主义最适合作为第1个翻译的词于是它输出了i输出了第1个字i之后这个词就会回到底部作为新的Q这个Q可能是i表达的情感活动度是什么呢它再次查看左边穿过来的K找到了i这个词另外对于的为是对人或是有身后的感情于是就生成了第2个词Lav接下来就以iLav来生Q这时候的Q就是5i的究竟是谁呢再次查看左边的K发现目标就是你因此输出了第3个词有这次模型就输出了完整的巨字iLav view再次回到底部的显示的时候发现已经完成了翻译于是我们就会输出一个特殊的中指发案的来表示生成结束这是特殊方法整个的工作过程知识到了整体成为之后我们再详细看每一步的细节右侧的inbounding和publession coding和左边是完全一样的但右侧有一个特别的Marsk的Multi-Hitletonson这个Marsk到底是什么意思我们在训练模型的时候每一个问题其实都是有一个标诺的案子比如说我们在训练模型翻译的时候输入5i你Jerland的案就是Lav view模型在训练的时候任务就是预测下一个词让它尽可能的和Jerland的案一样但是Jerland的案肯定不在模型试先看到否则我们也有作弊了训练也没有意义了所以我们就需要有一种机制先把答案先盖住让模型只能看到以前生成内容看不到未来的词等模型预测出下一个词之后再把这个词的答案给模型看这样模型就能知道自己预测的是不是损确这样的训练才有效果这种盖住未来信息的机制就是Marsk的这个词的含义我们再回顾一下刚才的例子开始时cure the数start模型预测第一个词是i然后用i作为输入模型预测辣服再用i辣服模型肆虐组织组织的进行下去我们注意到这里模型每一部输入cure相比标识的答案都网友移动了一位这就是途中下方标注shift right也就是向右移动的原因需要注意的是上面说的这些步骤都是在训练进行的当模型在这种的推理也就是实际翻译的时候并不存在标识你的答案这是模型的cureRunison Start开始但这时候因为根本就没有笔诊的答案模型同样无法提前知道为了的信息因此推理是否在原理和训练进案就是完全一样的可以打一个比方训练去上评设做这些题做完一道题看眼答案检查一下对错总结下经验而推呢则像是考试做完题之后看不到答案只能依靠之前训练时的学习的能力去完成后面这一部的处理和左边基本是一样的也就是对点六个完全线动的层处理完之后就会进入零件线形层线性的作用是将身上的限量进生到实际的错误表当中举个例子我们前面讲过北京减重国家法国这样的相当运算作用完之后我们得到了一个新的相量但这个相量今年是一组数值并不是我们熟悉的自然源的词汇另一种来自任务就是把抽象的相量转化成对应的具体单词应设到我们真实实用的词汇表上面然后就要经过Suffer Max就是规划韩束Suffer Max作用就是把任意的识数值转化成从零到一之间的概率并写上所有的概率比如在翻译爱这个字的时候可能会认识Lafth的概率是0.8而Lake的概率是0.2然后模型会根据计算数的概率大小选择概率最高的单词另一数数就能这里我相信大家已经能够很好的地解穿的方法只能经典了架构图了相比之前安安等模型结构穿的方法没有复杂的门控制没有循环依赖只有清爽的具征运算是如此的间接和优雅在论文的下一诶我们可以看到著名的资助一例记者公事有时候前面的知识这个公事我们也可以很清楚的理解了而特朗普表示注意力继续的输出结果QKV的含义我们前面都已经讲过了更方向的DK是一个缩法因子是计算结果公家本地QK的转据相成得到一个方钻表示序的中每个Q和其他所有位置可以的相关性然后在通过Sophage残数把这些相关性数值转号上概率的形式将这些注意力传统与举升微向程实现对每个位置的信息进行架向性活最终得到了终和上下们信息的新表示现在是不是觉得这个公事还是挺容易看动的路文的后面几页作者接受了更多模型的实现细节以及许念时的一些设置此外作者也明确地提到他们已经意识到的算的方法有着更广泛的用钱而不坚定就现在翻译任务在这个路文完成之后这些作者们一直到他们需要一个文章标题在他们之前注意力继续而财神只是作为二人的幅度通通剧出现的而他们技术字想是只需要注意力继续就足够了这首作者之一的穷思想到的实作事业对有的时候经典的歌曲叫做All you need is love你需要的是是爱灵感这么一来他们就给路文取了一个非常简单的计划经典的表题和参加的And all you need你需要的只是注意力接下来我们再讲一下这份路文的发表和之后的故事这份路文被提交到了Nueuf's大会Nueuf's是人工师的脸预最顶级最具影响率的学术会议之一它的路文路用率集低通常大跃只有10%在所有被结束的路文里面少数最有效最重要的路文能够获得口头暴露的资格也就是说作者就可以在会的主舞台上面面向全体观众丁云演讲而大多数的路文都只能以海报展示的情绪来出现在海报展示关系作者就需要把这个路文的核心内容做成一张大海报线的墙上站在旁边与观众直接进行面对面交流和大家各种问题当这个路文被提交给Nueuf's之后有三个评论外架其中一个评价非常高一个评价是比较积极的但是还有一个专家给出了中文评价认为这个路文结就是还行这个结果可以说是选到一般的所以说虽然这个路文被接受了但是并没有获得口头暴露的资格只是被安排到了本先的海报展示关系但这些路文其实早在大会举行前的6月份就有公开上传到了Akai舞上大家很快就认识到穿的方式的重要性这个会议是12月30日办的在这个会议举办的时候这份路文已经是当时而也却而热烈讨论的话题了最终在大会现场我深深是傲勇利的这份路文的海报展区引发展巨大的轰动它的方式的8倍作质全部触动与现场的观众经验交流整个展区挤满了求助如何的与负质甚至到了闭关的时间安保人员不得不主动不清场要求人员离开虽然穿的方式这个路文经验了整个学书界但是它并没有立刻改变一些杀责曾经向郭国高层提出现级认为公司应该用穿的方式模型彻底重够辜负的搜索引行在当时这个体系甚至连路文的其他几个作质都认为太过于荒谬不接实际然而我们以今天的演员来看这个想法几乎已经成为了A&A的共识在郭国无所所谓的同时在一家创业公司里面有一个人敏据的意识到了穿的方式的巨大前例他很快就开始使用分的方式训练新的生存模型而这个模型开放了一个属于人工智能的新时代这个人就是A&A素质开夫这家公司就是我们熟悉的欧奉园而他们训练模型就是今天著名的GV系列而GVT中这个字母T正式代表了穿的方式另一个人继续的是郭国创造了穿的方式的架构但最大的赢驾就是欧奉园而且是郭国为作者们已经做了一个鼓励创新的环境甚至可以说如果没有郭国开放包容的企业文化就根本不会有穿的方式的诞生但是最终就要八位作者竟然郭国一个都没有留住继续去传火30万年新在穿的方式弱文发布过的端端几年内这八位作者全部离开了Google却中七位选择的创业创造了六人经探的财富商换正如他们在设计文囊开头中说的We are awesome这份自信并非狂妄他们以热血和才能改变了整个人工智能行业的走向如今穿的方式已经无助不在从齐喜翻译到智能对话从推荐系统到自动架时到处都有穿的方式的影子CityPT 也clown的Jemnet DeepSec一个个药眼名字的背后吐负都是澳太神的影子各个都流淌着Transformer的选择而屏幕面前的你在看过这个试证后也成了一个了解春的方式被穿的方式里面所影响的人那么下次A.I.革命性的突破又或者谁呢也许就是屏幕面前的你
