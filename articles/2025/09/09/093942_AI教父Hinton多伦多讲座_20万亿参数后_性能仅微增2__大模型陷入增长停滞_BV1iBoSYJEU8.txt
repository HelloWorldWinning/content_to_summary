Timestamp: 2025-09-09T09:39:42.158513
Title: AI教父Hinton多伦多讲座：20万亿参数后，性能仅微增2%，大模型陷入增长停滞 BV1iBoSYJEU8
URL: https://b23.tv/7qCXEik
Status: success
Duration: 26:28

Description:
### 核心思想提炼与总结

#### 1. 物理学对深度学习的启发

*   **关联函数与可解释性：** 物理学中关联函数是描述宇宙的普适工具。对于万亿参数的AI模型，尽管其潜在有用，但如何运用物理学直觉来理解其内部运作机制仍是一个未解难题。演讲者对此持悲观态度，认为完全理解大型模型可能永远无法实现。
*   **标度律与其他物理学进展：** 物理学启发了多项深度学习进展，如受限玻尔兹曼机。网络性能随规模和计算量的增加而呈幂律（标度律）提升，这一现象与凝聚态物理等物理系统中的行为类似。物理学家在数学技术发展中也扮演了重要角色。
*   **变分方法与高效通信：** 物理学中的变分方法在机器学习中得到应用。以通信为例，相比于固定选择最廉价的编码方式，随机选择编码结合共享随机数生成器，实际上能更高效地传递信息（例如，通过玻尔兹曼分布），体现了统计物理学在编码理论中的价值。

#### 2. 人工智能的伦理与安全挑战

*   **AI与国际合作的困境（CERN类比）：** 物理学界有CERN等大型国际合作的成功范例。然而，鉴于AI的“双重用途”性质（如致命自主武器、网络攻击），各国政府在AI领域的合作面临巨大政治阻力，这与核武器研发初期的情况类似，难以效仿CERN模式。
*   **理解与安全之间的平衡——核武器类比：** AI的发展被比作核武器，其基础科学与潜在破坏性应用并存。演讲者强烈批评公开大型基础模型的权重，认为这等同于公开“裂变材料”。一旦权重公开，任何拥有少量资金的实体都能利用这些模型进行恶意目的，严重加剧了AI安全风险，且这一“船已出海”，难以挽回。
*   **对未来AI发展的悲观预测：** 尽管AI具有医疗、教育、材料科学等诸多积极应用，使其发展速度难以减缓，但演讲者对能否安全地发展AI持悲观态度，认为目前缺乏足够的政治意愿来制定保障安全的措施。他援引心理学研究，指出轻度抑郁者在预测负面事件概率时往往更准确。

#### 3. 深度学习的未来与大脑类比

*   **标度律的局限性与新方法的必要性：** 纯粹依赖标度律（增加计算和数据）可能面临收益递减，GPT-4后的进展显示出边际效益递减的迹象。但演讲者认为AI不会因此停滞，类比摩尔定律从提高速度转向并行化，新的理论和工程创新将继续推动AI发展。
*   **大脑学习机制的探索：** 探讨了大脑中是否存在类似反向传播的机制。提出“时间差分”可能作为误差导数在脑中实现反向传播的一种方式，例如在小脑中训练前庭系统。然而，这种机制在大脑中可能不如数字计算机高效，尤其是在处理信息瓶颈时。对理解大脑在感知学习中的作用，目前仍感到失望。玻尔兹曼机虽受物理学启发，但并非大脑的实际运作方式，也非最优工程方案。

### 核心结论

尽管物理学为深度学习提供了深刻的见解和工具，但AI的快速发展及其潜在的军事和网络应用，加上缺乏有效的国际合作和对模型透明度的担忧，使得其安全和可控发展面临严峻的伦理和治理挑战。

### 贯穿内容的总框架

**交叉学科对话：物理学视角下的深度学习与人工智能的伦理挑战**

<Mermaid_Diagram>
graph TD
    subgraph "I. 物理学对深度学习的启发"
        A["物理学启迪"] --> B["关联函数"];
        A --> C["标度律"];
        A --> D["变分方法"];
    end

    subgraph "II. 深度学习现状与未来"
        E["AI模型可解释性挑战"]
        F["高效通信策略"]
        G["深度学习发展"]
        B -- "启发理解" --> E;
        C -- "驱动性能" --> G;
        D -- "应用实例" --> F;
        F -- "提升效率" --> G;
        G -- "复杂性增" --> E;
    end

    subgraph "III. AI的伦理与安全挑战"
        H["AI伦理与安全"]
        I["国际合作 (CERN类比)"]
        J["AI双重用途风险"]
        K["模型权重公开争议"]
        L["军事/网络应用"]
        M["安全发展困境"]

        G -- "引发关注" --> H;
        H -- "呼吁但困难" --> I;
        H -- "核心顾虑" --> J;
        J -- "具体案例" --> K;
        J -- "主要威胁" --> L;
        K -- "加剧风险" --> M;
        L -- "阻碍合作" --> M;
    end

    subgraph "IV. 深度学习未来方向与大脑类比"
        N["AI未来发展"]
        O["标度局限与新思路"]
        P["大脑学习机制探究"]
        Q["脑中反向传播"]

        G -- "展望" --> N;
        N -- "路径一" --> O;
        N -- "路径二" --> P;
        P -- "探索可能" --> Q;
        O -- "持续驱动" --> G;
    end

    R["核心结论：AI发展的复杂性与挑战"]
    H -- "构成" --> R;
    M -- "导致" --> R;
    E -- "体现" --> R;

    style A fill:#DCE775,stroke:#333,stroke-width:2px,color:#333;
    style B fill:#C5E1A5,stroke:#333,stroke-width:1px,color:#333;
    style C fill:#C5E1A5,stroke:#333,stroke-width:1px,color:#333;
    style D fill:#C5E1A5,stroke:#333,stroke-width:1px,color:#333;

    style E fill:#FFCCBC,stroke:#333,stroke-width:2px,color:#333;
    style F fill:#B3E5FC,stroke:#333,stroke-width:1px,color:#333;
    style G fill:#90CAF9,stroke:#333,stroke-width:2px,color:#333;

    style H fill:#EF9A9A,stroke:#333,stroke-width:2px,color:#333;
    style I fill:#FFCDD2,stroke:#333,stroke-width:1px,color:#333;
    style J fill:#FFCDD2,stroke:#333,stroke-width:1px,color:#333;
    style K fill:#FFCDD2,stroke:#333,stroke-width:1px,color:#333;
    style L fill:#FFCDD2,stroke:#333,stroke-width:1px,color:#333;
    style M fill:#E57373,stroke:#333,stroke-width:2px,color:#333;

    style N fill:#BBDEFB,stroke:#333,stroke-width:2px,color:#333;
    style O fill:#CFD8DC,stroke:#333,stroke-width:1px,color:#333;
    style P fill:#CFD8DC,stroke:#333,stroke-width:1px,color:#333;
    style Q fill:#CFD8DC,stroke:#333,stroke-width:1px,color:#333;

    style R fill:#FFEB3B,stroke:#333,stroke-width:3px,color:#333;
</Mermaid_Diagram>

Content:
 This is where it becomes clear that I don't actually know any physics. Well, I hope to convince you to the contrary. So thanks so much for a really wonderful talk that I had a list of questions that I was thinking of asking you and then I'm tempted to just throw them all out and just go off what you said because it was really interesting. But let me start with one thing that you mentioned over and over is this idea of correlations and correlation functions being useful for either the learning procedure or for understanding what's happening. And in most areas of physics, correlation functions are how we describe the universe. Even the particle collisions that will go on at the Large Hadron Collider, you can think of them as correlation functions, these things called quantum fields, or what Professor Kushner studies with climate science, if you have a time series of data and it's random, some of the information you can get out of it is how much is whether at this time correlated with whether at that time. I'm wondering, we have these enormous trillion parameter models now. There's lots of numbers inside those things. Can we use any of this intuition for kind of physics inspired correlation functions to tease out any of the meaning or interpretability of what's happening inside those large networks? Probably, but I don't know how to do it. That is, many people say we never going to be able to trust these big neural networks until we understand how they're working. I think we may well never understand, indeed, how these big models are working. I mean, we program them, so we know roughly the architecture of the net, but how they work depends on what they learn from data. And when something with a trillion real value parameters makes a decision, it may be that there is no simple explanation of why I did that and the values of those trillion parameters. Now, I should say most people in the field think we can do better than that, but I'm not convinced we can do much better. So let me also ask, you know, you mentioned this idea of the restricted Boltzmann machine is like an enzyme that catalyzed some advancement in deep learning. And my understanding is there have been several of those physics inspired advances that have managed to do something pretty amazing. I think one example is this idea of scaling laws that as you increase the size of a network or you give it more and more computing time, it does better and better, but it does better and better in a particular way that physicists would call a power law and that if you make a certain kind of plot, it looks like a straight line. And my understanding is that that idea was inspired by that kind of behavior showing up all the time in physical systems, in condensed matter systems, in high energy systems. And the fact that this has happened, there are other examples. Several times suggest to me this is not an accident. Well physicists have thought a lot about things, right? And they're very smart. So like a lot of mathematics came from people thinking about physics. True. I think probably some of the new mathematics came from trying to figure out why the moon didn't fall into the earth and things like that. So it's not surprising that physicists are among the first people to come across fancy mathematical techniques. Well let me ask, where do you think the physics perspective can be most useful in understanding deep learning and maybe to contrast it with the perspective of say psychology or biology or traditional computer science? I wish I could answer that. So Ganguly at Stanford is an ex-physicist and has done very nice work on showing what's happening in the weights as models learn. So I mean my belief is insights from physicists are going to be useful. But let me be a politician and just change the subjects slightly and tell you a piece of physics that really is very useful. So people in machine learning use variational methods which come from physics. And I want to try and describe for the general public how you might use a variational method or how you get an insight from a variational method to understand how you could get efficient communication between models. So let's suppose you have two different ways of encoding exactly the same event and you want to communicate this. We have a sender and we have a receiver and the sender wants to communicate this event to the receiver using as few bits as possible, sending as few bits across the channel as possible. And so let's suppose you want to tell someone it's raining and being Canadian you can say it's raining or ill three. I don't speak French but I think that's right. I'm not a real Canadian. So a normal person would say well if it takes sort of 10 bits to say it's raining it takes 10 bits to say ill blue what you should do is just pick one and do it and it's going to take you 10 bits. But a physicist would say okay you've got two different ways of coding the same thing. What if we stochastically picked one of those ways? So we flip a coin and we pick one way. Now it looks like you don't win anything by doing that because you flipped your coin and you either say ill blue or you say it's raining and you still have to send 10 bits. But suppose that instead of flipping a coin what we do is we run a little random number generator to get our random bit and suppose that the receiver has the same random number generator. So now we're going to run the random number generator and decide whether to say it's raining or ill blue. The receiver can also run that random number generator and see which random bit we used and so actually we can communicate the random bit that we use for picking between these two codes. And so the receiver can actually get 11 bits of information. You can get the 10 bits of information that tell them whether it's raining or not and you can get another bit of information which might just be about a random number generator but instead of using a random number generator we can just take some other message we want to communicate. Suppose you're doing consulting on the side and you want to do something else at the same time you can communicate this other message one bit at a time by your choice between whether you say ill blue or it's raining. So it turns out that a stochastic choice is actually the right thing to do and if you look at a free energy function then the number of bits it takes to send the code is like the energy and the probabilities you're assigned to code is like the entropy and the best thing to do is use a Boltzmann distribution. So if you've got a code that's a bit longer than another code use that less often but you do occasionally use it and so what happens is results from statistical physics show up encoding theory like this if you've got alternative codes. It's actually better to use the code stochastic than to just pick the cheapest one. That's a little bit of physics showing up in. Which is you know it's a nice example of the kind of the confluence of fields you know the work that Shannon did on entropy has now found its way all into quantum information and the way that people think about information and physical systems. I want to riff a little bit on the subject of cooperation and message passing. So physics has this very long tradition of enormous worldwide collaborative projects of which CERN the European Organization for Nuclear Research is one of them. The history is really amazing you know kind of founded to induce cooperation after World War II but managed to do amazing things like discover the Higgs Boson which won the Nobel Prize a couple years ago and by putting a lot of very very smart people on a similar task was able to make amazing progress. It feels like we might need something like a CERN for AI nowadays. What would such a thing look like? How would people contribute to it? How might it be the organizing and what would it do? Given the current political situation it would clearly be in Europe. There's a big problem with it. So right now people are organizing a petition to say we should have a CERN for AI and I'm trying to decide whether I want to sign on to that. The main problem is AI is going to be useful for lethal autonomous weapons and all the countries that sell arms like the US and Russia and China and Israel and Britain are all going to want to develop their own lethal autonomous weapons and they're not going to want to collaborate. It's also good for cryptographic attacks and things like that. Cyber attacks. So I think there's a lot of reasons why it's going to be much harder to get countries to collaborate. Like imagine you try to set up CERN at a point where people thought okay we've got the atom bomb, we've got the H bomb and then there's some other kind of bomb we haven't quite got yet. Let's all collaborate to explore it. It wouldn't have worked like that. They had to believe that we got the H bomb now and doing this high energy physics probably isn't going to give us anything worse than that so we can collaborate. I think it will be a very good idea if we can make it happen but I think there's going to be a lot of political reasons why governments don't want to do it. Maybe we'll just ask one more question and then we'll get to some questions from the audience. The analogies between deep learning and big world changing weapons like the nuclear bombs, there's a lot of really interesting resonances there and I think one of them is the fact that the underlying physics that governs how atom bombs work is stuff that I teach in my graduate particle physics classes because it is part of how the world is described at the fundamental level but of course if you take that with the quantity of some material you can dig up somewhere and make something that is you know unimaginably destructive and I wonder how you think we should be balancing these desires for understanding this tool that is AI is going to do amazing things for good certainly in addition to all the things that it could do to make the world worse but how do we balance the understanding with the need for safety. Okay so there's one thing I think quite strongly which is you wouldn't make it so you could buy on the web fissile material because with nuclear weapons the difficult bit is getting the fissile material I mean it's all difficult but that's the most difficult bit that's very expensive and if you want to stop people having them you bomb the facilities and make the fissile material so you'd be crazy to release the fissile material so because maybe researchers say well I'd like some fissile material so I can play with bombs oh well we should give them it's unfair not to give it to researchers in universities when countries have it but it'd be crazy and meta started it but they now release the weights of these large foundation models and the point is the main constraint on what you can do with AI is that it takes a lot of money and a lot of data to train a large foundation model. Once you've got the weights of the model you can then use that model for doing all sorts of other things with not much training so I think they were completely crazy to make the weights public. People talked about it like it was open sourcing but it's not like open sourcing at all in open sourcing you release code and people look at that code and say wait a minute that line is a bit suspicious when you open source weights they don't look at it and say oh look this weights are a bit suspicious they just take those weights and they train them to do something else but they're starting from a very good starting point so that boat is sailed already but it was crazy to release the weights of these big models because that means any cyber criminal who can get his hands on a few hundred thousand dollars can retrain one of them. Sorry that's bad news right? Given that this stuff is already out there in the world do you think there are things that we can learn from it that will help us prevent some catastrophe that will come from the larger models that are trained after it? I'm quite pessimistic so my friend Jan Laker who works for meta is very optimistic and it turns out your personality has a big effect on what you predict the future is going to be like and it turns out I'm right so psychological research shows that if you take normal healthy people normal cheerful healthy Americans and you ask them to predict what's the chance that in the next five years you will remember if your immediate family will be seriously injured in a car accident. I don't know the action I can't remember the numbers but they'll say things like one percent and actually it's like ten percent and you ask them what's the chance that you or a member of your immediate family will get cancer in the next five years and again they're much too low they just underestimate the probability of these awful things happening. If you take paranormal schizophrenics they overestimate if you take mildly depressed people they get it just right. So maybe we have well it would be great to continue the conversation with some questions for the audience so yeah what do you go ahead and yeah. Really nice conversation. I don't know if we have a mic runner or not if not oh we do okay so questions from the audience and I see a hand raised in the back there. So all Dr. Hinton wonderful talk not only for the experts but also for the general public and would like to ask you specifically my name is Zim Rizvi there is a book also come and over recently Genesis Mr. Ksenja was also one of the authors and then other owners of. When you put your hand like that you put the mic the other way and I can't hear what you're saying. And there's a mention of an ethos based future development of AI where they talk about the human ethos being built as part of the whole regulatory framework and understanding it that you alluded to globally a corporation that need to be in place before we pursue the artificial intelligence to get to a point where the AGI is achieved or we go beyond that. We'd like to kind of know about like once there is a new tech comes there's always this debate about regulation and when that regulation is required and when it's too early about it and or we too late already for the artificial intelligence your answer to that will be appreciated. Okay so the first point is we're not going to be able to slow it down because it's got too many very good uses. I mean more or less all industries can make use of AI it's going to be very useful in healthcare it's going to be very useful in education it's going to be very useful in designing new materials which may be very helpful for things like climate change. So we're not going to be able to slow it down the question is are we going to be able to develop it safely and there seems to be not much political will to do that. People are willing to talk about things like discrimination and bias things they understand but most people still haven't understood that these things really do understand what they're saying we're producing these alien intelligences. For now we're in control but we're making them into agents so they get to do things in the world and they're very soon going to realize that a good thing to do to achieve your goals is to get more control and so I'm very worried that we're in a situation now where we'd like really strong sensible governments with intelligent thoughtful people running them and that's not what we've got. Okay go ahead yeah. Hi Jeff how are you? My question is about power laws and so there's been this theory or idea that as we scale more compute and more data we're going to have more performance models. I think Sam Altman mentioned that it's correlated with the log of the compute in the data but ever since the release of GPT-4 we've seen that every single update of LLMs has been unjurewhelming to a bit like we haven't seen the big jump that we saw from maybe GPT-3.5 to GPT-4 and most recently with the GROG 3 release there was a lot of hype about the amount of compute that went into it with yellow musk and xai but it was also not like that great model. I'm wondering if you think that we've sort of plateaued with regards to compute and data and that we need a different approach. I think it's quite reasonable to think we're getting diminishing returns from this scaling and basically you get a little bit better each time you double the amount of compute that's the log right so it's getting more and more expensive to get these small improvements. However scaling has taken us a long way we've got things that are really pretty smart now and new ideas are also going to take us further and new engineering tricks for making these more efficient going to take us further. So I'd be very surprised if it just plattos. People have been predicting for ages that it's just about to just plateau and it hasn't but it may be that we need more good ideas to make it keep developing. So if you look at Moore's Law for example, Moore's Law for as long as I've been almost as long as I've been alive and people are saying Moore's Law is about to come to an end and just computers just got faster and faster and faster and faster is it making smaller and smaller and smaller and then it got to the point where they were doing like the gigaflop and people said they're not going to get any faster it's the end of Moore's Law and then they went off at right angles and started getting more and more parallel and we got embedded GPUs and things and Moore's Law just kept on in fact even accelerated. So I think it's going to be like that I think scaling took us so far and scaling is maybe like making computers faster and faster but now making computers more and more parallel kept Moore's Law going for another 10 years or so maybe Moore and I think we're going to get new ideas that are going to make things work better. There's not so many smart people working on this but I can't believe there aren't good new ideas that are going to make it work better. Maybe one last question. Oh from the, can we actually, okay we'll have maybe in the back and final one to the front. Hi so the theory of active inference has sensory neurons running parallel to motor neurons and they're tightly coupled and you know the error signal in the sensory neurons activates a motor neuron to make some kind of correction which is, I mean that's about activations and not weights but I'm wondering is there something like back propagation possible here like if you have these actual parallel sets of neurons running is there something like that? Yes you can actually make a version of back propagation work in a brain. It just doesn't work as well as it does in a digital computer and the cerebellum is one place where people thought there may well be something like back propagation because you get this visual slip signal, you get a visual slip signal that can be used to train your vestibular apparatus which is much faster and so you can use visual slip to train vestibular and that's where you get the error signal for back propagation and people are speculated about whether something like back propagation goes on there but you can also do back propagation in a brain by using temporal differences. So back propagation initially seems very implausible because in the forward pass you're sending neural activations and in the backward pass you're sending neural sensitivities. The signal that's coming back is how sensitive is the error to a change in this neuron, a derivative. So that's completely different kind of information but you can use temporal derivatives to stand in for error derivatives. So you can have a system that has two passes and the difference between the activation in those two passes is your error derivative and the sum evidence the brain might be using temporal derivatives as error derivatives. So for example you have neurons that detect motion, you have neurons that detect location and then if you ask well how do you detect motion the obvious way to do it is to look at the difference in location over time but that's not how you do it at all. You have separate neurons that represent motion so why can't you use the difference of location neurons to represent motion and one possible answer is because you're using those temporal differences to represent error derivatives. So there's a whole paper by me and Yoshua Benjia and other people. I don't think Yoshua is an author of that. There's a paper by me and various people on using temporal differences as error derivatives and showing how you can implement a version of back propagation in a brain but the point is it doesn't work very well when you're trying to pack a lot of information into a few connections. It won't get information into bottlenecks. If you have big big generous neural networks with plenty of spectra, these techniques will work and will allow you to do something like back propagation but not as efficiently as in the digital computer. So it may be that the brain has ways of doing stuff like back propagation by using temporal differences but nobody really knows and it's been somewhat disappointing the attempts to understand what the brain is up to in perceptual learning. I don't think we've got there yet. Question from the Martin family. Thank you so much for the fantastic lecture. It was incredibly interesting. Just, I have so many questions. It's hard to choose one but we were just looking at image generation, not image recognition. I was wondering if there was some sort of relationship between the wake state and the sleep state. Like is image generation where the brain system in the wake state is generating the image based off of what it's learned? So I should emphasize that both machines were a nice idea about how you get a learning signal that used simple relationships that you get at thermal equilibrium is physical physics but I don't think there was actually going on unfortunately and they're not particularly good engineering. So I think it may well be that something like sleep-on learning is going on but not the most. Well we've come to the end of the evening so I really would love to thank Dr. Hinton again and Dr. Khan for a wonderful fireside chat and of course for the amazing lecture we heard in the first hour. So let's thank them again.
