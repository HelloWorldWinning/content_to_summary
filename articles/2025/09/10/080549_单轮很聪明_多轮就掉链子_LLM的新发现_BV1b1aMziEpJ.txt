Timestamp: 2025-09-10T08:05:49.076130
Title: 单轮很聪明，多轮就掉链子：LLM的新发现 BV1b1aMziEpJ
URL: https://b23.tv/zcJ1Aco
Status: success
Duration: 6:25

Description:
以下是对文本内容的总结：

### 大模型在多人对话中迷路：核心问题与系统性评估

#### **一、 引言**
*   **研究背景：** 大模型（如ChatGPT、Claude）虽专为对话设计，但在多人、信息逐步完善的对话场景中表现显著下降。
*   **现有评测不足：** 大多数评测集中于单人、指令明确的场景，或将多任务分解为独立子任务再拼合（"AppleSort"），未能真实考量模型整合分散信息的能力。

#### **二、 核心问题定义**
*   **现象：** 大模型在信息不完全说明（Under-specification）的多人对话中，性能普遍下降，且一旦偏离轨道（“get lost”）便难以纠正。
*   **命名：** "LLMs get lost in multi-turn composition"（大模型在多轮组合中迷失）。

#### **三、 研究方法**
*   **评估框架：** 提出"碎片模拟"（Shards Simulation）方法。
    *   将完整指令拆解为多个信息"碎片"（Shards）。
    *   不同"参与者"逐步透露部分信息，模拟真实世界的多人、渐进式对话。
    *   **示例：** 将“雪球战任务”拆分为多轮逐步提供目标、能力和环境信息。
*   **任务类型：** 涵盖6类常见任务（代码生成、数据分析、API调用、数学、表格转文字、长文总结）。
*   **评测指标：**
    1.  **平均表现：** 总体正确率或分数。
    2.  **Top-2 表现：** 最好情况下的能力（60分位成绩），反映模型潜力。
    3.  **不可靠性（UnReliability）：** 最好与最差情况的差距，反映稳定性。

#### **四、 主要发现**
1.  **性能骤降：** 单人完整指令下正确率超90%，多人不完全说明场景下直降至约65%（平均下降39%）。
2.  **能力保留，可靠性缺失：** Top-2表现仅下降约15%，表明模型能力并未大幅减弱；但不可靠性翻倍增加（>100%），达11%以上，说明模型极不稳定，易“迷路”且无法纠正。
3.  **普适性问题：** 即使是最强的GPT-4、Gemini、Claude等大模型，性能下降幅度也与小型模型相似。
4.  **原因分析（通过模拟日志）：**
    *   过早下结论，在信息不足时急于给出完整答案。
    *   依赖错误的早期假设，导致后续连锁错误。
    *   遗忘中间信息。
    *   输出冗长、内容过于啰嗦。
    *   过度假设，导致自身混乱。

#### **五、 补救方法及启示**
*   **Concat (完整指令拼接)：** 将所有信息碎片重新拼接成一条完整指令给模型。
    *   **结果：** 性能几乎与单人场景一样好。
    *   **启示：** 性能下降主要源于信息分散和多轮交互，而非模型本身能力不足。
*   **Recap (最后一轮重述)：** 在最后一轮对话中，将之前的所有碎片重新展示给模型。
    *   **结果：** 有帮助，但现实中难以预判“最后一轮”。
*   **Snowball (滚动式重复)：** 每轮对话都重复之前所有的碎片信息。
    *   **结果：** 性能提升15-20%，但仍远不及单人场景。
    *   **启示：** 简单的信息拼接或重复补救有限，根本在于模型需要增强内在的多人交互鲁棒性。

#### **六、 实践建议**
*   **对系统开发者：**
    *   不能只关注模型的单人能力，必须评估其在多人对话场景中的可靠性。
    *   单纯降低温度等参数无助于解决多人的不可靠性问题。
    *   未来模型设计需同时优化能力（Ability）和可靠性（Reliability）。
*   **对用户：**
    *   若发现模型“跑偏”，应直接开启新的对话。
    *   可让模型先总结当前需求，然后用总结后的完整需求重新开始新对话。

#### **七、 论文贡献**
1.  提出了“碎片模拟”框架，能够系统性评估大模型在多人、信息不完全说明场景下的表现。
2.  通过大量实验证明大模型在该场景下普遍存在“迷路”现象，平均性能下降39%。
3.  明确指出核心问题在于模型的可靠性（Reliability），而非其能力（Ability）。

---

**核心观点 (Core Point)：**
大模型在多人对话中迷失的核心问题在于其可靠性而非能力，亟需通过系统性评估和内在机制优化来提升其多轮交互的鲁棒性。

**Overarching Framework (总框架):**
大模型在多人对话中的“迷失”现象：评估框架、问题诊断与解决方案探讨。

<Mermaid_Diagram>
graph TD
    subgraph "大模型多人对话挑战 LLM Multi-Turn Dialogue Challenge"
        A("大模型多人对话挑战") -- "导致" --> B("性能下降与迷失");
        B -- "表现为" --> F("平均性能大幅下降");
        B -- "表现为" --> G("不可靠性急剧增加");
        B -- "但仍保留" --> H("Top-2能力保留");
        B -- "且是" --> I("普遍性问题");
    end

    subgraph "问题诊断 Problem Diagnosis"
        B -- "核心在于" --> C("可靠性缺失");
        C -- "导致行为包括" --> J("过早下结论");
        C -- "导致行为包括" --> K("依赖错误假设");
        C -- "导致行为包括" --> L("遗忘中间信息");
        C -- "导致行为包括" --> M("输出冗余/混乱");
    end

    subgraph "评估框架 Evaluation Framework"
        N("现有评测不足") -- "促使" --> O("碎片模拟框架");
        O -- "特点1" --> P("信息不完整性");
        O -- "特点2" --> Q("多人逐步输入");
        O -- "使用指标" --> F;
        O -- "使用指标" --> G;
        O -- "使用指标" --> H;
    end

    subgraph "解决方案与建议 Solutions & Recommendations"
        R("研究补救方法") --> S("Concat (完整指令)");
        R --> T("Snowball (重复历史)");
        S -- "结果" --> U("性能几乎恢复");
        T -- "结果" --> V("有限提升");

        W("实践建议");
        W -- "针对开发者" --> X("评估多人可靠性");
        W -- "针对开发者" --> Y("优化能力与可靠性");
        W -- "针对用户" --> Z("开启新对话");
        W -- "针对用户" --> AA("总结需求重述");
    end

    F -- "由...验证" --> O;
    G -- "由...验证" --> O;
    H -- "由...验证" --> O;
    U -- "揭示" --> C;
    V -- "揭示" --> C;
    C -- "需要解决" --> X;
    C -- "需要解决" --> Y;

    style A fill:#FFDDC1,stroke:#E67E22,stroke-width:2px,color:#333;
    style B fill:#FFCCCC,stroke:#FF0000,stroke-width:2px,color:#333;
    style C fill:#FF9999,stroke:#FF0000,stroke-width:2px,color:#333;

    style F fill:#F0F8FF,stroke:#ADD8E6,stroke-width:1px,color:#333;
    style G fill:#F0F8FF,stroke:#ADD8E6,stroke-width:1px,color:#333;
    style H fill:#F0F8FF,stroke:#ADD8E6,stroke-width:1px,color:#333;
    style I fill:#F0F8FF,stroke:#ADD8E6,stroke-width:1px,color:#333;

    style J fill:#FFEBCD,stroke:#CD853F,stroke-width:1px,color:#333;
    style K fill:#FFEBCD,stroke:#CD853F,stroke-width:1px,color:#333;
    style L fill:#FFEBCD,stroke:#CD853F,stroke-width:1px,color:#333;
    style M fill:#FFEBCD,stroke:#CD853F,stroke-width:1px,color:#333;

    style N fill:#DCDCDC,stroke:#696969,stroke-width:1px,color:#333;
    style O fill:#C1FFC1,stroke:#008000,stroke-width:2px,color:#333;
    style P fill:#E0FFFF,stroke:#4682B4,stroke-width:1px,color:#333;
    style Q fill:#E0FFFF,stroke:#4682B4,stroke-width:1px,color:#333;

    style R fill:#FFCC99,stroke:#FFA500,stroke-width:1px,color:#333;
    style S fill:#FFFACD,stroke:#DAA520,stroke-width:1px,color:#333;
    style T fill:#FFFACD,stroke:#DAA520,stroke-width:1px,color:#333;
    style U fill:#90EE90,stroke:#3CB371,stroke-width:1px,color:#333;
    style V fill:#FFD700,stroke:#FFA500,stroke-width:1px,color:#333;

    style W fill:#ADD8E6,stroke:#4682B4,stroke-width:2px,color:#333;
    style X fill:#B0E0E6,stroke:#6A5ACD,stroke-width:1px,color:#333;
    style Y fill:#B0E0E6,stroke:#6A5ACD,stroke-width:1px,color:#333;
    style Z fill:#DDA0DD,stroke:#8A2BE2,stroke-width:1px,color:#333;
    style AA fill:#DDA0DD,stroke:#8A2BE2,stroke-width:1px,color:#333;
</Mermaid_Diagram>

Content:
大家好今天我们来聊一篇认为名字叫做LLMS get lost in multi-ton composition那只一过来就是大模型在多人对话中迷路了这篇认为主要研究了大远模型在多人信息逐步机思的对话中为什么表现会明显的下降我们知道像CHA的GBT Cloudy 节目里这些大模型本账的就是一个对话借口现实中的用户不可能一次性地把需求讲清楚往往是先要给一个大概在逐步的补充说明但是大多数的评测都还是在担任这个指令完全明确的场景下进行的这就跟实际使用的差距就很大那做者发现在多人不完全说明的对话里面模型的性能会普遍下降他们把这个现象叫做LLMS get lost in multi-ton composition就是模型一旦走边就很难够纠正回来之前的研究比如MT-Bans,Botchart,Mastchart等等都尝试过评测多人能力但大多数采用都是这个AppleSort的任务也就是把每个人可以单独看出一个指任务然后再拼起来这种方法其实是高过了模型的多人表现因为他没有真的考量到模型能不能把理散的这个信息拼在一起这边认为强调的是一个真实的情况又会一开始可能讲的不清楚就是所谓的Anders specification就是信息不足做者要解决的就是想看看在这种场景下然后进行系统性的评估模型那做者提出了一个很有意思的一个办法就是把原本担任完整的一个指令拆成多个的碎片所谓的Shouts没有人只透露一部分的信息让模型逐步拼出做完整的需求主要是一种原来的问题是比如接着学求每小时做20个但是每十五分钟融化两个他要做到60个需要多久采分之后地人只说接要打雪帐要准备学求第二人再说他一小时能做20个第三人才告诉他目标是60个最后才接触会融化这样模型就必须去跨人去整合信息这种Shouts simulation能够严格的控制实验同时能也能够模拟真实的对话的场景他们选了6类常见的任务比如说覆盖代码生成数据固杀学API的调用数学籍表格转文字长文当总结每人都是从单独的成熟的单人的数据集中挑了100个左右的问题然后用分辨法去改造成多了在评测的指标上做的用了3个1是平均表现就是总体的正确力或者分数第二就是App2的就是60分位的成绩也就是说明最好情况下的能力第三个就是On ReliabilityU最好和最差的情况的差距反映稳定性这个结果是非常一致的单人完整的指领下很多模型能达到90%以上的正确力一旦换成多人不完全的说明性能直接掉到65%左右平均下降39%App2的下降只有15%左右但是不可靠性翻倍增加达到11%以上100%以上换句话说模型本身能力没有掉多少但是他们在多人的表现非常不稳定一旦换成就会一路出啊去没有办法就正更有一事是就连最强的GBTFullPoneWattGimiliandandandandpro掉的跟小模型一样多所以这个问题是普遍性的那么为什么会这样子呢做着分析了这个模拟的日子发现几个主要的原因第一就是过早的下结论再还没有拿到足够的信息情况下就急着给完整的这个达当时依靠错误的救的错误一旦早期假设错了后期还会不断的再错的机子上去修改然后中间会以及忘这个中间的信息直接开锣跟纪委忽视了中间的这个结论回答得非常的容常输出太萝卜加在假设导致自己反而混乱这些都加聚了走错路就回不来的这个问题路红里面也测试了一些补救的方法比如说说叫做ConCatConCat in the Name就是把错误碎片重新拼成一条完整的指令给模型结果证明性能几乎跟单人一样好这就说明性能下降确实是因为多人和信息分散还有就是Recapable把最后一轮把所有的碎片重新付碎片让模型再回答一次有帮助但是在现实中又或没有办法预指最后一轮对不对还有就Snowboard就是每人都把之前的碎片重新的重付一片就像滚血这样滚血球一样结果能够提升15%到20%但还是比不上单人这个表明用简单的框下来补救是有限的最根本还是模型要自己增加多人的这个逻报性所以我们了解这个过后对系统开发者来看不能够只看模型的单人能力必须评估多人的这个电话的可靠性仅仅调低温度比如说SatT2之约的也是没有用的多人的这个不缺的性不会消失未来的模型要同时优化这个能力就是Appity的和可靠性Reliability对用户也如果发现模型左偏别再应拉回来直接开始开一个新的对话这样也可以让模型帮你总结向模型的需求然后再重新开对话把需求完整的需求一次性的需弱这个游戏像在我们在鞋带马最新复杂的风险的时候都是这种样子的发现左偏了就直接把Problem重新写来过开一个窗口重新搞这这篇认文的这个贡献大概分为三点第一呢提出了这个分辨这个模型所谓下跌的声明有流行能够系统的评估多人不完全的说明这个场景而且大量的实验能够证明大模型会迷路性能平均会掉39%核心问题在于可靠性而非这个能力所以对于模型设计了者来说还是我们普通的这个中端用过来讲一定比较意识到多人等于高风险要通过这个框架荣誉来不救要么等到未来的模型原来生存来解决好我们今天的分享就到这里关注我或许更多的有趣的AI支持和工具我们下期视频再见
