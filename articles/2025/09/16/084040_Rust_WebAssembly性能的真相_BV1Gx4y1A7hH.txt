Timestamp: 2025-09-16T08:40:40.068536
Title: Rust WebAssembly性能的真相 BV1Gx4y1A7hH
URL: https://b23.tv/TNCLqlJ
Status: success
Duration: 29:47

Description:
**标题: Rust与WebAssembly前端渲染性能的真相**

**核心结论:**
Rust和WebAssembly前端框架的渲染性能现已足够强大，足以超越或媲美主流JavaScript框架，彻底颠覆了其“DOM渲染性能不足”的传统误解。

**Overarching Framework (核心框架):**
Rust与WebAssembly前端渲染性能评估与误解澄清

---

**总结大纲：**

**引言**
*   **主题：** 探讨Rust和WebAssembly（Wasm）在浏览器端渲染用户界面时的实际性能表现。
*   **范围限定：** 专注于浏览器内部渲染性能，不涉及Wasm边缘运行时等其他场景。

**一、关于Wasm性能的常见误区与真相**

*   **误区一：Wasm性能惊人，Rust比JavaScript（JS）快得多，将接管世界，并直接渲染到Canvas。**
    *   **真相：** JS通过即时编译（JIT）和优化同样能达到极高速度。Wasm在某些任务上可与JS媲美或更快，但对于浏览器渲染，瓶颈通常在于浏览器本身的渲染能力，而非Wasm与JS的执行速度差异。Wasm并不会带来5倍的通用性能提升。
*   **误区二：Wasm太慢，前端开发应继续使用JavaScript。**
    *   **真相：** 实际基准测试显示，许多Rust/Wasm前端框架（如Leptos、Dioxus）的渲染速度已经超越或至少媲美大多数主流JS框架（如Svelte、Vue、React）。Wasm的性能对于浏览器DOM渲染来说已完全足够。
*   **误区三（半真半假）：Wasm一旦能直接访问DOM，性能问题就能解决。**
    *   **真相：** 直接DOM访问会有一定帮助，但目前Wasm与JS的性能差距已非常小。主要的性能瓶颈并非DOM API调用路径，而是Wasm与JS之间字符串数据（从UTF-8到UTF-16）的复制和重新编码成本。

**二、JS框架基准测试（JS Framework Benchmark）分析**

*   **基准测试内容：** 衡量纯粹的DOM渲染速度，涵盖创建、更新、删除、选择、交换行等多种操作，作为框架的压力测试。
*   **总体表现：**
    *   **Rust/Wasm与Vanilla JS对比：** WasmBindGen（Rust参考实现）通常比Vanilla JS（原生JS参考实现）慢2-4%。
    *   **框架速度排序（从快到慢，部分代表性框架）：** Solid (JS) > Leptos (Rust) ≈ Dioxus (Rust) > Vue (JS) > Sycamore (Rust) > Svelte (JS) > You (Rust) ≈ React (JS)。
    *   **核心发现：** 框架所采用的架构和方法论（如细粒度响应式 vs. 虚拟DOM）对渲染性能的影响，远大于编程语言本身（Rust vs. JavaScript）的影响。

**三、Rust WebAssembly框架内部性能差异分析**

*   **字符串互操作成本：** Rust（UTF-8）与JavaScript（UTF-16）的字符串编码差异导致Wasm向JS传递字符串时需要额外的字节复制和重新编码，这在大量DOM元素创建时尤其明显。
*   **Dioxus (结合Sledgehammer)：**
    *   **Sledgehammer：** 一个低层库，专门针对Wasm-JS字符串传递和DOM元素创建进行了深度优化（例如，避免以字符串形式传递元素名称，使用特殊编码）。
    *   **编译器优化：** Dioxus利用Rust的编译时优势，区分模板中的“静态”和“动态”部分。更新时仅对动态部分进行差异比较，显著减少了计算量，从而实现出色的创建和更新速度。
*   **You (虚拟DOM框架)：**
    *   **机制：** 每次数据更新时，都会重新运行组件，生成一个全新的虚拟DOM树，然后与上一个虚拟DOM树进行全面递归差异比较，最后才更新实际DOM。
    *   **局限性：** 即使是微小的DOM变化，也需要进行大量的虚拟DOM树遍历和比较，导致额外的开销，使其在更新速度上相对较慢。
*   **Leptos/Sycamore (细粒度响应式框架)：**
    *   **机制：** 组件本身不会“重新运行”。当响应式状态发生变化时，只有受影响的最小代码逻辑块被执行，直接定位并更新DOM中对应的节点。
    *   **优势：** 这种机制避免了虚拟DOM的额外开销，实现了非常高效和细粒度的DOM更新。Leptos还利用模板节点克隆来进一步提升创建速度。

**四、其他性能考量**

*   **包大小（Kilobyte Weight）：**
    *   Wasm二进制文件通常比纯JS的小型框架（如Svelte、Solid）大。
    *   然而，大型JS框架（如React）的包大小可能与某些Wasm框架相当或更大。
    *   **挑战：** 当前Wasm生态系统在代码分割方面尚不如JS成熟，应用通常以一个较大的Wasm文件传输。
*   **启动时间（Time to Interactive）：**
    *   对于小型应用，许多Wasm框架的交互时间与JS框架持平，甚至更快。此时，页面的加载速度往往受限于CSS等外部资源的下载和渲染，而非Wasm或JS代码本身的执行（存在性能下限）。
    *   Wasm支持流式编译，即使是较大的Wasm包，也能在下载的同时进行编译，其加载成本已变得非常可控。一旦Wasm包加载完成，后续不再需要额外的网络资源请求。
*   **内存分配：**
    *   由于Wasm内存管理方式（一次性申请大块内存）与JS垃圾回收机制不同，Wasm框架在报告的内存使用量上通常更高。
    *   但在Wasm框架内部，Leptos和Dioxus等高效框架比You等虚拟DOM框架显示出更低的实际内存使用量。React的内存使用模式有时与Wasm框架有相似之处。

---
<Mermaid_Diagram>
graph TD
    A["Rust与WebAssembly前端性能的真相"] --> B(("评估核心：浏览器渲染性能"))

    subgraph "一、Wasm性能常见误区与真相"
        M1["误区1: Wasm性能惊人，将取代JS"] --> T1["真相1: JS与Wasm在JIT下可同样快，浏览器渲染能力是瓶颈，无5倍提升"]
        M2["误区2: Wasm太慢，应选用JS"] --> T2["真相2: 许多Rust/Wasm框架已超越或媲美JS框架，DOM渲染速度足够快"]
        M3["误区3: Wasm需直接DOM访问才能快"] --> T3["真相3: 直接DOM访问帮助小，主要瓶颈是字符串复制/UTF-8到UTF-16重编码成本"]
    end

    A --> M1
    A --> M2
    A --> M3

    B --> P1["性能基准测试：JS Framework Benchmark"]
    B --> P2["加载性能：启动时间 (Time to Interactive)"]
    B --> P3["资源消耗：包大小 (Kilobyte Weight)"]
    B --> P4["资源消耗：内存分配"]

    subgraph "二、基准测试结果与分析"
        P1 --> F1["参考基准: Vanilla JS (原生JS)"]
        P1 --> F2["参考基准: WasmBindGen (原生Rust/Wasm)"]
        P1 --> F3["JS高性能框架: Solid, Svelte"]
        P1 --> F4["Rust高性能框架: Leptos, Dioxus, Sycamore"]
        P1 --> F5["JS主流框架: Vue, React"]
        P1 --> F6["Rust主流框架: You"]

        F1 --- F2
        F3 -- "通常更快" --> F4
        F4 -- "通常更快" --> F5
        F5 --- F6 -- "通常更慢" --> F4

        F3 & F4 & F5 & F6 --> C1["核心发现: 框架方法论 > 编程语言"]
        C1 --> M2_T2_Reinforce{{"强化真相2：Wasm速度充足"}}
    end

    subgraph "三、Rust/Wasm框架内部技术差异"
        K1["关键瓶颈: Wasm-JS字符串互操作 (UTF-8 to UTF-16)"] --> D1["解决方案: Sledgehammer (Dioxus底层库)"]
        D1 --> D2["优化: 元素创建、特殊编码、减少字符串传递"]
        D1 --> F_Dioxus_Fast["Dioxus 优势: 创建与更新速度快"]

        V1["虚拟DOM (V-DOM) 方法: You, React"] --> V2["机制: 完整组件重渲染 -> V-DOM树对比 -> 差异更新"]
        V2 --> V3["V-DOM局限: 即使小改动，也需遍历整个树，开销大"]
        V3 --> F_You_Slow["You 劣势: 更新速度相对慢"]

        R1["细粒度响应式 (Fine-Grained Reactivity): Leptos, Sycamore"] --> R2["机制: 仅执行受影响的最小代码块 -> 直接更新DOM节点"]
        R2 --> R3["优势: 无需组件重渲染，更新效率最高"]
        R3 --> F_Leptos_Sycamore_Fast["Leptos/Sycamore 优势: 更新速度快，Leptos利用模板克隆提高创建"]

        C2["Dioxus创新: 编译器区分静态/动态模板部分"] --> C3["机制: 仅对比动态部分，静态部分直接复用"]
        C3

Content:
 Hey, this is Greg, creator of the Leptoast Web Framework for Rust. Today we're going to talk about the truth about Rust and WebAssembly Performance. Now to be clear, I'm only talking about WebAssembly Performance for rendering in the browser. I'm not talking about things like Wasm, Edge, Runtimes, and so on. I'm talking specifically about the performance of compiling a Rust application to WebAssembly and using it to render a user interface in your browser. But I often see in these conversations a few myths, so I thought it would be helpful to kind of walk through them and talk about the truth about WebAssembly Performance. Myth number one is that WebAssembly is amazing. Rust is much faster than JavaScript, so WebAssembly should be faster than JavaScript. And it will take over the world. In fact, you'll use WebAssembly to render directly into the canvas. We'll skip HTML, CSS, JavaScript altogether. There are a number of reasons I don't think this is the right approach for us to be taking. But the basic truth is JavaScript can be quite fast when it's just in time compiled and optimized by the VA engine or in the browser. It runs pretty well. WebAssembly can run just as fast, and there are certain tasks WebAssembly can do much faster. But in terms of rendering into the browser, the actual speed of WebAssembly versus JavaScript is not the main constraint. It really is the browser's ability to render something once you get to a certain point. So WebAssembly isn't going to have some 5x improvement over JavaScript. But myth number two is that WebAssembly is too slow. You should use JavaScript instead. I see this a lot in the Rust community where people tend to be pretty performance conscious. There's this narrative, I think, that sure, sure, sure, if you want a fast website, build the back end in Rust, build your APIs in Rust, but then use a fast JavaScript front end framework like maybe Svelte gets recommended a lot. Or if you're super performance conscious, Clued, and maybe solid.js. And this is kind of true, except you'll see when we dig into the actual benchmarks here that there are plenty of Rust and WebAssembly frameworks that are faster than almost all the JavaScript frameworks. So there is no longer really a constraint on WebAssembly performance in that way. WebAssembly is not too slow to use for rendering in the browser. There's a third kind of half myth, which is that WebAssembly will finally be fast enough once it's able to directly access the DOM. So you may know that right now WebAssembly can't directly call into browser APIs. It has to call out through JavaScript, and then JavaScript will call out to the DOM. There's a WasmBineGen that lets us do this without having to know about it, but it's there in the background. We can't directly do something like create a DOM element or set an attribute or set the contents of a text node from within WebAssembly. And so there's this perception that once we finally land one of the several proposals that will allow WebAssembly to directly access the DOM, then there will finally be good performance. And this one is partially true. It will probably help close the gap between WebAssembly and JavaScript. But that gap is really small right now. And the main constraint is not actually the ability to call DOM APIs from WebAssembly. It's actually the cost of copying strings from WebAssembly over into JavaScript. And we can talk a little bit more about why that is. So the truth about Rust and WebAssembly performance, if you want to stop watching the video right now, is that Rust and WebAssembly front end frameworks are fast enough right now. In fact, there are several of the newer and bigger ones that are faster than almost any JavaScript framework. So this whole argument about Wasm being too slow for DOM rendering simply doesn't hold water anymore. And I'm going to spend the rest of this video basically digging into some benchmarks to show you exactly why I say that is. We're going to look at the JS framework benchmark, which is a great benchmark just for pure rendering speed. And we're actually going to look through the Rust code of the implementation of those in a few Rust frameworks to understand what's going on in each framework, why they are different from one another. So let's jump in. I'm going to spend a lot of this video in this JS framework benchmark result table. If you don't know this benchmark, it basically is a measure of raw rendering speed. So I'll show you what it actually is. It does things like create 1,000 rows. Clear 1,000 rows. Update the contents of every 10th row by adding some exclamation points. Deleting rows, right? Selecting rows and highlighting them, which is harder than you would think to do quickly. And then my favorite one is swapping rows. So if you pay attention to the second row here, it's being swapped with the second to last row. So it's basically a stress test for different frameworks. And there are a huge number of frameworks that have implemented this benchmark. I can't even fit them all on one screen, right? We're just going to look at some of the most popular JavaScript frameworks and some of the most popular Rust frameworks to try to understand some of the differences. But here's the top line, right? If you look at this results page, you can see that the narrative that JavaScript is faster than WebAssembly for rendering in the front end is simply not true, right? So we start out, Vanilla.js, this is just a plain, vanilla JavaScript reference implementation. WasmBinGen is the plain vanilla Rust and WebAssembly reference implementation. Sledgehammer is a new, very low-level library, which we'll talk about when we talk about Dioxys. But this is JS Rust Rust, right? You'll notice the scores here are the sort of total score. That's 1.02 for Vanilla.js, 1.04 for Sledgehammer, 1.06 for WasmBinGen. So Rust can be maybe 2% slower. It's typically about 4% slower than the JavaScript equivalent. But then if you just look at the order of the rest of the frameworks we've got here, and there are a bunch of additional very fast JavaScript frameworks that are very small that people have basically never heard of, but the fastest of the kind of big ones is solid. And then we've got Leptos and Dioxys tied to Rust frameworks. Then we've got View, another big JavaScript framework. We've got Sycamore, another Rust framework. Sfelt, another JavaScript framework. You, another Rust framework. And you was pretty much tied with React. Of course, a huge JavaScript framework. And I think I actually want to have React hooks 18 up there instead of Vanilla React 17. And here we go. So we've got React hooks slightly faster than you, whatever, right? But you can see basically there's this order from React, you, these sort of big classic virtual DOM-based libraries are typically the slowest. Some of the things that you would think of as being a fast JavaScript framework like Sfelt is actually tied with something that's quite a fast Rust framework like Sycamore. And then these really performance conscious frameworks in Rust like Leptos and Dioxys quite fast, and really just a few percentage points slower than solid. But importantly, note that, you know, Leptos and Dioxys are faster than View. Sycamore is faster than Sfelt. And View is basically equivalent to React, right? So it's more about the approach that these libraries take than the language that you're writing in. But so what makes for these differences just between the Rust web assembly frameworks? We'll dig into the code a little bit, but let me just untick all our JavaScript frameworks here so we can really look at it. And I'll take out Sledgehammer 2 for a second. We'll leave in the Vanilla one for reference. Okay, so there's some pretty big differences here between these different Rust frameworks. If I zoom in a little bit, you'll notice that in terms of creation speed, right, how quickly you can create a thousand rows, those are pretty widespread. WasmBynGen comes pretty close to Vanilla JavaScript, right? Leptos is a few points behind that, but Dioxys is really fast. And so the Dioxys in creating rows is pretty close to Vanilla JavaScript. In fact, if we add in Sledgehammer here again, which is the library Dioxys is using under the hood, Sledgehammer is nearly identical to Vanilla JavaScript in creating rows. This is really, really good. In terms of things like the partial update, you'll see that, which is that adding the exclamation points to the rows, you'll see some differences. This is an unusually good result for Dioxys in this run. I'm curious. But you'll see basically there's not that huge of a difference until you get to you, which is again doing that virtual DOM based diffing. In terms of selecting a row, again, Leptos is pretty close here. The others are kind of bunched, right, a little slower. It's interesting to me when we get to swapping rows, right? So if you check out, we'll add solid back in, which is quite fast. If you notice here, in terms of swapping two rows, which is sort of a classic, you have to diff that whole list, but you only make one or two calls out to the DOM, right? So it's a lot of computational work to understand which very targeted changes you should make. The WebAssembly frameworks do really well. You don't see that 5, 10, 15% slowdown. In fact, you've got Vanilla JS at 28, Sledgehammer at 28, Wazenbine Jnet 29. These ones, as reference implementations, they're a lot of cheat, so they don't actually do the computation. They just swap the rows. Solid's at like 30.5, Leptos down at 29, Dox is 30.7, Sycamore 30.5, U 30.5. So in other words, in these cases where you're doing most of the computation in WebAssembly and then just one or two calls out to the DOM, the rust, Wazen frameworks are as fast as solid, even U, right? Which overall is quite a bit slower. Even U is exactly as fast as solid on this benchmark. And Leptos is the fastest. Likewise, with clearing the rows, right? Solid's at 38, Leptos 34, Dox is 38, Sycamore 40. U is quite a bit slower on this one, fair enough. But on these benchmarks where you're doing most of the work inside WebAssembly, right, to make small changes to the DOM, the rust, Wazen frameworks are actually doing a great job. It's just those things, particularly creation, right, where you have to generate a whole bunch of strings and ship them into the DOM, right, where you see the real cost. You'll notice, right, the way this works is you have a bunch of strings, you create a thousand rows with random combinations of them, and there may be 20 or 30 characters long. So the way that string interop works is the real cost of WebAssembly here. JavaScript, for reasons that I don't understand, but maybe do the fact that it originates in the 90s, uses UTF-16 as its default string encoding. Rust, like most other languages, uses UTF-8. This means that Rust strings in WebAssembly are UTF-8 encoded strings, but to pass them through a JavaScript, they have to be actually not only byte-wise copied from WebAssembly into JavaScript, but then re-encoded from UTF-8 into UTF-16. We think about creating a thousand rows here, right, and each of these are maybe, I don't know, 20 characters. That's 20 kilobytes of strings you have to do, and the way that the benchmark works is it goes create clear, create clear, create clear, create clear, right, and over and over, it's copying all this data, re-encoding it, doing it back, which is just an added overhead for all of these rust WebAssembly libraries, except that Sledgehammer is this custom renderer that Dioxus created amazing work on this, which has a lot of optimizations around the cost of passing strings from WebAssembly over into JavaScript. A lot of optimizations around the cost of creating elements, so they actually have their own special encoding that doesn't pass element names as strings, right? They do everything they can to speed up that cost of passing stuff when you're creating it from WebAssembly over into JavaScript, and that's where they get these amazing results that are almost on par in terms of creating elements almost on par with something like vanilla JavaScript, almost on par with solid. They're really cutting down a lot of the cost of doing that creation. But then what about these differences between the Rust frameworks, right? Because we all, you know, we know and love Rust, right? So what if you're choosing between these frameworks? I'm going to dig in mostly to you, Dioxus and Laptos in this example. Sycamore and Laptos are working in very similar ways. The main difference is that Laptos is using template node cloning to create the elements, so you'll see kind of small improvements in the creation speed, particularly when you have a lot of rows, right? This 603 to the Laptos 511. That's the main difference here. If you look at the other benchmarks are pretty similar. The Selectro one is also different. This is just a different primitive that Laptos is using to drive a particular fine-grained reactive change when you do this, and it highlights each one of those in red. Laptos is using an approach that really only triggers the two that are changing and doesn't even check anything for the other thousand of them. But other than that, Laptos and Sycamore are very, very similar performance, very similar approaches. The difference is between you, Dioxus and Laptos, or you, Dioxus and Sycamore are much bigger. If you're interested in Sycamore, just replace Laptos in what I'm about to say in your head with Sycamore, and it'll mostly all be true. Let's look at some code here. I'm going to start with you. The first thing you'll notice is that this is divided up into two different components. You've got this, it's actually three components, I guess. You've got this Jumbotron, you've got this sort of app component. The Jumbotron, of course, is just the one that has all the buttons in it. They've got an app component that manages all the rows, right? And then they've got a row component. So this is the first important thing to know about virtual DOM frameworks. What happens when you make a change in a virtual DOM framework like this is that it reruns the component, it re-renders the component, it creates a tree of virtual DOM nodes, and then it diffs that against the last set of virtual DOM nodes. So when you're thinking about performance for V DOM frameworks, the smaller the component is, the less diffing you have to do for a particular update. So components have a lot of meaning, particularly with regard to performance, because you want to make basically your component as small as you can to encapsulate the meaning of something while also not rerunning the whole thing. So in other words, when I just do a partial update, when I just add the exclamation points, I don't want to create a new virtual DOM for the Jumbo-Tron, right? So you split out the row from the Jumbo-Tron so that when the row, the label changes, you add those exclamation points, only this row component reruns. And this is the same for you for dialysis, both virtual DOM frameworks, right? But so what actually happens when I press that button to add those exclamation points? What happens is that this, you know, you sends a message, updates its model, reruns this view function. And this view function actually generates a new tree. And then it dips it against the last tree, like I said. Now here's the problem. This is why V DOM frameworks have traditionally been slower than more fine-grained reactive frameworks like Leptos, like Sycamore, like Solid. When you diff those two trees, the framework will literally say, okay, I've got a TR here. I've got a table row. Does the old one have a table row? Yes, good. Okay. I've got the class. It's danger. Is the old one danger? You know, oh no, the old one was nothing. Let's add the danger thing, right? This is if you're doing the selector, I guess. No, the old one wasn't danger. The new one isn't danger. Great. Let's see. We've got a TD here, right? Do you have a TD? Yeah, I've got a TD. Okay, we've got a class. It's call medium one. Is yours call medium one? Yeah. Okay. So props.data.id is zero. Is yours zero? Yeah, mine's zero, too. Right? It literally goes through every node. And this isn't that slow, right? It's basically a bunch of partial equal checks. But it's not fast, right? It's a bunch of work that you do over and over and over again. And if you're doing that exclamation point, that partial update, right? We know, as the users, as the people watching this, that the only node that's actually going to change during that is this one, this label, right? So context.props.label. So the only thing in this entire tree that's going to change is that one node. But you still have to go through and diff the entire thing because the framework doesn't know that, right? It's just creating all that. So that's part of why you and React, which take almost the same approach, are on the slower end, right? Because they're doing a lot of additional work on every one of those updates that, honestly, we know at compile time, you don't need to do. This is part of the insight of what has made dioxics so fast in its new 0.3 release, right? Really great work. They've basically taken the insight that actually you know at compile time, right? We have a compiled language. You know what in that row is going to change. So this looks very similar. I mean, they have a different syntax for their view macro, right? But if you think about it, right, when you're compiling a macro like this, you see, okay, that's a TR. This can literally never be anything different from a TR. That's just the element name. It's never going to change. However, this class, that's a dynamic thing. That's a variable. That can change. This TD can't change. This class also can't change, right? This is a string literal, right? The value of column medium one can never change. Oh, but here's label.key. That's a format string. So this can change. So what dioxics actually does is it goes through and instead of just compiling into a kind of naive virtual DOM node like you, right, you is going to represent this internally as a struct with an element string and some attributes in a vector and some children in a vector. So dioxics will actually distinguish between the static parts and the dynamic parts of the template, which means that when dioxics does a partial update, right? Dioxics doesn't check to see if this TR is still a TR. It doesn't check if this TD is still a TD. It doesn't check if this class is still column medium one because it knows it can't change. It checks a few things. It checks whether it is in danger is still the same. It will check label.key, I guess. It will check maybe label.label. That's it. It's only diffing like three things, right? When you when you rerun this component, it does the same thing where it re-renders the component reruns, but because it distinguishes between static and dynamic parts, right? The diffing is those three really fine grain nodes. It's a super smart approach pioneered by a kind of experimental JavaScript library called Blockdom, I think, but it's very similar to the kind of fine grain to reactive approach, even though it's actually not that. So this is why dioxics is so fast on updates. If you look, in terms of the partial update results, right? Again, I think this is a particularly good run for them. Like the fact that this is the same as the vanilla JS suggests that there's some statistical noise in this always, right? But, you know, that's quite fast, right? Compared to use virtual DOM, right? Which is about 10 points slower than Sycamore than leftos. So you can see the advantages there, but you also get big advantages in terms of the creation speed, right? Because they can do this analysis, right? And then they know, well, we're always going to ship the static parts. And so we can just store that once in JavaScript and then clone it over and over again, and then just update these dynamic parts. And along with Slicehammer, that's a big part of what gets them there on the creation speed. We do a very similar thing where you create an HTML template element that's like literally a template string, clone that template element, clone its contents to create a bunch of elements and then fill in just the dynamic parts. And it's really cool that with a virtual DOM library, you can still do that. Even though, in theory, it has these virtual DOM nodes, you're doing diffing and stuff. But it's because they're using the compiler, taking advantage of the fact that Rust is already a compiled language to do that diffing. And then something like Leptos or Sycamore, right? You'll notice one big difference here is, right? We just have it all in one component. Because components never rerun in this framework, right? When I make a change to the label, it doesn't re-render my whole jumbotron here, right? There's this stuff never, ever, ever changes. It's just some buttons with event listeners. But it's not reactive at all. This never changes. This never changes. It's only this for loop, right? Which is just a keyed list implementation. So we have the keys, which are the row IDs. And then we do, you know, some diffing. When those things move around, we need to know, do I need to move this row or not? Do I need to remove this row? And then each of the rows is just rendered. It's a template. It gets cloned. And again, very similar to the dioxis approach here, right? We know this TR is always a TR. It never changes. We know the TD is always a TD. We know the class is always the same, right? These things just don't change. So we don't need to think about them at all. The difference between something like Leptos and something like Dioxis is that when label changes here, when label changes here, right? When we press that button to add those exclamation points, what Dioxis is going to do, right, is a Dioxis is going to re-render this, diff those three points, and then update one of them, which is the text node. Leptos is just going to re-run this little block here, right, from here to here. And it's just going to update that text node over and over and over. Likewise, with this class danger, when this selector changes, right, when we select a new row, it just finds the exact rows that have flipped from selected is true to selected is false, or selected is false to select is true, and toggles the class on those. So we can do these much more fine-grained updates. You can still get really good performance with an approach like Dioxis in a scenario like this where there are only three small nodes that you're diffing, but it's not that true fine-grained reactivity in a sense, right? Those things do have to be diffed, but still a fantastic performance all around. I want to look at two other things in this benchmark, which are really important, though, when we talk about performance, because the other disadvantage of WebAssembly is that WebAssembly binary sizes are large. They're larger than JavaScript, and you also have to ship JavaScript, right? So, like, the amount of JavaScript that's required to kind of bootstrap and do the glue code for a Laptose app is almost as much JavaScript as you need for a solid app, but then we're also shipping all the WebAssembly, right? So I want to look at the startup metrics, which are from, you know, Lighthouse, Lighthouse mobile metrics, and look at what the cost of WebAssembly really is in loading. Now, granted, this is a very small and simple application, right? Your real app, you're going to be shipping a much larger bundle than this, and one of the main issues right now is that we don't have WebAssembly code splitting that really works. So unlike JavaScript where you can split your bundle into a bunch of pieces, WebAssembly, you're shipping one big chunk. However, if you look at this, so if you look at this total kilobyte weight, right, you can really see the differences here. You know, vanilla.js, shipping 150k, Waz and bind, Gen is 185, Sycamore, 279, Laptose is bigger, U is bigger, Docs is quite big. Let me add just like for reference, maybe Svelte and Solid back in. So here's Solid, here's Svelte, right? And let's give React to. React hooks, there we go. So in terms of bundle sizes, right, you immediately see that all of the JavaScript, this vanilla.js is un-minified by the way. So that's why it's a little bigger than these guys. But Svelte, Solid, Vanilla.js, all smaller, React is actually big enough that it's bigger than something like Sycamore, which is funny, it's shipping enough JavaScript that it's larger than Sycamore's WebAssembly. But you'll notice there's a disconnect here between the kilobyte weight, how much you're actually shipping, and consistently interactive. And I should say also that all of these WebAssembly examples, these are compiled with Op Level equals three, right, for speed, the highest speed you can in cargo, rather than Op Level equals Z, the smallest size you can. And none of this is G-Zipped, none of this is minified at all, right, in the rest ones. So these numbers are a little bigger than they would be. But let's organize it a different way, right, by consistently interactive. So this is time to interactive when the page loads. You'll notice two things. One is that React is the slowest dialysis. The other thing you'll notice is these numbers are all basically the same. 1877, 1876, 1877, 7777, 781, 784. What does it suggest? What this suggests is that there's a floor in this benchmark, we're using what bootstrap? There's certain amount of stuff like CSS that has to load before we can render anything on the page at all. And so there's a floor to how quickly, how much the JavaScript and the WebAssembly actually affect it. And what you're seeing here is that Leptos, Sycamore, WazimBynGen, they're all coming you, right, they're all coming in underneath that floor. So for a small application like this, right, what's blocking the loading time is not actually a JavaScript or WebAssembly, it's actually the CSS. So I think that's pretty interesting. In a larger application, right, it's a very different story because WebAssembly just scales linearly where JavaScript would do some bundle splitting and so on. But again, the speed of WebAssembly streaming compilation is such that even four or five years ago Firefox was saying, look, we can basically compile this at the speed it can come in over the network. So the cost of 500 kilobytes of WebAssembly, or say it's a megabyte, maybe it GZips to 400 kilobytes. Not exactly the cost of just downloading 400 kilobytes, but it's pretty close, right? So you have to think if you're in something in a resource constrained environment or you are in an industry like e-commerce where, I mean, milliseconds are dollars, right, maybe it's not time for WebAssembly yet. But if you're building an application where people can wait, I don't know, you know, a tenth of a second, a quarter of a second for something to load, you know, maybe that's different. And once one Wazam bundle has loaded, that's your entire application. So there are no subsequent fetches for more stuff. And it's all just compiling streaming as it's coming in. The other thing I want to look at if you're looking at this benchmark yourself and you're curious is just the memory allocation results here. You'll notice there are huge differences between the JavaScript and the WebAssembly ones. That's because basically the way that it's measured WebAssembly sort of claims pages of memory. You can almost think of it, big chunks of memory from the browser. And then, you know, the metrics here aren't necessarily introspecting that to see sort of, okay, what's actually being used so that whereas the JavaScript stuff is garbage collected. So you'll see all of the WebAssembly ones are much higher. But then within the WebAssembly ones, right, you'll notice that they're all pretty close. They all have this sort of stable 1.8, 1.9 megabytes of memory that they get on first load. And then you can see some differences. So let's look at the round memory here. You know, leptos and dioxys are pretty memory efficient. You and Sycamore are a little bit less so there's probably some kind of weirdly happening here in leptos when we're creating and clearing a bunch of stuff. I think I may be delaying reclaiming some of that memory or whatever. But in terms of the updates, right, you're running at a pretty steady pace in terms of, you know, creating 10,000 rows, right? What's actually kind of funny here is that the React memory usage looks a lot like the WebAssembly memory usage, even though the other JavaScript libraries have a totally different profile, right? So React is really using a lot of memory here. But you'll notice, you know, leptos and dioxys are both just using a lot less memory than something like you, for example. And I think it is because of that different approach that's not doing as much work on every run. So, yeah, I mean, what's the truth about Rust and WebAssembly performance? What can I say? You know, if faster rendering than Svelte and an equivalent load time for a small app doesn't convince you that RevAssembly is ready for use, then I don't know what can. But, you know, I hope that you enjoyed this video. I hope you got a little bit out of it as far as what the actual performance constraints are on WebAssembly. People come to me all the time and ask, you know, I want to start any commerce site. Should I use leptos? And it's like, no, probably not. Like, you got to really think about what are the performance constraints in your environment. But for a lot of applications, you know, the kind of rendering speeds we're talking about here, that difference between solid at 1.08, 1.09, and a leptos or a dioxis at 1.14, that's something that you will never, ever, ever notice in actual use of your application. If you're creating a thousand rows in a table, you're going to virtualize that table. You know, like this stuff is about pushing things to the extremes. But what that shows is that basically WebAssembly is ready for primetime. So, I hope you have a great day and I hope you're building great applications. Take care. Thank you.
