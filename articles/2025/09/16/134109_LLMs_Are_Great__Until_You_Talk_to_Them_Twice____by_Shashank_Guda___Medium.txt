Timestamp: 2025-09-16T13:41:09.721910
Title: LLMs Are Great, Until You Talk to Them Twice! | by Shashank Guda | Medium
URL: https://shashankguda.medium.com/llms-are-great-until-you-talk-to-them-twice-5ea11543befa
Status: success
Duration: 0:00

Description:
以下是根据原文提炼的核心思想总结，并以中文呈现：

### 核心要点总结

**一、 研究背景与动机**
大型语言模型（LLMs）在处理单个、完整指令的问答任务时表现卓越。然而，现实世界中的对话往往是多轮的、渐进式的，用户会分步提供信息或修正需求。当前LLM的评估多集中于单轮任务，导致一个空白：LLM在实际多轮对话中的表现如何？本文研究旨在填补这一空白，探讨LLM在指令逐步揭示的对话场景中的表现。

**二、 多轮任务模拟方法**
研究团队设计了一个受控的模拟框架：
1.  **指令碎片化模拟：** 将完整的任务指令拆分为多个“碎片”，在对话中逐轮揭示给LLM。
2.  **自动化多智能体模拟：** 一个LLM扮演用户，逐轮提供指令碎片；另一个LLM作为被评估的助手模型，接收信息并尝试回应。
3.  **性能衡量：能力值（Aptitude）与不可靠性（Unreliability）：**
    *   **能力值：** 模型在最佳情况下的潜在表现（例如90th百分位），反映其技能上限。
    *   **不可靠性：** 模型性能在不同运行（或细微变化）之间的波动程度（90th - 10th百分位），反映其稳定性。

**三、 核心研究发现**
1.  **性能普遍下降：** 在多轮、指令不完全明确的对话中，所有测试模型的表现均显著低于单轮、完整指令的场景，平均性能下降约39%。
2.  **“对话中迷失”现象：** LLM一旦在对话早期做出错误的假设或判断，便难以自我纠正，容易“迷失”且无法恢复。
3.  **性能下降的主要原因：** 主要是由于**不可靠性显著增加（112%）**，即模型在处理相同任务时表现出高度不稳定性；能力值仅略有下降（约15%）。这意味着模型并非完全不具备处理能力，而是其表现高度波动，成功率不可预测。

**四、 LLM在多轮对话中的常见失败模式**
1.  **过早尝试与错误假设：** 模型在信息不完全时过早尝试提供最终答案，并基于错误的假设填充缺失信息。
2.  **过度依赖先前内容：** 模型倾向于固守其早期答案或推理，即使后续信息表明其不完整或错误，也仅尝试修补而非重新开始。
3.  **冗长回复导致语境混乱：** 模型在多轮对话中倾向于产生冗长、重复的输出，这会稀释关键信息，使其难以追踪重要语境。
4.  **忽略对话中间信息：** 模型对对话最开始和最新轮次的信息关注度高，但容易忽视或遗忘对话中间的关键细节。

**五、 缓解策略及其局限性**
研究探索了两种缓解策略，均显示出一定改进，但未能完全恢复性能至单轮水平：
1.  **RECAP策略：** 在对话结束后，将所有指令信息重新总结并提供给模型，给予其最终修正的机会。
    *   **局限性：** 实际应用中难以判断对话何时结束。
2.  **SNOWBALL策略：** 在每轮对话中，用户除了提供新信息外，还会重复之前所有的指令碎片。
    *   **局限性：** 导致输入过长，可能超出上下文限制，增加成本和延迟，且用户体验不自然。

**六、 对各方的重要启示**
1.  **对LLM构建者：** 应将多轮对话的可靠性视为与单轮性能同等重要的目标，并在模型训练和评估中纳入多轮场景。
2.  **对AI产品与工具开发者：**
    *   鼓励用户一次性提供尽可能多的细节。
    *   实施内部的“总结”或“核验”步骤（类似RECAP）。
    *   采用上下文管理策略（如总结历史对话）。
    *   监控模型混淆迹象并及时干预，可能需要重置对话。
3.  **对终端用户：**
    *   当模型表现不佳时，尝试重新开始对话而非试图纠正。
    *   尽可能将所有要求整合到单次提示中。
    *   若对话中忘记提供重要信息，可要求模型总结现有理解，然后在新对话中提供完整上下文。

**七、 核心结论**

大型语言模型在多轮、渐进式对话中表现出显著的性能下降和不稳定性，主要原因是其“迷失”并难以从早期错误中恢复。

### 框架 (Overarching Framework)

大型语言模型多轮对话挑战及其应对策略 (Challenges and Countermeasures for Large Language Models in Multi-Turn Dialogue)

### Mermaid 概念图

<Mermaid_Diagram>
graph LR
    subgraph "问题陈述 Problem Statement"
        A["核心问题：多轮对话挑战"]:::problem
        A --> B["单轮对话：表现优异"]:::good
        A --> C["多轮对话：表现挣扎"]:::bad
    end

    subgraph "研究方法 Methodology"
        C -- "研究动机" --> D["指令碎片化模拟"]:::method
        D --> E["自动化多智能体模拟"]:::method
        E --> F["评估指标"]:::metric
        F --> G["能力值 (Aptitude)"]:::metric
        F --> H["不可靠性 (Unreliability)"]:::metric
        G -- "最佳表现" --> H;
    end

    subgraph "关键发现 Key Findings"
        E --> I["性能普遍下降 (平均39%)"]:::finding
        I --> J["无法从错误中恢复 (迷失)"]:::finding
        J --> K["过早尝试与错误假设"]:::failureMode
        J --> L["过度依赖早期内容"]:::failureMode
        J --> M["冗长回复导致语境混乱"]:::failureMode
        J --> N["忽略对话中间信息"]:::failureMode
        I -- "核心原因" --> O["不可靠性显著增加 (112%)"]:::finding
        I -- "次要原因" --> P["能力值略有下降 (15%)"]:::finding
    end

    subgraph "缓解策略 Mitigation Strategies"
        I -- "尝试解决" --> Q["RECAP 策略"]:::strategy
        I -- "尝试解决" --> R["SNOWBALL 策略"]:::strategy
        Q --> S["RECAP 局限性：非实时"]:::limitation
        R --> T["SNOWBALL 局限性：冗长/成本"]:::limitation
        Q -- "有限改进" --> I;
        R -- "有限改进" --> I;
    end

    subgraph "对各方影响 Implications"
        J --> U["对LLM构建者：优先多轮可靠性"]:::implication
        J --> V["对AI产品开发者：设计防护措施"]:::implication
        J --> W["对终端用户：调整互动方式"]:::implication
    end

    U --> X["构建者：改进模型/训练"]:::action
    V --> Y["开发者：前端引导/内部总结"]:::action
    W --> Z["用户：重新开始/整合信息"]:::action

    X & Y & Z --> AA["人类-机器协作模式"]:::conclusion
    AA -- "目标" --> BB["LLMs迷失，人类不必"]:::conclusion
    J --> BB;

    classDef problem fill:#FFCCCC,stroke:#FF0000,stroke-width:2px,color:#333;
    classDef good fill:#D4EDDA,stroke:#28A745,stroke-width:1px,color:#333;
    classDef bad fill:#F8D7DA,stroke:#DC3545,stroke-width:1px,color:#333;
    classDef method fill:#E0F2F7,stroke:#17A2B8,stroke-width:1px,color:#333;
    classDef metric fill:#D1ECF1,stroke:#007BFF,stroke-width:1px,color:#333;
    classDef finding fill:#F5C6CB,stroke:#DC3545,stroke-width:2px,color:#333;
    classDef failureMode fill:#FEEAEA,stroke:#DC3545,stroke-width:1px,color:#333;
    classDef strategy fill:#FFF3CD,stroke:#FFC107,stroke-width:1px,color:#333;
    classDef limitation fill:#E2E3E5,stroke:#6C757D,stroke-width:1px,color:#333;
    classDef implication fill:#CCF8F1,stroke:#20C997,stroke-width:1px,color:#333;
    classDef action fill:#CCECF8,stroke:#007BFF,stroke-width:1px,color:#333;
    classDef conclusion fill:#D8BFD8,stroke:#8A2BE2,stroke-width:2px,color:#333;
</Mermaid_Diagram>

Content:
LLMs Are Great, Until You Talk to Them Twice! Why Chatbots Struggle in Multi-Step Dialogues Shashank Guda 23 min read · Jun 1, 2025 -- Listen Share Press enter or click to view image in full size Large Language Models (LLMs) like ChatGPT or Claude have amazed users with their ability to answer complex questions in a single turn. But what happens when a conversation stretches over multiple back-and-forth turns, with the user refining or revealing the task incrementally? Recent research finds that even top-performing LLMs get “lost” in multi-turn conversations, suffering steep drops in performance compared to one-shot prompts. In practical terms, an LLM might ace a well-specified single prompt, yet falter or produce inconsistent answers when the same information is given piece by piece in a conversation. This article dives into the key findings of the paper “ LLMs Get Lost in Multi-Turn Conversation ” and what they mean for AI developers, LLM builders, and users. We’ll explore the motivations behind studying multi-turn interactions, the clever simulation strategy used to evaluate models, how researchers measured aptitude versus unreliability, and why strong single-turn performers still stumble in multi-turn settings. Finally, we’ll discuss mitigation strategies like RECAP and SNOWBALL , their limitations, and the implications for anyone building or using AI tools. Why Multi-Turn Conversations Matter (and Confuse LLMs) Press enter or click to view image in full size One of the promises of conversational AI is that users can iteratively clarify or refine their requests over several turns. In real life, people often don’t get their instructions perfectly precise on the first try, they might start with an underspecified query and then add details or adjust requirements as the conversation continues. Ideally, an LLM assistant should help interpret and refine these needs through dialogue. However, most LLM evaluations today focus on single-turn tasks with fully specified prompts. This leaves a gap: Are our chatbots actually good at those realistic multi-turn exchanges, or are they just good at one-shot Q&A? The paper’s authors were motivated by this gap. They note that conversation logs confirm underspecified instructions are common in real usage, yet evaluation rarely tests the models under those conditions. To investigate, the researchers posed a fundamental question: “ How do state-of-the-art LLMs perform when instructions are given in a conversational, step-by-step manner instead of all at once?” If a user only reveals part of the task at first and provides additional details in later turns, will the LLM patiently incorporate the new information, or will it go astray? Understanding this is crucial for both LLM builders (to know where models fall short) and end-users (to adjust how they interact with chatbots). As we’ll see, the results show a significant performance gap that has big implications for the design of AI systems. Simulating Multi-Turn Tasks with Sharded Instructions Studying multi-turn performance is tricky because real user conversations are messy and varied. The authors designed a controlled simulation framework to fairly compare single-turn and multi-turn scenarios. Their approach revolves around sharded instruction simulation, essentially, taking an existing fully specified task instruction and breaking it into pieces ( shards ) to be revealed turn by turn. To simulate a multi-turn conversation from a task that was originally framed as a single prompt, the researchers introduced the concept of instruction sharding. Instead of giving the full problem at once, they split it into smaller “shards” each shard revealing just one aspect of the problem. Press enter or click to view image in full size On the left is the original single-turn instruction: a complete problem statement about snowball-making that includes all relevant details. On the right is the same instruction broken into five shards. In a multi-turn simulation, each shard is revealed to the LLM one by one mimicking how users gradually provide information in real conversations. The model must accumulate context over time rather than processing all constraints at once. Here’s how it works in a nutshell: Imagine a complex prompt that normally would be given all at once (e.g., “ Write a Python function to do X, meeting requirements Y and Z ”). In the sharded version, the same content is split into multiple parts. The conversation might start with only a high-level request (“ I’m trying to implement X. ”), then on subsequent turns the user (simulated) provides additional requirements or clarifications (“ Actually, it needs to also do Y ” … “ One more thing: make sure it handles Z ”). The assistant LLM sees a gradually unfolding problem, much like a real user who forgot to mention some details up front. Importantly, the sum of the shards contains all the information of the original full instruction, just spread out. This ensures that if the model handled the conversation perfectly, it would have what it needs by the end to produce the correct solution. To cover a broad range of scenarios, the researchers curated six diverse generation tasks (both programming-related and natural language tasks) and sourced instructions from high-quality benchmarks for each. The tasks included code generation, database query generation (SQL), API call composition (actions), math word problems, data-to-text description, and document summarization. For each task, around 100 fully specified prompts were chosen from benchmarks like HumanEval (code function writing), Spider (SQL queries), GSM8K (math problems), ToTTo (table-to-text description), a document summarization benchmark, etc. Each of these was then transformed into a sharded instruction set following a semi-automatic process (an LLM helped propose possible shard breakdowns, which were then validated by the authors). This yielded a bank of 600 multi-turn conversations (sharded versions) paired with their single-turn equivalents. Press enter or click to view image in full size Six sharded tasks included in the experiments Once the instructions are sharded, the researchers simulate a conversation between a user and an LLM to test the model’s behavior. But instead of using human participants (which is expensive and hard to scale), they construct a fully automated, multi-agent simulation where one LLM plays the role of the user and another is evaluated as the assistant. Press enter or click to view image in full size The simulation starts with a User Simulator, which reveals one shard of the instruction per turn. The Evaluated Assistant (the model being tested) generates a response, which is then passed through a Strategy Classifier to determine whether it attempted an answer or not. If the model makes an answer attempt, the Answer Extractor isolates the actual output, which is finally judged by the Task Evaluator. The simulation ends when either a correct solution is generated or all shards have been revealed and processed. Paths are color-coded: red for incorrect answer attempts, yellow for non-answer responses (like clarifications or hedges), and green for successful completions. Crucially, the simulation involved an automated user simulator and a system to evaluate the assistant’s responses. The user simulator “knows” the full instruction and strategically reveals one shard per turn. The assistant LLM being tested does not know upfront how many turns there will be or that the instruction is incomplete (it’s not explicitly told “more info coming”). It just sees whatever the user says each turn and responds. This setup forces the model to either ask clarifying questions or attempt partial solutions as information trickles in closely mirroring what happens when real users give details bit by bit. The conversation continues until all shards have been given, at which point ideally the model should produce a final answer incorporating everything. To benchmark performance, each conversation (and its single-turn counterpart) is scored on the task’s relevant metric. For coding tasks, for example, they checked if the generated function was functionally correct (passed the unit tests). For math problems, did the model get the correct numerical answer (exact match)? For data-to-text or summarization, they used metrics like BLEU or a custom LLM-based judge for summary quality. This way, the researchers could quantitatively compare how well the model did when the same information was delivered via conversation versus in one prompt. They defined three main simulation settings for each instruction: The top portion illustrates how a fully-specified single-turn prompt (blue) is broken into a sequence of shards (gold) for multi-turn conversations. The bottom visual compares five simulation types. FULL : A single-turn setting where the entire instruction (fully specified) is given at once. This is basically the standard prompt used to measure the baseline upper-bound performance when the model has all info upfront. SHARDED : The multi-turn conversational setting, where the instruction is broken into shards revealed over several turns. This is the core test of underspecified multi-turn performance. CONCAT : A single-turn control condition using the sharded phrasing. Here, all shards are concatenated into one long prompt (often formatted as bullet points), preceded by a note like “ Take into account all these points: ” Essentially, CONCAT gives the model all the information (so no underspecification) in one go, but preserves any rewording or structure changes that happened during sharding. This helps isolate whether any performance drop is due to the multi-turn interaction itself, as opposed to the prompt being rephrased or split up. If a model does fine on CONCAT (where it sees everything together) but not on SHARDED, we know the conversation aspect (gradual reveal) is the culprit. Additionally, the authors later experiment with two augmented conversation strategies called RECAP and SNOWBALL , which we’ll discuss shortly, aimed at mitigating failures by reminding the model of context. But first, let’s see how the models fared in the standard settings. Aptitude vs. Unreliability: Measuring Performance in Multi-Turn Dialogue Press enter or click to view image in full size An innovative part of this research is how it evaluates model performance beyond just an average score. The team was interested not only in how well an LLM performs, but also in how consistently it does so across different conversation trajectories. In multi-turn scenarios, the same model and task can play out very differently depending on how the model responds e.g., one run it might ask a clarification and get to the right answer, another run (or with a slight prompt variation) it might misunderstand and go off course. To capture this variability, the researchers ran 10 simulated conversations for each model-task-setting (with different random seeds affecting the model’s generation). This gave a distribution of outcomes for each scenario. They defined metrics to quantify both aptitude and reliability: Average Performance : Simply the mean score over all runs for that instruction. This is a high-level success rate or quality measure, averaged across the stochastic variations. Aptitude : Essentially the best-case performance the model can achieve on that task. Specifically, they use a high percentile of the score distribution (the paper uses 90th percentile, denoted A₉₀) as an estimate of the model’s capability when things go well. Intuitively, if you ran the conversation many times or in the best-case scenario where the model doesn’t get tripped up, how high could it score? This reflects the model’s potential or skill on the task when it doesn’t get lost. Unreliability : A measure of how much the performance fluctuates or degrades in the worst cases. They define this as an inter-percentile range (90th minus 10th percentile score, denoted U₉₀₋₁₀). In simpler terms, unreliability is the gap between the model’s best-case and worst-case outcomes for the same instruction. A small gap means the model is consistent it performs similarly every time, whereas a large gap means sometimes it does well but other times it fails badly. Press enter or click to view image in full size Think of aptitude as a model’s top potential (if everything in the conversation goes smoothly) and unreliability as how far it can fall off that potential in the worst conversational mishaps. A highly reliable model would have almost the same result every time (low variability), whereas an unreliable model is like a volatile car: It might speed ahead or veer off the road unpredictably. Press enter or click to view image in full size Averaged Performance (P) of LLMs on six tasks ( Code, Database, Actions, Data-to-text, Math, and Summary). For each task, conversations are simulated in three settings: FULL, CONCAT, and SHARDED. Models are sorted in ascending order of average FULL scores across tasks. Background color indicates the level of degradation from the FULL setting. The last two columns average the performance drops from the CONCAT and SHARDED compared to the FULL in percentages across the six tasks. Press enter or click to view image in full size (a) Visual introduction to the concepts of Aptitude and Unreliability when overlaid on a box-plot visualization, (b) Reliability results based on experimental simulations with 15 LLMs, (c) Summary of results from gradual sharding experiment, with instructions sharded in gradually larger shard sets (from 1 to 8 shards). By measuring these, the researchers could decompose why the average performance might drop in multi-turn settings. Is it because the model’s best possible performance (aptitude) is fundamentally lower when instructions are split up (perhaps the model just can’t integrate info as well)? Or is it mainly because there’s a higher chance of making mistakes sometimes (even if it could do well, it often doesn’t) i.e., an unreliability problem? When LLMs Take a Wrong Turn: Key Findings from the Experiments Press enter or click to view image in full size After running over 200,000 simulated conversations across 15 different LLMs and 6 tasks, the headline result is striking: Every single model tested performed worse in multi-turn, underspecified conversations than in single-turn, fully-specified prompts. On average, model performance dropped about 39% in the multi-turn setting compared to the one-shot setting. In some cases, models that scored above 90% on a task when given the full prompt in one go would plummet to around 60% when the same information was given via a conversational exchange. This degradation showed up even in just two-turn conversations, not only long chats. In other words, splitting a query into just a question and a follow-up clarification was enough to significantly confuse the models. Importantly, this wasn’t only a problem for smaller or weaker models, it affected all models, including state-of-the-art ones. The researchers tested a range from open-source models like LLaMA 3.1 8B, to Anthropic’s Claude 3.7, to Google’s Gemini 2.5 Pro and OpenAI’s GPT-4.1 (a variant of GPT-4). The “ lost in conversation ” effect was universal. In fact, the authors note with some surprise that even the most capable models “get equally lost” in multi-turn conversations as less capable ones. A model like GPT-4 (which had very high single-turn scores) still suffered a comparable relative drop and became just as inconsistent in multi-turn tasks as smaller models. Simply making a model bigger or more powerful doesn’t solve this reliability issue in dialogue. So what exactly does “ getting lost ” look like for an LLM? A recurring pattern observed was that when an LLM makes a wrong assumption or a misstep early in the conversation, it tends not to recover from it. The model might latch onto an incorrect interpretation of the task or produce a flawed intermediate answer, and then each subsequent turn it kind of doubles down or works around that flawed basis, instead of correcting course. The researchers put it succinctly: “When LLMs take a wrong turn in a conversation, they get lost and do not recover.” Press enter or click to view image in full size Illustration of the “lost in conversation” phenomenon. In a fully specified single-turn prompt (left), the LLM has high aptitude and typically produces a correct solution on the first try. In a multi-turn underspecified scenario (right), the LLM may start out okay (even asking a clarification), but often jumps into a solution too early, makes incorrect assumptions, and ends up with a convoluted, incorrect answer. Empirically, across many models, multi-turn conversations lead to lower aptitude (about a 15% drop in best-case performance) and much higher unreliability (over 100% increase, meaning the worst-case outcomes are far worse). In simpler terms: the model’s peak capability only slips a bit, but it becomes far less consistent and trustworthy in multi-turn dialogue. The performance degradation was found to consist of “A minor loss in aptitude and a significant increase in unreliability. ” On average, the model’s best-case scores in multi-turn were somewhat lower than in single-turn (they estimate around a 15% drop in that upper percentile performance), indicating a slight dip in capability. But the lion’s share of the problem was unreliability: The gap between the model’s successful runs and failure runs widened dramatically, in fact, the study reports an 112% increase in unreliability in multi-turn settings. Practically, for a given query, one run the model might solve it, and another run (or slightly different phrasing) the model completely bungles it a variability more than double what happens in single-turn usage. Another way to put it: They observed about a 50 percentage-point spread on average between the best and worst outcomes for the same instruction in multi-turn mode. That is huge. In single-turn mode, a model might reliably score, say, 90% every time on a task; but in multi-turn, sometimes it might still hit ~90% (best case) yet other times only ~40% on the same task if the conversation goes awry. What does going awry look like? The researchers qualitatively analyzed the conversation logs and identified several common failure modes that explain why the models get lost. These key issues are worth highlighting: Premature Attempts and Incorrect Assumptions: LLMs often jump to provide a final answer too early in the conversation. After the first turn or two (when not all details are given yet), the model tends to assume what the user wants or fill in the missing pieces on its own, and then proceeds to produce a solution based on those assumptions. Because the assumptions can be wrong, the answer is off-target. For example, an LLM might attempt to write the full function after only the first requirement, guessing the rest, rather than waiting for all requirements. This leads to “bloated” answers or off-track solutions that don’t actually satisfy the user’s later-specified needs. Over-Reliance on Previous Content: Once the model has produced an answer attempt, it gets anchored to it. LLMs would stick to their earlier answer or reasoning, even when new information in later turns shows that answer to be incomplete or incorrect. Instead of scrapping the flawed attempt and starting fresh with the new info, the assistant typically tries to patch or append to its existing answer. This often just compounds errors. Essentially, the model behaves like a person who hates to admit their initial solution was wrong, and so keeps modifying it in a piecemeal way which can end in a convoluted mess. Overly Verbose Responses (Context Muddling): The study observed that models in multi-turn settings tend to produce long, verbose outputs at each turn. They may ramble, restate information, or include extraneous details. These lengthy responses can muddy the conversation’s context, making it harder for the model to keep track of what’s important. It’s like the model is talking itself in circles. Notably, the researchers found a correlation between verbosity and performance: In simulated chats where the assistant’s answers were shortest, the success rates were significantly higher, whereas overly long responses were associated with much lower performance. The verbosity might be a byproduct of the model trying to be helpful or cover its bases, but it inadvertently causes it to lose focus on the key requirements. Ignoring Middle Turns (“Lost in the Middle”): Another subtle issue is attention bias within the conversation. Models appeared to pay a lot of attention to the earliest instructions and the very latest turn, but could neglect information revealed in the middle of the conversation. This is analogous to the known “lost in the middle” problem with long text inputs the model over-weights the beginning and end. In a dialogue, if an important detail is given in, say, turn 3 of 5, the model might overlook or forget it by the time it’s constructing the final answer. This contributes to the final solution being incorrect or incomplete (since the model latched onto its initial idea and whatever was last said, but not some nuance from earlier clarification). The paper alludes to this effect by noting models sometimes give disproportionate weight to certain turns and fail to correctly integrate mid-conversation clarifications. Taken together, these behaviors paint a picture of why multi-turn conversations are tripping up LLMs. The model is eager to help (so it tries answering quickly), but this eagerness leads it to guess parts of the problem that weren’t told yet. Once it’s on a track (even if wrong), it has no mechanism to truly backtrack its previous output lingers in the conversation, biasing subsequent replies. And the sheer verbosity can bury useful info or cause the model to lose the forest for the trees. It’s worth noting that these issues were largely task-agnostic and model-agnostic. The effect was seen across all six task types and virtually all models. The only exception was that for tasks which are truly episodic, meaning each turn is independent (for example, straightforward machine translation turn by turn, or a conversation where each user query is unrelated), multi-turn doesn’t hurt because there’s no incremental build-up of one query. But whenever a single problem is split into parts over turns (which is common in realistic use cases), the degradation appears. Even approaches like using special “reasoning” models that chain intermediate thoughts (some models in the test like DeepSeek or O3 attempted to internally reason) did not eliminate the problem. This suggests the problem isn’t simply solved by prompting the model to “think harder” it’s more fundamental. Also, interestingly, the authors experimented with turning down the randomness (temperature) of the model to see if deterministic behavior would help consistency. One might think if the model always takes the highest-probability response (greedy decoding), it might at least be consistent in its mistakes. However, unreliability persisted even at temperature 0. In theory, a fixed deterministic model would produce the same outcome for a given conversation every time, so where is the variability? The answer is that even small differences in how the conversation unfolds (like exactly how the user phrased a clarification, or minor non-determinism at one turn cascading) can lead to divergent paths. The authors point out there’s a compounding effect of subtle non-determinism over multiple turns. Essentially, multi-turn interactions amplify instability. So while setting temperature to 0 may remove randomness at each local step, it doesn’t guarantee the model won’t get lost; if it deterministically makes a wrong assumption early, it will deterministically be lost each time for that conversation. The unreliability the authors measure is often across different instructions or models, so a model that’s perfectly deterministic can still have a wide gap between best and worst outcomes across different conversations. In short, the multi-turn reliability issue isn’t just random fluke it’s a systemic weakness. Attempts to Keep Models on Track: RECAP and SNOWBALL Press enter or click to view image in full size Given the stark drop in performance, the researchers explored a couple of intuitive strategies to help models do better in multi-turn conversations. RECAP Strategy: The idea behind RECAP is simple, after the conversation is done, have one extra turn where all the information is recapped to the model to give it a final chance to answer correctly. In practice, this means running a normal SHARDED conversation to the end (user gives shards one by one, model responds each time), and then adding a final user prompt like, “ Okay, here is everything we discussed: [list all requirements]. Now please give the solution. ”. This is effectively a combination of a multi-turn dialogue followed by a single-turn CONCAT prompt. The model thus gets to see a consolidated view of the task (as if it had all info at once) at the very end, hopefully allowing it to correct any earlier misunderstandings in its final answer. SNOWBALL Strategy: SNOWBALL takes the idea of repeating information even further it does it at every turn. In a SNOWBALL simulation, each time the user provides a new shard, they also repeat all previous shards in that turn. So the context “snowballs” larger each turn: by turn 2 the user message contains shard1 + shard2, by turn 3 it contains shard1+shard2+shard3, and so on. The assistant is constantly reminded of all prior details. This is like the conversation itself having a running summary that grows, ensuring nothing gets forgotten from earlier turns. It’s a bit like an over-explicit user who restates all prior requirements whenever adding a new one (“ Let’s recap: you need to do X and Y, and now additionally Z… ”). Redundant, but the hypothesis is it could help the model stay on track by keeping the full context explicit at each step. So, do these strategies help? The authors tried them out with a subset of models. Both RECAP and SNOWBALL did show some improvement over the standard SHARDED conversations. Essentially, performance in those multi-turn scenarios got a bit closer to the single-turn baseline when using these interventions. RECAP in particular often outperformed SNOWBALL in their tests, likely because giving a clean full recap at the end allowed the model to fix mistakes and produce a good answer. SNOWBALL, while helpful, sometimes may have overwhelmed the model with increasingly long inputs every turn (imagine reading an ever-growing prompt, not easy for the model either!). However, neither method fully restored performance to the level of a single-turn prompt. Even with these aids, the models still lagged behind the FULL or CONCAT conditions. For example, if a model was scoring 90% in single-turn and 60% in a normal multi-turn, maybe SNOWBALL or RECAP could bump it up to, say, 70–75%, but not back to 90%. There were diminishing returns, some errors persisted. Moreover, there are few practical limitations of each approach. RECAP is somewhat “unrealistic” in real use because it assumes we know when the conversation is over. In a real user chat, the system won’t magically know that the user has finished giving all requirements and it’s now time to recap. You could design a chatbot to always ask “ Let me restate everything… ” at some point, but triggering that at the right moment is non-trivial unless the system is certain no more info is coming. It’s more of an evaluation tool (to see if having the full info would help at the end) than a user-friendly solution. By the time you realize the model is lost, damage may be done. SNOWBALL, on the other hand, is something a system could try to implement (automatically repeating context each turn). It doesn’t require future knowledge, it just requires a willingness to be verbose. But it has downsides too. It makes each user turn much longer (potentially hitting context length limits or increasing latency/cost). And it might annoy users or be unnatural to repeat everything every time. Even with SNOWBALL, the study saw improvement but still not a complete fix. The fact that performance still fell short of the single-turn scenario indicates that while reminders help, they cannot fully eliminate the confusion or errors introduced by the multi-turn process. In summary, these mitigation experiments show that the multi-turn reliability problem is not easily solved by simple prompt engineering or conversation tactics. Giving the model an extra chance with a recap helps somewhat (suggesting that at least the model then recognizes some of its mistakes when shown the whole picture), and repeating info each turn can keep it on track better. But neither approach is a silver bullet. The LLM can still “get lost,” albeit maybe a little less often. The authors also mention that reducing generation randomness (temperature) or using “agentic” reasoning approaches had limited effect on the core issues. All of this implies that the problem runs deeper, possibly into how the models are trained or how they handle conversational context internally. Implications for LLM Builders, Users, and AI Tool Developers Press enter or click to view image in full size These findings carry important messages for different stakeholders in the AI community: For LLM Builders (Model Developers): The clear call-to-action is to prioritize multi-turn reliability as a first-class goal alongside raw performance (aptitude). Until now, a lot of model development has chased state-of-the-art single-turn benchmark results. This research highlights that a model that’s brilliant at one-shot tasks might still deliver a poor user experience in a dialogue. Model builders need to incorporate multi-turn scenarios into their evaluation and training. This could mean developing new training data where the model is explicitly taught to handle underspecified queries and not rush to answer. It might involve architectural changes or memory mechanisms to better maintain and update a knowledge state across turns. Organizations building conversational AI products should conduct multi-turn specific evaluations (even small-scale experiments can reveal reliability issues) and treat a model’s tendency to go off-track as a critical bug to fix. In short, don’t just ask “Can it solve task X?” but also “Can it carry on a coherent, correct conversation about task X over multiple turns?”. For AI Product and Tool Developers: If you’re building applications on top of LLMs (chatbots, coding assistants, etc.), you should be aware of these limitations and design around them. The results suggest that current LLMs cannot be naively trusted to handle a stepwise clarification process. As a developer, you might consider implementing some guardrails or strategies to mitigate this: Encourage users to provide as much detail as possible up front. For example, prompt users with fields or follow-up questions to gather requirements before generating the final answer. Since we know LLMs do much better when everything is in one prompt, structuring your UI/workflow to achieve that will lead to better outcomes. Implement an internal recap or verification step. For instance, after a user and model have had a dialogue, before using the model’s answer, you might feed the conversation history (or a summary of it) back into the model and ask it to double-check or re-solve the problem. This is analogous to the RECAP strategy. It costs an extra turn, but can catch mistakes. Use context management tactics: If the conversation is getting long or complex, consider summarizing past turns and feeding the summary (instead of raw full history) to the model to focus its attention. This might alleviate the “lost in the middle” effect by keeping context digestible. Monitor for signs of the model getting confused (e.g., contradictory answers, the user repeating themselves) and proactively intervene, perhaps by starting a clean slate or asking a direct clarification. The bottom line is that system developers should not rely on the model to self-correct reliably once it’s on the wrong track. From the findings, persisting in a flawed conversation seldom leads to recovery. It may be better to detect the situation and, for example, prompt the user to restate or consolidate. For End Users: If you’re just a user chatting with an AI, these insights can help you use the tool more effectively. If things go wrong, try a fresh restart. If a conversation didn’t yield the right answer, don’t waste many turns trying to get the bot back on track. It might just be “lost.” Instead, start a new session and pose your query again (perhaps in a more direct or detailed way). Often, you’ll get a much better result on a second try or with a clean context. The variability of LLMs means a fresh run can succeed where a tangled conversation failed. Consolidate information into one prompt whenever possible. Rather than feeding requirements bit by bit, list out all your needs in one go if you can. If you realize mid-conversation that you forgot to include something important initially, it might be better to literally ask the assistant to summarize the current understanding and then start a new chat with that full context. For example, you can say, “Let’s start over. Here’s everything we need to do: …” This strategy leverages the model’s strength in single-turn aptitude by removing the multi-turn ambiguity. In fact, the CONCAT experiments showed that models did much better when the same info was given as a single compound prompt than scattered, so combining into one prompt can boost both accuracy and reliability. These are somewhat workaround solutions, and as the paper notes, they’re not ideal user experience, they put extra burden on the user to manage the conversation’s clarity. In the long run, we’d want LLMs to handle natural multi-turn instructions gracefully so that users can just converse naturally. But until then, being aware of these failure modes means savvy users can adjust their behavior to get better results (and avoid frustration when the bot starts acting confused). Experienced users already suspect this issue: for example, some coders using LLM-based tools have learned to frequently start new chats “whenever they can” rather than carry on long threads. This intuition matches the study’s findings a reset often clears the accumulated errors. The fact that multi-turn degradation is so pervasive might also hint at why some AI applications haven’t seen as high adoption or satisfaction as expected, especially with novice users. If someone doesn’t know they should be explicit upfront, they might naturally converse and then get disappointed when the AI gives wrong or inconsistent answers. This could erode trust and discourage use. Therefore, solving this issue is not just an academic exercise; it could make conversational AI more reliably useful to a wider audience. For the research community, these results open avenues to explore new training methods (like fine-tuning on synthesized multi-turn data), conversation-aware architectures, or even interactive alignment techniques where the model learns to ask for missing information rather than guessing. It’s a reminder that human conversation is contextual and iterative, and aligning AI behavior with that is non-trivial. LLMs Get Lost, Humans Don’t Have To. Press enter or click to view image in full size This study doesn’t just expose a technical flaw, it reinforces a critical truth: LLMs are not replacements for humans, but tools to be used by humans. Despite their remarkable capabilities, even the most advanced models today can fumble a multi-turn conversation that any attentive human would navigate with ease. When the task is underspecified or evolves gradually as most real-world communication does LLMs stumble, often irreversibly. And that’s exactly why human oversight, intention, and interpretation remain indispensable. But it’s not just about what machines can’t do it’s about how we choose to use them. This research reminds us that humans and LLMs must co-adapt. While model builders must improve reliability and context handling, users, developers, writers, analysts must also learn how to communicate effectively with machines. That means designing interfaces that guide the model clearly, knowing when to reset a conversation, and understanding how these systems fail so we can build guardrails around them. We’re entering a world not of full automation, but of human-machine collaboration. LLMs won’t replace people who can think critically, communicate clearly, or adapt flexibly but they will amplify those who do. The future of AI isn’t about handing off problems, but about working alongside tools that are powerful but imperfect. As long as LLMs keep getting lost in conversation, it will be up to us to keep the map and the compass. Sources LLMs Get Lost In Multi-Turn Conversation Github/Lost In Conversation HuggingFace Dataset
