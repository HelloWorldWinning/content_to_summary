Timestamp: 2025-09-05T08:54:55.608458
Title: [Ilya Top 30] Ilya Sutskever 推荐的第一篇神作：复杂熵 - 复杂动力学第一法则  混沌与秩序之间的新物理定律 yukH6wUAX58
URL: https://youtube.com/watch?v=yukH6wUAX58&si=qulLppEwyaCZW-Si
Status: success
Duration: 33:26

Description:
以下是对文本内容的总结与提炼：

### 核心要点总结

“复杂动力学第一法则”提出了一种在资源受限条件下，衡量系统复杂度会经历“先增后减”过程的新物理猜想，它统一了从物理现象到大型人工智能模型的信息压缩本质，为理解宇宙和智能的演化提供了新的视角。

### 贯穿框架

本内容围绕的贯穿框架是 **“复杂动力学第一法则”**，该法则推测在封闭系统中，尽管熵持续增加，但系统复杂度（特指“资源受限老练度”或“复杂商”）会呈现出“先增加后减少”的演化曲线，并将其应用于理解宇宙、生物智能及大型人工智能模型的信息压缩与组织原理。

### 结构化摘要

1.  **伊利亚·苏茨克维尔 (Ilya Sutskever) 简介**
    *   **背景与声誉：** OpenAI 联合创始人、前首席科学家，深度学习领域的关键人物，以解雇 Sam Altman 事件及对 AI 安全的执着而闻名。
    *   **主要成就：** 参与发明 AlexNet；在 Google 期间发明 Sequence-to-Sequence 并推动 LSTM；深度参与 AlphaGo 项目；创立 OpenAI 后领导 GPT 大模型研发；现创立 Safe Superintelligence 公司。
    *   **专业特长：** 精通模型、基础设施、实战编码、软硬件、强化学习，是全能型天才。
    *   **推荐意义：** 其推荐的论文（系列首篇为“复杂动力学第一法则”）反映了对 ML 深刻的理论与实践理解。

2.  **“复杂动力学第一法则”核心内容**
    *   **法则概述：** 挑战传统熵增定律，提出在封闭系统内，熵持续增加，但复杂度会经历一个“先增后减”的过程。
    *   **典型示例：**
        *   **牛奶与咖啡混合：** 初始（低熵低复杂度）→ 混合中（高熵高复杂度，出现复杂结构）→ 最终混合均匀（高熵低复杂度）。
        *   **宇宙演化：** 大爆炸（简单）→ 当前（复杂）→ 热寂（简单）。
    *   **提出者：** 物理理论学家 Scott Aaronson 在 FQXI 会议上提出，此概念改变了对复杂度的直觉理解。
    *   **与热力学第二定律的对比：** 熵（混乱度）是持续增加的，而复杂度（结构、模式、意义）并非如此，纯粹的随机状态（高熵）并不被认为是高度复杂的。

3.  **复杂度定义的演进与局限性**
    *   **定义一：熵 (Entropy)**
        *   **定义：** 波尔兹曼熵 (S=kB log Ω) 和香农熵 (信息压缩程度)。
        *   **局限性：** 熵持续增加，无法捕捉“先增后减”的复杂度模式；高熵的完全随机状态缺乏结构和意义，却被视为高熵，不符合直觉上的“复杂”。
    *   **定义二：科尔莫哥洛夫复杂度 (Kolmogorov Complexity)**
        *   **定义：** 描述一个对象所需的最短计算机程序（图灵机）的长度。
        *   **局限性：** 与熵有相似特性，纯粹的噪音（随机序列）因难以压缩，其科尔莫哥洛夫复杂度很高，仍未完美体现“先增后减”。
    *   **定义三：老练度 (Sophistication)**
        *   **定义：** 产生一个属于特定集合的元素所需的最短程序的长度，而非精确复制原对象，通过“判别器”确定集合成员身份。
        *   **优势：** 初始简单和最终随机状态的老练度均较低。
        *   **局限性：** 有数学证明，老练度无法超过 Log T + C (T为时间)，这意味着中间的高复杂度状态也可通过模拟初始状态+Log T步的短程序描述，未能产生预期的中间高峰。
    *   **定义四：复杂商 (Complex Entropy / Resource-Limited Sophistication)**
        *   **核心思想：** 在老练度的基础上引入了“资源限制”（如计算力）。一个系统有两种描述方式：直接描述其状态（可能很长）或描述其初始状态及演化过程（需要计算资源）。这两种描述的“成本”构成复杂度，且无法同时最小化。
        *   **成功之处：** 资源限制使得在中间状态，直接描述复杂结构冗长，而通过模拟生成又耗费计算资源，从而使得“程序复杂度 + 计算复杂度”的总和在中间达到高峰，完美符合“先增后减”的曲线。

4.  **复杂商与大模型 (LLMs) 及人工智能的关联**
    *   **伊利亚的视角：** 大模型（LLMs）被视为对互联网上所有信息的高度压缩。
    *   **压缩与理解：** 最有效的压缩需要最深层的理解。LLMs 通过捕获数据中最本质的规律，实现了信息的极致压缩，其模型权重是信息的“秘密集合体”。
    *   **大模型的“复杂商”特性：** 大模型既不随机也不简单，且难以进一步无损压缩（任何改变都会影响性能）。描述它要么需要庞大的权重数据，要么需要描述其训练过程（消耗大量计算资源和数据存储），这使其处于“复杂商”的高峰。
    *   **DNA类比：**
        *   互联网 ≈ 生物圈（包含所有复杂生命）。
        *   大模型 ≈ DNA（最本质、最压缩的模式，能“生成”复杂事物）。
        *   无生命星球或宇宙热寂（完全随机）≈ 低复杂度，无需复杂DNA描述。
        *   人类大脑：因其思想、世界模型和知识，难以被完全压缩和复制，因此具有更高的“复杂商”。

5.  **总结与未来展望**
    *   **“复杂动力学第一法则”：** 被视为一个统一物理学、基础信息科学和人工智能的新物理规律（尚为猜想）。
    *   **深远影响：** 对信息压缩、图灵机问题等基础科学领域有深刻启发。
    *   **开放性问题：** 该法则仍是一个待验证的猜想，强调了提出问题比解决问题更重要，鼓励进一步研究。

<Mermaid_Diagram>
graph TD
    A["伊利亚·苏茨克维尔"] --> B["推荐 'Top30' 论文/博客"];
    B --> C["第一篇: 复杂动力学第一法则"];

    subgraph "核心概念"
        C --> D["核心思想: 复杂度 '先增后减' 过程"];
        D --> E["封闭系统"];
        E --> F["熵 (Entropy)"];
        E --> G["复杂度 (Complexity)"];
        F --> H["持续增加 (遵循热力学第二定律)"];
        G --> I["先增后减曲线"];
    end

    subgraph "示例与类比"
        D --> J["牛奶咖啡混合示例"];
        D --> K["宇宙演化类比"];
    end

    subgraph "复杂度定义演进"
        subgraph "被否决的定义"
            L["定义1: 熵 (Shannon/Boltzmann)"] --> L1["无法体现 '先增后减' 曲线"];
            M["定义2: 科尔莫哥洛夫复杂度"] --> M1["噪音仍被判为高复杂度"];
            N["定义3: 老练度 (Sophistication)"] --> N1["中间状态峰值受限 (Log T+C)"];
        end
        O["定义4: 资源受限老练度 (复杂商)"] --> O1["克服局限，符合 '先增后减'"];

        D --> L;
        L --> M;
        M --> N;
        N --> O;
    end

    subgraph "复杂商与人工智能"
        O --> P["大模型 (LLMs)"];
        P --> Q["信息压缩的本质"];
        Q --> R["深层理解实现高效压缩"];
        R --> S["大模型拥有高 '复杂商'"];
        S --> T["DNA类比 (LLMs似DNA, 互联网似生物圈)"];
    end

    subgraph "总结与展望"
        O --> U["新物理规律 (猜想)"];
        U --> V["统一物理、生物、AI"];
        U --> W["开放性问题 (未解决)"];
    end

    style A fill:#ADD8E6,stroke:#333,stroke-width:2px,color:#333;
    style B fill:#ADD8E6,stroke:#333,stroke-width:1px,color:#333;
    style C fill:#F9F7D8,stroke:#333,stroke-width:2px,color:#333;
    style D fill:#F9F7D8,stroke:#333,stroke-width:2px,color:#333;
    style E fill:#D3D3D3,stroke:#333,stroke-width:1px,color:#333;
    style F fill:#FFFFCC,stroke:#333,stroke-width:1px,color:#333;
    style G fill:#FFFFCC,stroke:#333,stroke-width:1px,color:#333;
    style H fill:#FFB6C1,stroke:#333,stroke-width:1px,color:#333;
    style I fill:#90EE90,stroke:#333,stroke-width:1px,color:#333;
    style J fill:#D3D3D3,stroke:#333,stroke-width:1px,color:#333;
    style K fill:#D3D3D3,stroke:#333,stroke-width:1px,color:#333;
    style L fill:#FFB6C1,stroke:#333,stroke-width:1px,color:#333;
    style L1 fill:#FFB6C1,stroke:#333,stroke-width:1px,color:#333;
    style M fill:#FFB6C1,stroke:#333,stroke-width:1px,color:#333;
    style M1 fill:#FFB6C1,stroke:#333,stroke-width:1px,color:#333;
    style N fill:#FFB6C1,stroke:#333,stroke-width:1px,color:#333;
    style N1 fill:#FFB6C1,stroke:#333,stroke-width:1px,color:#333;
    style O fill:#90EE90,stroke:#333,stroke-width:2px,color:#333;
    style O1 fill:#90EE90,stroke:#333,stroke-width:1px,color:#333;
    style P fill:#BAE8C4,stroke:#333,stroke-width:1px,color:#333;
    style Q fill:#BAE8C4,stroke:#333,stroke-width:1px,color:#333;
    style R fill:#BAE8C4,stroke:#333,stroke-width:1px,color:#333;
    style S fill:#BAE8C4,stroke:#333,stroke-width:1px,color:#333;
    style T fill:#BAE8C4,stroke:#333,stroke-width:1px,color:#333;
    style U fill:#B0E0E6,stroke:#333,stroke-width:2px,color:#333;
    style V fill:#B0E0E6,stroke:#333,stroke-width:1px,color:#333;
    style W fill:#B0E0E6,stroke:#333,stroke-width:1px,color:#333;
</Mermaid_Diagram>

Content:
大家好,我是近,今天来给大家带来一系列有序文章的第一篇这一系列有序文章是伊利亚SusKaver推荐的30篇论文这个故事大概是这样的,伊利亚SusKaver对他的Open-Eye的同事说如果你学会了这40篇论文你就学会了这个世界上关于RML重要的那种90%然后呢,尽管这个文章一系列一开始是40篇文章但是并没有一个官方的这么一个list然后后面传织传织就变成了30篇然后无热心的网友和他这个同事一共找到了27篇所以尽管他非常有名的叫做Top30但其实只有27个文章而且这20篇文章并不都是论文其中包括很多这种博客然后我呢还是简单介绍一下了伊利亚SusKaver伊利亚SusKaver最有名的时间呢其实是他解估了Open-Eye的C.O山茅特曼然后呢在那一次正面中呢,他彻底出圈然后做一个科学家有一些这种执着然后对于这种技术安全的这么一个执着然后这是他最出圈的一个时间但在这之前呢,其实他已经有一些非常有名的一些work了然后首先呢是Alex Knight他在加拿大失从Jephrie Hinden然后Jephrie Hinden就是AI教父刚拿了诺贝尔物理学奖的那个人然后他跟这个他的师父还有Jephrie Hinden还有Alex一起发明了这个Alex Knight然后在这个比赛中取得了巨大突破,并且拉开第二名很多很多这种很多粉然后伊利亚呢跟他这个师兄和Jephrie Hinden就一起加入了Google然后也算是先起了深度学习的这一波浪潮然后就是然后他加入Google以后呢他就开始研究NRP自然语言处理他发明了Secrease to Secrease然后把RSTM推到了时代的最前一年然后他并且深度参与了AlfaGo的项目然后也是未列AlfaGo的维促不多的作者之一然后在这之后呢,他离职了然后去跟Sharm Automatically还有Alomask一起创立了OpenAI然后他作为首席科学家呢带领各种团队呢引领了Skelling Law引领了这种GPT大模型的这么一个研发然后就是他著名的去结构Sharm Automatically事情因为他现在比这种Sharm Automatically更注重安全然后他就发动了正面然后结构了Sharm Automatically尽管后续呢Sharm Automatically又重新掌权重新多权多回了OpenAI的控制权然后伊利亚也在这个事件之后不到一年就离职了然后他现在呢是开了一个新的公司叫做Safe Superinteligence然后他是本来继续当CSO但是在这个CIO被卖他重新挖兜以后他自己开始当任了CIO然后伊利亚呢可以说是在MLG呢计懂模型又懂Info并且是一种实战派比如说他不仅理论还有很多很多这种实战的Coding的这么一个能力然后他是幼冬软件儿幼冬硬件儿然后幼冬这种模型幼冬这种强化学习是一个全能型的天才然后我们今天呢就要讲他推荐的第一篇论文其实是一个伯克这个伯克的名字叫做这First Law of Complex of Dynamics也就是复杂动力学的第一法则然后这个文章讲的是怎样的一个事情呢就是说在一个封闭系统内我们通常会看到商是在不断增加的但是他的复杂性却经历了一个先增后检的一个过程然后我们看一个例子就是这个牛奶和咖啡红盒的例子我们在最左边的这张图可以看到牛奶在上面咖啡在下面在混合之前它的商也是非常的低也就是它的混乱度非常的低然后它的复杂性也非常的低它非常的简单然而在混合的中间状态就是它的商混乱度稍微增加了一点以后它的复杂度其实是非常高的因为我们可以看到中间的见面中间的这种锅柳这种复杂的结构然后最终就是混合好的一个状态它的商达到了最大尺但是它的复杂度似乎又降了下来因为它就是完全混合的一个状态没有非常复杂的结构也就是说我们在商混乱的程度一直在增加然而它的复杂性却经历了一个先增后检的一个过程我们直觉上可以看到这种很多的这种类似的这种拍摊包括宇宙就是大爆炸的时候非常简单最终宇宙热迹也非常的简单但是现在我们复杂的宇宙非常的复杂然后物理上就是说似乎我们能够感受到有一个更深层的物理规律像热力学第二定律一样有一个非常非常复杂的这么一种热力学热力终力学然后生质包括这种信息论人工智能都在这其中包含了一些角色然后我们之后可以聊到为什么这个这个复杂度跟信息论以及生物智能有着深刻的联系然后我们把这个问题算是正式化的用物理的角度来聊一下也就是说我们可以看这张处也就是说它的横着是时间它的重着是复杂度或者是这种伤也就是混乱度混乱层度根据说李学第二定律是不断增加的但是它的复杂度经历了一个先增后检的这么一个过程然后这个这件事情是有非常著名的物理理论学家上carl提出的它在一个FQXI conference中提到了这么一件事情这个FQXI是Fundational Question Institute然后这一篇会议是在一个从挪威开网各版哈奔的一个游轮上举行的也是非常的豪华然后它提出的这么一个理论其实是彻底上彻底改变了我们对于复杂度的这么一个理解尽管我们对复杂度还没有一个非常精确的定义但是直觉上告诉我们复杂度是似乎是经历了一个先增后检的这么一个过程然后我们在接触一下背景知识就是热力学第二定律这Second Law of Surnal Dynamic也就是商增定律然后它的一个封闭系统内的商是指踪不减的然后一个例子就是比如说你一个直觉的理解方式比如说你一个污字如果你不收拾就会越来越乱东西会不断的变复杂然后在信息论里就是说一个一串0或者一串1或者一串0101的组合也是低伤的一个状态然后如果它是随机的0和1的话它就变得伤的复杂度混乱程度就变得非常的高然后我们可以看到这种复杂度似乎是有一个叫做不同的这么一个Python因为完全随机的状态似乎也没有那么复杂然后如果一个污字它彻底的混乱了我们似乎觉得它也没有那么复杂因为它就是随机的嘛也就是说我们可以看到这种复杂度经历了先增后检的这么一个过程所以这里的核心理就是说是否有一个新的物理规律然后来统治了这么一个复杂度的这么一个进化的这么一个过程这么一个问题我们就需要一个精确的一个定义首先我们会讲这种复杂度有四个定义然后我们先也就是伤然后是科学复杂度然后是老练度然后是最终做者提出的这么一个Complac 照顾负杂伤的这么一个定义然后在这四个定义中我们会先讲到就是说为什么前三个定义都不够好就是关于复杂的定义为什么都不够好首先第一个就是说为什么不能只用伤来取代来代表这个复杂度我们刚才其实已经核心原因其实刚才你说到了就是因为伤的一直是在增加的然后他随着这种复杂度的增加我们想要看到一个地件的一个Pyton然后我们这个伤是到底是怎样的一个定义然后在波尔兹曼给了一个波尔兹曼上的一个定义也就是最常用的这么一个定义就是S等于KBLogMiga它给了这么一个公式KB也就是涂中的K就是一个日理学常数然后LogMiga就是它这种所拍的祖国这么一个过数我们不用去太深入地去理解这个公式但是直觉上就是说它通过这么一个公式然后能够测量这东西到底有多么混乱然后当它祖国混乱的话它的排列祖和各处就会多一点然后它的伤就会大一点然后它就是通过这么一个微观的尺度排列祖和的方法来定义了一个伤然后这里其实可以提到一下这种San No Anchipi也就是乡农伤也就是信息伤也就是物在我们进行信息压缩的时候如果它不能被压缩了它就是高伤的如果它可以被Significant的压缩然后0110101的这种祖国然后它的这种乡农伤其实就是很低的然后在这种数学上其实这两伤是等价的然后伤的具体就是我们提到的伤指增加的具体是怎样的一个特点的就是说在这种伤的视角里我们可以看到它不断的增加并且在这种Uniform Random也就是完全随意的这么一个状态然后它是看起来是更加的混乱的也就是说它对于我们想要的一些属性包括Stracher、Pitron甚至包括Meaning也就是结构模式还有意义都是没有一些理解的然后它反而给这种完全混乱的程度增加了很多的伤这个显然不符合我们想要的这种复杂度的定义就是说复杂度我们想要的复杂度跟这种混乱程度还是有很多的区别的因为我们想要这种复杂度在中间状态是更高的所以我们就需要另一个工具然后我们就会提到ComableColor of Complacity也是科式复杂度科式复杂度本日上就是这么一个定义它的定义是说Jandle Rate这么一个东西所需要的最短的一个程序的一个描述也就是说对于我们可以用一段自服券来描述这么一个我证比如说011010101然后我们就可以把它变成01成研也就是说我们用121成续就可以写出来这么一个自服券如果都是0的话当然就是0成研如果都是1的话就是1在写n次如果是完全随机的话它的也非常的复杂这个科式复杂度ComableColor of Complacity显然它跟N-TOP是非常的相似的它跟N-TOP也是伤有着一个共同的属性对于纯粹的噪音它是没法而完全的Complacity然后即使这样它其实引入了一个非常有趣的概念就是说它引入了一个Complacity的概念然后我们稍微介绍一下ComableColor of这个人科式复杂度的这么一个创立的这个人这个人其实是一个大的一个数学家它是比相农也就是信息论的开创者稍微晚了20年所以它的这种理论都是基于这种相农上的然后它这种ComableColor of复杂度它对复杂度有着非常非常深刻的理解然后推动了这种复杂度的发展我们后面提到的东西其实也是基于这种科式复杂度基于这种压缩的这种压缩其实就包含了这个算是图零机的这么一个设定然后你写一段程序来去抵犯这个技术然后生成一个这么一个系统所需要的最小的一个自负创然后也就是说像这种分型的图形其实在相农上的视角甚至是无限的然后但是在这种ComableColor of the复杂度的点点你可以写一个分型的一个程序就能创造出这种图形也就是说把它的压缩成了一个更短的自负创然后它对于这种复杂度深刻的理解包含了它提出了一个概念叫做Sophistication也就是老练度这么一个概念所以它这个老练度其实也是ComableColor of提出来的直觉上讲Sophistication就是说也是最短的程序能Describes such as a X这个X就是任何一个系统包括Stream 任何一个系统但是它家里怎么样一个设定呢它家的一个设定就是说我不一定要完全生成这么一个Element而是说这个元素属于一个更广阔的一个集合然后只需要生成这个集合中的一个元素就可以了然后这样的话它摆脱了紫阳的一个问题就是说当你Stream或者说是当你这个完全合合好的咖啡是完全随机的时候我不需要完全生成Exlex 里生成完全精确的一个匹配精确的一个负现我只需要生成一个负现它的这么一个我们有一个这种上币判别器就是这种Oracle能直接判断说你跟另一个元素同时属于这么一个集合然后我们就能定义这个负杂度我们就能定义这个你的Sophie Cation 就是你的生成这么一个东西的一个最胆程序也就是说我们对于一个非常非常简单的Stream就是Kamagaro 负杂度可是负杂度非常小的时候它的这么一个Stream也是非常短的我们保持了这么一个优秀的说性然后它最终非常负杂的时候也就是不是最终非常混乱的时候也就是说完全随机的时候它的这么一个Sophie Cation也是非常的低也就是说它在歧视状态和最终状态它的Sophie Cation 负杂度都是非常低的这个我们似乎看起来是非常的Promise 然后我们就是想要验证一下它的在中间状态的这么一个Sophie Cation是高的然后我们想要这么一个数据它是否满足了呢非常不幸的是它并不能满足一个非常厉害的数学家证明了这个Sophie Cation 无法超过Log T加 CC是一个场数Log T就是时间 也就是说我们有一个Hack我们有一个解 就是说我们直接从初是状态模拟 再加上Log Tstab我们就能得到这么一个Sampo它可以完全代表现在的状态然后它并且能可以Trick这种Auerlake 说我中间的牛奶半混合的状态也是可以与通过Log T加 C来描述的也就是说它的复杂度在中间状态也没有变得非常高所以它不符合我们想要的复杂度先增后检的这么一个属性但是没有关系它也算是带我们走了巨大的一步因为它满足了歧视状态和最终状态复杂度都非常低的属性只是它的中间状态也可以通过一个非常厉害的程序可以破解这个作者就是想在老链度的状态上再打一个不定它就是说这个真正的突破就是说要在这个程序中要设定这种Resource Bomb地的Zohr我们想要负现这个我们必须要Limit也就是限制它的资源因为你从一开始那个状态继续去跑这么一个模拟然后跑T步它其实是非常费资源的我们只需要再加上一个资源的限制它中间的这种复杂度就上来了然后我们再重新规划一下这个定义就是说我们这个复杂的这么一个定义也就是最短的程序能够抵抗这个Object是一个GinnexMember并且它是有限的资源然后它到底有怎样的Implication呢就是说它一开始的这么一个状态显然是低伤的并且是低复杂度的因为我们可以用一个非常简单的程序也就是出使状态比如说全是零这么一个状态去描述它最终状态也是非常简单的因为它是完全随机我们只需要说Random3跑就可以了只有中间状态它是非常复杂的然后我们可以想到它其实有集中方式来描述这么一个状态一种情况的就是说我们一个B提个Bit的描述0和1然后我们把整个的完整的状态给记住下来另一种方法就是说我们去软Simulation我们去跑一个实验然后跑到T部我们就得到这么一个状态然后我们回到咖啡和牛奶的例子出使状态我们可以明显地看到它最简单的描述方式是牛奶在前咖啡在后一个非常简单的程序最终的状态我们最好的描述就是用咖啡和牛奶1比1然后完全随机中间状态描述起来就非常的吹气了它有两种描述方式一种是用纯粹的比如说上面牛奶中间多少层是多少牛奶和咖啡的红和笔然后我们去一个进寺或者我们甚至取一个令人的关系但是这个仍然可能会就是说不能精确的描述状态然后我们可能需要更多的这种描述方式从描述时钟状态另一种状态我们就是取一个出使状态然后我们去跑一个Simulation然后它在跑T部以后我们就会得到类似近似于这么一个中间状态的状态然后也就说我们可以要么是把原本的描述变得非常的贵要么我们就是把资源费在跑实验的一个Compute上计算力上然后它的这么一个资源限制的老连度的这么一个状态显然你无法同时减少计算了减少它的计算的复杂度或者以及这个程序的复杂度你只能减少其中的一个然后我们的这么一个商的一个复杂度的定义是他们俩的贺也就是说在中间状态它的值就非常的高了它就完全符合了我们想要这么一个曲线然后这就是它想要的这么一个复杂商的这么一个定义也可以教复杂就是资源就是算力限制的复杂度算力限制的老连程度然后这里我们就要延伸一下就是说这个复杂商的这么一个定义到底跟大模型跟人物质能有怎样的关系为什么伊利亚会推荐这篇文章呢这是因为它把这个R&M的本质就看成了一个对于世界上所有信息的网络上所有信息的一个压缩然后伊利亚SusKWR和伊美达的CEO黄人熏真算黄有一个非常有名的一个桑残2023年他们就讨论了人工质能还有这种信息论的一个本质然后大模型的就是一个算是一个叫做复杂商的一个颠缝它的既不随机也不简单然后并且它很难继续压缩如果你继续压缩它就是有损的一个压缩然后它伊利亚就把整个的这么一个训练的一个过程看成了一个对信息的压缩的这个过程把它压缩进这么模型的权重之中然后为什么说它的复杂商无法再被减少呢就是因为如果你想要描述它的话你要么用零合一这种二斤制的数据去描述它你无法再压缩它了因为如果你随便改变其中的字节的话你会影响模型的性能然后或者另一个描述方法就是你把这个训练过程给描述出来然后这个显然是更无效的这么一个描述因为它这么一个训练过程的需要包含它的所有训练数据这种数据显然是消耗了更多的寸出空间更需要更多的Biz然后一亮就说到如果你想要把这些互联网的数据compress的非常的好的话压缩的非常好的话你就需要找到一些隐藏的一些秘密也就是你想最有效的压缩你反而需要最深的理解然后RM就是大模型的就是作为这么一个秘密的一个集合体然后它使得这种信心的无法被更多的被压缩了是因为它其实已经抓取到了这种数据上最本质的一个规律然后这里我会提出一个非常有趣的一个类比就是说大模型的其实更像是DNA也就是我们这个世界包括这种所有的生物呀都是互联网然后我们想要把比如说一个动物呀一只猫一只搞一个大枪呀给压缩呢其实我们只需要它的DNA我们就可以复刻这个动物尽管它不完全一样但我们能让它分辨出来它是大枪也就是说大模型在这里呢更像是DNA然后是最本质的这么一个区别最本质最核心的这么一个模式然后所有的这些动物呢包括我们生存的这种生态呢就是互联网然后它是其实呢可以被压缩的尽管它的伤呢是已经是比较比较的这种复杂度呢已经是比较高了然后如果是没有生命的一个星球呢它显然它的这么一个压缩的程度呢更高的因为你可以就是通过一些基础物质就能表述出来它而不需要DNA这么复杂的东西也就是一个星球如果毁灭了的话然后它也是完全的造生包括宇宙的这种热机状态呢它的复杂度也是非常的低的也不需要DNA这么复杂的这么一个事情去来描述它所以大模型就是像是我们就像是DNA互联网的就像是动物园这里有一个非常有意思的点就是人类人类到底能不能被压缩呢如果是原始人的话我们似乎觉得它更能够被压缩然而一个有思想的人它似乎就更难以被压缩了因为它其实会有一些更多加深刻的思想也就是说它脑海里还有一个模式世界模型它还有一些知识被引拜的了进它的大脑中也就是说它的大脑是最难被压缩的因为你很难完全复现出来就是说一个具体的人的一个大脑所以人的这种复杂商其实是更高的Compalachophyy更高这也是一个非常有趣的一个观察这就是相当于是说这种复杂商对于这种大模型的这么一个影响之后我们还有怎样的一个延伸呢就是说我们似乎发现了一个宇宙的新的物理规律也就是复杂商先增后检然后这个复杂度跟这种混乱度也就是伤似乎是不太一样的并且这个复杂商似乎是统一了就说在物理上包括基本机科学还有人工智能呀都是有影响的都是发现了统一样的规律然后它也算是对以后的这种发展的有着非常非常深远的一个影响然后而且最牛的就是说它是一个提出问题的人它并没有解决问题就是说这个上并没有去给出这么一个问题解因为我们都知道像这种科学研究最最最最牛的人都是初提的人也就是提出猜想的人包括但不限于这种费马大定理之前是费马猜想然后包括这种离曼猜想呀都是最牛的人都是提出猜想的人然后像这个复杂商的这么一个定义对于这种信息压缩呀包括这种GZB呀都是有非常深远的影响的然后它甚至可以牵扯到这种听机问题就是图灵机的这么一个计算机科学非常本质的一个问题然后还有最近的一些发展呢包括显示复杂都会就是Apparion的complacity也是最近的一些研究然后我们后续呢可以再给大家做延伸的一些阅读然后这一个热力学第一不叫热力学就是这么一个复杂动力学的第一定理仍然是一个conjecture 仍然是一个猜想然后它是一个开放式问题就说它还没有答案然后有兴趣的观众和读者可以继续去深入研究这个问题然后说不准下一个诺别这样就是你呢谢谢大家
