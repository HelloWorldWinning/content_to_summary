Timestamp: 2025-02-02T23:29:58.452598
Title: We’re releasing OpenAI o3-mini, the newest, most c
URL: Text file upload
Status: success
Duration: 0:00

Description:
好的，这是对您提供的文本的总结，包含所有要求的元素：

**1. 核心结论（Core Point）：**

OpenAI o3-mini 是一款在成本效益、速度和 STEM（科学、技术、工程、数学）推理能力方面优于 o1-mini 的新型小型推理模型。

**2. 根本结论（Fundamental Point）：**

o3-mini通过支持开发者功能和在多个领域（尤其是STEM）中提高性能，正在重新定义小型推理模型的能力。

**3. 总体框架（Overarching Framework）：**

本文主要介绍 OpenAI 新推出的 o3-mini 模型，从以下几个方面展开：

*   **模型概述：** o3-mini 的定位、主要特性和功能。
*   **开发者支持：** o3-mini 对开发者功能的支持，包括函数调用、结构化输出等。
*   **用户访问：** o3-mini 在不同用户群体中的可用性，以及对免费用户的支持。
*   **性能评估：** o3-mini 在 STEM 相关领域的性能评估，包括数学、科学、编码等。
*   **与其他模型的比较：** o3-mini 与 o1-mini 和 o1 的对比，包括性能、速度和准确性。
*   **用户反馈：** 人工评估和用户偏好结果。
*   **速度和效率：** o3-mini 在速度和效率方面的提升。

<Mermaid_Diagram>
graph LR
    subgraph OpenAI Model Evolution [OpenAI模型演进]
        A[o1-mini] -- 基础模型 --> B(o3-mini)
        B -- STEM推理优化, 更快, 更准确-->C(o3-mini-high)
    end
    
    subgraph o3-mini Features [o3-mini 特性]
        D[函数调用] --> B
        E[结构化输出] --> B
        F[开发者消息] --> B
        G[流式传输] --> B
        H[推理努力程度: 低/中/高] --> B
        I[搜索集成] --> B
    end

    subgraph o3-mini Performance [o3-mini 性能]
        J[STEM能力] --> K[数学]
        J --> L[科学]
        J --> M[编码]
        K --> N[AIME竞赛数学]
        L --> O[GPQA博士级科学]
        M --> P[Codeforces竞赛编码]
        M --> Q[SWE-bench软件工程]
        M --> R[LiveBench编码]
        S[通用知识] --> B
        T[用户偏好] --> B
    end
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
     style J fill:#9f9,stroke:#333,stroke-width:2px
    style K fill:#9ff,stroke:#333,stroke-width:2px
     style L fill:#9ff,stroke:#333,stroke-width:2px
     style M fill:#9ff,stroke:#333,stroke-width:2px
      style D fill:#ff9,stroke:#333,stroke-width:2px
     style E fill:#ff9,stroke:#333,stroke-width:2px
       style F fill:#ff9,stroke:#333,stroke-width:2px
    style G fill:#ff9,stroke:#333,stroke-width:2px
      style H fill:#ff9,stroke:#333,stroke-width:2px
        style I fill:#ff9,stroke:#333,stroke-width:2px
      style N fill:#aff,stroke:#333,stroke-width:2px
      style O fill:#aff,stroke:#333,stroke-width:2px
      style P fill:#aff,stroke:#333,stroke-width:2px
      style Q fill:#aff,stroke:#333,stroke-width:2px
      style R fill:#aff,stroke:#333,stroke-width:2px
      style S fill:#aff,stroke:#333,stroke-width:2px
      style T fill:#aff,stroke:#333,stroke-width:2px
</Mermaid_Diagram>


Content:
We’re releasing OpenAI o3-mini, the newest, most cost-efficient model in our reasoning series, available in both ChatGPT and the API today. Previewed in December 2024⁠, this powerful and fast model advances the boundaries of what small models can achieve, delivering exceptional STEM capabilities—with particular strength in science, math, and coding—all while maintaining the low cost and reduced latency of OpenAI o1-mini.

OpenAI o3-mini is our first small reasoning model that supports highly requested developer features including function calling⁠(opens in a new window), Structured Outputs⁠(opens in a new window), and developer messages⁠(opens in a new window), making it production-ready out of the gate. Like OpenAI o1-mini and OpenAI o1-preview, o3-mini will support streaming⁠(opens in a new window). Also, developers can choose between three reasoning effort⁠(opens in a new window) options—low, medium, and high—to optimize for their specific use cases. This flexibility allows o3-mini to “think harder” when tackling complex challenges or prioritize speed when latency is a concern. o3-mini does not support vision capabilities, so developers should continue using OpenAI o1 for visual reasoning tasks. o3-mini is rolling out in the Chat Completions API, Assistants API, and Batch API starting today to select developers in API usage tiers 3-5⁠(opens in a new window).

ChatGPT Plus, Team, and Pro users can access OpenAI o3-mini starting today, with Enterprise access coming in February. o3-mini will replace OpenAI o1-mini in the model picker, offering higher rate limits and lower latency, making it a compelling choice for coding, STEM, and logical problem-solving tasks. As part of this upgrade, we’re tripling the rate limit for Plus and Team users from 50 messages per day with o1-mini to 150 messages per day with o3-mini. Additionally, o3-mini now works with search to find up-to-date answers with links to relevant web sources. This is an early prototype as we work to integrate search across our reasoning models.

Starting today, free plan users can also try OpenAI o3-mini by selecting ‘Reason’ in the message composer or by regenerating a response. This marks the first time a reasoning model has been made available to free users in ChatGPT.

While OpenAI o1 remains our broader general knowledge reasoning model, OpenAI o3-mini provides a specialized alternative for technical domains requiring precision and speed. In ChatGPT, o3-mini uses medium reasoning effort to provide a balanced trade-off between speed and accuracy. All paid users will also have the option of selecting o3-mini-high in the model picker for a higher-intelligence version that takes a little longer to generate responses. Pro users will have unlimited access to both o3-mini and o3-mini-high.
Fast, powerful, and optimized for STEM reasoning
Similar to its OpenAI o1 predecessor, OpenAI o3-mini has been optimized for STEM reasoning. o3-mini with medium reasoning effort matches o1’s performance in math, coding, and science, while delivering faster responses. Evaluations by expert testers showed that o3-mini produces more accurate and clearer answers, with stronger reasoning abilities, than OpenAI o1-mini. Testers preferred o3-mini's responses to o1-mini 56% of the time and observed a 39% reduction in major errors on difficult real-world questions. With medium reasoning effort, o3-mini matches the performance of o1 on some of the most challenging reasoning and intelligence evaluations including AIME and GPQA.
Competition Math (AIME 2024)
The bar chart compares accuracy on AIME 2024 competition math questions across AI models. Older models (gray) score lower, while newer ones (yellow) improve. "o3-mini (high)" reaches the highest accuracy at 83.6%, showing significant progress.
Mathematics: With low reasoning effort, OpenAI o3-mini achieves comparable performance with OpenAI o1-mini, while with medium effort, o3-mini achieves comparable performance with o1. Meanwhile, with high reasoning effort, o3-mini outperforms both OpenAI o1-mini and OpenAI o1, where the gray shaded regions show the performance of majority vote (consensus) with 64 samples.
PhD-level Science Questions (GPQA Diamond)
The bar chart compares accuracy on PhD-level science questions (GPQA Diamond) across AI models. Older models (gray) perform lower, while newer ones (yellow) improve. "o3-mini (high)" reaches 77.0% accuracy, showing notable progress over earlier versions.
PhD-level science: On PhD-level biology, chemistry, and physics questions, with low reasoning effort, OpenAI o3-mini achieves performance above OpenAI o1-mini. With high effort, o3-mini achieves comparable performance with o1.
FrontierMath
A black grid with multiple rows and columns, separated by thin white lines, creating a structured and organized layout.
Research-level mathematics: OpenAI o3-mini with high reasoning performs better than its predecessor on FrontierMath. On FrontierMath, when prompted to use a Python tool, o3-mini with high reasoning effort solves over 32% of problems on the first attempt, including more than 28% of the challenging (T3) problems. These numbers are provisional, and the chart above shows performance without tools or a calculator.
Competition Code (Codeforces)
The bar chart compares Elo ratings on Codeforces competition coding tasks across AI models. Older models (gray) score lower, while newer ones (yellow) improve. "o3-mini (high)" reaches 2073 Elo, showing significant progress over previous versions.
Competition coding: On Codeforces competitive programming, OpenAI o3-mini achieves progressively higher Elo scores with increased reasoning effort, all outperforming o1-mini. With medium reasoning effort, it matches o1’s performance.
Software Engineering (SWE-bench Verified)
The bar chart compares accuracy on SWE-bench Verified software engineering tasks across AI models. Older models (gray) perform lower, while "o3-mini (high)" (yellow) achieves the highest accuracy at 48.9%, showing improvement over previous versions.
Software engineering: o3-mini is our highest performing released model on SWEbench-verified. For additional datapoints on SWE-bench Verified results with high reasoning effort, including with the open-source Agentless scaffold (39%) and an internal tools scaffold (61%), see our system card⁠⁠.
LiveBench Coding
The table compares AI models on coding tasks, showing performance metrics and evaluation scores. It highlights differences in accuracy and efficiency, with some models outperforming others in specific benchmarks.
LiveBench coding: OpenAI o3-mini surpasses o1-high even at medium reasoning effort, highlighting its efficiency in coding tasks. At high reasoning effort, o3-mini further extends its lead, achieving significantly stronger performance across key metrics.
General knowledge
The table titled "Category Evals" compares AI models across different evaluation categories, showing performance metrics. It highlights differences in accuracy, efficiency, and effectiveness, with some models outperforming others in specific tasks.
General knowledge: o3-mini outperforms o1-mini in knowledge evaluations across general knowledge domains.
Human Preference Evaluation
The chart compares win rates for STEM and non-STEM tasks across AI models. "o3_mini_v43_s960_j128" (yellow) outperforms "o1_mini_chatgpt" (red baseline) in both categories, with a higher win rate for STEM tasks.
The chart compares win rates under time constraints and major error rates across AI models. "o3_mini_v43_s960_j128" (yellow) outperforms "o1_mini_chatgpt" (red baseline) in win rate and significantly reduces major errors.
Human preference evaluation: Evaluations by external expert testers also show that OpenAI o3-mini produces more accurate and clearer answers, with stronger reasoning abilities than OpenAI o1-mini, especially for STEM. Testers preferred o3-mini's responses to o1-mini 56% of the time and observed a 39% reduction in major errors on difficult real-world questions.
Model speed and performance
With intelligence comparable to OpenAI o1, OpenAI o3-mini delivers faster performance and improved efficiency. Beyond the STEM evaluations highlighted above, o3-mini demonstrates superior results in additional math and factuality evaluations with medium reasoning effort. In A/B testing, o3-mini delivered responses 24% faster than o1-mini, with an average response time of 7.7 seconds compared to 10.16 seconds.
