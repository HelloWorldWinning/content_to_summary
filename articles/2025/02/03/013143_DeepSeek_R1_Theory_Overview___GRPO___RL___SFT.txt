Timestamp: 2025-02-03T01:31:43.932029
Title: DeepSeek R1 Theory Overview | GRPO + RL + SFT
URL: https://youtu.be/QdEuh2UVbu0?si=13X-0nYzp2axvr4n
Status: success
Duration: 25:35

Description:
**Summary:**

1.  **Introduction:**
    *   The video analyzes DeepSeek's R1 paper, focusing on how they developed reasoning models.
    *   The presenter uses a map to navigate the paper's complexity.
2.  **Model Overview:**
    *   The base model is DeepSeek V3.
    *   The goal is to generate three reasoning models: DeepSeek CAR1, DeepSeek CAR1 Distilled, and DEC CAR1.
3.  **DeepSeek CAR1:**
    *   It is a large reasoning model, achieving performance close to OpenAI's 01 model.
    *   It's better than the base DeepSeek V3 model, indicating the added reasoning capabilities.
    *   It's primarily achieved through post-training rather than pre-training, based on the pre-trained DeepSeek V3 model.
4.  **Reasoning-Oriented Reinforcement Learning:**
    *   The core method involves reinforcement learning, specifically using Group Relative Policy Optimization (GRPO).
    *   GRPO aims to teach the model reasoning without human feedback using carefully crafted reward functions.
    *   The reward function is rule-based and deterministic, focusing on accuracy and format.
    *   The model is prompted to use a "think" tag for reasoning, with Chain of Thought output within the tag.
5. **GRPO Details:**
    *  GRPO optimizes a complex formula involving the model's confidence in its outputs, normalized rewards, and a clipping mechanism to prevent overly chaotic steps.
    *  It also includes a Kullback-Leibler divergence term to prevent the model from diverging too far from the base model.
6. **DeepSeek CAR1 Refinement**
    * After initial reinforcement learning, the model tends to produce lengthy reasoning outputs.
    * Supervised fine-tuning is used to align the reasoning output for better human readability but at the cost of some performance.
    * Data generation and filtering: an intermediate model is used to generate reasoning prompts with a judging model to filter out problematic data.
7. **DeepSeek R1 Development:**
    * DeepSeek V3 is again used, this time, combined with more data for supervised fine-tuning followed by reinforcement learning.
    * Preference rewards (helpfulness and harmlessness) are incorporated into the reinforcement learning loop.
8.  **Distillation:**
    *   The large DeepSeek R1 model is used as a teacher to distill knowledge into smaller models (student models).
    *   Distillation is done using the full supervised data and the teacher model's log probabilities, leading to better performance than directly training the smaller models.

**Core Point:** DeepSeek developed a reasoning model (CAR1) comparable to OpenAIâ€™s 01 through reinforcement learning with a unique rule-based reward system and further refined it through supervised fine-tuning and distillation.

**Fundamental Point:** Reinforcement learning, combined with innovative techniques like GRPO and strategic data distillation, enables significant improvements in language model reasoning abilities without needing extensive human feedback.

**Overarching Framework:** The overarching framework is a multi-stage process that starts with a base model (DeepSeek V3), enhances its reasoning capability through reinforcement learning, refines it with supervised fine-tuning, and then distills the knowledge into smaller, more practical models.

<Mermaid_Diagram>
    graph LR
    subgraph DeepSeek V3 (Base Model)
        A[DeepSeek V3]:::base
    end

    subgraph Reasoning Model Development
        B[Reinforcement Learning with GRPO]:::rl
            B --> C{Rule-Based Reward System}:::reward
            C --> D[Accuracy, Format Rewards]
        B --> E[Prompt Format (Think Tag)]:::prompt
    
        E --> F[Chain of Thought Output]
    end

    subgraph DeepSeek CAR1 Refinement
        G[Supervised Fine-Tuning]:::sft
        G --> H[Chain of Thought Data Alignment]
            H --> I[Improved Human Readability]
                I --> K[Slightly Degraded Performance]
        J[Data Generation and Filtering]:::data
            J-->L{Judge Model (V3)}
             L-->M[Filtered Data]
    end
    
     subgraph DeepSeek R1 Development
        N[Supervised Fine-tuning]:::sft2
           N-->O[DeepSeek V3 Base Model]
            O-->P[Combined Fine-tuning Data]
        Q[Reinforcement Learning]:::rl2
             Q-->R[Preference Rewards(Helpfulness,Harmlessness)]

    end
    subgraph Distillation
        S[DeepSeek CAR1]:::car1
        T[Smaller Models (Student)]:::student
        S --> U[Distillation Process]
            U --> T
            T--> V[Improved Performance]

    end

    A --> B
    B --> G
    G-->J
    J-->N
     N-->Q
     Q-->S
    S-->U

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#cff,stroke:#333,stroke-width:2px
    style E fill:#9cf,stroke:#333,stroke-width:2px
    style G fill:#cfc,stroke:#333,stroke-width:2px
    style J fill:#9f9,stroke:#333,stroke-width:2px
    style N fill:#cff,stroke:#333,stroke-width:2px
    style Q fill:#ccf,stroke:#333,stroke-width:2px
    style S fill:#fcc,stroke:#333,stroke-width:2px
    style T fill:#ffc,stroke:#333,stroke-width:2px
    style U fill:#9cf,stroke:#333,stroke-width:2px

    classDef base fill:#f0f0f0,stroke:#333,stroke-width:2px
    classDef rl fill:#e0e0ff,stroke:#333,stroke-width:2px
        classDef reward fill:#e0ffff,stroke:#333,stroke-width:2px
    classDef prompt fill:#d0e0ff,stroke:#333,stroke-width:2px
      classDef sft fill:#f0fff0,stroke:#333,stroke-width:2px
    classDef data fill:#d0ffd0,stroke:#333,stroke-width:2px
       classDef car1 fill:#ffd0d0,stroke:#333,stroke-width:2px
     classDef student fill:#ffffd0,stroke:#333,stroke-width:2px

</Mermaid_Diagram>


Content:
hi everyone in today's video I wanted to check out quickly one of uh the new paper that has been put out by Deep seek so the R1 paper uh we're going to go through a walk through because it's a very interesting paper with a lot of like neat information uh the only issue is that um there's a lot of things going in parallel so I have a little map that I found out and I'm going to use this to uh walk through the paper and uh check out like all of the interesting bit so this way when you're going to actually read it you're going to be able to uh sit yourself a bit uh a bit more so if we just take a broad overview of what's going on we start with a base model here deep V3 base and the whole output is to generate these three things so deeps car10 deeps car1 distilled and DEC car1 these models are reasoning models um like uh the 01 model from open AI so if we dive into the result here uh you can see here deep c car 1 is this big blue bar and you have like these six Benchmark and the the tldr here is that it's very close to open AI 01 so close top one that you could argue that it kind of reproduced it a bit except on this one um but it's very close in in general right and it's better than like just the deeps V3 uh which is was the model that is based upon so the reasoning that we're adding on top of dcq tree is helping in all of these different complex benchmarks one thing to note here here is that this is mostly just doing posttraining it's not doing pre-training of it's using the pre-train model deep seek V3 which uh was uh recently put out right so like when you look around and see like uh the different tech company kind of freaking out about deep seek it's not like the deeps R1 that they're freaking out it's it's literally this one the deeps V3 which is used as the base uh reproducing the r10 result is not that uh complicated that much um it's this one that is a bit more complicated to reproduce um and where is um um a lot of the the hype is around so we're not going to go into deeps V3 but like know that this is a mixture of expert uh model um that is also open source let's start our um little um uh walk through this paper so we're going to start with a deep seek V3 base and then we're going to go over here right we're going to go into this little block and then we're going to do the reasoning oriented reinforcement learning right to learn uh reasoning not through like a supervised fine tuning uh with human feedback and whatever just with the reinforcement learning so just with this and proper reword functions the researchers at Deep seek managed to get this model to uh becoming a reasoning model on par with o1 this is what they did right so let's take a look at it there's two block that are important here there's uh the reward function that they're using right the reward system and then there's this thing the grpo that we were calling both of these things play together in order to make it so that we don't need a human in the loop or more data here to do what we need to do so the group relative uh policy optimization is not coming from this paper actually the idea came a bit earlier and this outo paper so deep seek math pushing the limit of mathematical reasoning and open language model so this was in April 2024 um and you see here deeps mat is uh better at that this particular mat Benchmark so I think that they SE seen what they did with group relative uh policy optimization on the mat Benchmark and then they were like hey we could do so much more uh with it so DC car10 was uh built out um we're not going to go into this paper either but like you know that this is not a new idea it's coming from here and then uh it's very useful so what's going on is this very huge uh formula um and um in ANL we're trying to do reinforcement learning and reinforcement learning is is quite simple is you have a an agent of some sort with some policy which is its parameter set of action that they can do a set of state and then the environment is giving it reward about like is it doing good or not and if it's going good the policy should be updated so that the agent can move through the environment and like get more of the reward and less of the bad stuff right right in this case what they're doing is they're trying to uh teach the model to uh uh reason through the task properly and then make it emerge so that like the performance is better that's what they're doing and then with the a certain format of answer to be uh put out remember this is deeps V3 so they're trying to make de deeps V3 better at reasoning very important they're not trying to make a model that is pre- from scratch to do reasoning trying to make the deeps V3 model base model be better at reasoning The Prompt that they're using is this thing right so blah blah blah conversation between user and assistant you see it's a very very tight prompt and then between uh here this is where it's saying like hey you should be doing reasoning process closing with in think and think here and then here and think reasoning process go here so they're telling the model to follow this format they're not going crazy in term of like restraining the model and like uh like uh manipulating it so it does that there is some reward function to make it stay there stay in this relative format in term of tank but this is where in the think that it will output this Chain of Thought um that it will feed back into itself with reasoning token we're going to go through the reward model and then we're going to go back to grpo um so here the important point in this paper is that it's a rule-based uh reward system so there's no critic model in this part uh in the other part there will be but in the r10 there is no critic model model that is happening so there's not another neural network that is trained to critic whatever the model this model is outputting it just deterministic uh reward uh functions it's only dist terministic stuff so here you have the accuracy reward on some of the benchmarks so for example for lead code problem there's like is it compiling if it's compiling cool you get the point is it like clearing up the test if it's clearing some test but not others you get some point so it's very deterministic and so this is where the the link to uh the reinforcement learning loop from the envirment is they have this for mat they have this for like uh the C the engineering things they have this for the coding stuff and then there's feedback loop from the RO system for the format and the main thing that they're trying to do is like keep your reasoning between the think uh tag um and that's kind of that like this is uh this is what's going on with um the reinforcement uh learning uh loop so we have this and then this is getting fed um to this uh formula uh that reward that we've saw previously is this thing it's the advantage um that we call in reinforcement learning and it's uh disadvantage is just like the rewards normalized basically with the mean card deviation so keep that in mind because when we're going to go over here we're trying to maximize U this whole Beast uh over here trying to get the biggest number uh here and then the policy see in in this setup is pi and you see here we have two Pi we have the old pi and then the current Pi so this is what we're the parameter that we're trying to uh change right now so in this Pi old is uh the parameter that LE led to the output that we're using at the moment so you see here we have these O's and then these Q so for a given question the model will output many output right they will they will it will shoot out many of its output and then these output will have some sort of confidence log probability associated with them these log probability are in here right so this if we summarize it it's like how confident are you about whatever you're outputting in term of string of text at the moment right and then this is um uh multiplied by the advantage here that we're calling which is the normalized reward so how confident are you what's the reward that you get and you're trying to maximize that technically right um you're doing a normalization here based on like what the old policy was doing and then this is what you're doing right now uh so you're trying to do better than the old policy if you're doing worse or you're less confident and whatever there's like a weight weight that is happening between these two so yeah so if you had only this it could work right but it's it's going to go all over the place so what what we're doing if we take the other um element over here you see we have a minimization over there between two things right between what we just discussed right so our confident times like the uh the actual reward from for like each of your answers and then this thing over here which is the clipping um the clipping means if you look at it it's exactly the same stuff right it's the same stuff as this yeah only also have the times the advantage except that you only are allowed to take like a small a certain a step of certain size in both Direction and this is a hyper parameter that you can tune if you want to but what this effectively means is like don't take too big of a step that's all right they're trying to limit the like the the the different movement that are too chaotic that the reinforcement learning can uh it can occur in a reinforcement learning agent this is the only thing that this thing is meaning the second part that you have over here is the KE Divergence over there and then if you look you have different stuff you have like the current model that you're trying to improve and then a ref model right and then you have the parameter beta which is like controlling how much of the K Divergence you want to um to uh take into consideration but the ref model here is a deeps V3 model in effect right K Divergence is like the distance between two distribution and then the Deep seek V3 three and then the this deep seek R1 that we're trying to to train should not be too far apart so you should not start to like train and change like uh your your your weights and your behavior too much that you're diverging uh very far away from uh the base model that's all it means so in a nutshell the DRP what it does is that it's um it's doing its work here on uh uh with the with the feedback from the environment and all this stuff and everything else here and here is just to make the model not go crazy too much that's all there is right so this is with respect to um the previous model to the base model and this is with respect to the previous old model don't move too far away from it h if you want to go a bit deeper on the grpo because this is like the key of this uh whole paper I suggest two video I'll link them them into the description so we're done with this part over here so we have our accuracy and our formatting so this is the path that we're uh choosing this path over here include that but is not present in uh in this uh particular case and that's kind of that just with reinforcement learning we get the following result we get deep seek r10 to uh be very close to uh1 mini And1 uh that number in uh all of the different Benchmark right and if you look at like the training step for this reinforcement learning uh loop uh we see that the performance goes from like really normal for like a a non- reasoning model to reasoning uh close to u to the different kind of o1 that we have here and even surpassing it at this point at like more than 8, 000 steps um so this is cool this is great the part that is very interesting is this curve over here right so what this curve shows is that as the training step increase the model is giving more and more lengthy reasoning response within its think t but in no moment did we uh force it to um to do that really it just learned that like through the reinforc M Loop if I increase the amount of text I have in my reasoning I'm getting better result and more rewards the only part really where we're incentivizing it to do that is here in the prompt and it's not too crazy we're just asking it to reason and then it's doing it stuff to reason here so if you take a a small look at like what type of response you're and you can tried this by the way with AMA on your own computer you can say like hey here is my question and you can see like the the T response um you can see like here it's trying to do its best and then at some point it's like hey wait wait there's AA moment I can flag it figured out something it go back into its step and then like he try to figure out afterwards so there's something bigger that is emerging from the reinforcement learning loop on this deeps V3 base model that was already trained right so it's as as if like reasoning was already there in um the Deep seek V3 kind of big huge model with six 100 like billion parameter and then the reinforcement learning learning Loop kind of like Juiced it up so that like that reasoning was more at the Forefront and now like since it's at the Forefront uh you get like this type of uh uh more in intricate uh Behavior so this is good this is very interesting because you don't get that with 01 so so open AI decided to hide the Chain of Thought even though like it could be useful for you like to see like the reasoning of the model it's a choice that they that they've made um I think it's a good business choice because like you could just technically use a one to disal the Chain of Thought and like generate a data set that will help you create your own reasoning model so o1 will not be that useful as we going to see in the distillation aspect um that part uh makes small model very very good so we have that the issue that uh is happening with the deeps car1 is zero model right at this point we we have a train DC r10 model is that it's very difficult to read the chain of of uh thought prompt it's not made for you right it's it's made to maximize the reinforcement learning Loop that we have which is very interesting like there's um it's using its token to kind of the reasoning token to get to the result and uh sometime it's mixing different languages it does doesn't matter for it if it's like say in English or like I say in in Chinese or or whatever what they taught at this point is that hey if we are uh going to um align it a bit better the reasoning step will be more coherent for people they will be able to see it better and technically if we gear it properly the performance should be even better because it will it will have less of a random walk to do in reinforcement learning land before finding like the finding the reasoning for real this this is why we're over here they they did at the first supervise fine tuning right with some cold start uh uh Chain of Thought data right so they had this data and then they didn't say how many sample they said thousands I don't know if it's like n 99, 000 or just one um so they did that and then they Ed supervised fine-tuning to fine-tune this model before doing the reinforcement uh learning uh thing that we're discussing so at this point they uh this this supervised fine tuning with this data set and then they do uh this what we call reasoning oriented RL with like grpo but they adding a Chain of Thought language consisten consistency reward so they're forcing the model to not do this language mixing thing because we can't understand it there's an interesting uh tidbit of information here if you see here um ablation experiment show that such alignment result in slight degradation in the model performance this reward align with human preference making being more readable um very cool it means that like the model was better when you could mix the language and stuff and when we forc it to not mix the language it was slightly worse uh than before then you have a whole bunch of accuracy right uh reward accuracy of different things so the format so the language consistency you have like the deterministic accuracy about like did I get the code wrong or like did I get the the math problem right like all of this H is fed into like the feedback loop with the the reward now bab because at this point there's multiple model that comes in at when we're out of this we don't have deep seek r10 you can forget about it doesn't exist anymore right we have this intermediate model deep seek V3 base uh that was trained with reinforcement learning and with the supervis fine tuning we have that right what we're doing is we're going to use that model to generate some reasoning prompt it will generate the data at this point in time the its only value is to generate data right to get to the other model we're going to do something else we're going to start with deeps V3 base over here and then we're going to go all the way down here for a supervised fine tuning step and then again a reinforcement Loop here but using like the data that we're using so right now we're on making data land we're not into training anything right we're using that model intermedi model to uh create some reasoning prompt and then there's another model that we're not seeing here which is this guy again it's this dude deeps V Tre and then de deeps V3 is acting as the judge so in this step right it's reasoning data and then uh before with deeps r10 it was just like a deterministic reasoning data here they're adding other sort of reasoning data using the judge kind of setup where you have like another model that is able to give you feedback about like your the data that is being generated plus they're filtering out some data that they is out of whack with like mixed language long paragraph code blocks um yeah so do then you have a data set that you can use for doing supervis fine twing like we did in the previous step for this particular model over here right so that thing that we're doing we're try going to try to do it again but with more data effectively right we're going to try to do it again over here so they do that and they also try to generate non- reasoning data for like writing QA self conition translation and then they still use deep cv3 to generate these and um yeah then they try to generate the um some uh Chain of Thought data for for some stuff but this is not that important so at the end of the day you get like 600 and then 200 uh K uh data set that you can combine so here you have like the deeps V3 uh some leftover data that like they could use for supervisor fine tuning and then you have a V3 over here that is using like Chain of Thought prompting in order to generate some non- reasoning data over here so 200k sample over there 600k sample over there you add them up and then you have this combined um uh supervis vun data we're we're there at this point now what they're doing is doing reinforcement learning on all of this stuff right and to do that I think um see here for reasoning data we Adder to the metal G line in deeps R1 zero which uh use rule base reward so for some of the set they're using this and then for General data we Restort to reward model to capture human preference in complex and Nuance scenario so there's this going on and then they're adding two things to the reinforcement learning so they're adding helpfulness and then harmlessness so helpfulness is like isy output helpful for the user right there's some sort of like a metric that you can get out of this and harmlessness is where you can end do some biases on the model to not talk about certain thing right yeah this is this is what's Happening Here the harmlessness actually will tank a bit the performance and some of the Benchmark because the model will not be able to Output the information that deep in its weight it has um because it it followed and then the weights are set up in a certain way to not output these things so yeah so this is this part so at this point we're here so there's this combine uh uh supervis F tuning data you uh do supervise fine tuning on the deeps V3 base over here right you do that over there Boop with the 800k sample and once you supervised fine tune after that use uh reinforcement learning here so this is where we're doing the preference reward that we just talked about and through this Loop which is the same as the deeps R1 um zero over there but with like not just rule based stuff you get deeps R1 that's that's how we got the stuff so if we look at the result here for d seek uh r one uh it's actually good like it's not too bad you see we're over there it's doing good in like English it's going good in code it's doing good here in matte it's do good in Chinese um I believe like it could have done better here but it was nerfed um yeah so overall it's it's pretty it's pretty good it's on par with open AI o1 the cool thing that they' done after that is that they um use all of these uh model that are much smaller and and um what they did is that they took all of the uh uh supervised fine tuning data that they had and they did supervised fine tuning on these right they did two things actually they tried to do that with the full reinforcement learning Loop right like so this whole Pipeline with the smaller model and it didn't go that well we're going to see in a minute but like when you use like all of the data based on this and then you uh using this for uh distillation so taking this big R1 model in in the Loop to do the uh the distillation of that model into that uh you get a much much better uh performance uh with uh with these model um so distillation is is not too complicated you have the teacher model over here right and then like you have the log probability you have access to the log probability of like the output right of the model as the input are passing and you have a much smaller model and then the model the smaller model so this is a student so the student has access to like uh the input output right throughout this but it also has access to the log probability on the big one and then they they try to mimic that also right so this makes the student uh learn much faster distillation is a very like a a standard process in deep learning so with this right with this data that was made with this whole Pipeline and then deep C car1 dis solation you get this T power one distill into quen or Lama and then different B that you have and then these you can go right now in amaama like download some some of them and start to use use them you can use like the 1. 5b over here um it will work in most um in most setup and here is the result where uh they tried to um do the uh r10 on quen directly just quen and then distill from R1 and as you can see the distl is better and uh it just it just seems like the uh the reinforcement L Pipeline on a smaller model model is not like uh as effective as doing it on the much much huge models because remember DC car1 was made on the 600 billion parameters of V Tre right um so it seems like there's like there's the big model right and then when you're doing reinforcement learning it's like you're you're try you're making like the reasoning core pop off a bit and now it's doing better when you have a smaller model it seems like this reasoning core is not popping off in a meaningful manner or some sort like don't quote me on any of this that I just said but the feeling is uh seems to be like this so that's it so this is the paper uh it's pretty cool pretty cool paper I suggest you to read it with a map next to you because you're going to need that uh but overall like uh the core of the of the paper is really that part like grpo right I suggest you to check out the two videos I'm going to link it to the description they explain it much better than me um so if you get that you're going to understand like what the reinforcement L Loop is actually doing understand also it's like rule base reward right so this explains some of the speed up that they had over here so I hope you enjoy the video thank you for watching leave a like if uh you find this useful so also big thanks to the subscriber that I'll also sponsor I really appreciate the support thank you very much so if you have any question don't hesitate to leave a comment I'm really here to help and have a great week everyone [Music] [Music]
