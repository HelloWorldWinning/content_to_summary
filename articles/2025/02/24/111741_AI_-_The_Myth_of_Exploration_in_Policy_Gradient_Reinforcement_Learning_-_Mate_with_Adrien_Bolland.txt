Timestamp: 2025-02-24T11:17:41.712086
Title: AI - The Myth of Exploration in Policy Gradient Reinforcement Learning - Mate with Adrien Bolland
URL: https://youtube.com/watch?v=nqp1LUaFecQ&si=WYOtItEwkH8NQqqo
Status: success
Duration: 1:07:13

Description:
## Summary of "The Myth of Exploration in Policy Gradient"

**1. Introduction:**

*   Reinforcement Learning (RL) aims to optimize decisions in an environment via an agent interacting through actions, state transitions, and rewards.
*   Policy Gradient methods directly optimize a parameterized policy (e.g., neural network) to maximize the expected discounted sum of rewards.
*   A common observation is that policies tend to become overly deterministic during optimization, hindering exploration and leading to sub-optimal solutions.

**2. Exploration Bonuses (Intrinsic Rewards):**

*   To maintain policy stochasticity, a surrogate objective function is often used: Original Return + Intrinsic Reward Bonuses.
*   Two main categories of intrinsic rewards:
    *   **Uncertainty-based:** Penalize or incentivize actions/states based on prediction errors of a learned model (e.g., penalizing where a model predicts poorly).
    *   **Entropy-based:** Encourage diverse actions and state visitations by adding the entropy of the policy or state occupancy measure to the reward.

**3. Decoherence Criterion & Pseudo-Concavity:**

*   **Decoherence Criterion:** Measures the sub-optimality gap between the optimal policy of the true return and the optimal policy of the learning objective (true return + intrinsic reward), ensuring that optimizing the surrogate objective yields a policy close to the true optimum.
*   **Pseudo-Concavity:** Requires the learning objective to have a unique global maximum, guaranteeing convergence towards the global optimum using stochastic gradient ascent.
*   Illustrative example with a driving problem shows that specific intrinsic rewards (state visitation entropy) can satisfy both criteria for specific weights, while others (action entropy) may not.

**4. Stochastic Approximation & Efficiency:**

*   Even with a pseudo-concave and coherent learning objective, optimization can be difficult due to using stochastic gradient approximations.
*   Intrinsic rewards can be easily optimized, leading to convergence towards the maximum of the intrinsic return rather than the true return.
*   **Efficiency Criterion:** Defines the probability that a gradient ascent step improves the learning objective. An exploration strategy is delta-efficient if this probability is greater than delta.
*   **Attraction Criterion:** Specifies that in a region around the global maximum of the learning objective (containing the maximum of the intrinsic return), the probability of improving the true return should be sufficiently high.
*   Maze environment example demonstrates how the choice of intrinsic reward affects the probability of improvement and highlights the interplay between efficiency and attraction criteria.

**5. Conclusion:**

*   Exploration terms (intrinsic rewards) are proxies for achieving a good learning objective, not necessarily for exploration itself. Reward shaping, if feasible, could serve a similar purpose.
*   Entropy bonuses have good smoothing properties.
*   More research is needed to better understand exploration, especially distinguishing the effects on the learning objective vs. the gradient estimate.

**Core Point:** Exploration bonuses, while often added to encourage stochasticity in policy gradient methods, should be carefully chosen and tuned to ensure that the resulting surrogate objective is both coherent (leading to a near-optimal policy) and efficiently optimizable (ensuring gradient updates effectively improve the return).

**Overarching Framework:** The content presents an analytical framework based on decoherence, pseudo-concavity, efficiency, and attraction criteria to understand and evaluate the impact of exploration bonuses on policy gradient algorithms, highlighting potential pitfalls and offering insights for better design and implementation.

<Mermaid_Diagram>
graph LR
    subgraph RL Fundamentals
    A[Environment] --> B(Agent)
    B --> C{Actions}
    C --> A
    A --> D[States]
    D --> B
    A --> E[Rewards]
    E --> B
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#fff,stroke:#333,stroke-width:2px
    style D fill:#fff,stroke:#333,stroke-width:2px
    style E fill:#fff,stroke:#333,stroke-width:2px
    end

    subgraph Policy Gradient
    F[Policy Parameterization] --> G{Policy (Ï€)}
    G --> C
    G --> D
    H[Maximize Expected Return] --> G
    style F fill:#ccf,stroke:#333,stroke-width:2px
    style G fill:#afa,stroke:#333,stroke-width:2px
    style H fill:#ccf,stroke:#333,stroke-width:2px
    end

    subgraph Exploration Challenges
    I["Policies Become Deterministic"] -- Hinders Exploration --> H
    style I fill:#fcc,stroke:#333,stroke-width:2px
    end

    subgraph Intrinsic Rewards
    J[Surrogate Objective] -- Combats Determinism --> H
    J --> K{Original Return}
    J --> L{Intrinsic Reward Bonuses}
    style J fill:#ccf,stroke:#333,stroke-width:2px
    style K fill:#afa,stroke:#333,stroke-width:2px
    style L fill:#afa,stroke:#333,stroke-width:2px
    L --> M[Uncertainty-Based]
    L --> N[Entropy-Based]
    style M fill:#fff,stroke:#333,stroke-width:2px
    style N fill:#fff,stroke:#333,stroke-width:2px
    end

    subgraph Evaluation Criteria
    O[Decoherence Criterion] -- Ensures Near-Optimal Policy --> J
    P[Pseudo-Concavity] -- Guarantees Convergence --> J
    Q[Efficiency Criterion] -- High Probability of Improvement --> J
    R[Attraction Criterion] -- Attracts to Optimal Return --> J
    style O fill:#fff,stroke:#333,stroke-width:2px
    style P fill:#fff,stroke:#333,stroke-width:2px
    style Q fill:#fff,stroke:#333,stroke-width:2px
    style R fill:#fff,stroke:#333,stroke-width:2px
    end

    subgraph Takeaways
    S[Exploration as Proxies] -- Not Necessarily Exploration Itself
    T[Entropy Bonuses Smooth]
    U[Need to Distinguish Effects]
    style S fill:#ffc,stroke:#333,stroke-width:2px
    style T fill:#ffc,stroke:#333,stroke-width:2px
    style U fill:#ffc,stroke:#333,stroke-width:2px
    end

    A --> F
    I --> J
</Mermaid_Diagram>


Content:
welcome back everyone to another episode um this episode we have Adrien uh who is from the University of Le Adrien will be talking to us about the myth of exploration in policy gradient such an intriguing title so Adrian we're very excited to have you with us uh the floor is yours and uh please start go ahead yes thank you for the invitation so indeed the title sounds a bit intriguing and what we're going to discuss is the fluence of adding intrinsic reward bonuses when you do a policy grance algorithm so to start a bit with introduction I guess you have seen this figure a thousand time but reinforcement learning is about making decision when you have an environment in which you can apply actions that will make evolve a state and provide reward through the evolution of of the states and we call the the a agent the the thing that is going to take this action depending on the the observation of the state it's Gathering through time so one interesting thing about LL is that it require an oracle model so meaning that you have to be able to sample from something or to to have a a data set that you have built at some point and even though the optimization might may take a lot of time in fact the execution may be fast because you have your policy that is some one to one mapping from the state to the actions so some quick notations we will call S the state space a the action space P0 and P for the transition and the initial distribution of the the marov chain that is built and row is the reward function and we are looking at marov stochastic policies so policies that take as input a state the current state in fact of the system and provide the distribution over the the action space and for the sake of Simplicity we will only look at continuous cases so we will work on the density of the of the policy of the distribution of the actions and so in policy gradient or in direct policy optimization direct policy search what we try to maximize is the infinite discounted sum of rewards That You observe so we generate trajectories from State Z as Z to the infinity and we want to maximize the discounted sum of rewards which we can write as the expectation of the reward function on expectation over this distribution which is called the occupancy the state occupancy measure which represents basically some kind of average of state that is going to be visited Through Time I know that there is already a first typer here there is a factor 1 over 1 minus gamma that is that is missing and so in policy gradients what we do is we parameterize the policy with typically a neural network and we perform stochastic gradient assent on this objective function so there is one typical observation when we do policy gradient is that if we want the the algorithm to converge the policy shall remain sufficiently stochastic during the optimization procedure so typically if at Point the distribution of actions over the actions evolves towards something that is direct over one action well it's main stop working and so what we see is that in several optimization procedure if we simply maximize this previe previous objective what may happen is that it converges towards something that is locally Optimum because uh it has become too deterministic to to explore as it is usually called but we are going to see that basically it's more like an artifact of the optimization function that you you work on yes come again sorry yeah so this is interesting what you say so you're saying that as we optimize the policy becomes more deterministic right yes this is so this is not theoretical stuff it's more of observation and so typically so the intuition behind that is that usually if you are at some bad points in your policy space well and you have a lot of variants while reducing the variant usually will improve the performance of the policy so typically if you're doing very bad actions it's better to to do something that is less bad more often and so this can happen by reducing the the stochasticity of the policy so the variance when it's parameterized by a neural network and so at the end of the day you end up with something that is very deterministic and that is in a kind of local Optimum I see okay and and in terms of what you just said about the intuition would this also hold if I have sparse reward set up because how do I know that kind of locally uh my action is bad you know if I just get zero reward most of the time well this is a good question so we are going in fact to cover The Spar rewards problem a bit later and what we may see is that there are two things to distinguish which are the local Optimum in term of the objective function itself and then the fictive local Optimum if I may call them like that just that result from having a bad estimate of the the gradient so when you perform Monte Carlo estimation for getting an estimate you may have something that is usually zero that's because it's a SP problem and so it May indicate that you are in a local Optimum or that the saddle point or basically you have a zero gradient but it's a statistical problem it's not a real estimate so we're going to distinguish these two effects okay okay okay cool thanks and so in practice in policy gradients in order to to keep the policy sufficiently stochastic in order to implement these exploration stuff what we do is to optimize the surrogate objective and so this surrogate may look may look like that so you have basically this expectation so here the factor is is not missing anymore and so you optimize basically the expectation of the reward function so it's the same plus so K intrinsic reward bonuses so basically K additional reward bonuses that are designed in order to increase the stochasticity of the policy and so how are these reward bonuses or intrinsic reward exploration bonuses selected well there are two classes of reward bonuses the first is to have a model that will basically provides you some kind of uncertainty over the model so basically if you have for example a neural network that you have trained for predicting the next state well what you can do is to have an uh to during the imization penalized basically if you are going to have a bad prediction and so what you are going to do is to enforce going to states where your model is not well uh fitted in fact and so these are the exploration bonuses that are based on prediction error as I call them and the second class of uh uh intrinsic bonuses that you may add are entropy based so typically so the most classical I guess is the entropy of the policy that you add to your uh to your objective function which basically can be written as having a reward bonuses that is proportional to minus the log of the probability of the action in the current state and uh there is also the same for the state occupancy measure so you can also say Okay what is the log of the probity to have a state when that I have observed in this trajectory and usually we don't look at the whole state but at some feature of that state so this is the second type of intrinsic reward bonuses which are called entropy based in opposition to the uncertainty based which were this first kind when you have a model and you want to penalize or to enforce the the errors of the the between the model you have train and between what you truly observe and so I distinguish two terms which is the return expected return and the intrinsic return as I call it in in the paper which is basically due simply to the linearity of the of the expectation and you see that it's basically the sum of the expectation of rewards function so you may think of it as one is the true return and the other is the intrinsic return so what you you enforce with these exporation terms and so the thing is we know optimize the surrogate learning objective but what we want is that the policy that we get when optimizing this surrogate objective function assuming that it is possible and easy to optimize it we want it to be close to the original optimal policy basically if you here you add some reward that completely enforce behaviors that are not optimal what is going to happen at the end of the day that you're going to converge towards a policy that is just of poor quality compared to the initial measure which was the return of the policy and so the first research question was about is it possible to get condition that link the learning objective function and the return such that when we optimize the learning objective we get some policy or we would expect to get a policy that is of some good quality and so the first thing that's I'm going to to look at so maybe a side note is that we optimize this learning objective with stochastic radiant ascent and stochastic radient asent so in general if we forget that we are in an in mdp and stuff like that well it's guaranteed to converge towards a local maximum under some condition and if the function is pseudo concave well we know the pseudo concave is some generalization of concavity which means that basically there is a single local maximum well then we are guaranteed to converge toward This Global maximum which is the the only points where the gradient is going to be zero and So based on these two dis intuition we can derive a first condition which I call the decoherence Criterion and I said that a learning objective L is Epsilon coherent if and only if the return of a policy maximizing the return minus the return of a policy maximizing the learning objective is less or equal to obsin so I optimize one policy maximizing the return assuming that it's possible to do so so I look at the best policy maximizing the return and the best policy maximizing the learning objective and I look basically at the suboptimality gap between these two policies and I say it's Epsilon coherent if it bounded by Epsilon and so basically if Epsilon is equal to zero it mean that maximizing the learning objective will provide you an optimal policy and the learning objective is uh is the the one with the return plus the intrinsic motivation or which one is the learning exactly exactly okay so here this Theta C let's call it that like that is the parameter that maximizes the return plus the intrinsic return okay I see so so this is a measure of comparing how good the optimal policy or the policy under Theta star from the true return versus the policy from the return plus intrinsic motivation so to say exactly okay okay it means that assuming that you can maximize the learning objective to to the global maximum well if the coherence Criterion is respected for a given Epsilon well then you know that this policy is at most sub optimal by Epsilon okay understood and and when you say l uh oh Epsilon coherent is just this meaning that this Epsilon is the uh difference between the two right uh why there is no absolute value or something like that like couldn't this be negative for example J Pi uh what do you call that pi plus pi Theta no because so you could say okay there is the absolute value but this is always positive because the first J Pi Theta star is the maximum okay I understand yeah so it's evaluated in the AR Max so it's the Max and so when you subtract some thingal is going to be positive whatever so you don't need to to to take an absolute value yeah and so the thing now let's let us assume that we have this Epsilon cerence the question is under what condition could we find this U this P Theta cross that maximizes the the learning objective well we can think again about the two condition I gave for stochastic gradient asent to converge and well basically the function must be pseudo concave so it means that a learning objective must have a single Global maximum and so I say that the function is pseudo concave if there exists a unique so this exist with this weird exist there is mean there exist a unique Theta cross where the gradient is equal to zero and at that point while it's equal to the maximum of the function l so the learning objective that you maximize and so if this this Criterion is respected what you can expect is that stasy gradient Ascent is going to converge towards the global maximum of the learning objective and so if this learning objective is furthermore Epsilon coherent well then you can say that you are able to maximize or you should be able to maximize the learning objective and this Max is going to provide you a good policy in term of return so if these two criteria makes sense I'll just show you a quick example and so basically we are going to consider U sorry Adrian yeah sorry to interrupt you again but so uh are you going to show later why the l uh that you consider which is return Plus in intrinsic ISO concave I'm going to show you now that it is indeed the case okay and the why well this I don't have the answer I see okay okay no problem so here I say that if it is true well then it will work and what I'm going to show you is that on a simple environment where we have nonlinear Dynamics while adding the entropy may help and then we can discuss a bit why it helps in this case and for what kind of system is going to help but I believe it's very complex to a prior say where it's going to help and where it's not going to help in in a very general case at least and so this this toy example is basically a kind of driving problem where you have a a valley that looks like this so like the blue curve and you have a car that you want to to bring to the Target position and you start at the in initial position X initial and so the thing is that if you want to go to the Target position you have to accelerate to climb up this cliff and then to go to go back into the valley to the Target position and what I do is that I provide a reward to the agent that is proportional to its depth in the uh in the valley and so basically initially it has a reward that is going to be one because the depth is minus one then when it climbs the the valley it's going to provide zero reward and finally you get rewards of two when you are close to the Target so it's proportional to minus the the depth in the in in the in the environment in the in the hill and um for representation purpose I'm going to consider a very simple policy which is this policy here Pi GP K Sigma which says that I have a normal distribution over the action and the mean is K times the distance to the Target position and it has a constant Co variance of Sigma variance in this case and so I have two parameters K which is the the mean if I may say parameter and sigma which is the variance parameter and what I consider are the two intrinsic reward we saw at the beginning which is the log of the probity of a feature of the state and so here we consider the log of the prity to be at a certain position so X in the in the valley and we also consider a second intrinsic reward bonus which is the probability so minus the log of the probability of the action given in the state and so the learning objective is the expected over this policy and state distribution that it creates of the reward so minus the depth in the policy plus the waiting of these two intrinsic reward bonuses namely the uh minus log visitation priority of the position and the L quity of the action provided with the state and so what I'm going to show you here are six plots and in each plot I consider different values for the Lambda one and Lambda two so the weights of the intrinsic reward and I plot basically the heat map of the learning objective so the darker the higher the learning objective and there are two additional information this green area which represent the area in which the uh learning the different the learning objective is at most Epsilon so here Epsilon is one of one smaller than the return so basically if you have a policy that is within this area you know that you are suboptimal by at most one in term of expected return and the black dot is the global maximum of the learning objective and the gray dot is if any the um local Optimum so if I take the first picture here the a I've taken a waiting of 0. 05 for the uh minus Lo probability of the state visitation and what I see is that the learning objective has two local Optimum one close to zero and one close to minus one for k all having zero standard deviation and in addition I see that the global maximum of the learning objective is within a range of one so this policy corresponds to a policy that is at most to Optimal by one so it's in the green area so what I can conclude is that here I am Epsilon coherent for Epsilon equal to one because this Global maximum is within the green area but I'm not U pseudo concave because I see that I have two local maximum so this learning objective respects the first criteria but not the second and if I look at the extreme most figure so F here I look at at another combination of the weights so I give a lot of weight to the in return corresonding to the probity of actions and what I see is that here I have a single global maximum but that corresponds to a policy with a high variance and so it is Epsilon it's not Epsilon coherent for EPS equal to one but it is Pudo concave so we already see that there is some tradeoff so some for some values of Weights Lambda I have one uh criteria respected for some not so if I go back to a b and c figures so these present uh three weights so I have put zero for the uh prity of actions but I have a value that is small and becomes larger 01 and one for the waiting of the probity of the state visitation and what I see is that when I have a large weight I lose the Epsilon coherence but I gain some pseudo concavity but if I have a good intermediate value well for B you can see that you have a single Global maximum for the learning objective close to minus one and close enough meaning that it is within the range for the Epsilon coherence for Epsilon equal to one and so when you add this exploration bonus based on the state visitation entropy it is possible for a well tuned value of Lambda to get the two object the two criteria are respected and so you may assume that it is going to be easy to to optimize but when you look at the other entring reward bonus so the probity of the action which is basically the the most standardly used exploration bonus so we usually see in the algorithm that it's necessary to add the the entropy of the of the action which is going to be equal to the intrinsic return corresponding to this intrinsic reward but you see here that it's impossible to get whatever the value of Lambda you choose both respected at the same time so it's possible for some case to have one of the two criteria the other and sometimes depending on the intrinsic reward that you consider it is possible to have both that are respected or n and so the the first observation is that within this kind of simple environment it's we can already illustrate that sometimes the respected and sometimes not which may explain why U these exploration reward bonuses work in this cases does that make sense because I know it's a lot of information and figures at once uh yeah I think yeah I think it's clear so okay may be some intuition about why uh adding this exploration reward bonuses is going to be good if I go back to a figure so to the previous figure this U the prity to be in a state is going to be uh large when you have a policy that travels uh along the the whole valley and so for a large poity I mean a spread distribution rather so you're going to to have a spread distribution when you have a policy that is going to be optimal and so we kind of see that it is possible to maximize a bit if I may say the entropy of the state visitation and get an optimal policy on the other hand when you have a policy that does a bit of random you have a low priority to reach this uh low point and so this is the reason why the entropy bonus based on the state visitation is going to be good and why the entropy bonus based on the the action priority is not going to be good and so we may think that the story is um basically over so when you have these two criteria the learning objective is going to to have good properties and so we optimize it and it's okay it's working but in fact this when we assume this is true if we had access to the to the real gradient of the learning objective but but we don't because we build stochastic approximations at one at the first hand and also we so addition to building stochastic approximation we build a very bad stochastic approximation because we have to neglect some partial derivative so typically we neglect the influence of the U the uh depend dep of theta in the reward in the intrinsic reward because I haven't written it explicitly but the intrinsic reward now depends on Theta because probity the state visitation probity depends on the policy and obviously the probity of an action depends on the policy to so it depends on the parameter Theta and this dependency is is basically never captured when we do a policy gradient so we going to optimize this function so even this pretty looking function but with arguably poor stochastic approximation of the gradient and so just to summarize a bit what we have done so far so we've seen that there was some kind of trade-off between the two previous criteria and one way to to balance this trade-off maybe to to schedule the weights so for those of you who have implemented the policy gradients you know that sometimes it's interesting to Discount the entropy penalization um and maybe the most interesting point is that when we interpret entropy bonuses like this well it's absolutely not the same role as in value based errl in value based errl you know that it's exploration but because you have to to keep your policy sufficiently stochastic it is not really a question of stochasticity but it's more a question of learning objective and indeed the smoothing effect of entropy regularization so policy entropy regularization has been long known or known I don't know but recent work have studied this very well at least depending on how it modifies the the optimization procedure and in particular there are several works that show that it is linked to to robust optimization and so the story is not over over as I said because even pseudo concave and coherent learning objective function can be in fact very hard to optimize and this is due to the fact that we use stochastic approximation of the gradients and here what we want to to look at is if we can build once again some condition such that we expect the stochastic gradient Ascent to be be fast enough when we optimize it and by fast enough so I'm going to Define it I'm going to say that it's going to be fast enough if it has a high priority to improve the learning objective so if one stochastic gradient asense step has a high probity to the learning objective so once again I start with a quick reminder of optimization so the Improvement of the learning objective when we perform a an Ascent step in the update Direction hat D is provided by this random variable X which is equal to the difference of f theta plus the update in that step plus Alpha at D minus F of theta so this Alpha is the learning rate and this to the first order is equal to Alpha times the scalar product between the update Direction so the stochastic gradient estimate and the true gradient of the function f and so in practice when we uh look at asymptotic convergence what is common is to compute the expectation or try to bounce the expectation of this random variable and then we know that one Ascent step will on expectation update by that uh Valu so this Bounds at most and then we can uh get um convergence guarantees but this is very complex to do and so what I'm going to do is to look at the probability that this random variable is positive mean the probability that we have a policy Improvement so that one step of updating the policy in that direction uh is going to improve the policy and so there are two way of interpreting that the first is to say okay in fact the conversion is going to so what each asent step is going to improve by a constant value the function be it positive or negative and so for having a large convergence guarantee we must have a large priy of improvement or the second way of seeing it is to say okay what I'm going to what I don't want is that the Astic convergence is driven by events that have a low priority so you know that so it's going it's exactly what is happening when you have sparse reward environment on expectation you have a positive gradient but in fact this expectation is positive simply due to the fact that that you have the probity over the probity of the lucky event of a correct update is small but it has a high value and so an expectation is positive it's high and so we look at the priority that this random variable is positive so the prity that we improve the function f and here the function f is going to be the learning objective and so the remaining slides is about Computing or illustrating criteria on the priority that a gradient asent step is going to improve the learning objective or improve the return may be easier if we directly Drive dive into the the Criterion so I say that an exploration strategy is Delta efficient if and only if the probability that a gradient asend step in the direction at d improves the learning objective and that this probility is greater than Delta and so this I say it like that for almost all Theta the probity of random variable D positive is larger than Delta and this the random variable is the scalar product between the the direction in which we we perform a step and the true gradient of the learning objective and so if this is large it means that very often when we perform an Ascent using Direction D on the learning objective is going to improve the learning objective typically if D is equal to one It means that each stochastic gradient asent step improves the learning objective uh Adrian sorry can you can you go back one one slide please yes so so can you explain again uh please about what is be hat in your case like in in our in our setup of RL what would this vad be because I thought that would be the gradient uh uh of the it is not the gradient it's the stochastic estimation of this gradient I see rainforce typically it's the rainforce estimate if we perform okay and and and your definition in the second slide is just saying that if my uh estimation is correlated or if it has an angle uh correlated with the true gradient end right uh then I will get an efficient exploration strategy right exactly and by efficient it means that stepping in that direction will improve the learning objective with high probability yeah yeah with yeah so you know that there is noise on this hatd so This reinforce estimate and that in practice sometimes you perform several estim several update in the good direction and several in the bad Direction due to that noise to that distribution over reforce estimator in a general case Ascent estimate I see so so in other words this gradian Theta is like assuming you can compute the Oracle expectation yes and B hat is just like a montic sample or mono estimate yes exactly following on policy I guess right yeah yes yes everything is on policy okay okay yeah okay got you okay and so we can expect if this Delta is large that the St the Monte Carlo gradient Ascent is going to be very fast because we will very often improve the learning objective so here in practice you may not look at the greater than zero but greater than some threshold in order to avoid that you have reached some Plateau for example yeah okay so this is interesting but there is a last criteria Criterion that is probably much less intuitive which is the following and so I may be going to first introduce the intuition and then to to look at this Criterion and so the intuition is that in practice it is very easy to compute good estimate of this intrinsic return so it is very easy to have estimate of the intrinsic return that provides you with a good direction and on the other hand it may be very difficult to compute estimates that will improve the return and so if you perform stochastic gradient Ascent in that case what you may have at the end of the day is that you converge towards the global maximum of the inic return and not of the policy of the return of the policy because you will see that there is no improvement of the policy because this uh uh efficiency for the policy in some sense is going to be low but on the other hand but since it is easy to optimize the intrinsic return well you still see an improvement of the learning objective but simply because you are improving one of the two terms and what you want is that when you perform this optimization well um you want at some some point to start optimizing also the return and so what you may say the first idea is to say okay what I want is some equivalent of the efficiency Criterion that would say that okay some point the update hat D is going to provide me a good high priority to improve the return in addition to improving the the the intrinsic return and the learning objective but this is uh again a bit naive because you don't want this to be high everywhere what you want is that once you have reached the global maximum with high prity in some sense of the intrinsic return then you want the uh probity of improving the return to be sufficiently high and so by this I mean that we want for a bow that is centered in the global maximum of the learning objective and that contains the maximum of the intrinsic return we want in this region of the parameter space that the probability of improving the return following this asend Step at D is efficient energ so all of this Barbarian notation is basically telling you that in some region of the space if I follow the ascent direction direction hat D so the reinforcement the learning objective well I also improve with probity Delta the the return of the policy and this region of the space is not randomly chosen but is the one that first contains the uh true maximum of the learning objective and also the maximum of the inic return which is the point toward which you subject to converge if you have a low prity to improve the the return everywhere else right so could you could you remind us of what was theta plus Theta dagger what what was that yes Theta dagger is the AR Max of the learning objective of L Theta right so so you're saying okay you take the the learning you take Theta dagger right then then you define a ball around it right this is B um and Theta dagger is the is the return plus the intrinsic part right so that's the return plus the intrinsic part that's what it optimizes right yes right and now and now oh just consider and then consider that Theta intrinsic is within that ball right uh and then and then you're going to say that the attraction criteria is that if I start from there I will actually get attracted get attracted to maximizing the true return objective exactly so it's like a it's like a stability criter in a way no yes yeah okay okay so is it was basically the intuition between this attraction name is that stochastic gradient estimate are going you know to provide you trajectories that are attracted toward some kind of stable point with high priority which is the global maximum yes and so this is someh so the first criteria was the equivalent so for the whole state space you want trajectories to be attracted to the global maximum of gradient of L of L Max in fact sorry okay and one last question so maybe it's a naive question but why is Theta intrinsic belonging to uh to the ball around Theta the dagger or is it because I can grow the ball radius as much as I want really no so this ball so why should Theta in be within this B is the intuition that I said that as it is easy to compute gradient estimate that will improve the intrinsic return you are likely to converge towards that point or to be attracted to that point when the efficiency Criterion is respected okay okay so if so if it's not really a guarantee but intuitively efficiency Criterion say okay you are likely to go towards the intrinsic Max or towards the global Max but you want to be attracted toward the global Max and so you have to to guarante this attraction in addition thanks and so this may sound a bit like two criteria that falls a bit out of nowhere but I'm going to illustrate on a simple example again that in fact these two criteria are respected when you when you work on a simple environment and so this simple environment is a is a maze a maze environment but that is simplified to to a point that basically it's simply a horizontal Corridor where you start at left and you want to reach the the the point that the the highest tile and so they are s tile and you have two actions left or right and then you move in the in the corridor like that and so you have a priority to stay idle when you perform an action and so basically you have to be optimal to perform always action I want to go to right and then you will reach as fast as possible the target position where you have a positive reward while it is zero everywhere else and so the thing is that you already see it that if your policy have a low priority to go to the right you will never observe a positive reward and so the policy that we consider here is a very simple one it is one parameter Theta that gives you the proportion of action that are going to to to the right and so if Theta is equal to one you are going with one to the right and then well uh you going to go as fast through the horizontal Corridor and so again I consider the two intrinsic return that we previously discussed so which is going to be the visitation priority to be in a state or in one tile and the priority of the action uh that ised by the previous policy and so here I represent in Black in figure a the return of this policy you see indeed that for small values of theta you have zero values so you don't observe you it's basically not zero But You observe a very very small return and when the probility is high While You observe a high return that cumulates when you reach the terminal State and I also represent the uh entropy basically so the expected uh logity so the this intrinsic return in red for the prity of actions so that is the entropy of the policy basically is going to be maximum in 0. 5 so a uniform policy and minimum for policies that go always way to the left or all way to the right because they are deterministic it's a bit more complex when you look at the priority to be in a certain state so it's going to be maximized around 0. 7 and it increases a bit from 0 to 07 and then it decreases again and so this is a weird effect of the gamma that is going to wait this distribution and then if I sum both intrinsic both the intrinsic return and the return I get the learning objective in the right figure so in Black again the return of the policy in red the return plus entropy of the actions and in blue the return plus the entropy of the state visitation measure and what you see is that the three functions are pseudo concave so basically if you have the true gradient you can always go in the right direction and they all have their Global maximum and unique maximum to in Theta equal to one so basically with respect to the two previous criteria there is no re reason to prefer the one or the other learning objective because they all respect Epsilon coherence for equal zero and they are all pseudo concave and so what I'm now going to look at is to look at the PRI of improvement and so here it's a very simple environment so the poity of improvement is simply the poity that the derivative of the learning objective is positive derivative so it's no longer a gradient because there is a single parameter and so this is this figure and so in Black I have the probity that the rainforce gradient Monte Carlo gradient of the return is positive in red the Monte Carlo estimate is taken for the uh learning objective with the entropy of the policy and in blue the learning objective takes into account the entropy of the state and so there are two two things that are interesting to observe first is that indeed when you look at the reinforce estimate of the return for small parameter Theta it has a POC of improvement that is zero so if you start with a polic that is U that is a low probity to go to the right you may optimize as long as you want you will never reach the optimal policy because you're in a sparse environment you never see the reward and so it doesn't work on the the other hand when you look at the learning objective taking into account the entropy of the policy so the red what you see is that when you have an entropy that is a parameter that goes often to the left so a small parameter Theta the probity of improvement is equal to one and this can be understood because basically the return has no influence on the learning objective because the probity of improving it is going to be zero whatever and so what you compute when you perform rainforce updates is to compute the estimate of the gradient of the entropy which is very easy to compute and provides you a direction that leads to increasing the parameter Theta going towards 0. 5 the maximum which also happens to be a direction in which you uh increase the return and so for that region of the space for zero up to around 0. 4 well you are efficient with Delta equal to one because you perform policy Improvement at every step and what is interesting is that when you come around the global maximum of the entropy of the policy well you have a sudden drop of the efficiency so meaning that you have a much lower probability of improving the return and why is that it's because basically the rainforce estimate of the uh the the intrinsic return so the entropy provides you a gradient that goes backward so if you want to maximize the entropy you have to reduce Theta so basically there is no longer this collaboration between maximizing the return and maximizing the entropy and so here the second criteria comes uh into account because we are typically in the region where we have a ball so is an interval that contains the global maximum so Theta equal to 1 and the maximum of the intrinsic return so Theta equal do five and you can say that in that interval well we are lucky enough to have an efficiency that is about 0. two and so it means that there is still some probability that you improve the return and what you see is that the farther away you go from the uh the uh the local the global maximum of the entropy of the policy while the higher the probability of improving the return return and this simply comes from the fact that you see so many times the target position when we have a Theta that is close to one that is going to dominate with high priority in some sense the estimate of the of the gradient of the uh the entropy of the policy so here we can see the interaction of the two criteria that are needed if you want to to have a good convergence and interest interestingly if you look at the entropy of the state visitation well it is almost uh always large but it's never equal to one so what it means is that first as it is always large it can be assumed to be a good uh exploration bonus considering that you want to have high priority of improvement but yet you never reach this one and this is likely to to the fact that in order to estimate this distribution you have to use a more complex model so you have to build the model over the state distribution which here is done using some uh uh what we call it a histogram sorry and so the histogram comes with an approximation and so this approximation makes it such that sometimes you don't perform improvements and so here interestingly is that the uh second criteria is not really necessary so you see that indeed you are still sufficiently efficient you don't have this drop of efficiency when you reach the global maximum of the of the uh INF return which is likely due to the fact that at this point you already see very often the target position so you have a parameter Theta that is sufficiently large such that what dominates in this interval is the influence of the return and the inic return is somehow less influent dominates less the learning objective and so interestingly so this is the conclusion slide we have seen that uh exploration has two effects one on the learning objective on expectation and one on the uh on the estimate of the gradient and how it is going to influence the optimization but probably the most important is to remember that these exploration term are only proxies so to to have a good learning objective it's not basically you can have anything else it does need to uh to be really exploration uh based in some sense you can simply do reward shipping if it's possible and use the same criteria because simply these are proxies that appear to have good properties but it's not objective is not to explore it's just to have a good learning objective and uh well interestingly for the simple example that I have presented entropy bonus have good smoothing properties and if you go looking in the article you performed extended experimented experiments on more complex environments and we see that again entropy bonus in particular the state visitation copy is is pretty good for guaranteeing that the four criteria works and uh I'm convinced that exploration is not that well understood in particular well usually we don't distinguish between the two effect I've never seen a paper that we say okay here we deal with spse environment so we look at the estimate and here we look at environment with complex Dynamics so we look at the return function this I've never seen so I believe that the there is a need to if I may do the joke to explore in that direction and U I truly think that there are steps to go to to to get with formal proof so no I've introduced four criteria but in fact there is no real theoretical guarantee behind that it's more based on the intuition and observation and I think that building up that could help alleviate some some common folklore as I like to call it in in the literature so if there have question feel free to to ask thank you thank you so much Adrian this was this was very interesting actually so so your work is more let's say on the analysis fundamental level right trying to fundamentally understand what is going on you know when we do these exploration uh bonuses inside our L which I think is pretty amazing and we we need to do more of that research so well done I think this is awesome um so I just want to ask you a question uh in in general so um so so this has been let's say an analysis paper maybe right like trying to do some definitions and trying to look for them in experiments right um do you think in the future this could be transformed into a design paper where where we actually when we are optimizing NRL uh we actually do constraints like the ones you mentioned uh uh in order to have a better exploring policies well yes I think what would be so the criteria as such are basically not possible to use in practice right because you have to get this Oracle of the gradient or of the learning objective which you don't but what would be nice is to say okay if I use this exploration term and maybe with this schedule on the parameter Lambda that then you have some guarantee that at some point the criteria are respected so this could be but I guess that is going to be very dependent on the environment right because you can assume that for some environment having a stochastic policy is a bad ID while you may typically if you initialize directly your policy on the optimal value when all these exploration is going to be unnecessary and it may drive you away from the from the solution okay so this would be nice to have so an exploration that guarantees the but to do to do all right I see got it and U do you plan to have a broader set of experiments like Beyond uh the setups you considered uh because I understand this is fundamental work so it means you know to start somewhere uh but do you believe in the future you want to try this on deep RL settings for example with the I don't know like complex domain or something like that so one limitation is that you have to select your environment such that well you can somehow look at these criteria and so if the environment is too complex well you lose everything and what I've been doing in the appendex of the paper is I took a policy that was a neural network of several headden layers and what I look is that I optimize the policy I get a trajectory of parameters and I look at the criteria over this trajectory and so basically over the parameter that so basically over a trajectory in the in the parameter space I can somehow analyze the criteria and look at okay is the prity of improvement sufficiently large for this trajectory and stuff like that and so this I've already done every we generalizing towards deep deepl but still the environment was sufficiently simple such that I could basically get rid of some problem that arise due to the optimization for example you need a model of the state visitation distribution which in practice is absolutely not trivial to get right I see okay um yeah and one last question for me so I do see that you've been trying to understand RL you know in this perspective but do you believe we could use your Technique to actually design better optimization algorithms um because because you know I I saw this uh like two two local Minima function you consider it right so uh so so so that's that's interesting because because you kind of are designing a method which allows you to you know avoid this local B bad local minimum and go to the better local minimum right um do you think criteria like this would actually be useful in analyzing like non-convex optimization problems in general this yes I believe that maybe it will not help to build new optimization algorithm but provided with one optimization algorithm I think that the analysis generalizes to any any any problem so many problems I would rather say where you optimize a surate learning objective so typically a lower bound or upper bound if you you maximize or minimize of your true objective function or if you have a stochastic estimate of the gradient which is usually the case in in machine learning so yes and interestingly for example the criteria do not use the assumption that we have a sum of return so basically you can use it for any surrogate in already and so we don't really use neither the fact that the uh the the return has some structure so it's likely to generalize to some extent to to other stochastic optimization problems with a surrogate learning object with theate learning objective yes very interesting yeah yeah that is that would be cool actually oh nice yeah um so uh philipos Alex mat do you have any questions hey and yeah I do have some questions than thank you for the talk thank you was very interesting I was wondering how would how did you model the state visitation so I understand that in the last experiment it's histogram and so in both experiment is in histogram because in the first we only look at the position so it's unid dimensional and in the second experiment we uh look at the the tile so it's also un dimensional and so in both case I used an histogram that was built by extensive sampling so I sample a lot of trajectories to get the histogram and then I can estimate the rest and in the extended experiment I did I used the Goan mixture models and in practice you can have have a neural network that you optimize in parallel to your policy to fit this distribution does it answer a bit the question but in the case of the neuron Network what would be the objective you mean for the U the uh learning the state visitation yeah well uh what I did was naive so I simply looked at the trajectories that was that were sampled for building the gradient estimate and I fitted uh a Goan mixture on that using ES so probably the most real thing you can do but if you want to have something that is a bit more intelligent you can look at the gamma models it's called like that I believe that were developed by Sergey leine team so they have a neural network and they do basically some kind of Monte Carlo estimate with a bootstrap to to learn this distribution with another neural network that is that takes us input the state and provides you the the likelihood I see you and just to be sure in the in the one slide before this one in the clubs you you add here you add the two Criterion with a weight of one right for the acceleration yes so basically it's a we it's not a weight of one but here so this is the entropy weighted by I believe 0. 1 but it's simply the sum of the return and the inic return which are both presented on the figure but if you look at the definition here the intrinsic return already where was it here the inic return captures the weight so what I plot is the inic return so weight counted yeah okay I see and you mentioned also that the maximum of your J Criterion is always positive I don't understand one I'm missing the like so the come again the maximum of of your J CR so the maximum of your crit oops so here know it was it was before it was a question yeah so if you go if you go back to where you defined this Epsilon what do you call it coherence yes so I think mat is trying to understand why J Pi is is always positive right it's not always positive well because you uh this is a maximum function minus something else so it can only be yeah yeah okay I okay thank you thank you much that's all on my side thank you for the questions y thank you mat yeah any other questions if not I will have one more but let's see Alex philipos any questions on your side thank you but nice talk thank you thank you all right cool okay uh right so so one last question maybe not so related to the specific uh talk here uh but we're asking everybody who's giving us a talk to tell us more about how their interview process has been going right because we know that how the review process sorry has been going uh so we know that uh a lot of people are complaining about the review in the machine learning community and what has been your experience with that and maybe if you have any ideas to improve uh that would also be cool so maybe you can say a word or two on on how your experience was interesting question so uh this paper was uh rejected at I clear so you can see the review online so go go ahead and what you will see is that the it was a typical problem where there were not enough experiments and so fair enough at some point because indeed we were lacking some interesting experiment that we did and I believe that now we have enough evidence but the question is always since I'm in a problem as I told you that I have to keep the exp under control to be able to show things that uh I hope I won't get the same remarks if he submitted and we'll see but indeed I think that uh there is a it's always complex when you do papers that are a bit in between something that is very experimental and something that is very theoretical because it's a bit none of both world and sometimes you get hard reviews yeah no how to improve that well it's a good question I'm a naive PhD student so I'm not sure I can give something very okay okay well that's a very political answer I see um no no okay yeah that makes sense however I would definitely urge reviewers to consider papers like this more importantly because they're like they're really important analysis papers and I think there isn't enough of them right there's more like I have this new algorithm look at my method you know but rather than why things work I think this is important so so yeah that's a good point and uh yeah it's hard to get these papers accept it however yeah I'm sure you'll get it in it's it's actually awesome analysis paper I think yeah thank you awesome so Adrien I want to thank you again for coming and talking to us in this episode this was really really exciting uh my send my sincerest greetings to deia and uh and I know this is deia style so I'm sure he likes what what you're looking into so so yeah so thank you again and uh yeah thanks everybody for joining us this has been our Uh current episode of me with the researcher and I will see you next time so thank you D again
