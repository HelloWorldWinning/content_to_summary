Timestamp: 2025-02-24T09:54:43.868051
Title: Step-by-Step AUTOENCODERS: The ONLY Tutorial You Need !
URL: https://youtube.com/watch?v=HcDIU_YqwVo&si=OKxkEB6LDK0HjBJP
Status: success
Duration: 1:08:09

Description:
好的，我将用简体中文对内容进行总结，并提供大纲、核心观点、框架和 Mermaid 流程图。

**总结：**

**一、Autoencoder 简介**

   *   **定义:** 是一种神经网络，用于编码和解码特定项目。
   *   **目标:** 使输出尽可能接近原始输入。

**二、Autoencoder 架构**

   *   **组成:**
        *   **编码器 (Encoder):** 将高维输入数据（如图像）压缩成低维嵌入向量（也称上下文向量或隐藏表示，记作 Z）。
        *   **解码器 (Decoder):** 将嵌入向量解压缩回原始域。
   *   **损失:** 编码和解码过程中存在信息损失，表现为重建损失。损失大小取决于上下文向量的维度：
        *   维度越高，损失越小。
        *   维度越低，损失越大。
   *   **潜在空间 (Latent Space):** 在潜在空间中选取一个点，通过解码器可以生成新的图像。解码器学习了如何将潜在空间中的点转换为可用的图像。

**三、Autoencoder 的设计与训练 (Keras 实现)**

   *   **数据集:** Fashion MNIST (灰度服装图像，28x28 像素)。
   *   **预处理:** 缩放像素值到 0-1 之间，并将图像填充到 32x32 像素。
   *   **编码器设计:** 3 个卷积层，步长为 2，用于减小输出尺寸。
   *   **解码器设计:** 编码器的镜像，使用转置卷积层。
   *   **训练过程:**
        *   使用 Adam 优化器和二元交叉熵损失函数。
        *   使用回调函数保存模型检查点和 TensorBoard 日志。

**四、Autoencoder 的应用与局限**

   *   **应用:** 生成新的图像，图像压缩。
   *   **局限:**
        1.  潜在空间中不同类别区域大小不均，导致采样时容易偏向某些类别。
        2.  难以确定潜在空间中随机点的选取方法。
        3.  潜在空间存在空白区域，对应于训练集中未充分表示的图像。

**核心观点：** Autoencoder 通过编码器将高维数据压缩到低维潜在空间，再通过解码器重构数据，从而学习数据的潜在表示，可用于生成新数据，但其性能受潜在空间分布的影响。

**Overarching Framework:** The overarching framework of this content is a comprehensive tutorial on autoencoders, starting from its definition and architecture, going through the implementation details using Keras, and finally analyzing its applications and limitations.

**Mermaid 流程图:**

<Mermaid_Diagram>
graph LR
    subgraph Data Preparation
        A[Fashion MNIST Dataset] --> B(Preprocessing: Scaling & Padding)
        style A fill:#f9f,stroke:#333,stroke-width:2px
        style B fill:#ccf,stroke:#333,stroke-width:2px
    end

    subgraph Encoder
        C[Input Layer (32x32x1)] --> D{Convolutional Layer 1}
        D --> E{Convolutional Layer 2}
        E --> F{Convolutional Layer 3}
        F --> G[Flatten Layer]
        G --> H(Dense Layer: Embedding)
        style C fill:#ffc,stroke:#333,stroke-width:2px
        style D fill:#cff,stroke:#333,stroke-width:2px
        style E fill:#cff,stroke:#333,stroke-width:2px
        style F fill:#cff,stroke:#333,stroke-width:2px
        style G fill:#ccf,stroke:#333,stroke-width:2px
        style H fill:#ffc,stroke:#333,stroke-width:2px
    end

    subgraph Decoder
        I[Input Layer (Embedding)] --> J{Dense Layer: Reshape}
        J --> K{Conv2D Transpose Layer 1}
        K --> L{Conv2D Transpose Layer 2}
        L --> M{Conv2D Transpose Layer 3}
        M --> N[Output Layer (32x32x1)]
        style I fill:#ffc,stroke:#333,stroke-width:2px
        style J fill:#ccf,stroke:#333,stroke-width:2px
        style K fill:#cff,stroke:#333,stroke-width:2px
        style L fill:#cff,stroke:#333,stroke-width:2px
        style M fill:#cff,stroke:#333,stroke-width:2px
        style N fill:#ffc,stroke:#333,stroke-width:2px
    end

    O[Training: Adam Optimizer, Binary Crossentropy Loss]
    P[Latent Space Visualization]
    Q[Image Reconstruction]

    H --> I
    B --> C
    N --> Q
    H --> P

    style O fill:#afa,stroke:#333,stroke-width:2px
    style P fill:#aaf,stroke:#333,stroke-width:2px
    style Q fill:#aaf,stroke:#333,stroke-width:2px

    O --> H
    O --> N

    subgraph Limitations
    R[Uneven Category Representation] --> S(Sampling Bias)
    T[Undefined Latent Space Distribution]
    U[Gaps in Latent Space]

    style R fill:#fcc,stroke:#333,stroke-width:2px
    style T fill:#fcc,stroke:#333,stroke-width:2px
    style U fill:#fcc,stroke:#333,stroke-width:2px
    end

    Q --> R
    Q --> T
    Q --> U
</Mermaid_Diagram>

希望这个总结对您有所帮助！


Content:
hello everyone welcome to data fish analytics in the previous video we learned about generative AI model families today we will be learning everything about an interesting and important generative AI topic which is called as autoencoders Auto encoders are a type of approximate density functions so let's learn Auto encoders today so in this video we will explore architecture of autoencoder learn how to design and train Auto encoder right from scratch using Keras we will unlock Auto encoders creative potential in generating new images and also we will examine their limitations reminding us that even in the world of AI there are boundaries to explore let's get to know the basics of Auto encoder an auto encoder is simply a neural network that is trained to perform the task of encoding and decoding a particular item now this output from an auto encoder should be as close to the original item as possible now let me explain you these two points with the help of an architecture diagram of an auto encoder so this is the entire architecture diagram of autoencoder now Auto encoder is mainly made up of two components which you can see on the screen the first important component is encoder and second important component is decoder an encoder Network compresses High dimensional input data such as image into a lower dimensional embedding vector or we call it as a context vector or even some people call it as hidden representation this Vector is represented as Z in in this diagram a decoder Network decompresses this embedding or this context Vector back to this original domain now you might have observed that output image is slightly blurred now this is to depict the loss of information during encoding and decoding well it also depends upon the dimensionality of context Vector higher the dimensionality of context Vector less will be the loss lower the dimensionality of context Vector higher will be the loss this loss is about the Reconstruction loss I am talking about now this context vector or embedding vector or this Z is an interesting component or it is an interesting part now remember this the concept is that if you pick up a spot or a point in the latent space or hidden space we can create new images by running this point through the decoder now this works because the decoder has learned how to turn points in the latent space you into usable images now in order to implement it let's design our encoder decoder architecture the input image that we will be dealing with while implementing station will be of resolution 32 cross 32 it will have one channel because it will be black and white images this encoder will consist of three convolutional layers and we will be using a stride of two to half the size of output each layer now this decoder is the mirror image of this encoder instead of convolutional layers we will use convolutional transpose layers and finally the output image that we will be getting from this encoded decoder architecture or from this Auto encoder would be 32 cross 32 the resolution will be 32 cross 32. now let's start with implementation of this encoded recorder architecture let's start implementation of Auto encoders in Python I will be using Google collab to implement this before implementation I will set my runtime to some GPU so T4 GPU is available okay so for this implementation we will be using fashion mnisd dataset now this data set is a collection of grayscale images of clothing items and each image is of size 28 crores 28. let's start this implementation by importing few libraries which we require so first library that we require is numpy so import numpy as NP second Library we require to display images or to plot images import matplotlib then we require tensorflow Keras so these are all the libraries which we require for today's implementation of Auto encoders the first step is data preparation this data set which we discussed is fashion mnisd dataset comes pre-packaged with tensorflow so it can be downloaded by using this line so explain y train then test data X test y test equal to we will use the data sets module datasets Dot fashion mnisd Dot load underscore data and we will run this block of code now these images contained in this data set are originally 28 crores 28 in grayscale with pixel values ranging from 0 to 255 to make this image a suitable for our neural network we will pre-process them by scaling the pixel values between 0 and 1. additionally we will pair the images to a size of 32 cross 32 to simplify how we work with the data as it goes through the network during forward pass and so on so let's write a preprocessing function preprocess images and here we will input images as argument so images is equal to images dot as type node 32. and we will divide it by 255 so that they will scale between the values 0 and 1. then images is equal to we will pad these images so that the size of the image becomes 32 cross 32. images 0 comma 0 then 2 comma 2 then 2 comma 2. and constant values equal to 0. 0 then images is equal to NP Dot expand expand dimensions we will give images comma minus 1. and then we will return images okay let's pre-process extreme and Y train sorry X strain and X test we will call this function preprocess images and we will give input as explain again for processing pre-processing X test we will call this function and we will give X test let's run this block of code okay now that our images input images are pre-processed successfully now we will write few helper functions to display images because we want to visualize what is exactly going in our neural network and we have to analyze our data as well so we will require few helper functions 2 display images now one tip to all you guys all of now one tip to all of you guys you can simply copy paste this to helper functions from The Notebook which I have pinned it in the description box from my GitHub repository but I'll write it for you from scratch let's say sample batch we will give data set and batch equal to dataset dot take get single element I will not describe this code because this is out of scope for this particular session as we have to learn Auto encoders so if you have any doubt in this particular code please ask me in the comment section and we can solve it together we will simply return this batch but in numpy okay now the actual real function of display images uh input will be images then number of images to display number of images to display we will set it as 10 as a default then size would be let's say 20 gross free then cmap we will select cmap as let's say gray underscore r then type would be float32 and we don't have to save it anywhere so same to equal to none so these are the input keyword arguments now let's write the actual function in display images it is Max greater than 1. 0 then images equal to images divided by 255. 0 that mean is less than 0. 0 then images equal to images plus 1. 0 and then divided by 2. 0 2. 0 then we will call PLT dot figure and will require figure size precise then we will write a for Loop so for I in range this number of images to display let's take X1 equal to PLT Dot subplot 1 comma number of images to display and then number of images that is I Plus 1. then PLT dot I am show images of I Dot S Type we have already decided decided our type as this float just type it here and then C map equal to cmap let's set PLT dot X is equal to pl2 dot access is off and then PLT Dot show we want to be requiring this save to keyword for now let's run this now let's display this theme images from training data set so we will call this display images and will simply pass explain here let's see how our function works it will display 10 images because we have set 10 as default number so these are the images which are present in the training data set also we can display images in our test data set we just have to Simply replace this with X underscore text so these are the images from test data set now let's design our Auto encoder let's design Auto encoder so as we have discussed during the presentation we will Design Auto encoder in similar manner but before that we have to list down few parameters so image size equal to 32 okay we have channels as one we will set epochs training networks will be let's say five um embedding dimensionality that is the dimensionality of uh the hidden vector or context Vector we will set it to for now just to visualize in two Dimensions so we will set batch sizes uh 100 let's say and then we will have buffer size as 1000 then validation split as 0. 2 I think that's it for parameters if we want to declare more parameters for training we can go to this block of code and we can edit this block of code now let's uh code for actual encoder first is we need to Define input layer so encoder let's define input layer of encoder so let's see encoder input equal to we will use the Keras layers and we will simply use dot input and we will give shape as this image size image size image size and number of channels we will give a name to this layer let's give name as encoder input so let's say that then we will define stack of convolutional layers so we will Define three convolutional layers this convolutional layers will be on top of each other so let's say x equal to layers. com 2D functionality from layers of Keras the input would be 32 3 cross 3 now strides would be to and we will set activation as value rectified a linear unit and padding would be same so if you want I can make a separate video on convolutional neural network if you want if if you want to understand what is tried what is padding equal to c means and so on please write in the comment section whether you want this type of video or not and we will give this as encoder input okay we will connect this layer to encoder input then we will get another layer of convolutional 2D I'll just copy paste it so that I don't have to write again and again and I'll just change this values so instead of 32 I'll write 64. and everything will be same except here we will give the preceding letter that is X okay then third convolutional layer again I'll copy paste instead of 64 we will write here 128 and everything will be same including this X so that's it now we have now we will flatten the output flatten the output from this last convolutional layer so flatten the output from last convolutional layer um to a vector so this will be a vector so we will take a variable that is shape before flatten equal to K Dot int underscore shape will pass it as X now this will be required in future required in future by decoder layer by decoder block so keep this in mind so then we will flatten layers Dot flatten we will pass it the last convolutional layer okay now after flattening uh we will design a dense layer dense layer which will connect this which will connect this flattened vector to let's say 2D embeddings or context vector so encoder output equal to layers Dot dance and then we have to pass the embedding embedding Dimensions so embe DP ing embedding Dimension which we have already specified here this embedding dimension and we will name this layer as encoder output layer and we will pass X to this particular encoder output layer now the Keras model which defines now we will have encoder variable then models. models encoder input and encoder output now let's print model summary or let's print this encoder model summary encoder Dot summary let's run this block of code okay we are getting an error invalid syntax maybe you want okay Let's uh solve this particular error the error is in this line layers dot input shape equal to image size comma image size comma channels okay so this will be the shape okay and then we have to name so name equal to encoder is a part of this layers. input argument let's run okay again an error in next line I don't think this is necessary okay now uh it is running okay we got another error x equal to layers dot platen X required in future by decoder okay module this has no attribute flatten uh let's solve this code sir let's solve this error I think it should be written in capital layers dot flatten and again we got another error that is it has no attributes model it should be model okay now that we have solved all the errors uh this is the architecture of encoder so this architecture is similar to what I represented it with the help of a diagram uh this is the layer type encoder input convolutional convolutional convolutional flattened layer and the output layer here you can see the total number of parameter total number of trainable parameter and non-trainable parameter and if you want to see how many number of parameters are there in each layer then you can see it here okay now let's code and design for decoder let's define input layer for decoder so decoder input will be equal to layers dot input uh shape will be 2 cross 2 that is 2 cross and we will give a name to this decoder here let's say decoder input layer so similar type of coding we will do for decoder but this decoder will be a mirror image of encoder now we will connect input to dense layer so here we will Define X and layers. dents NP Dot prod and now we will use this shape before flatten so here I have already told you like uh require this variable required in future by decoder so we will uh require this now uh when actually designing and implementing this decoder and then we will input here decoder input okay now we will reshape now we will reshape this particular Vector which we have which we will get from this layer uh into a tensor that can be fed as input to the first convolutional transpose layer so reshaping to field as input to First convolutional 2D transpose layer so we will use layers Dot reshape and we will again use this shape before flatten variable and then X as input now let's design a stack of convolutional 2D transpose layers and these layers will be on top of each other okay so here x is equal to layers Dot com 2D transpose first will be 128 dimensions and then 3 cross 3. and then strides will be equal to 2 we will set activation as rectified linear unit we will set padding equal to same so again I am repeating if you want to know what is this 128 3 cross 3 is try its activation padding in convolutional neural network please write a comment whether I should make a separate video on that or not then we will have we will pass this x okay this is the first convolutional uh transpose layer in order to code the second trans uh this conventional transpose layer I'll just copy paste it and instead of 128 we will have 64 here and everything will be same now we will Define third convolutional layer I'll just paste and instead of 64 here we will have 32 now we will have the decoder output layer so decoder output equal to layers Dot conf 2D here it will be 1 cross 3 comma 3 strides will be equal to 1 activation will be equal to sigmoid padding will be equal to same and name of this layer will be decoder output layer and we will pass X to this particular layer okay now decoder equal to models Dot model similar thing which we've done while designing and implementing encoder so models dot model decoder input and decoder output then we will simply print a summary of decoder let's run this okay so without any error we have executed this again you can see for each layer you can have number of parameters and for decoder the total trainable parameters are this and non-renewable r0 okay so about 2 lakh forty six thousand two seventy three parameters are trainable parameters for our decoder now the the most important part is joining this encoder this encoder to this decoder let's join it so let's join encoder to decoder let's define this as our Auto encoder so Auto encoder equal to models. model encoder input comma decoder and here we will select encoder here we will give encoder output to this decoder then we will simply print the summary of entire architecture autoencoder dot summary encoder output is not defined okay so here is the encoder output all right okay I have made a spelling mistake here okay so this is the total uh architecture of encoder plus decoder so the total number of trainable parameters are now roughly about three lakh 43 000. now why to wait let's train this model so training our Auto encoder uh before training we have to compile this entire Auto encoder architecture which we designed it now in order to compile we have to give two arguments first is Optimizer and second is loss so let's compile Auto encoder Dot compile Optimizer equal to Adam optimizer and then we will select loss as binary cross entropy we can use rmse loss as well but this means that the generated output will be symmetrically distributed around the average pixel values but on the other end when we use this binary cross entropy the loss is a symmetrical and thus it penalizes error towards extreme more heavily than error towards the center so that is why we will be using this binary cross entropy loss let's compile this model now we have to save this model right somewhere so let's create a model checkpoint variable and then we will use callback functionality in this we have to give file path so I'll simply give file path as my current directory it will automatically make a folder which is named at checkpoint in my current working directory and I'll tell him to save weights only as false because we want everything to be saved and save frequency uh would be based on Epoch and not on steps we will set monitor as loss mode will be equal to minimum and let's also give this argument that is saved based only as true in order to save a memory and let's set our boost as zero okay then we will use this tensor board so we can visualize training loss validation loss and Etc equal to callbacks Dot tensor board and we will set logging directory as let's say tensor board locks let's run this okay now uh let's actually train the model so we will use the fit functionality encoder dot fit So In traditional machine learning algorithms also we use fit functionality when we use uh scikit-learn package and all so here uh this is same like the first we have to give the training data X train and uh y train then we will set up a box then we have batch size then we have then we will let Shuffle as true and then validation data equal to X test and Y test and finally callbacks callbacks will be this model checkpoint and tensorflow callback so here tensorflow callback let's train this model the name Auto okay Auto encoder my mistake so here we are encountering an error logits and labels must have same shape but received 130 to 32 versus 100. so here uh the input and output would be the same because we are trying to reconstruct images okay now uh the code is running so this is the first Epoch which is taking about 10 seconds to run here validation data will be again X test and X test we are not predicting anything right we are just trying to reconstruct the original image with minimal reconstruction loss so keep this in mind we are not classifying uh images into n number of labels as we traditionally we can see that the loss is slowly reducing from 0. 2675 to 0. 2525 validation loss and the training loss started at 0. 3 and after the end of fifth Epoch the loss is 0. 2517 we can give it for more epochs to train uh the loss is decreasing but this is just a prototype method or prototype approach or a prototype model we can see and that's why we will just train it for five epochs now let's save this particular models save the checkpoints or Trend model we will use Auto encoder dot save we will save this in models directory in this Auto encoder then if you want to individually save this encoder then you can also save we will require this individual encoder and decoder when we actually have to regenerate new images and to see how encoder is storing this hidden uh representation or context vector so that's why we are saving this encoder and decoder separately as well and then decoder dot save in models directly and then decoder so if you are using Google collab it will be stored in your current working directory in this models folder in encoder all the encoder files will be saved in decoder all the decoder file will be saved and in encoder now all these encoder file will be saved so we have saved our Auto encoder entire encoder decoder then we have decoder and encoder now uh uh we have trained our model we have also saved our model now let's actually reconstruct images so that that was the whole point right like reconstruct images uh using our trained Auto encoder I'll close this particular tab let's write the code to reconstruct images so let's say um we want number of images as one thousand then example images we can take it directly from our testing data and from our training data here we will use this variable similarly so example images we have got and then just to visualize the labels we can use these labels y test and then images so we have selected first thousand images from this testing data set test or test data set so let's say predicted images as Auto encoder we will simply use predict functionality predict and we will input this entire example images so it will predict for this 1000 first 1000 test images let's run this it is pretty fast right within one second it predicted for 1000 images but we have to uh also remember that the images are of only 32 cross 32 pixels so they are not huge images as we currently have in our mobiles like 4K image or 24 megapixel 4800 megapixel images and so on let's display our predicted images so we will use this display methodology which we already wrote before this display images we will call this functionality so this is a very handy helpful handy uh helper function predicted images so it will predict sorry it will display the first 10 images uh if I if I want to uh like display 20 images then I'll just set this number of image to display as 20. 20. and it will display first 20 test images which it predicted so this these are the images that it predicted so predicted in the sense uh reconstructed images or generated images now the next part is visualizing embeddings or the latent space how it represented image in the context Vector that is z which we already saw during our presentation so let's write the code for that so visualize embeddings or context vector [Applause] let's not call it as context Vector let's call it as latent space so here embedding is equal to we will use this encoder and we will predict all of the example images let's give this example images as a bigger number let's say 5000. so that we will have more data to visualize our latent space so it only took few seconds to predict this particular image from this encoder okay so now we have constructed embeddings for each and every image let's print embeddings for let's say first few images embeddings let's say first image so this first image is represented by this two dimensional vector okay now this 32 cross 32 pixel image is represented by these two numbers imagine uh the compression that we have achieved so we don't have to store the entire image we just have to store one model and this particular representation of image so it it really helps in image compression so many methods are related to this like just you just have to store these embeddings and with this embeddings or latent space you can reconstruct an image from these two numbers Itself by using decoder imagine if if you want to print let's say embeddings of um this 10 images now this is this is a very cool approach right guys just by using this two image I am representing my first image and just by using this two numbers I am representing the 10th image in a data set this numbers or this vector or this hidden Dimension or this hidden latent space might not be understandable to us but this is pretty much understandable by the decoder of the trained model of your system now let's plot all these points okay uh so I I'll again write a few lines of code to plot this so let's define the figure size um why not keep it as let's say 2L then PLT Dot figure uh fig size equal to let's give it as 12 cross 12. and then let's print a scatter plot of embeddings and we need to have x and y axis and that's why we are writing like this we will set c as black and white lettuce black and Alpha has 0. 5 let's select SST uh three sorry PLT dot show um it got an unexpected uh okay let's solve this error uh this hyphen should not be there in this argument so this is how Let's uh decrease the size of this let's say eight so we can read it in one entire screen okay now this is visible okay so this is each dot represents an image so this dot is an image this dot is an image this dot is an image but they are now compressed in a 2d representation so if you want to plot this we just have to pick this point give this point to decoder and decoder will regenerate image now we can take any point here in this entire box now guys here suppose this is my cursor right I can click here I can take this point and I will give it to decoder decoder will generate anything uh it will try to generate something okay whether it might be meaningful or it might not be meaningful that we will be seeing in in this session that we will be seeing uh in some time whether it generates meaningful or not but uh this this is a simple black and white graph right we don't know uh what are label ideas we don't know what is let's say shirt or this pant and so on we don't know what it is now let's color this particular embeddings by their clothing type so we have total 10 labels in this M1A St data set fashion mnisd dataset let's color these embeddings so that we can represent it in a more meaningful way but you got right uh you all got what this points mean and so on let's color this embeddings color the embeddings as per label IDs so example labels equal to Y underscore test and then um we have got a number of images this number of images okay so we have example uh images and example labels then we will set as figure size we already said that then PLT dot figure fixed size equal to let's keep it let's let's copy that particular variable fig underscore size 8 cross 8 so that it can be visible without zooming in or out then again PLT dot scatter uh the same thing we will do like embeddings zero and this one for x and y axis then we have uh cmap equal to Rainbow so each and every ID will be represented with different uh shades of a rainbow then c equal to example labels we will again set Alpha is 0. 5 and ss3 PLT dot I will let's say we will also have a color bar just to get an idea about what color represents what ID and then PLT dot show tada come on this is a very uh interesting and cool diagrama guys so here this green part is of id4 this is a different type of clothing this uh Violet this orange and red are different type of uh embeddings and so on so this in this way uh uh the features in Hidden dimensionality are represented by this encoder but here we can observe few things like some clothing items let's say these items they are represented over a very large area right and few items uh let's take these items they are clustered in a very small area so this is one limitation of this Auto encoder few items cover very huge area and few items cover very small area uh now the the distribution is again not symmetrical about the point zero cross zero so this is my point zero right uh most of the distribution is in the negative side uh some of the points are lying towards positive uh side right and also we can see that this this huge piece of Chunk here this white Gap is there again this white Gap is there so encoder don't know whether so encoder doesn't have any idea about this white patches dispatches which we have there are you these patches are very huge so if I select a point from this patch which we will do in the coming minutes uh the encoder will confuse okay which clothing item it has to generate but suppose if I select a point from this Orange okay this orange is of Ida 9 right 8 and 9 so this red and orange it it will generate this particular label IDs but it will fail to generate a point uh if if I take a point from this white patch or the the we call it as holes okay so this is a very cool uh graph that we have plotted a very cool scatter plot so we can actually uh uh so so I am excited here now because where actually we have actually compressed this image after compressing we have all we have plotted all this uh dimensionality that is hidden latent space and we will we have gotten to know what is exactly going in the back end of this encoder right now let's try to generate new images from decoder we will not generate a test image we will generate now in order to generate new images we just have to select random points from this box from this box entire box you can just select random points let's say here here here it can be between this colored embeddings or it can be outside these embeddings um but we have to get the range of the existing embeddings and so on that we will be coding in this and then pass this two dimensional Vector to decoder decoder will generate superb images okay so be with me uh now let's come to the most uh interesting part of this uh video like let's uh let's try to generate new images let's Force our Auto encoder the a trained Auto encoder to be more creative in generating images okay now we can generate images within this bounding box within this bounding box of this hidden uh representation like latent space so we have to Define boundaries so means and Max means and uh Max equal to NP dot minimum minimum of what this embeddings and axis zero then NP dot Max so NP dot Max again embeddings axis as zero then uh we have to select something called as grid width and grid height actually I'm like more excited to generate new images I wanted to see uh how these images are getting generated and what this images looks like then sample equal to NP Dot random let's say random dot to uniform and here we will input means Max and size equal to this grade is width and height and here we will write embedding dimensionality let's run this let's solve this error okay here NP dot min it has to be closed and then NP dot Max okay uh let's uh decode the sample let's say points so reconstructed images equal to we will use decoder dot predict and let's say sample okay before that let's print this sample right so these are the sample images first image second image sorry first uh random point which we took from that particular bounding box this latent space and so on so we are doing like this so for each and every two dimensional Vector this Vector an image will be generated by this decoder let's display these images right this reconstructed images so display images and then reconstructed images let's run this okay so this is the images which are actually generated by decoder how cool is this stuff right this is the these are actually the new images that we are forcing decoder to generate but still it is not a clear image and so on but and see this one uh this doesn't look anywhere to the clothing thing and also this one okay if we make it more clear then this can be a trouser but right now it is just a rectangular box now we will see why this is happening uh in order to see this we need to check uh which points this sample uh points it's selected from this image we need to plot this point uh on top of that particular hidden space let's plot this let's now we are trying to reason why we are getting this kind of output now this is a good image a footwear image and this is also a good image a trouser image but this kind of images uh is not what expected from an auto encoder right there's meaningless images let's plot this let's say my figure size we already specified then PLT dot figure size equal to underscore size comma quick underscore size which we already defined this fixed size then we will again write a scatter plot what I'll do is I'll straight away take this code and I will paste it here okay now we will also plot the newly generated points this newly generated points is this this sample so PLT Dot scatter then again we will call it for a y-axis one let's say c equal to let's give it a color okay let's copy this and paste it here hash and Alpha will be one let's set as s is equal to 2. okay let's set S as 40 then we'll show this let's run so these are the points see these are the points which is randomly sampled uh and most of the points are lying on this white patches see this point one two three four these are the points which are lying in the white patches and that's why we are getting this type of un meaningful images or meaning less images but the points which are generated from this cluster like this Four Points these four points and these two points they are actually they will be meaningful right but the points which are generated from these white patches and the decoder will try to generate something but it will not be meaningful so let's uh try to improve this particular graph by adding a grid of decoded images as well to it so let's code h space we will give it 0. 5 let's say and w space we will also give 0. 5 then we will write for in range grid width into grid height which we already defined it earlier then ax equal to fig dot add subplot grid height grade width and I Plus 1. as Dot axis we will set it as off and AI dot text we need to add text like from which point that particular image is generated so we'll write here foreign font size font size as let's say it then it will be Center alignment and transform equal to as Dot trans axis ax dot IM show a reconstructed images we will just copy paste this variable name uh just sorry for this this particular misspelling okay reconstructed images I comma and then cmap equal to Gray let's run this hopefully it won't give any error oh we have got it so here we can see that this particular sample 9. 9 and minus 3. 4 it gave us this particular image of trouser this minus 6. 4 minus 8. 5 get give this image of a sandal so in this way you can actually generate images or generate new images from your decoder some images which are falling on this white patch might be senseless or meaningless uh but the images which are sampled or the points which are sampled from this cluster now that will be more meaningful now we will see the limitations of this uh Auto encoder now that we have completed implementation of Auto encoders let's understand the limitation of Auto encoders so there are three important and three major limitations of Auto encoders the first limitation is we can see that if we pick points uniformly in a bounded space that we Define we are more likely to sample something that decodes to look like a back that is id8 then an ankle boot that is id9 now because the part of the latent space carved out for backs is larger than the ankle boot area so this backs is represented by Orange Point and ankle boot area is represented by Red points the second major limitation is it is not obvious how we should go about choosing a random point in the latent space since the distribution of this points is undefined and the third important major limitation of Auto encoders is we can observe that there are gaps in the latent space where none of the images are represented the autoencoder doesn't prioritize making in these regions correspond to recognizable clothing items because very few training set images are encoded in this gaps or in this space so that's it guys for this Auto encoders video hope you all got something to learn about Auto encoders if you have any doubt please comment in this in this comment section if you have watched video till now like this video And subscribe to the channel your subscription supports me a lot thank you guys
