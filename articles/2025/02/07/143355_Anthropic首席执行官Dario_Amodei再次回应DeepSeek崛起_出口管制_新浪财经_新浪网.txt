Timestamp: 2025-02-07T14:33:55.896318
Title: Anthropic首席执行官Dario Amodei再次回应DeepSeek崛起|出口管制_新浪财经_新浪网
URL: https://finance.sina.com.cn/roll/2025-02-06/doc-ineiphxe1731617.shtml
Status: success
Duration: 0:00

Description:
好的，我将根据您提供的文章内容进行总结，并生成一个结构化的概要和 Mermaid 流程图。

**总结概要：**

1.  **核心结论：** 美国对华芯片出口管制不仅是必要的，而且应该更加严格，以确保美国在AI领域的领先地位和安全缓冲，从而更好地引导AI的全球发展和安全应用。

2.  **根本结论：** 在中美AI竞争中，美国保持技术领先，并通过出口管制争取时间，是实现AI技术安全、负责任发展的关键前提。

3.  **总体框架：**
    *   **中美AI竞争与出口管制：** Dario Amodei认为，DeepSeek的成功并非美国出口管制失败的证明，反而突显了其重要性，因为AI技术具有军事和经济双重价值。
    *   **AI安全与治理：** Amodei担忧中国可能滥用AI技术，主张通过出口管制拉开技术差距，为AI安全治理争取时间和空间。他认为，美国应保持领先地位，以更好地规范AI发展，避免“竞赛困境”。
    *   **开源与闭源、模型蒸馏：** Amodei认为，开源与闭源的区别被过度强调，模型能力才是关键。他关注模型蒸馏的风险，并呼吁加强对模型窃取的防范。
    *   **AI人才与跨国合作：** Amodei欢迎全球人才加入美国AI公司，但也强调要警惕中国政府对AI的潜在滥用，希望中国也能重视AI安全问题。
    *   **AI的民主化潜力与全球治理：** Amodei认为，AI有促进民主化的潜力，但需要正确的治理和应用。他主张美国应在AI领域保持领导地位，以更好地推动全球AI治理，实现技术惠及全球的目标。

<Mermaid_Diagram>
```mermaid
graph LR
    subgraph AI竞争与安全
        A[中美AI竞争]-->B(出口管制：芯片限制);
        B --> C{美国保持领先地位};
        C --> D(AI安全治理);
        D --> E{全球AI安全标准制定};
    end

    subgraph 技术要素
        F[模型能力];
        G[开源/闭源];
        H[模型蒸馏风险];
        I[算力资源];
        F -- 影响 --> G;
        F -- 影响 --> H;
        B --> I;
    end

    subgraph 人才与合作
        J[AI人才流动];
        K[跨国合作];
        L[对中国政府的担忧];
        J --> L;
        L --> K;
    end

    subgraph AI的潜力与治理
        M[AI的民主化潜力];
        N[全球AI治理];
        C --> M;
        C --> N;
    end

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#9f9,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#9f9,stroke:#333,stroke-width:2px
    style F fill:#ffc,stroke:#333,stroke-width:2px
    style G fill:#eee,stroke:#333,stroke-width:2px
    style H fill:#eee,stroke:#333,stroke-width:2px
    style I fill:#eee,stroke:#333,stroke-width:2px
    style J fill:#eee,stroke:#333,stroke-width:2px
    style K fill:#eee,stroke:#333,stroke-width:2px
    style L fill:#eee,stroke:#333,stroke-width:2px
    style M fill:#eee,stroke:#333,stroke-width:2px
    style N fill:#eee,stroke:#333,stroke-width:2px

    classDef highlight fill:#fec,stroke:#f66,stroke-width:2px;
    class C highlight;
```
</Mermaid_Diagram>

这个 Mermaid 流程图以图形化的方式展示了文章的核心观点和各个要素之间的关系，包括AI竞争与安全、技术要素、人才与合作、AI的潜力与治理等方面。


Content:
1月30日，Anthropic首席执行官Dario Amodei写了“题为DeepSeek和出口管制”的万字文章，核心观点是DeepSeek的成功不能代表美国对华芯片出口管制的失败，反而恰恰证明了出口管制的必要性。其观点在美国和中国的AI圈引发了广泛讨论。国内也有业内人士针锋相对写了一篇流传甚广的回应文章“评Dario檄文：AI全球扩散已不可逆”，也同样得到了美国那边很多的关注。 对于这场争论，Dario显然没打算就此偃旗息鼓。今天，他受邀做客美国知名中国研究播客“中国说”，与主理人 Jordan Schneider做了一次访谈， 对他那篇DeepSeek檄文做出了更详细的解释和回应，其中涵盖了中美AI竞争、出口管制、模型蒸馏、开源vs闭源、AI安全治理、中美AI人才流动等广泛问题，谈得非常具体和深入。 Dario Amodei强调“ 中国不会把AI用在好的地方”，如果美国没能保持住领先地位，就不仅要担心中国的模型不安全，还得担心中国用这些模型对美国进行威慑。 通过出口管制在芯片层面拉开差距，美国就能腾出更多空间去做安全工作，获得足够的安全缓冲。他再次强调， 因为现在的“推理”在新模型中变得越来越重要， 应该禁止具有很强推理能力的H20对华出口。 他还觉得DeepSeek的模型很不安全，说Anthropic评估了DeepSeek的模型，发现它在避免生成能制造生物武器的资料这块，“几乎没有任何安全措施，是所有测试过的大模型中最糟糕的之一”。他甚至还公开挖人：“ 希望 DeepSeek的人才能来美国，为Anthropic或其他“负责任”的公司效力。要是还决定留在中国，希望你们能重视这些问题。” 由于访谈的内容十分深入，干货挺多，我还是本着开源的精神，把全文实录附后，供参考： 出口管制与 AI 的善意 Jordan Schneider： 让我们先谈谈，如果 AI 进展非常快，会对国家实力产生什么影响。 Dario Amodei： 几个月前，我写了一篇名为《Machines of Loving Grace》的文章，重点讨论了超强 AI 的许多积极应用。我在文中用了一个定义，描述我所认为的“非常强大”的 AI 的模样。我用了这样一个短语——“数据中心里的一整个天才国度”，来形容所有公司都在尝试构建的东西。这个比喻非常具有启发性，能让我们理解其中的影响。 就好像在某个地方突然出现了一个有一千万居民的国家，而且这些人都是在任何领域都堪比诺奖级的全才。那会对一个国家的实力产生什么影响？显然会带来许多影响：大幅提升经济能力，加速科学发展。或许不幸的是，这也可能会影响情报和国防——比如操控无人机集群，或者分析情报信息。总的来说，拥有大批极其聪明的实体可以“虚拟地”控制各种事物，这将成为多方面实力的源泉。 Jordan Schneider： 那你为什么要写一篇关于出口管制的文章呢？ Dario Amodei： 在看到人们对 DeepSeek 的反应，以及我自己在这个行业里、作为研发这项技术的人之一，我看到很多见解并不正确。 这些见解来自一些之前并未密切关注 AI 技术发展轨迹的人，但如今因为“中国公司首次发布了某个新模型”这样的新鲜事实而开始关注。 他们错过了之前许多关键进展，对行业动态理解不够透彻。 他们的反应往往是： “哦天哪，这东西真的很便宜。 ” 也许出于刻板印象，认为“中国制造就是廉价”，就把这个模式套用在了 DeepSeek 身上。 但实际上，正如我在文章里提到的，这个领域一向有着成本不断下降的趋势。同时，我们也在投入越来越多的资金来训练模型，因为这些模型非常强大，也非常有经济价值。所以即便成本在走下降曲线，但我们为了获得更优秀、更强大的模型，投入还在不断增加。总的来说，这条“提高性能、提升智能程度所需高投入”的趋势，依旧领先于“训练成本随时间下降”的趋势。 在这样的时间点上，DeepSeek 的确发布了一个在成本方面有所创新的模型，但它本质上依旧处在过去多年里早已存在的“成本下降”曲线中。这并不是说他们用 600 万美元就做出了别的公司花数十亿才能做的东西，更多的是：基于过去的趋势，我们每年大约有 4 倍左右的成本效率提升（正负有所浮动），而 DeepSeek 的做法正好踩在了这条曲线上，跟一年前、半年前的模型相比，似乎“便宜”了很多。 我们会看到，未来会有多个团队能以相对较低的成本做出与之相当质量的模型，而与此同时，也会有多家机构（包括 DeepSeek 在内）愿意花更多钱去训练更强大的新模型。目前的新情况是：出现了一家新的参与者。原本的顶尖 AI 研发企业名单上只有 Anthropic、OpenAI、Google，也许还有 Meta 和 xAI，而现在又多了 DeepSeek，也许今后中国还会有其它公司加入。对我而言，这是一个非常具有里程碑意义的事件，此前从未发生过。它确实值得我们关注，但我看到有些人反应过度，并且他们的理解方向也不太对。 Jordan Schneider： 你最大的更新是对一家中国公司的实力，以及对其他可能中国玩家的实力有了新的认识。在模型方面，你觉得我们应该如何重新评估这种实力差距？ Dario Amodei： 我先说明一下：我们对 DeepSeek 的追踪已经有一段时间了。大约一年多来，我们一直把它视作中国在这方面最有潜力的玩家。这一点早就影响了我们对行业发展趋势的判断。对于现在才注意到 DeepSeek 的人来说，“更新”就是：在此之前，美国可能有三到五家企业可以开发前沿或近前沿模型，如今则变成了美国有三到五家，中国也出现了一家。接下来他们能否继续做出接近前沿的模型，要看他们能拿到多少芯片、以及能否获得大规模芯片，这与过去能接触到的规模相比可能会有巨大差异。 Jordan Schneider： 过去几年里，你和整个 AI 安全社群都在提醒大家 AI 竞赛可能带来的风险。 我想听听你是如何一步步得出对出口管制的当前观点的。 Dario Amodei： 其实这两个想法并不矛盾。 我依旧担心“竞赛”本身，尤其是美中双方对这项技术实力相当、步步紧追的情况。 如果真出现了这种情形，对于双方来说，继续快速推动技术前进绝对是“合理”的，因为 AI 既有巨大的经济价值，也有巨大的军事价值。 除非有非常强烈的安全证据证明 AI 系统的危险性，不然各方都会有强烈动力继续前进。 尤其是，我担心美国在立法层面的进展。 过去一年里，针对 AI 风险（包括被个人滥用，或者其自主行为的潜在危险），美国曾有过一些立法尝试。 然而，反对这种立法的理由之一就是： “如果我们减缓进度，中国就会迎头赶上，超过我们。 ” 这种说法其实完全可以理解。 我确实认为，不仅因为我觉得中国和其它 一些 国家可能不会善加利用 AI，也因为如果我们没能保持领先地位，就会落入一种“竞赛困境”——不仅要担心对手的模型不安全，还要担心对手用这些模型对我们进行威慑。所以，我觉得我们必须要在中国面前保持领先。也许有人会说这与“要小心地发展技术”相矛盾，但我认为出口管制是一种可以同时兼顾这两点的方式。如果我们能形成一个时间差，比如说领先两年，那么我们可以把其中的六个月拿来做安全审查和管控。这样我们还处在领先地位，同时又能让我们自己打造的 AI 更安全。如果两边进度不相上下，我们又得同时担心对手做出的 AI 不安全，又担心他们用 AI 压制我们，一下子就陷入进退维谷的局面。 其实这个思路我已经想了很久——只不过这是第一次对外直说。我一直觉得让美国在 AI 上领先非常重要。一方面，我们需要谨慎地研发这项技术；另一方面，如果能通过出口管制在芯片层面拉开差距，我们就能腾出更多空间去做安全工作。否则，如果仅仅靠自己拼命加速，而对手也在加速，那么我们就不可能获得足够的安全缓冲。确实，这里面充满了权衡取舍。 Jordan Schneider： 我们可以再深入一点吗？你能详细解释一下，怎么才能达到“与中国形成良性竞争”，以及如何在西方内部同行之间形成“良性竞赛”？ Dario Amodei： 西方公司之间比较好协调，因为我们可以通过同一个法律框架来约束大家。 如果美国或欧盟通过了一项法律，那么 OpenAI、Meta、Google、xAI、Anthropic 这些公司都得守。 这样我们就能确保每个参与者都采取必要的安全措施。 即便现在没什么强制性法律，但也有一些“自愿标准”之类的东西。 一旦有一家大公司明显更负责任，而另一家明显更不负责，就会激发监管部门去关注后者。 这种机制在西方运作还算有效。 但是美国和中国之间不存在这种机制。 这是一个典型的霍布斯式无政府状态（Hobbesian anarchy）——即国际政治上的现实主义。 倒也不是说一点合作都没有机会。 我确实认为我们应该尝试一下，但我比较悲观。 我觉得也不是完全不可能取得进展，至少边际上的进展可以试试。 假如在将来真的出现了某种非常明确的证据，证明 AI 系统会对人类文明构成极度危险，可能两国才会坐下来认真谈谈。 但目前为止，我们并没有这样的铁证。 有些从事安全研究的人自认为有非常有力的理由，但在我看来这些论证并不足以让两个大国就此罢手。 此外，模型随着能力的增强，我们也会逐步看清它们到底有多大风险。接下来一年、一年半，我们会对这个问题了解更多。如果真的发现了特别有说服力的证据，证明这些模型已经对全球构成“立刻且重大”的威胁，也许会改变局面。 到目前为止，我所了解到的情况是，美国政府曾经派代表团去跟中方沟通与 AI 安全相关的议题，但似乎中方兴趣有限。我希望这种态度能改变。我们自己也一直在努力对外科普 AI 安全问题，包括如何用可解释性技术、如何用“宪法式 AI”（constitutional AI）、以及一些可扩展的监督方法去更好地掌控和引导 AI 系统。我们当然希望这些想法能传到中国，让他们也采用这些更安全的方法。 但回到现实角度： 美国和中国是两个政治制度不同、长期存在对立关系的国家。 他们很可能会在这项技术上互相竞争，看谁能跑得更快。 这应该是“默认走向”。 Jordan Schneider： 我能想到一个有点类似并且相对“乐观”的案例是，中国决定重视气候变化问题。但气候技术跟 AI 不同，它并不具备太强的军民两用属性。虽然绿色技术也能带来经济收益，可它缺少很直观的军事价值。 Dario Amodei： 据我所知，中国在履行气候承诺时，也有时断时续，这在我看来并不意外。 对比公司之间的情况也是如此——大家会签署一些自愿准则，但是执行力度可能有差异。 不过就算只能起到 10% 的积极效果，也值得一试。 现实主义地看待国际关系：如果真的出现了足以威胁美国、中国乃至所有国家的非常明确的证据，那么合作的可能性才会真正提升。我想对那些希望大家暂停研发 AI 的人说：你们最重要的任务其实是证明 AI 的威胁确实到了非停不可的程度。 Jordan Schneider： 你觉得美国应该向中国卖哪些芯片、不应该卖哪些芯片？ Dario Amodei： 首先，美国的出口管制从来都不是为了阻止像 DeepSeek 这样规模的企业获得几万块 GPU。我们也要理解，总会有一些走私或转运渠道存在。SemiAnalysis 的 Dylan Patel 之前报道过 DeepSeek 大约有 5 万块 GPU，但并未得到官方确认。DeepSeek 本身也没明确公布，不过以 SemiAnalysis 在半导体领域的消息渠道，可信度还是相对高的。 据说他们使用了三种芯片：H100、H800 和 H20。就按照 SemiAnalysis 给出的信息来说吧。第一种是 H100（美国常用的高端芯片），大约 1 万块左右，这些显然是通过某种走私或境外代理的方式流入中国。 第二种是 H800。它是为了绕过 2022 年首次出口管制而出现的降速版本——也就是把芯片间通信带宽降下来，以满足管制规定。随后在 2023 年的新版管制中，这个漏洞又被堵上了。我们很早就注意到，通过某些训练方法可以绕过通信带宽的限制。2023 年的新管制策略更有针对性，不那么容易被规避。所以，DeepSeek 手头的 H800 芯片大概是 2022 年和 2023 年新规之间的那个窗口期运进中国的。 第三种是 H20，它不适用于训练，适合推理（inference），所以不可能用来训练他们最初阶段的 Base V3 模型。但在他们第二阶段的 R1 训练中，可能也有一部分训练过程混入推理环节，此时 H20 就能派上用场。 H20 目前没有被禁，但我建议也应该禁止向中国出口，因为现在的“推理”在新模型中变得越来越重要。 总之，我的看法是应该把范围尽量覆盖全面。当然也要把握好尺度，不可能连游戏机都不让出口——毕竟那会对经济活动造成过度干扰。所以还是要精细设计，但像 H20 这样的芯片应该被纳入出口管制范围。 Jordan Schneider： 英伟达的论点是：如果我们不向中国出售这些芯片，那华为就会有更大的国内市场来发展自研芯片。比如以后 DeepSeek 宣布能高效地在华为的 910B 芯片上运行之类。你怎么看中国在芯片制造方面的自主能力？这对对半导体制造设备层面的管制又意味着什么？ Dario Amodei： 从 10 到 15 年的长周期看，英伟达这个说法大概率是正确的，中国可能最终会赶上。但芯片供应链极其复杂，而且我们对半导体制造设备和维修也有限制。想在短期（几年）内做出能媲美英伟达最新 B100、或者我们在用的 Trainium、TPU 之类的芯片并不容易。华为的软件生态也还没跟上，不过那倒是次要的。 我认为华为芯片要在短期内追上美国主流，基本不太可能。如我在《Machines of Loving Grace》以及那篇探讨出口管制的帖子里所说，我觉得最关键的时间窗口是在 2026、2027，最晚 2030 前后。在那之前能否保持关键技术优势才至关重要。AI 领域变化太快了，10-15 年简直是“天荒地老”的尺度。 Jordan Schneider： 拜登政府的政策基本是允许中国公司在境外的西方云服务商处使用高性能芯片，这样做你觉得问题大吗？ Dario Amodei： 他们后来出台了一个“扩散规则”（diffusion rule），限制中方通过在其他国家设立空壳公司的方式来绕过管制。也可能你说的情况略有不同。 只要这些芯片物理上不在中国本土，其实短期内问题还没那么大。因为训练模型需要能直接访问大量芯片做大规模运算，一旦它们想把模型训练好，就需要更紧密的硬件掌控。而且如果模型是在美国的云上，一旦发现违规，美国可以马上切断访问权限。不过从长远看，这也需要设法堵住，不能让对方规模化地使用。 Jordan Schneider： 你对 DeepSeek 开源模型怎么看？ Dario Amodei： DeepSeek 的发布有几层特性： 1）他们公开了模型的权重； 2）他们发布了一个在成本和性能上有一定创新的模型，而且是首次由中国公司做出的同等水准的模型； 其中，第二点更重要。大部分我在那篇关于出口管制的文章里写的内容，其实即使他们不开放权重、而只开放 API，也几乎不会有改变。核心影响还是在于这是一个强大的模型，而中国公司第一次在同一竞技场出现。对于他们能否进一步大规模扩展，需要看出口管制是否允许他们获取数百万颗芯片。 从商业角度看，模型能力才是决定性因素。如果模型能力很强，它就有竞争力，占到 80-90% 的影响因素。而至于是否开源权重，多数时候影响相对有限。况且“开源”在这里也不完全等同于“开源代码”，只是放出了权重矩阵（大量数字），跟常规的开源项目还不太一样。 在商业层面，很多之前一直做“开权重”的公司，最终也会想办法获得盈利，所以往往会限制权重的使用或不再完全开源。对我来说，更重要的是又出现了一家中国公司能够推出强大模型。单纯把权重开放对商业竞争的影响其实并没那么大。 Jordan Schneider： 再进一步聊聊——你觉得各国政府什么时候会开始担心开源模型这件事？ Dario Amodei： 从商业角度来看，开源或闭源的差别被夸大了。从安全角度，“权重开源”与“权重未公开”也常常被夸大。最根本的问题是：这个模型究竟有多强。一旦模型非常强大，我们当然不想让它被对手窃取，无论是通过黑客还是通过公开发布都不行。如果模型没那么强，无论是否开源都不会引起大风险。 Jordan Schneider： 说到被窃取，你怎么看模型蒸馏（model distillation）的风险？ Dario Amodei：有传言称（我在博客里也提到了，我自己没做判断），DeepSeek 可能是蒸馏了 OpenAI 的模型。他们声称自己有证据，但我没深入研究过，不确定真伪。蒸馏确实是可行的技术手段。它当然违反了使用条款。 这里要注意几点： 一是我们可以研究如何检测是否存在蒸馏，比如对比两个模型的大量输出，看它们是否存在同源关系。蒸馏过的模型经常在语言风格、回答方式上表现出明显相似的“指纹”。若能将此做成可量化的统计检测方法，就会很有价值。 二是可以通过监控技术尽量防范别人用 API 蒸馏模型。但就和网络安全的任何问题一样，这永远是一场攻防博弈。 这也是为什么有些公司选择不让用户看到模型的推理链条，避免别人更方便地进行蒸馏。即使如此，仍有人能越狱（jailbreak），因此也有人在研究如何让模型更难被越狱。我们公司今天也刚发布了一个让模型更难被越狱的方案。 间谍防范与跨国合作 Jordan Schneider： 先撇开蒸馏不谈，你在大约一年半前做客 Dwarkesh 的播客时曾说，如果一个国家把“偷模型权重”当作头号任务，那它就能做到。如果真如你所预期，AI 变得至关重要，吸引了来自北京和其他国家级对手的攻击，这对顶尖实验室和追赶者之间保持差距意味着什么？ Dario Amodei： 我能想象到的最大差距也许就是两年，而这两年其实就价值巨大。 能给我们带来非常多的技术和安全管理上的回旋余地。 超过两年就不太现实了，所以我们的目标应该是尽力争取这两年时间。 至于阻止国家级对手在两年之内窃取顶尖模型，这确实难度很大，但我觉得并非不可能。关键要借助两个强力抓手： 1）美国政府，尤其是其反间谍能力； 2）AI 模型本身——因为随着模型变得越来越强，它们也可以帮我们做网络防御和自我保护。 我们在大约一年半前就提出了一个“负责任的扩展计划”（RSP），里面有不同的安全级别，比如目前我们处于 ASL2（普通的科技公司安全标准）；ASL3 主要防范强大的非国家黑客；ASL4 或 ASL5 则是应对国家级威胁。当模型达到某些能力门槛时，我们就必须升级到这些极高水平的安保措施。最初设计这个流程时，就考虑到了美中竞争，也考虑到我们需要在实验室安保上投入更多。 Jordan Schneider： 美国政府自己的邮箱都可能被黑客攻破，中国也能监听大部分美国电话。 把希望寄托在“美国+AI 模型自身的安全能力”上，靠谱吗？ 也可以换个方式问： 如果真的有人偷到了权重，DeepSeek这种公司会不会在缺乏模型研发的“隐性知识”情况下难以复现你们下一代模型的功能？ Dario Amodei： 运行现有模型其实不算很难； 但要在此基础上做新的模型，就会遇到重重困难，因为这些模型通常是为了特定的集群环境和底层系统做的高度优化。 除此之外，你也可以做一些别的事情，比如直接将其部署到某些场景进行推理或微调等，但距离真正赶超仍有差距。 Jordan Schneider： 我们聊了计算资源竞争，也聊了模型层面。其实模型是由人造的。如果你去 NeurIPS 那种会议，会发现有很多在场的人说着普通话。想听听你想对那些在西方学习却持有中国护照的学生、工程师说些什么，毕竟他们可能会想：“我为什么要为这个人工作？” Dario Amodei： 首先，我一定要说清楚：我们讨论的“对中国的担忧”，绝不是针对所有中国人，而是针对中国政府。我们公司非常欢迎来自世界各地的优秀人才。我们跟在中国或别的国家的研究者和工程师，本质上属于同一个科研社区。如果有可能的话，我也支持通过非官方渠道（Track II）进行学术上的沟通和合作。 我们在意的核心问题是： 中国 政府会如何利用强大的 AI。中国本可以在过去十几二十年走上一条更自由化的道路，但显然并没有。很多“对中国有研究”的人说这中间曾有个转折点，无论如何，结果就是他们走向了和美国相反的模式。如果当初他们选了另一条路，也许我对中美之间 AI 竞争的看法会完全不同。总之，这与对某个国家或某个民族的成见无关，而是与对某些制度可能如何滥用 AI 的担忧有关。 Jordan Schneider： 好吧，那看来这篇访谈不会在中国走红了。那你有没有什么想对 DeepSeek 本身说的？ Dario Amodei： 我觉得他们是非常有天赋的工程师。我想跟他们说：认真对待 AI 系统自主性带来的潜在风险。 当我们对 DeepSeek 的模型做评估时，发现它在“是否能生成超出谷歌或教材可查信息的生物武器资料”这一项上，几乎没有任何安全措施——在我们测试过的所有大型模型中是最糟糕的之一。 我并不认为现在的模型已经到了能马上造成大规模危害的程度，但 AI 正在指数级地进步，或许今年底或明年就会不同了。所以我对 DeepSeek 的主要建议是：关注 AI 的安全与风险问题。目前美国多数顶尖 AI 公司都已公开表态，认为 AI 的自主风险以及被不法分子滥用的风险都值得重视。我的第一选择是希望 DeepSeek 的人才能来美国，为我们或者其他负责任的公司效力。要是他们还是决定留在中国，我还是希望他们能重视这些问题。 Jordan Schneider： 当前有一种主流的叙事方式：人们担心 AI 会强化非民主政权，比如让中国成为“千年帝国”之类。但也有人希望，AI（尤其是开放式的 AI）可能会带来某种“民主化”效果。你怎么看？ Dario Amodei： 我确实能想象 AI 会促进民主，我在《Machines of Loving Grace》也谈过。但我不认为这与“开源/闭源”有多大关系。就算是中国公司做的模型，在形式上对外“开放”，如果他们掌握了最大规模的计算能力，也能进一步微调模型来加强社会控制，那也未必能“民主化”。我举个极端例子：如果我是一个处于非民主国家的普通公民，只拿到一份模型权重的“U 盘”，就能对抗整个政权吗？恐怕没那么容易。开源在社交媒体时代的价值也不完全相同，就算你拿到了 Twitter的源代码，也不代表你能撼动其对社群舆论的实际影响力。 民主化更多地取决于我们如何正确使用这项技术。我在那篇文章里提到过几个方向，比如在司法领域的应用。AI 或许可以让执法与司法过程更加统一和公平。有人担心算法会带来偏见，但如果做得好，AI 反而可以帮助我们更接近“人人平等面前的法律”。 此外，在协助公众讨论和民主决策方面，AI 也能发挥作用。我们和一个名叫 Polis 的组织合作过，他们用 AI 帮助分析、归纳公众意见，引导人们找到共识，这对民主社会是有益的。再者，AI 也能在科学、医疗、心理健康等领域带来巨大进步，从而在更宏观层面帮助社会以更健康的方式运转。 Jordan Schneider： 你写了两篇博文，似乎存在某种张力：一方面你说要严格管制，另一方面你也相信 AI 会广泛造福人类，而中国有五分之一的人口。你怎么同时兼顾这两点？ Dario Amodei： 我觉得这两者其实是同一个世界观的两面。在《Machines of Loving Grace》的“AI 与民主”那一部分，我就提到过要“锁定供应链”（这就包括出口管制、实验室安防等）。它们完全可以相辅相成。 首先，我们完全可以让世界上所有人（包括中国人民、包括其他一些国家的人民）享受到 AI 的好处，同时又不向这些政府输出军事级能力。因为 AI 是“通用双用”技术，如果你直接把一个强大的通用模型一股脑儿送过去，那对方就能用来做各种可能的军事用途。但如果你在自己掌控的服务器上运行模型，通过 API 提供给中国科研或工业使用，比如他们想用来研发抗癌药物或提高能源效率，你可以让他们“租用”模型能力，从而帮助中国乃至全世界获得经济和民生的好处。但若对方企图让模型去研发氢弹或者追踪核潜艇，你可以直接封锁这类请求。这样，我们实现了基于应用层面的筛选。 长远看，确实需要一个全球化的治理方案，因为不太可能永远只靠美国一方控制技术。目前，我的设想是，如果美国处在领导地位，能够制定规则，那么我们就能更好地谈判一个“对所有人都有利且相对安全的国际秩序”。如果我们处于劣势，就容易被对手主导谈判，局面会更糟。所以拥有技术领先会给我们更大的包容度。最终目标并不是“打压对手”，而是希望在一个实力更有利的起点上，与全球各方达成尽可能安全、人人受益的应用规范。只要我们能从优势地位出发，更有机会做出大方的决定，让这项技术真正惠及全球，也将更有能力去化解它的负面效应。  （转自：东不压桥研究院） 24小时滚动播报最新的财经资讯和视频，更多粉丝福利扫描二维码关注（sinafinance） 新浪财经意见反馈留言板 新浪简介 | 广告服务 | About Sina 联系我们 | 招聘信息 | 通行证注册 产品答疑 | 网站律师 | SINA English Copyright © 1996-2025 SINA Corporation All Rights Reserved 新浪公司 版权所有
