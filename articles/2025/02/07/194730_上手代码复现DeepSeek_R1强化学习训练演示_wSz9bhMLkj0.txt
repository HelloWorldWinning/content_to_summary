Timestamp: 2025-02-07T19:47:30.518976
Title: 上手代码复现DeepSeek R1强化学习训练演示 wSz9bhMLkj0
URL: https://www.youtube.com/watch?v=wSz9bhMLkj0&ab_channel=LLMer
Status: success
Duration: 32:34

Description:
好的，这是根据您提供的内容生成的摘要，包括核心结论、框架和 Mermaid 流程图。

**1. 核心结论 (Core Point):**

通过基于 GRPO 的强化学习，即使是小型语言模型 (如 QN 2.5B)，也能在数学推理任务上展现出类似 CoT (Chain-of-Thought) 的推理过程，并提高答案的准确性。

**2. 根本结论 (Fundamental Point):**

强化学习通过奖励函数引导模型生成包含正确答案的推理过程，从而使模型能够在特定任务上泛化，而无需复杂的模型结构或中间推理步骤的监督。

**3. 总体框架 (Overarching Framework):**

内容围绕使用强化学习 (特别是 GRPO 算法) 训练小型语言模型进行数学推理展开，重点包括：

*   **问题提出:** 传统模型在复杂推理任务中表现不佳。
*   **解决方案:** 使用 GRPO 强化学习训练小型模型，使其具备 CoT 推理能力。
*   **训练过程:**
    *   选择基础模型 (如 QN 1.5B)。
    *   准备训练数据 (如 GSM8K 数据集)。
    *   设计奖励函数 (包括准确度奖励和格式奖励)。
    *   使用 Hugging Face 的 TRL 库进行训练。
*   **实验结果:** 训练后的模型能够生成推理过程并给出更准确的答案。
*   **未来方向:** 探索更大的数据集和模型，以及泛化到其他类型任务的可能性。

**4. Mermaid Conceptual Map:**

<Mermaid_Diagram>

```mermaid
graph LR
    subgraph 数据准备与模型选择
    A[GSM8K数据集]
    B[QN 1.5B 模型]
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    end

    subgraph 强化学习训练过程
    C[系统提示词] --> D(模型生成回答)
    D --> E{奖励函数评估}
    E -- 准确度奖励 --> F[更新模型参数]
    E -- 格式奖励 --> F
    style C fill:#ffc,stroke:#333,stroke-width:2px
    style D fill:#fff,stroke:#333,stroke-width:2px
    style E fill:#cff,stroke:#333,stroke-width:2px
    style F fill:#cfc,stroke:#333,stroke-width:2px
    end

    subgraph 实验结果与分析
    G[训练后的模型] --> H{CoT推理}
    H --> I[更准确的答案]
    style G fill:#efe,stroke:#333,stroke-width:2px
    style H fill:#eee,stroke:#333,stroke-width:2px
    style I fill:#afa,stroke:#333,stroke-width:2px
    end

    A --> C
    B --> C
    F --> D
    linkStyle 0,1,2,3,4,5,6,7 stroke-width:2px;
```

</Mermaid_Diagram>

希望这个摘要对您有所帮助!


Content:
上个视频呢我们连续讲两个论文都是关于Dipsic RE这个思维量推理模型的那今天我们就来动手实战验证一下通过训练这个过程能体会得到更深刻的体验到基于GRPO这种强化学习是如何把这个中间推理过程给它放化出来的那也是非常简单那个先看个效果然后我们再反过来说过程这个呢是本地运行的一个qn 2.5b就是5e 才是我的最小的qn模型然后我们问他的一个问题就是这个是元数级理的英文问题就什么意思呢就是我有一桶水现在三分之一满的然后为了把它填满我又加了16加轮的水进去那么请问这个满的状态下是多少加轮的水也就是说填了16加轮水呢是我填了3分之二嘛那也就是对吧3分之一是8加轮所以3分24整体应该是24那这个qn 2.5b呢默认的这种模型效果是这样的还给你答案是36%通过我们训练完之后得到的基于qn 2.5b的推理模型同样的问题是推的这么回答的如果填满我们就要分析一下然后什么什么什么一堆分析的过程这就有这个4v2推理的过程然后呢一堆计算现在结果里后24然后所以给助了一个总结答案整体加满是24加轮水那么这个就是正确的一个效果它训练之后的模型和之前的qn 0.5b它来的不同在于训练之后它是有的这个4v2推理过程并且答案也准确了这样的一个过程那我们就看看首先这个蛋码是基于这个就是哈根费斯他们率先的把grpotrinit几程到他们这个tr的库里面了所以就可以直接用那么这是一套模板然后我们改了改就在这里面我们演示一下看了这个怎么过程训练的过程第一就是也有一些库然后主要看那我讲一下我们现在做的什么呢现在是模拟这一步就是我们上个视频讲dpsychre0这个纯前化学习的这个过程也就是什么我们选一个基作模型这里面我们选的是q15E参数那个模型然后我们的推理样本选的是这个gs38k就是全部数据体大概是有8800盘这样的题目全英文的还可以看一下比如说就这种题就是用远描述的一些题目这其实算数的然后呢它后面给了你一个答案然后并且就是所有的答案后面有4个信号后面这个这就是答案所有答案都是证据看一下比如这道题答案35这个题是85就是比较简单的这种语言描述的数据题在我们这个训练共产党中我们是把问题拿出来并且把这个答案抽出来就是作为我们的这个光处次然后我们写我们的这个re1r方式就是我们的奖励韩述去判断一下证不证确这样的过程所以它那是训练级大概是8000盒然后我们就是这样的首先组织我们的没说完然后我们强化训练这个里面用的是哈克菲斯包装好的GRPO然后用的是激活韩述这点时候确信奖励加上格式奖励加上那个两个另外一个幅度奖励最终我们得出来的就是我们的一个基于q15E模型得出来的这样一个推理的小模型然后我们就可以去做一些尝试测试看个效果如何没有其他这些东西那么看看待马首先我们组成是训练之前每一个样本前面我们要给它加一个什么系统提示词就跟dbcg一样我们先要把系统提示试尝就是你们回答个问题要求你在reasoningreasoning或者这个sinkingsinking之间然后写内容然后你的答案呢又在 also  also 之间什么意思呢我们看这个效果那么我们这里面把特殊偷套给scape掉了scape掉了所以没显示出来那比如说这个就是sinkingsinking之间这个呢就是unser最终的答案这也没有话所以体现在我们dbcg聊天窗口里面的话它会把这一部分给怎么样就是这个灰色的这思考过程弹化输出而最后一部分呢是结果的部分它可以加粗然后加大自号在底下输出这样的一个逻辑好了这个是帮助simple format就是帮助我们去通过simple充取匹配里面的内容的是一个辅助的然后有四有两个这个辅助寒霜一个是如何从这个ximple到达案里面抽取充取答案辅助寒霜就是在把unser和unser这两个中间的东西的抽损这意思然后这个寒霜是如何在训练级里面抽正确答案就是说这个训练级里面所有这个四个信号后面的这个是正确答案我们要把它抽出来作为什么的最后我们训练共产党中那个奖励还是所对比的那个值就是看看对不对对了的话就给我们奖励指一份两份不对的话就给领本这个意思所以这是一个辅助寒霜然后就是利用deadset这个默认的库我们直接掉用就是open a ag s 八K就是这个东西hug and face的open aopen ag g s 八K这个数已经自动就给掉进来了掉进来以后我们就组织我们的promp组织什么呢根据q1的这个promp这个template首先绝色是系统然后系统的内容是什么呢内容就是我们上面告达的这部分system prompt并且我们要往里面加我们的内容一会然后用货的部分就是你回答的部分就是questionquestion x questionlambda x question的意思就是我们数据集理这个x 代表一行question这个是oncer所以xquestion就是这个就是我们的一个问题那就是把这个问题在模拟到我们训练样本中的提问当中同时我们要把这个答案xoncer我们这个16给他抽出来作为我们这个证据答案所以就这xtrack 对吧上面这个这个韩述把我们的oncer里面的四个星号后面的那个证据答案给他抽出来这个就是证据的答案所以一会儿用到证据答案去对比模型推出的推理出来的结果做奖励韩述那么奖励韩述这个是人家老外写的我们就是已经拿过来用了有四奖励韩述这个说实话也不是原这个原则文只说用了这个奖励韩述并且是有准确度奖励和格式奖励并没有给出原代码所以大家有各种不同的方式去做这种匹配在这个匹配里面它主要针对的就是这个GN38K这个数据级去做奖励韩述的匹配其实奖励韩述这块我觉得才是dps1花了很大时间的部分因为针对不同的数据级你的奖励韩述判断的准不准去把这个很关键因为有些题目范化的结果它并不是在后面的给你一个证据答案你需要从它的文字中把证据答案抽出来还有的时候有些题目比较复杂那个答案不是一个数字那个答案可能是一个韩述一个方程或者说小段话所以这个时候如果判断样本级里面的证据答案抽据出来这个是一个技巧合它里面怎么判断呢就是非常简单就是正确性的判断怎么判断就是你看中间的过程是一个辅助答应出来以后我们的训练的时候我们把每一个回答的问题都给它答应出来了包括了原本的样本问题和我们抽据出来的答案和模型声诚的回答以及我们从声诚回答当中抽据出来的证据答案所以我们要匹配这个抽据出来的证据答案和我们这个样本里面证据答它俩匹配是不是证据如果是的话这两个东西匹配自以比较把它们在呆起然后分别作为R和A然后判断如果相挡的话我就给你2.0就是奖利值2.0不想挡的话0.0奖利值0.0所以这个韩述为主要的一个判断韩述其实就是判断什么的我们模型在训练的时候让模型就动态的输出了很多被选的推理的选项最终推理出来以后有个结果结果里面要包含一个正确的答案这个正确的答案要等于我们这个里面每一道题里面证据答案的数值就这个意思所以其中有两步就是第一步模型首先要能输出来这个正确答案是正确的第二就是这个格式要能被我们的韩述给它抽出来对吧 express这个这个韩述我们要从Answer和Answer之间把我们的答案输出来所以就是说如果模型在训练的共产党它输出的正确答案但是答案的格式并不是在Answer之间的话那我也是抽不出来的这个韩述是无法判断的所以这个道题它还是奖利韩述奖利值0.0什么时候能奖利分呢就是你既答对了而且还要把答案放在Answer之间这就是相当于隐性的一个格式奖利一样然后就是Answer就是整数的一个奖利是个辅助奖利0.5给0.5分另外一个是严格的格式奖利和一个软格式奖利就是判断我们的所有的回答和推理过程是不是在我们这些特殊脱捆当间之间比如Rise and Reset on the Sun之间这个终于主要判断什么的强制我们在训练的共产党中要模型输出倾向于标准的回答格式的这个训练完之后我们在用的时候就会发现在U.I.街面里面就是都很一致分别是这个问题我应该推理下什么方向然后到大几步然后最后总结什么什么什么然后结果是什么通过这样的一个格式约定的方式给它控制在这个输出当中然后我们最终在U.I.街面里展示的时候我们通过CSS这种宣展给高伦人看的感觉更清晰一些这个道理好了然后这个是主要的训练部分首先的Base Model就是Q1 2.55E才是这个Instagram的模型它EU有些基本能力然后这个依据是什么呢这就是我们这个模型然后推理样板就是我们刚才GSM这个是8K的数据样板然后我们来定义我们前华学习节参数学习率好这个0.5个0一个5然后我们这个两倍它一倍它我们的全中摔减我们的这个预热的比率等等我们的这个叫做学习率的Scheduler它们是预选学习率就是这样的一个过程上下的不是现性的然后就是一堆了而我们用的精度是BF16整体的Purder Device Trin Badge Size 1那我们就是一个库达那我们就Batch Size 1但是我们的这个Gridian Accumulation Stab 14就是我们4个相当于我们4P以后去反向更新一次那也就是说我们这个样板是有8000个样板对吧8000个样板除于4就是2000部我们一共要训练2000部在这个里面我们每200部的时候就停了就是MarscomptiCompliScience我们只能输出200这个是ProMpt的长度Token的长度这个你可以给它射长一些为什么是短呢因为就快整体的训练速度快你要射长的话就像原论文里说的它应该很长或者甚至于说这个达到10万Token100万Token级别因为越长退理时间原论文也说了越到后面退理叔叔的Token越长越准确对吧它自然很形容长了包括OPi最新的这个O3O3也是有后面时间很长有的时候退一个题目复杂一点然后是3分钟10分钟那你说3分钟10分钟每秒就按了10个Token的话对吧1分钟就是600个Token10分钟的67000个Token那可能1分钟不止10个Token1分钟可能100个Token所以10分钟6万个Token所以其实Token数量比较大这个可能数数比较长那我们在这里面就射得比较短比较的有一个限制就是我们这个训练除了的模型效果推理都是比较短的推理推理比较长的我们没有时间然后整个训练一个E-Poke就是我们这个8000个样品全部全部全部全部还一遍然后每100个步骤我们一共是2000部每100个步骤我们就存一下这个保健康点Triconite我们就可以在过程当中直接看效果其他都是一些我没有用VRM然后直接就炒三树给它放到孤大里面去然后我们就开始训练训练的主要是这个是GPU Trinit是TRMTRLHackinFaceGun出的他们都帮我们封装好了看了一下这个原带马里面其实就是我们在上两个视频所讲的这些GRPU的公式那么它默认的是分了16组这个Number Generations16各样本机然后我们把我们封装好的上面的几个辅助的这种叫奖励韩术给它放进来它每次训练的时候会通过我们所有的辅助韩术的总和去这个怎么样去做这个T度更新总和例子就是我们比如说这个准确度奖励我们可能一旦对了我们不是给二点点吗然后有这个格式奖励给0.5所以加在一起综合的奖励作为每一轮反向更新的奖励新号最后我们把这个模型保存下来就是保存到这个矛炉就白身了那其实这个两个都要使了之前你看一小时一起一个都要使当时是这个没训练完所以我就直接录了它已经看到效果了所以就没结束现在一共是运行类1000部还有一半没有训练完那么主要看什么其实主要这个过程非常好就是自己去动手训练的时候主要看它输出的过程你看我们来解读一下这个是question就是问题这到这这个问题是什么都是我们样本级里面的这个东西这个是问题然后我们并没有用到样本级里这个结果我们没有用到这个东西我们只是抽取了这个最终正确弹因为什么因为我们这个强化学习的过程我们不需要训练过程没有PRM冒动也没有ORM也没有每一部的奖励值我们只是知道最终的正确弹匹碑被整理的奖励对了我就给你奖励值两分然后整个的推理过程是让模型自动区域考虑的那这个自动考虑的过程是由于我们在训练的时候先随机生成的多个被选的推理其实也就是基础模型的能力就这段话其实什么呢其实是基础模型的能力你看我这是训练的第一个循环对吧第一个样本所以这个出来没有任何的这个训练的强化学习训练的反向更新的它只不过是q1是5E模型的这个基础能力所以它输出什么东西看一下正确弹是100就上面到提一对应的正在是100然后这个是模型的回答这个模型的回答首先第一个我们注意到这个基础模型能力它也可以做一些简单的CUT回答对吧因为我们在我们的系统提示则里已经告诉了你要依不依不考虑但是你看它没有做到什么呢这个把结果放在我们的标签里然后Answer对吧没有放在标签里所以这个还有一个问题这个到这就结束了因为我们控制的输出200个头盆你看这个比较长它就没输出完所以我们去抽取的结果也没抽取成功好了这个就是个例子我们看一个成功的例子短一点的例子好了前面这些例子其实都是没有任何的奖励质你看春天落死为脸因为我们根本就没有去更新没有得到奖励信号到100多部的时候对它这时候90部左右的时候我们才会有一些奖励消失进来往后看这个是一个问题然后这个证确答案知道问题对应的证谈是3007入然后模型的回复是这些然后我们去抽取出来的没抽取出来为什么没抽出来呢因为第一它没有把它放在证确的格式里我们抽取的时候抽取这个夸耗然后夸耗是两个之间的数字所以你看前期其实前就三个之前虽然都有这些东西但是我们都抽去不出来抽去任何东西都抽去不出来它没有形成什么没有形成一个我们希望的这个COT思考过程的模式所以这些都是训练过程中的打验也就是说前90度前100个这种循环步骤没有任何的对于模型训练的时候没有什么意义的价值对吧我没有更新任何采术在这过因为我没有说到任何的奖励信号来看一下这个到这为止也是这个问题随意拿一些然后证确答案是57然后这是模型生成的这个回答之1然后呢这个是我去抽取的这抽取的你看抽取的没抽出来抽取一堆文字出来所以这个就没有任何奖励信号所以一直往后往后往后我观察了很久将近英小时也没有任何变化到什么时候变化这很有意思就观察过程大家看看有一段过程当中它就输出的这个证确答了就是把我们的证确答放在Answer里面看看这个挺长的得翻一会儿这还没有但是你看这个已经有box的对吧都把结果放在box的里面box的什么意思就是这是一个特殊投赶最终答案非常明显的显示来说一般都放在box里你看它这已经有一些格式的奖励我再往下看最重要的什么呢最重要是在回答当中就这些Response的后面要有sync或者是resign的标签和Answer的标签这个是我们训练的关键目的那也是我们的格式奖励原刘文文里面的所谓的格式奖励我们看看到现在未来还没有对吧也没有没有没有报道机要耐性一下一会儿我们在说说这块的一些发现还是没有所以说你说运营这么长时间没有特别大的意义和价值没有我快速走一走这个是个比较训练模型是个比较耐性的伙儿看一看好来这个是个比较好的例子你看这个这个问题这是一个问题这个问题的证据答案是18然后这个是模型输出的其中一条答案那么也是有思维的过程它第一步第二步所以然后最终结果是对吧box是吧这个计算是正确的但是它什么呢它格式不正确它的格式应该把这个是8包在哎呦包在这个Answer和Answer的两个标签里这是我们希望它输出的格式所以呢我们就没有extract没有抽取出来正确的答案那么没有抽取出来也就意味着这个这次的循环没有得到奖励信号但是这个格式box应该得到奖励信号了我们就继续往下看什么时候它永线出来一次碰巧碰对了之后模型的永远朝着那个方向去走了看一下我们就大块布的往下走一走这个好这个跨过来了看看从哪开始呢从这开始吧把这上面都已经开始了你看这是一个问题然后证据答案是23然后我们的模型回答已经开始有格式了就是Risen和Risen中间这是我思考的过程然后我的答案是23对吧我放在Alser这里这个时候我就可以用我的抽取的服助感受把这23抽出来这是我的23所以我抽出的答案和我的正确答案样们极力给出正确答它是相等的这个时候这一条去年就得到的2.0分并且能得到了这个格式奖励所以就加起来是2.5还是3.53.5那么这一个样们反回去更新模型参数的时候它下一次永远都是在这种看这种Risen的RisenAlser当中就输出了我看看什么时候开始你看这就开始基本上都分问题了RisenAlserAlser也就是这开始看到没有这个是问题答案然后Risboss回答就开始RisenAlserAlser然后这是Alser这还有点问题你看这个Alser里面多了两个这个英文其实答案的是2400这是2100但它多了一个这个所以慢慢往后的时候这些被优化掉了直接变成数字在里面很神奇的一个过程你看往上去的时候已经有Risen和Risen这种感觉了对吧Risen中间是思考过程但是我们提出出来的没有因为它没有这个回答的部分AlserAlser的部分所以一点点同时开始就特别整齐奖励信号回去了以后强化学习就非常整齐你看AlserAlser11虽然在这个不对你看这个Alser正确的还是2但是它推理说结果是11所以这个样本集里面就没有得到这个准确度的两分奖励而只得到了什么格式的零点五分奖励所以一直往后看前都是Risen的Risen就保持了这个对线就是强化学习的强大支出当然我们推理过程我们也限制了200个投坑之内所以它这个推理过程还是比较短的如果我们有比较大的算力的时候可以把它限制在比如说两万个投坑那你就会看到比较长的一个推理过程好吧 这个后面都是挺正确的我观察了一下其实这个运行的500部左右就是有这种效果了只不过你的答案是不正确的那么答案不正确它就会反复推过来重新去生成接近于答案正确的这个推理那么一旦你推理的结果正确格式的正确的时候就给你讲例值然后永远保持这样的一个参数空间区集型推理那么类似的问题的时候它也能同样的这个部署给你推理出来这个就是往后去它就往后去基本都是挺好的一个可是复杂一点它已经推出来3333这个是正确的1125这个正确的当然是不对这种简单一点你看比较短的基本上还都是正确的所以这个就是我们训练出来以后的这样的一个效果那么就有两个问题了你看这个第一个问题就是我前100部左右的时候其实这些资源都浪费了这些极算的浪费了那也就是为什么它们在训练这个21这个21的时候它先有离不什么SFT之后再做RL这个SFT用了很多个推理样板去怎么样去做监督微调也就是说这一部很关键能减少我们前期这个慢循环这个时间就假如说我在这个用的基础模型大家回头往上看看太长了把这个一面放上去我用的这个基础模型是在训练的样板里这个QRL50.5B Instruct那我第一步先对它做SFT我先拿一些咱们说就是我这一批训练完的对吧 已经好的效果的样板了也就是我刚才下面你看到输出的那些正确的东西我把正确挑出来先拿它做一次SFT好了 这个就相当于这一步它的料比我们要大了一个上半倍甚至上几十万倍有了这个之后我们拿它比如说出来这个模型我们保存一下叫做这个叫接着接着我们就从接着开始因为这个接着已经有一些输出的格式了那我们再以它为基础去做强化学习这个时候我们往下就不用经历前面这些过程了就是一个比较痛苦的等待它自然而然有限做正确打案和正确推理不减过程那么它刚开始输出的时候就会尊敬一个我们希望它得到的过程然后我们下一步SFT增加我们样板数量其实它就更多什么更多做能力的方法了这个是得到的第一个起发那第二个起发就是其实这个真的很简单你看在这过程当中我们没有用第三第二个不同的模型我们只用了什么奖励韩朔和我们在训练的这个模型没有其他的模型所以整体的运算量很低也不需要在中间的推理过程上做什么指导只需要指导最后的正确打案的一步就行了就告他正确打案是对不对的对吧这一步让做到奖励韩朔只要给回去那么也就是他谈判话那你在一个这么小的数据级他能看到犯话的效果所以说在一个更大范围的数据级上在一个基于V3几千亿参数数级上基于几十万条的样板数据那样板数据包括的推理数据和非推理数据一起去做犯话的时候对吧这个是不是效果是更好呢那么一定是看到了基础效果以后看到这个训练的过程然后能给他犯话掉并且他犯话过了那种他加入了非推理样板和推理样板推理样板还分两部分一部分是这个数学代码推理样板一部分是逻辑推理样板这个逻辑推理样板也是人工筛选出来的通过V3生成的很多这种非数学和代码所以这种东西能让大家用的时候做了一些犯话好了那这些东西加上这个推理和非推理之后一起给到我们的这个sft坚度微调一步就让模型生成出来就你问推理的问题他给你生成的推理步骤你问普通问题比如说你好今天周急今天天怎么样他就会给你生成普通简单点的这个回答不需要推的回答因为经过了sft但是在这之后又经历了一次强化学习这个强化学习主要是干嘛的提升模型能力这个能力提升的部分你看看我认为这没说就是一个drpo同时把两种样本结合在一起那么动态判断美如果你这个样本是非推理的话我这个drpo里我们讲过了调这个t2策略t2的细术coefficient然后我们就可以用把drpo改成类似偏向于sft的这样的一个t2公升方式如果你是强化学习的那我就用正常的对吧我的drpo的公式最终得受我们最终的r1但是虽然这个过程在pypyline流程设计上比较复杂但是原理我们可以就比较简单的给它复现出来当然书记习很重要如果说这些你都改成中文的书记习那么它出来的效果就是中文的没有问题如果你把它改成泛化一点的就是不是书记提了代码也是一样只要你能验证这个最后的输出的结果对不对比如说通过test class对吧验证我们的代码对不对只要能给回讲力信号就能做到这件事再研设一点也就是他们接下来要包括opunai刚刚发的o3和dips一个可能会做的心模型就是做更多的这个泛化也就是说我们要有更多的数据级能够验证的或者打号标签的这种推理数据级有的话这个通过线我的这种强化学习模式能看得出来就能够做一些泛化但这种泛化是基于他们底层所有训练过的这些文字的每一个文字就是一个项量在高为项量空间中在推理过程当中的一种进式对吧进式在一种投影投出来了谁和谁哪一两个相对在一起那两个文字就会连续性地出来那么形成这个推理过程整个是一个逼近的方式我们通过大量样板训练出来它的一个逼近方式表达形式的逼近方式在这个共产里面是不是智能没法定义看我们怎么定义智能但有没有意识肯定是没有意识的我们训练一遍我们就知道这个东西有什么意识上肯定没有意识但是你把它放到一定大的量以后几千亿财数上万亿财数的时候相当于现在加了M1就模拟人当脑一样M1就是你把6千亿财数猜上了20个3百亿财数的将我们脑子里面20个共产区域对吧通过多摩太模拟人眼睛耳朵通过M1模拟脑子里面不同的处理区域也是这个方向但是肯定从效率上来说最终的人工质的AGA一定不是这个方式的你看这个多慢的训练所以还是有一天路要走的但是目前这个是大家认可的路所以怎么样的加算力也就是说我加数据加算力那么通过强化学习你看这不都能算对了吗而且还有推理扩证所以在这里面我们可以在展示展示其他的推理过程比如说随机找一些问题我们可以我试一下中文效果不太好推理的效果不太好可以试一下我们先问一些英文问题比如说刚好50多少回答这种问题这是个四号过程我们同样问题我们放在这个里面去这个是没有经过我们刚才被调的原始模型2.2362.236对答案是一样的但明显这个是四号过程更长我们把这个只不过是看起来比较麻烦我们给它放在这来它应该能输出一个是没输出一个是几个换一个问题用中文问一下试试数上一只猴这个可能不一定能做得很好的范化因为我们这个训练样本集是这个数学题的样本集一只猴数下你看我从你给你整个问题数下一只猴数上其个猴其这其一个猴请问你一共几只猴这个不是很好问题就是赵本山的一个搞笑的问题我们看它推理过程从题一来看已经固定了你看这个就是推理了对吧这里对对对对对对那么整句的答案它不一定能总共不有推三米咱就不看答案正确性但是这个过程给你一顿分析这个过程是不是很有意思就是我们看到这种COT推理的模型当然它效果准不准你需要通过更长时间的训练我们把这个题目放到原始的没有经过我们COTB条的这里就开个新传口别让它影响我们这个效果这个是没有经过训练的看看总共一只猴回答也不对其实有两只猴子你看它就没有整个的推理过程这个一大堆给你推理还有酸树的过程就感觉很不同所以可以试试大家这个代码我都放在视频的评论里面去同时这个很有意思过程当中对现在还得跑这个是A100的服气过程当中大概以我的这个超三数的话大概是能扩大一倍扩大不了了我40g的那一寸基本上就是一台机器得跑四五小时能出来一个最终的效果模型现在是一切因为2000部但是这个给我们很好的一个起事就是替换不同的数据集只要你后面能有个厌证的奖励还是用方法的话这个是最简单的通过强化学习犯化模型等力的一个过程并且这个模型是非常小的Q-1只有5亿才数的模型所以非常快也就是说通过Q-1直接做强化学习比他们原论文里说的比从这个这个这个Depthic23去争流到Q-1那点5b的效果应该更好是不是这个我们计算的吧应该是这个它得到这样的一个结论就是也是这么多拜拜拜拜
