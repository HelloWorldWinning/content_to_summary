Timestamp: 2025-02-07T21:05:01.385175
Title: 大模型的 蒸馏 (Distillation)究竟是什么，关于deepseek BV1xaNnegEhz
URL: https://b23.tv/vSIbinN
Status: success
Duration: 24:40

Description:
好的，这是对您提供的内容的总结，并根据您的要求进行了组织。

**核心思想总结：**

*   **核心要点：** 知识蒸馏是一种模型压缩和迁移技术，它允许小模型（学生模型）从大模型（教师模型）学习，从而在保持性能的同时降低计算成本。
*   **根本要点：** 知识蒸馏的本质是知识的迁移，而不是简单地复制模型结构，学生模型可以通过优化架构、知识结构和学习过程，甚至超越教师模型的性能。

**内容概要:**

1.  **引言：**

    *   讨论了围绕DeepSeek模型及其使用的知识蒸馏（Distillation）技术的争议。
    *   争议点包括DeepSeek是否通过知识蒸馏从OpenAI等大型模型中提取知识，以及是否大量访问OpenAI的API来获取数据。
2.  **知识蒸馏（Distillation）的解释：**

    *   强调知识蒸馏是“知识”的蒸馏，而非模型架构的复制。
    *   类比为老师教学生，通过训练数据让学生模型模仿老师模型的输出。
3.  **知识蒸馏的过程：**

    *   Teacher AI（教师模型）：已训练好的大模型，具有丰富的知识。
    *   Student AI（学生模型）：结构简单，通过比对Teacher AI的输出，调整自身参数进行学习。
    *   训练数据（Trinity Data）：用于训练模型的数据集，输入到Teacher AI和Student AI中。
4.  **知识蒸馏的优势：**

    *   **架构优化：** 学生模型可以去除教师模型中冗余的部分，使结构更紧凑。
    *   **知识结构：** 学生模型可以设计更清晰的知识结构，提高学习效率。
    *   **学习过程：** 学生模型可以通过调整学习过程，获得比教师模型更集中的预测结果。
5.  **DeepSeek的技术创新：**

    *   **混合专家模型（Mixer of Experts, MOE）：** 将模型分成多个专家模块，针对不同问题启动不同的模块，提高运行效率。
    *   **多头注意力机制（Multi-Head Attention）：** 通过多个注意力头，使模型可以同时关注多个知识领域，提高学习速度和知识整合能力。
    *   **多Token预测（Multi-Token Prediction）：** 一次预测多个Token，提高预测质量。
    *   **流水线并行（Duror Python）：** 充分利用计算资源。
    *   **混合精度（FPA）：** 根据情况使用不同精度的浮点数，减少内存占用，提高性能。

**总体框架（Overarching Framework）:**

内容围绕DeepSeek引发的知识蒸馏争议展开，通过解释知识蒸馏的概念、过程、优势和DeepSeek的技术创新，旨在帮助读者理解知识蒸馏的本质和技术价值，并对相关争议做出自己的判断。

**Mermaid 流程图：**

<Mermaid_Diagram>
graph LR
    subgraph Teacher AI (已训练好的大模型)
        A[Trinity Data 输入] --> B(Teacher AI);
        B --> C{输出结果};
    end

    subgraph Student AI (待训练的小模型)
        A --> D(Student AI);
        D --> E{输出结果};
        E --> F[比对 Teacher AI 输出];
        F --> D;
    end

    subgraph 知识蒸馏 (Distillation)
        direction TB
        S[优化架构] --> T{提高效率};
        U[设计知识结构] --> V{提高学习效率};
        W[调整学习过程] --> X{获得更集中的结果};
    end
    C --> F
    Y[MOE] -- 专家模块化 --> T;
    Z[Multi-Head Attention] -- 多头注意力 --> V;
    AA[Multi-Token Prediction] -- 多Token预测 --> X;
    AB[混合精度] -- FPA --> T;
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#fff,stroke:#333,stroke-width:2px
    style E fill:#fff,stroke:#333,stroke-width:2px
    style F fill:#fc0,stroke:#333,stroke-width:2px
    style T fill:#fff,stroke:#333,stroke-width:2px
    style V fill:#fff,stroke:#333,stroke-width:2px
    style X fill:#fff,stroke:#333,stroke-width:2px
    style S fill:#fff,stroke:#333,stroke-width:2px
    style U fill:#fff,stroke:#333,stroke-width:2px
    style W fill:#fff,stroke:#333,stroke-width:2px
    style Y fill:#fff,stroke:#333,stroke-width:2px
    style Z fill:#fff,stroke:#333,stroke-width:2px
    style AA fill:#fff,stroke:#333,stroke-width:2px
    style AB fill:#fff,stroke:#333,stroke-width:2px

</Mermaid_Diagram>

希望这个总结对您有所帮助！


Content:
大家好 这里是Jeff科技视角李老师又拿出白板来给大家讲一些硬核一点的知识了今天的话题是DipSick以及DipSick争议过程中间大家讨论的一个词叫做Distulation或者说叫做争流那么最近两天外媒针对这个恒空出事的DipSick有了一些争议争议主要集中在两点第一点就是DipSick采用了叫做Distulation的技术从这个OpenAI或者其他的一些大模型里头把知识体验出来然后变成自己的DipSick模型看起来就好像是梯庭是这个欺骗是复制或者说仿制别人也有的成果第二就是外媒引起争议的就是OpenAI说在去年秋天的时候他们发现和DipSick相关的一些大量的对OpenAI GPT模型的访问当然访问是通过API来进行的这些访问获取了很多很多的OpenAI模型的这些知识就是问答的方式就各种各样的问题批量很大相当于把OpenAI里头的知识去给弄出来然后训练的DipSick的模型基本上争议就是这两点那么我作为吃瓜看细的加拿大居民我不想去判断这个到底事情是真是尾我只是给大家介绍一下在大模型里头实际上不光是大模型几乎所有的使你马路的模型都有所谓的Distirrelation的过程这个过程到底讲的是什么我希望能够让如果您是文科生都可以听明白的方式来解释懂了这个概念懂得基本的原理你自己来判断到底这个是什么样的行为首先说什么是Distirrelation你们媒体通常会说是争流那么我更愿意加上知识争流它争流的并不是大模型的架构它只是争流的大模型里头包含的Nolage包含的信息这个首先要厘清所以这就是为什么我特地强调Distirrelation是知识争流不是整个系统加多的超级好那么首先争流这个动作是怎么来做的实际上它就像一个老师教一个学生一样就跟我们平常学习尤其是课堂学习是非常类似的当然这个学习更多的是以做息体的方式来进行那么首先我们会有很多的Trinity Data训练模型的数据那么训练模型的数据我会把它分别Input 输入分别网我们叫做Teacher AI就是有的大模型我送这个比如说是Gpt我假设或并不是说一定是Gpt我只是说假设它就叫GptTeacher AI另外这个绿色的我们叫做Steel的这是一个学生系统好之前刚开始学习的时候Teacher AI已经训练好了里头充满了各种样的知识它的系统加速大概就是这样的很多的输入的使宁愿中间有很多的Heated Layer隐藏使宁愿最后有一个输出比如说我前面把一个图片输进去最后输出是猫还是狗好那么Steel的AI刚开始在这个做Distillation的时候它里头是空的它只是有一个结构但是结构里头所有这些点实际上每一个使宁愿它里头都有一个机会韩数有相关的参数那么通过机会韩数和参数的这种计算最后得到结果之前肯定是一张白纸进来的是一个从来没学过相关知识的一个人那么训练数据进来一条记录这前面很多输入然后一个输出那么输入到Teacher AI里头它就会有一个相应的奥土同样的当我配置好的出始参数之后我同样的一个训练的一条记录也是输入一个图片然后进来可能这个系统BIRD弄出来Teacher系统预测的是一个猫结果Steel的AI预测的是一头猪好了毛根猪之间有共同点对吧都是四条腿但是还有很大的差异这时候Steel的AI就会按照Teacher AI输出的结果去调整自己的这些参数里头的包括这个几乎韩数的这些相关的Pyramid一路跳过去 往回调所谓的BP那么这种方式情况下好我每一条记录是不是都可以比对我Teacher AI和Steel的AI的结果注意这两个系统之间的结构可能是完全不一样的因为Teacher AI有可能是完全封闭的就是一个黑盒子你不知道里头到底是一个什么东西那么作为Steel的AI来说它只是去按照训练数据它的输入和输出去比照调整自己的这个结构然后这样自己的结构的输入跟输出尽量符合Teacher AI的系统的输入输出的结果就有样学样 寒难学步那么这时候你就很容易知道首先它真正学的是什么学的是Nollage 学的是知识它并不知道里头的结构是什么结构你需要去自己去设计第二网网Steel的AI在做征流的时候做Decealation的时候它的这个结构会比Teacher AI会更简单为什么呢因为Teacher AI学的东西很砸而且它未必是一个最优的结构它这个结构里头可能会有些荣誉的地方或者说有些地方比如说这块亚根就没用上比如说这块亚根就没用上但是它还是存在那时候每次计算的时候它还是参与计算但是计算完了之后到这边所有的输出比如说都成了一个零那你前面不管算了多辛苦后面都是摆搭因为成了零就等于全部都清了那么Teacher AI里头系统可能会有很多很多地方都有类似这样的情况那么通过Decealation我可以让我的Steelation AI更紧凑我可以用一个相对简单的比如说我的Teacher AI从说要更少是美元跟数量也更少同时我的架构之间有可能通过一些优化的方式我后面会讲到有关的优化的内容通过一些优化的方式使得我的性能更好计算速度更快同时我的结果几乎跟Teacher AI是一样因为它怎么预测我也怎么预测那么这时候当Trinity Data足够多的时候或者说Trinity Data对于Teacher AI里头的知识是覆盖的足够权的时候我的Steelation AI就是可以无限逼近于Teacher AI好如果说注意我这是如果说Decealation的方式从OpenI模型里头提取这个知识出来去构建自己的模型它有没有可能超过Teacher AI或者是超过GPT的现有的表现这个是非常有可能的为什么我们需要从三个角度来看首先系统架构刚才我其实已经提了Steelation AI的系统架构是经过优化的Teacher AI的系统架构里头可能会有一些我刚才讲过的这些论语的地方其实没有什么用但是你在过程中间你并没有真正的去有时间去做优化那么如果说Steelation AI了解到Teacher AI里头的一些这些问题它就会手动的先把它做一些优化把这些我认为它应该就是没有用的我首先叫Cut掉我们叫做剪织就像一个树一样有个这个结果子那个结果子有10个Brunch有10个树差子结果有3个树差子根本就不结果子那么我就所幸手工把这3个树差子剪掉这样子我重树的时候养分也不需要消耗那么多对不对同样的Teacher AI就是这样一个10个树差子的树其中有3个不结果那么Steelation AI在构建的时候我就直接把这3个剪掉了对我来说一定是它的这个我是肥得到的这个果子的这个效率肯定会更高对吧首先这系统架构层面它是可以做优化的第二知识结构方面Teacher AI里头的知识非常丰富的大模型里头各种各样什么样的知识多语言的知识各种领域的知识都有那么如果你没有得到Teacher AI的这个系统设计层面的帮助的话你想做Steelation的话其实挺难的为什么它到底有多少知识成什么样的结构的分布你是不知道的还是个黑盒子这时候你需要自己有一个清晰的知识结构的突补我知道好这个Teacher AI里头一共的知识可以分门别类是比如说25种每种里头可能又分成多少个子类然后我有一个股皮匙脑的里头然后我每一类去分别比如说假设用1万个问题来覆盖那么25类的话就是25A个那么25A个问题可以覆盖Teacher AI里头所有的知识然后我提完这25万个问题拿到相应的答案我就可以用这25万个问题以及相应的答案去Train我的Steelation AI如果我的知识结构设计的特别合理的话那有可能我这25万个问题真的就可以覆盖到Teacher AI的百分之99的知识内容25个大类的那么这时候我的Steelation AI我可能训练过程就非常简单因为Steelation AI如果它的知识结构设计的没有那么合理的话我生成这个系统可能花了没必100万个问题才真正把这个模型训练出来但是Steelation AI因为对知识结构理解更加深刻它用25万个问题就可以覆盖了你100万个问题覆盖了所有的知识内容了那么这时候你的训练速度是不是更快你模型也可以更小而得到和Teacher AI相近的这样的性能对不对好这是知识结构方面的提升第三个就是学习过程学习过程刚才实际上也提到了一点就是除了这个简直之外整个的学习过程最终的结果大家不要以为是简单领或者猫或者狗它有可能输出结果比如说Teacher AI的系统输出的结果是这样的一个分布中间最高的这个是Hyperpublicity就是最高概率的结果比如说是零年我好了这个是预测Teacher AI预测结果是这样但是Steelation AI我可能预测的结果是这样一个情况没必这样的OK好那么这种情况下Steelation AI跟Teacher AI之间有差别吗当然有差别对吧这个差别就是这两个封之间的Gap这就是两个系统的差别但是随著训练的次数的增加我的Steelation AI不断随著Teacher AI的结果去做调整调整的过程就是绿色的封就慢慢往这边逼近Teacher AI的终均值应该叫命那么最后结果就是Steelation AI最后的是这样的好了我们你谁更好Sony的话Steelation AI应该会更好因为它的预测指的情况下它的标准差更小这个讲大白话就是更集中对吧那么在这种学习的过程就有可能Steelation AI会生成比Teacher AI更合理的更集中的这种概率密度韩数所以呢这个也有可能对于Steelation过程中间Steelation AI会得到比Teacher AI更好的performance所以我们从系统架构从知识结构的设计到学习过程和相应的结果调整都可以发现Steelation绝对不是大家自面上理解的蒸流我就一盆这个化学的药品然后自己加热然后让它更浓一点不是这样的这个东西对于它来说有可能是一个黑盒子你只知道它的输出出中间所有的你自己的系统设计知识结构设计给学习过程都需要你自己单独的去抵抖所以说一文DeepSec是通过Steelation去生成这个模型仍然有相当大的难度或者说有相当大的技术海量所以这个就是我想表达了那么至于DeepSec通过有没有通过其他的一些更多的手段去或许比如说 OpenAI的一些东西我不知道我只是告诉你DeepSec is what is the definition of distillation以及不是DeepSec is the definition of distillation一招你就可以靠配很多东西了这些东西全部都要自己去抵抖好再说DeepSec这次的技术上心有那些除了这个DH AI它能做到的Steelation AI也能做到之外它还在工程方面做了很多新的革新注意这个革新技术并不是DeepSec自己首先发明的但是它在模型里头目前它的应用方式是效率是领先的比别人做得更漂亮明白吗那么其中包括那些首先是 Mixer of Expert我们叫做MOE或者叫做专家模化什么含义呢像提示AI里头你知道现在的大模型动折就是几百个比例的残数甚至还有可以到一个吹脸的残数这量是非常大的你这样大的一个模型你要乱起来的话做任何一个预测所有的残数全部算一遍的话系统消耗是很大的同时你时常也很常那么对于DeepSec来说它就做了一些区域化分整个DeepSec的R1模型应该是671个兵临的残数6710亿残数它在里头做了一些专家系统的化分这个专家系统就说这个巨大的模型里头我这块可能专门是算数学的好我这块可能是专门讲军事的这块我可能是专门讲八卦的这块是音乐这块是美术因为它的这个残数量那么所以它设了很多的Expert的模块每个模块的基本上它模块大小就是37个比例基本上只是全部所有残数的来购20分之一这样的话我针对不同的问题我就可以启动不同的专家模块来回答当你去比如说绘画的时候它可能只会E-Nable这绘画专家模块里头的40个比例的残数其他比例的残数根本不参与计算那么这时候你的运行要力是不是会更高因为你不需要的引涡那么多残数来计算了所以这就是Mixed of Expert优势你可以看作是三个臭皮胶组成一个猪蜡对不对一堆20几个每个人承担不同的角色然后对外是用一个声音来说这个就是Moe另外就是MultiHalluLate Tension所谓的这个多头的潜在注意力的机制我们知道整个的Large Language Mode核心就是传统的这种Transformer的计算那么在Transformer计算过程中因为它引入了一个Attention机制就是你在学习过程中始终它可以根据学习的内容去Focus的一些Attention的领域那么有时候学习的内容因为比较多所以如果你Attention的领域相对比较狭窄的话你每次学习就只能学习一个方向的东西那么你的学习速度就会比较慢而且知识接的观点有时候就很难去令他到一起那么MultiHalluLate Tension的这种通过潜相量的方式它可以动态调整你的Attention讲到这里已经有人晕了你到底Attention去到底是什么告诉你有一个效果你肯定听说过一个灵鲁的一个灵鲁的一个灵鲁我问你一个问题你自己算一算你看能不能回答出来小明上了公共汽车到了第一站上来三个人下了两个人到了第二站上了这个三上了五个人下了三个人然后一直说下去说完之后被问的人在行李头计算到底加加减减多少人最后问题是什么经过了多少站那么这就是一个点行的也不能叫点行的就是让你更加容易理解什么事你再听这个问题的时候你在心里你的Attention是车上有多少人但实际上这个问题最终问的是你过了多少站你的Attention如果只Focus在人数上你一定会听完之后马上就拿到问题就傻眼了但如果你Attention是同时戒Attention到车上有多少人同时又Attention过了多少站你这个Multi-Head两头的Attention你就可以回答这个问题了不管它问什么你都可以知道那么可能还会有更多的情况这只是打个比方让大家知道为什么我在知识学习的过程中间我Multi-Head的Attention可以起到这么大的作用除此以外就是Multi-Token Prediction大家知道Lunch Language Model在运行的时候它是以Token的方式去读内容然后给你回答回答的时候也是Token的方式来进行那么以前所有的Lunch Language ModelToken都是一个一个Todd我预测下一个Token是什么然后预测完之后再预测下一个Token所以就是像怎么说了总之它就是以顺序的方式这么做预测的现在是可以Multi-Token现在Multi-Token可以一压一两个两个Token同时按顺序输出的好处是什么因为你预测这个Token之后你预测下一个Token始终有机会会出现一些意外情况导致你飘移了原来的主题就你预测的下一个Token跟这个Token之间关联性可能没有那么强那么这时候你的预测质量就不高但如果你是一次就吐一压一的话那你前面一个Token跟后面一个Token的关联性一定是很强的因为你是在同一个计算过程中生成的一压一的Token这就好像我一个困子我这个困子本来是一米一米一米接起来的我另外一个困子是两米两米接起来的你觉得每个困子更接时在同样长度的情况当然是两米两米两米两米的更接时因为它的连接更少那每个Token之间如果你是单个Token吐的话有多少个Token就相当于有多少连接我现在是毛巨Token的话我两个Token的话我的连接数就直接减少一半我不知道有没有解释清楚大家凑或者理解了其他的一些包括DurorPython通过这个剥管道的方式你相当于每一次计算处理过程都是一个Python当你这个Python发现这个计算的效率就是你计算的时候突然发现这个Python空间的时候我可以把另外一个Python的内容直接拿过来进行计算所以这个是DurorPython去更加充分地利用计算资源另外还有FPA的混合精度这边多讲两句能听到这都已经是真爱了所以最后把稍微有点技术难度的内容放到最后面讲不想听线就可以测了大家知道计算机里头基本上数字就是两种一种是整数整形的一种是福点的整形的就是八位福点的就是有八位的有16位的有32位的对于神经网络的模型来说输入通常都是福点的比如说32位福点除此以外每一个神经园里头的这些机构寒数和相关的参数也都是FP32的缺省的所以这种情况下所有你在这个正常的模型里的碰到的所有的Number所有的数字都是FP32的它占用的空间是很大的如果我要想做一些系统优化的话我是不是可以去取一些相对比较短的比如说特定的输入我可能就直接去FP16我不要去FP32我只要去FP16我的这个要占的这个位置立马就少了一半我如果所有的这个输入和神经园里头的机参数全部都用FP16的我整个的内存占用就是正常FP32的一半如果我要再恨一点我用FP8我又少了一半FP8是FP32的1%但是你这么做有没有风险当然有风险很多时候预测的它有可能它会有一些例外时间比如说我的数据分部本身是这样的数据分部可能是这样的这个是大部分情况下数据都集中在这个区域里头但是很不幸我这会有一些Outline的值一些例外值如果我要去FP8的话那我可能FP8这些值我都都能取进去但是把Outline就丢了如果是FP32可能就是所有的包括Outline的值也能够取进去那么对于FP8的这个模型来说我可能90%8%的情况下我都能够得到准确的回答但是有2%的情况下就是这些Outline的情况下我都回答是错的那么FP32的情况下我可能百分之百的回答或者是99.99%回答都是对的但是只有很小概率会错那么这种情况下就是你的取舍了对吧你到底是用FP32消耗更大一点但是我只去度更高还是说我个别情况下我会用FP16在个别的情况下我用FP8来减少整个系统的这个资源开销提高这个性能所以 overall来说呢Disturulation它不太会就是通常的Disturulation不太会超过你被征流的这个系统的这种机效评测指标可能会差一点差的不会太多尤其是在这种基于这个混合精度的情况下但是你通过一些特定的设计对吧你是可以把你的最终的结果比提高到比以前的这个Disturulation系统还有好的情况所以这是一个非常复杂的问题我觉得大家都看多想多了解这方面的知识对于你得出自己的判断还是会非常有好处的好吧今天就讲到这里说大家有什么问题的话可以再评进去提出来以后这些东西有时间的话我再给大家细讲了今天只是 overall 主要核心在讲Disturulation是神仗大家没有这方面的知识也了解到底是怎么回事好 今天到这里了谢谢 拜拜
