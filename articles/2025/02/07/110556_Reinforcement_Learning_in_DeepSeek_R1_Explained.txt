Timestamp: 2025-02-07T11:05:56.398129
Title: Reinforcement Learning in DeepSeek R1 Explained
URL: https://youtube.com/watch?v=H20Hd6Xb7Qo&si=bxz408u3c860yAz_
Status: success
Duration: 11:30

Description:
Here's a structured summary of the content, along with a conceptual map:

**1. Outlined Summary:**

*   **I. Introduction**
    *   DeepSeek's R1 model.
    *   Understanding Reinforcement Learning from Human Feedback (RLHF).
    *   Group Policy Optimization (GPO) algorithm.
    *   Prerequisites: RL, PPO, Transformer knowledge.
*   **II. Reward Model Training with Human Feedback**
    *   Agent interacts with environment (state, action).
    *   Collecting trajectories (state-action pairs).
    *   Reward model assigns scalar reward to each state-action pair.
    *   Human preference guides reward model training.
        *   Maximizing probability of preferred trajectory.
        *   Using negative log likelihood for optimization.
    *   Single Reward Value Estimation.
        *   An alternative approach is to estimate a reward function with a single reward value for the entire response.
        *   Forcing the reward model to generate a higher reward for the response preferred by the human.
*   **III. Proximal Policy Optimization (PPO) Algorithm**
    *   Objective: Update policy to maximize reward.
    *   Advantage value: Determines if action is beneficial.
        *   Positive advantage: Encourage action.
        *   Negative advantage: Discourage action.
    *   Policy ratio: Ratio of the new policy to the old policy along with the advantage function.
    *   Clip function: Limits policy change.
    *   Reference policy: Prevents divergence from the original policy using KL Divergence.
*   **IV. DeepSeek R1 and Group Policy Optimization (GPO)**
    *   R1 uses RL for reasoning without supervised data.
    *   Addresses issues with reward hacking (rule-based rewards).
        *   Math problems: Compare answer with actual answer.
        *   LeetCode problems: Rewards based on test cases.
        *   Correct formatting: Rewards for thinking/answers.
    *   GPO: Encourages better-than-average responses within a group.
        *   Normalizing rewards.
        *   Encouraging rewards above mean, discouraging below.
        *   Averaging the objective term over the entire group.
    *   R1 model specifics: Base model (DeepSeek V3) trained with GPO, R1 uses initial supervised CoT data.

**2. Core Point:**

DeepSeek's R1 model leverages Group Policy Optimization (GPO), a reinforcement learning technique, to improve reasoning abilities by encouraging better-than-average responses relative to a group, guided by rule-based rewards and an initial phase of supervised chain-of-thought training.

**3. Fundamental Point:**

The key innovation lies in optimizing the policy directly through group-relative feedback, moving away from solely relying on potentially exploitable reward models to enhance reasoning capabilities.

**4. Overarching Framework:**

The framework consists of transitioning from Reinforcement Learning from Human Feedback(RLHF) which leverages reward model to Direct Preference Optimization(DPO) which leverages Group Policy Optimization to optimize reasoning in language models.

**5. Conceptual Map:**

<Mermaid_Diagram>
graph LR
    subgraph RLHF [Reinforcement Learning from Human Feedback]
        A[Agent Interaction] --> B(Collect Trajectories);
        B --> C{Human Feedback};
        C --> D[Reward Model Training];
        D --> E(Policy Training);
        E --> A;
        style RLHF fill:#ADD8E6,stroke:#333,stroke-width:2px

    end

    subgraph PPO [Proximal Policy Optimization]
        F[Old Policy] --> G{Advantage Value};
        G -- Positive --> H[Encourage Action];
        G -- Negative --> I[Discourage Action];
        H & I --> J(New Policy);
        J --> K{Clip Function};
        K --> L(Reference Policy);
        L --> F;
        style PPO fill:#90EE90,stroke:#333,stroke-width:2px
    end

    subgraph GPO [Group Policy Optimization]
        M[DeepSeek R1] --> N{Rule-Based Rewards};
        N --> O[Response Group];
        O --> P{Normalize Rewards};
        P -- Above Mean --> Q[Encourage Response];
        P -- Below Mean --> R[Discourage Response];
        Q & R --> S(Policy Improvement);
        S --> M;
        style GPO fill:#FFA07A,stroke:#333,stroke-width:2px
    end

    RLHF --> PPO -- Guides --> GPO
    style RLHF stroke:#333,stroke-width:2px
    style PPO stroke:#333,stroke-width:2px
    style GPO stroke:#333,stroke-width:2px

    subgraph Key_Elements [Key Elements]
        style Key_Elements fill:#FFFFE0,stroke:#333,stroke-width:2px
        direction LR
        T[Human Feedback]
        U[Reward Model]
        V[Advantage Value]
        W[Reference Policy]
        X[Rule-Based Rewards]
        Y[Policy Optimization]
        T --> U
        V --> W
        X --> Y
    end
    GPO --> Key_Elements

</Mermaid_Diagram>


Content:
recently deep seek published its paper we are going to explore how the algorithm behind their latest R1 model Works however to understand this we first need to grasp how reinforcement learning through human feedback functions finally we will examine the group policy optimization algorithm which is the algorithm deep seek used to train their recent thinking model r one that said there are some prerequisites for this video you should have a basic understanding of reinforcement learning you should also have some knowledge of the proximal policy optimization algorithm and also Transformer architecture in large language models for a simple overview I explained the PO algorithm in my previous video however there are many other videos available online on the same topic you can watch those as well now let's get started let's first understand how a reward model is trained using human feedback consider a simple reinforcement learning agent interacting with its environment the agent observes the environment we also use the word state to refer to its observation based on that observation the agent takes an action in the environment normally the agent receives a reward value from the environment after taking an action but here we want to train the agent using human feedback so how do we design the reward system with human feedback we simply collect a list of trajectories from the data which is essentially a collection of states and the actions taken on those states in the case of llms you can consider the output response of the llm where every token generated using the context is in action and the context itself is the state the reward model takes an observation action pair and provides a scalar value as output which represents the reward for the action taken in that state but how do we train this reward model let's say we have collected two trajectories now we receive feedback on both trajectories from a human in this case let's say the human preferred the first trajectory as the better one we feed each pair in the trajectory to the reward model and it predicts a scaler reward value for all the pairs in both trajectories our goal goal is to guide the reward model to assign a greater reward to the trajectory preferred by the human in terms of language models the answer preferred by the human should receive a greater total reward in short we want the total reward given to the trajectory preferred by human feedback to be greater than the trajectory not preferred by human feedback initially we assumed that the first trajectory is preferred by the human so it should receive a higher reward for that purpose we calculate the probability of the total reward for both trajectories using the soft Max equation this involves taking the exponent of the total reward for each trajectory and dividing it by the sum of the exponents of the total reward for both trajectories now if the human preferred the first response we maximize the probability of the first trajectory using gradient descent since both probabilities are dependent increasing the probability of the first trajectory will decrease the probability of the second one and if the human preferred the second response we maximize the probability of the second trajectory using gradient descent for this we usually take the negative log of the probability we want to maximize the goal is to maximize the probability of the total reward we actually guide the reward model to assign a higher total reward to the trajectory preferred by the human and we assign less reward to the trajectory that is not preferred by the human now you can see that we have all three ingredients for a reinforcement learning algorithm we can train any RL algorithm using this reward model this was the basic reward model concept to understand how reward models are trained it generates a reward for each step in the trajectory you can also generate one reward for the entire trajectory you can see the details in this paper from 2017 deep reinforcement learning from Human preferences let's study another way of estimating reward function with a single reward value for the entire response consider we have two responses as input for our language model we feed both responses to the reward model to get the reward for each response the reward model predicts two reward values one for each response with this simple loss function we are forcing the reward model to generate a higher reward for the response preferred by the human the goal of this negative log is to maximize the inner value which ranges from 0 to 1 we use the sigmoid activation function we take the negative log of the output after the activation function by doing this the model will predict a higher reward for the response preferred by the human and it will assign a lower reward to the response not preferred by the human the reward model will learn to give higher rewards to human preferred responses and these rewards will guide the policy Network to prioritize human generated responses this was a basic overview of the reward model let's begin our main proximal policy optimization algorithm this is the equation we will study consider this simple state of two words at time step T where our goal is to predict the next words probability distribution using a language model for Simplicity let's say we have three words for the next token prediction the next word policy for the current state is is simply a softmax distribution over the next tokens we represent it as our old policy which we aim to update during training to maximize the reward let's say we sampled the first action from this policy which has a probability of 0. 32 now we need to determine whether this action is beneficial for us or not for this purpose we use the advantage value which we calculate using the reward model and the value model the reward model provides the reward for this policy while the value model determines whether the reward is higher or lower than expected I have explained in this video how we calculate the advantage value using the reward and value networks but for Simplicity if the advantage value is positive it means that the action we took provides more reward than we expected so it's a good action therefore we should encourage this action if the advantage value is negative it means that the action we took provides less reward than we expected so it's a bad action therefore we should discourage this action let's say this is the new policy we want to learn initially it is the the same as the old policy but we will update it using the advantage the ratio of the new policy to the old policy along with the advantage function can be given by this equation here let's say we have a negative Advantage which indicates that this action was not a good one to discourage this action in the future we adjust the policy for this action remember our goal is to maximize the output of this equation we change the current policy to maximize this output if the advantage value is negative we adjust the policy to decrease the likelihood of that action in the future we discourage this action by decreasing its action probability for the given state if the advantage is positive it means this is a good action then we should increase the probability of taking this action by doing this we encourage this action in the future you can see that the new policy is updated for the given State this policy encourages the action taken because its Advantage is positive indicating it's a good action for the given State this clip function is used to ensure that the new policy does not change too much this means the ratio of the new policy to the old policy should be clipped within a certain range this minimum function ensures whether we need the clipped version of the policy Ratio or not but remember after we change the policy many times our language model can forget what it has already learned right so for this purpose we add a reference policy to guide policy updates at every step this reference policy is taken from a base model with frozen weights so that our new policy does not diverge too much from its original policy let's rewrite the original equation with the KL penalty to prevent excessive Divergence from the original policy this KL Divergence penalty will ensure that the reference policy and the new policy remain close to each other we also have a hyperparameter beta associated with the KL Divergence term which controls the strength of the penalty and the degree to which the new policy is allowed to diverge from the reference policy so in summary we had responses using human feedback we categorized those responses then we trained a reward model on those responses after this we trained our language model policy to prioritize the responses which are preferred by human but there are some issues with this reward model which were solved with direct preference optimization algorithm this paper shows that we do not need to implicitly train a reward model instead we can directly optimize the the policy according to human preference so let's first understand deep seek r10 which uses reinforcement learning to reason without any supervised data for reasoning Purp purposes previously we used to train the reward model and then train the language model policy with reinforcement learning but we faced issues like reward hacking in the reward model so the authors used a rule-based reward for training their thinking model in the case of math problems with deterministic results the model provides answers in a specific format and the results are checked by comparison with actual answer similarly for leak code problems they assign rewards based on test cases they also assigned rewards for the correct formatting of thinking and answers by the language model so if we represent our current policy as the old policy we sample a list of responses for a query using the language model with this old policy for each response we also receive a reward indicating how accurate the language model's response is for the given query now we calculate the advantage by normalizing all the rewards you can see that we are subtracting the mean value from each reward this means that all the advantages less than the mean value will be negative and vice versa for understanding consider this normal distribution you can see that the normalized rewards above the mean value are positive and all the normalized rewards below the mean value are negative remember when we multiply the advantage by the ratio of the new policy and the old policy if the advantage is positive it encourages the new policy to take this action in the future and if the advantage is negative it discourages the new policy from taking this action in the future so in short we have a group of responses for a query we also have their rewards we normalize those rewards now if the reward for a given response is greater than the mean reward value it means this reward is a good reward relative to the group of rewards we have then we encourage our language model to generate that response if the reward for a given response is less than the mean reward it means this reward is not good relative to the group of rewards we have then we discourage our language model from generating response for this reward value in the future in short we are encouraging better than average responses for each group of responses and vice versa so this algorithm encourages the good responses and discourages the bad responses relative to the group of responses for a given query that's why it is known as group relative policy optimization you can see here that this is averaging the objective term over our entire group then we have the ratio of the new policy over the old policy for every response and their corresponding Advantage values you can see that the same clipping is applied to it for small policy updates at the end we just have the KL Divergence penalty to keep the new policy close to our reference model as we discussed earlier so with this group relative policy optimization equation we try to improve the policy from our current policy every time now for the r10 model we have base model you can consider it deep seek V3 we train it with group relative policy optimization but for the R1 model we first start with some supervised data for chain of thoughts before going for reinforc learning training
