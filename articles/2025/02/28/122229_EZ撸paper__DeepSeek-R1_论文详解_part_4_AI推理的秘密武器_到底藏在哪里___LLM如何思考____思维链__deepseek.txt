Timestamp: 2025-02-28T12:22:29.398031
Title: EZ撸paper: DeepSeek-R1 论文详解 part 4：AI推理的秘密武器，到底藏在哪里？| LLM如何思考? | 思维链 #deepseek
URL: https://www.youtube.com/watch?v=JHrt8_YnmWA&ab_channel=EZ.EncoderAcademy
Status: success
Duration: 58:38

Description:
好的，这是对您提供的内容的总结：

**总体目标：** 解释DeepSeek-R1如何使大型语言模型（LLM）学会自我思考，以及思维链（CoT）技术如何提高模型性能，特别是对于需要推理的任务。

**I.早期LLM的问题与人类思维模式：**

*   **A.早期LLM的局限：** 早期的大型语言模型，如GPT-4，在需要推理的问题（如符号问题和数学问题）上表现不佳，因为它们缺乏推理能力，并且分词方式可能导致理解偏差。
*   **B.人类的两种思维模式：**
    *   System One：快速、直觉、有时是情绪化的思考方式，类似于快速提取记忆。
    *   System Two：缓慢、需要长时间思考的思考方式，类似于解决数学问题。早期LLM在需要System Two的问题上表现不佳。
*   **C.Scaling Law的失效：** 随着模型参数量的增大，某些任务上的性能提升并不明显，尤其是在数学问题上，因为模型缺乏System Two的思考能力。

**II.思维链（CoT）的引入与发展：**

*   **A.CoT的提出：** 为了解决LLM在推理问题上的不足，研究者提出了CoT方法，即在提示词中手动注入思考过程，使模型能够模仿人类的思考方式。
*   **B.Zero-shot CoT：** 通过在提示词中加入“Let's think step by step”等指令，使模型在没有示例的情况下也能产生思维链，从而解决问题。
*   **C.CoT与Scaling Law：** 在CoT的情况下，模型的性能随着参数量的增大而提升，展现出scaling law的特性，并且在一些任务上甚至超过了人类水平。

**III.CoT的优化与改进：**

*   **A.Self-Consistency：** 通过让模型生成多个答案，并采用多数投票（majority voting）的方式选择最终答案，可以进一步提高CoT的效果。
*   **B.Self-Improve：** 通过训练模型自己产生CoT，并利用高质量的CoT数据进行监督微调（SFT），可以使模型更好地掌握推理能力。DeepSeek-R1使用了类似的思想。

**IV.强化学习（RL）与CoT：**

*   **A.RL在LLM中的应用：** 将LLM视为agent，CoT的每一步视为action，通过强化学习使模型产生更有效的CoT。
*   **B.奖励模型（Reward Model）：**
    *   Outcome Reward Model (ORM)：对整个思维链的最终结果进行评分。
    *   Process Reward Model (PRM)：对思维链的每一个步骤进行验证。
*   **C.Verifier：** 在测试时（test time）使用，用于选择最佳的CoT答案。

**V.DeepSeek-R1的启示：**

*   **A.回归原始：** DeepSeek-R1没有沿用前人研究的奖励模型思路，而是回归到最原始的方法，即只使用数学问题的标准答案作为奖励进行训练。
*   **B.Scaling Law的重要性：** DeepSeek-R1的成功可能与其更大的模型规模有关，这使得模型在没有奖励模型的情况下也能展现出强大的推理能力。
*   **C.科研的批判性思维：** 科研结论需要在特定情境下思考，不能盲目相信前人的结论，需要进行验证和批判性思考。

**核心结论：** 通过思维链（CoT）技术，大型语言模型能够模拟人类的思考过程，从而在需要推理的任务上获得显著的性能提升，DeepSeek-R1通过回归原始的训练方法并结合更大的模型规模，实现了卓越的推理能力。

**Overarching Framework:**
The content delves into the evolution of reasoning capabilities in large language models (LLMs), with a focus on the development and application of Chain of Thought (CoT) prompting. It traces the progression from early LLMs lacking reasoning skills to more advanced models that utilize CoT techniques to achieve human-level performance on complex tasks. The framework encompasses various methods for enhancing CoT, including self-consistency, self-improvement, and reinforcement learning, while also critically examining the role of reward models and the scaling law in achieving optimal reasoning abilities. The discussion culminates in an analysis of DeepSeek-R1, which represents a departure from previous approaches by foregoing reward models in favor of a larger model size and a more direct training methodology.

**Mermaid Conceptual Map:**

```mermaid
<Mermaid_Diagram>
graph LR
    subgraph A [LLMs]
        A1[Early LLMs (e.g., GPT-4)] -- Lacks Reasoning --> A2(Reasoning Tasks: Symbolic & Math)
        A1 -- System One Thinking --> A2
        A1 -- Inefficient System Two --> A2
        A3[Modern LLMs]
    end

    subgraph B [Human Cognition]
        B1[System One] -- Fast, Intuitive --> B3((Response))
        B2[System Two] -- Slow, Analytical --> B3
    end

    A2 -- Scaling Law Failure --> A1
    A3 -- CoT (Chain of Thought) --> A2

    subgraph C [Chain of Thought (CoT)]
        C1[Initial CoT] -- Prompt Engineering --> C2(Reasoning Process in Prompt)
        C2 -- Few-shot Examples --> C3(Improved Reasoning)
        C4[Zero-shot CoT] -- Let's think step by step --> C3
    end
    A3 -- CoT --> A2

    subgraph D [CoT Enhancements]
      D1[Self-Consistency] -- Majority Voting --> D3((Improved Accuracy))
      D2[Self-Improve] -- Training Data from Model --> D3
    end
    C3 -- Increases --> D3

    subgraph E [Reinforcement Learning (RL) with CoT]
      E1[ORM (Outcome Reward Model)] -- Reward Signal (Final Result) --> E3((CoT Optimization))
      E2[PRM (Process Reward Model)] -- Reward Signal (Each Step) --> E3
      E4[Verifier] -- Evaluation of CoT --> E3
    end
    C3 -- RL Integration --> E3

    subgraph F [DeepSeek-R1]
        F1[No Reward Model] -- Simplified Training --> F3((High Performance))
        F2[Large Model Size] -- Scaling Law --> F3
        F3 -- High Reasoning Ability --> A3
        F3 -- High Reasoning Ability --> A2
    end
    E3 -- Exploration Lead to-->F
    A3 -- Scaling law emerge --> F2
    A -- Scaling law emerge --> F2

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#9f9,stroke:#333,stroke-width:2px
    style D fill:#ffc,stroke:#333,stroke-width:2px
    style E fill:#cff,stroke:#333,stroke-width:2px
    style F fill:#fcc,stroke:#333,stroke-width:2px

    linkStyle 0,1,2,3,4,5,6,7,8,9,10 stroke-width: 1px,color:black;
</Mermaid_Diagram>
```

希望这个总结对您有帮助！


Content:
大家好 欢迎来到 EZ.Encoder 今天开始前 我们先思考三个问题 第一个问题 DeepSeek-R1 是如何让大语言模型学会自我思考 这背后的秘密武器到底藏在什么地方 第二个问题 大语言模型思考十秒钟获得答案 为什么比快速只思考一秒钟获取的答案要好题 如何通过像人类一样的 slow thinkings 如何解决一个关键 问题 的 答案 一样 thought 技术到底是如何诞生的 今天这个视频 我就想围绕这些问题给大家撸一撸 请大家坐稳扶牢 我们直接开始 我们先来看一下比较早期的大语言模型 它存在什么问题 我们就以 GPT-4 为例 网上流传著几个大模型 检测一下比较早期的大语言模型它存在什么问题我们就以 GPT-4 为例 网上 流传著几个大模型 GPT-4 回答 strawberry 里面有两个 r 其实这个单字里面是有三个 r 的 我们可以再追问一下 它是否确定 GPT-4 非常肯定是有两个 r 第一个出现在 straw 的部分 第二个出现在 berry 的部分 这个答案是错误的 早期的时候 大家特别喜欢用这个大模型智障检测的问题 来 测试模型的推理性能 。 project 也就是草莓专案 所以这就是草莓专案的来历 同样的问题 我们再用 o1 来测试一下 o1 是一个带有推理能力的模型 可以看到 o1 模型在思考 然后它产生了一个思维炼 也就是它的思维过程 逐个单词进行检查从 straw 一直到后面的每个字母 然后最后数对了 通过 cot o1 模型是答对了 strawberry 里面是有三个字母 r 第二个问题是问九点八和九点十一谁大 如果我们问 GPT-4 的话 他是瞬间就给出答案 九点十一是比九点八大 但其实从数学的角度来看 九点八应该比九点八大 因为 九点八分可以看成九点八八八分八十多 是错的 我们可以再问一下 GPT-4 是否确定 GPT-4 说它非常的确定 我们再来换成 o1 同样的问题来问一下 因为 o1 同样的需要一段时间的思考 大概花了十五秒钟 然后 o1 是给出了正确答案 九点八比九点十一大 第一个处 strawberry 有几个 r 的问题 就是 symbolic 的问题就是 第二个比较九点八和九点十一谁大的就是数学的问题 这两类问题 早期的大语言模型像 GPT-4 表现都不是特别好 很大一部分原因就是因为模型缺少 reasoning 的能力 这些问题一部分原因也是来自于 tokenizer 简单来说就是大语言模型看到的 strawberry 由于分词方式的原因 和我们人类看到的 strawberry 是不一样的 这也导致大语言模型容易出错 我们来想想 这个问题从人类的角度来讲 即使我要回答这两个问题 我也要思考一下 给我 strawberry 这个词 然后让我立刻回答 有几个 r 的话 我是回答不出来 这里就引入心理学上的一个概念 叫 system one thinking 和 system two thinking 前有一本畅销书叫 thinking fast and slow 这本书是一个非常有名的人类心理学家写的 这个作者 2024 年刚去世了 这本书里面就讨论人类思考的两种范式 system one 是那种非常快速的 非常直觉的 有的时候甚至是 emotional 思考 的方式 比较 one 就有点类似于我们平时的语文或者是生物考试 很多记忆的东西 然后考试的时候你能很快的从你的大脑中提取这些记忆 直接回答这些问题 对于 system two 来说 就有点像数学一类的考试 面对这类问题的时候 你 需要 长时间的去思考 对于早期以及现在的大模型来说 对于 他 如何 很快回答 two 的这些问题 需要思考的问题 模型就做得不太好 所以领域类大家开始从关注模型的 system one 思考能力 慢慢的转变到了更关注于模型 system two 的思考能力 这里有一篇 paper 刚好讨论的这个问题 我就用 fig one 来给大家解释一下 对于有一些任务 比如说这些任务涉及到模型的技术 sensewo system sense one 对于数学以及这种 symbolic 需要推理的一些问题就表现的不太好 所以对于这一类的问题 system two 会比 system one 要好 但是 system two 它的问题就是会比较的慢 那导致了一个后果就是 system two 这种 common sense 的问题可能并不是特别适合 因为太慢了 另外对于比较偏向于 one 的模型的这个 答案 estimate 会比较的接近 也就说模型不会反思也不会修改它的答案 但是对于那些比较偏向于 system two 的模型产生的答案 模型是会修改自己之前的答案 也就是说模型会反思 会自我纠正 system one 和 system two 是人类思考的两种范式 但这并不代表著 system one 比 twosystem 因为 我们遇到的问题是多种多样的 问题 two 的这种能力 就好比于我走在路上 一个人给我打招呼说 how are you 我还要想半天不知道怎么回答 别人就可能觉得这个人很没有礼貌 这个时候 system one 就应该启动立马条件反射式的回答 fine thank you and you 所以 system one 和 system twosystem 什么是同样重要的 目前也有一些研究在探讨如何让大语言 system 何时在讨论如何在人类身上 进行 选择 two 我们再从另一个角度来看一下这个问题 在上一个视频里面 我们讨论了 scaling law 的问题 随著计算量、 数据以及模型的大小不断的增大 模型的性能是随之提高的 但是也发现这个 scaling law 在某些 任务 上面似乎失效了 模型 在 2023 年的时候搞了一个数据叫 big bench 这个资料集包含了两百个任务 就看其中的一个就好了 横坐标是模型参数量 也就是模型的大小 动作标可以理解为模型的性能 这个蓝色的虚线表明是平均的人类水平 这个紫色的虚线是最好的人类水平 黑色的虚线是随机的性能 也就是百分之五十 不同的线代表的是百分之五十不同的模型 随著参数量的增大 在这类别 law 模型的表现并没有随著模型的增大而上升 甚至有些模型当参数量变大之后 性能下降 甚至比 random 还差 注意这里使用的模型都是偏早期没有利用思维链技术的模型 为什么 scaling law 在数学的问题上面就突然失效了呢 原因也比较简单 模型还不具备有很好的 system two 的思考能力 这也类似于人类的思考 对于需要思考和推理的数学问题 如果让我在短时间内直接给出答案也是非常困难的 我需要打草稿 需要时间去思考才能回答 为了解决这个问题 有人就提出了 cot 进而慢慢的演化出 test time compute 这个概念 相对应的就是 training time compute cot 这个概念的提出最早来自于 google 的一篇 paper 因为这个 pdf 是 v6 版本 这个工作最早大概是在 2022 年左右 也就是 GPT3 发布之后 GPT3.5 发布之前的那段时间 一作是一个叫 Jason Wei 的小哥 我想稍微介绍一下这个小哥 Jason Wei 是 2020 年到 2023 年在 braingentemgenter ability 有关系 随后他在 2024 年的时候加入了 OpenAI 主要就是负责 o1 和 deep research 的工作 可以说 OpenAI 推出的推理模型 o1 o3 以及随后的 DeepSeek-R1 模型 很多思想都是基于 Jason Wei 的前期的工作 所以我认为这个人对领域的发展起到了非常重要的作用 Jason Wei 提出的这个 cot 方法也特别简单 他们就是在提示词里面手动的注入思考的过程 如果不添加 cot 的话 也就是左边的方法 给定一个问题 就是这里的数学问题 我们可以在里面用 in-context learning 的方法 简单的说 也就是在这个 prompt 里面我们加一个例子 这个例子是由问题和答案 合在一起的 给模型作为参考模型 然后再把这个例子放在这个例子 前面的例子是由问题和 text 放在这方面 的给模型 learning 但是用这种简单的 prompting 的方式 模型的答案是错误的 Jason Wei 提出的方法就是 我在这个例子里面不光提供答案 我还提供一个思考过程 也就是这个蓝线部分表明的 他把这个简单数学问题是如何解决的 详细的写在了这个里面 这样当模型有了参考的 思维过程之后 模型 在回答的时候 自己就可以产生一个类似的思维过程 也就是这个 绿色所表明的模型有了 这样 learning 的方法 提供一个思考过程 提供一个 cot 的范例 这样模型就可以仿照这个范例生成相对应的 cot 从而解决问题 我们再来看一下这个图 GSM8K 数据集上面的不同方法的比较 GSM8K 是一个小学水平的数据集 坐标是一个解题成功的 数学题当时是小学水平的数学题 当时 只有百分之十五 的结果也只能达到 十五 6 shots 加上 cot 的方式 这性能就已经能到百分之五十七了 已经超过之前最好的性能了 这里作者还做了一个实验 也就是把当时最好的 GPT 模型 GPT3 给拿过来 在类似的问题上面进行 finetune 也 就是这里提到的 finetune 的 GPT3 即使在 类似 的 问题 上面 finetune cot 和 output 这里展示的是他们这种方法对于不同的 task 都适用 比如说对于一些数学问题 common sense 的问题 还有像 data understanding sports understanding symbolic 的问题 都可以这么做 但是大家看这种方法其实也有一个问题 就是你需要在这个 prompt 里面人为的插入这些 cot 那这些 cot 是怎么来的呢 在这篇文章里面是人为提供的 但你不可能对每一个问题都人为的设计一些 cot 放到 prompt 里面 又提出了一个更为简单的方法 我们就先为简单 的 方法 shot 意思是 说给定一个问题 不提供任何的范例 而直接让模型提供这个答案 这种往往是比较难的 尤其是对于一些数学问题 所以模型在这个时候就回答错了 few shots 的意思就是说在这个地方提供一个范例 如何去解决类似的问题 也 就是 in-context learning 模型 可以从 这个 范例 cot 也就是我们前面提到的 在这个范例里面加入思考的过程 这样的话模型就可以回答的对 并且模型在回答的过程中自己也会产生思考的过程 但是这样的有一个问题, 就是对每一个问题你都要提供这样的思考的范例 所以 这篇文章就提出来 我不需要提供任何的范例你 在这个问题上 的意思 是 你现在的意思 呢 step by step 也就是让模型一步一步的思考 有了这样的一个指示之后 在没有提供任何范例的情况下 模型也会产生一些思维链 因为他要遵从人类指示 think step by step 模型最后也能得到正确的答案 所以通过这种方式就避免了人为的提供信息 stlet 而 更加 的简单 我们来看一下 cot 也就是加上这句话 我们就来看看 GSM 这个数学问题集上面效能 加上这句话之后 模型的性能从十点四直接就飙升到了四十点七 这个改善的幅度是相当的大 但是我们比较一下 zero shot cot 也就是加入 let's think step by step 的方法和 few shot cot 也 就是在 prompt 里面加入示例的那个方法 我们 可以看到 shot cot 好 随后 Jason wei 和 google 的其他同事把上述的两个方法给结合起来了 并且又做了一个 big bench hard 的数据集去评价有 cot 和没有 cot 模型的 scaling law big bench hard 就是从前面提到的 big bench 数据集里面选了三个非常难的任务 这里作者比较了两种 前面 procewting 的方式就是一种 叫常规词 promp shot 的方式 也就是在你感兴趣的问题之前 就是这里他们提到的 test time question 加入一个例子 这个例子包含了问题和答案 但是没有思考的过程 chain of thought prompting 就是在这个 few shot 的基础之上加入了 chain of thought 思考过程 也就是这里蓝色所高亮出来的部分 同时它们也结合了第二个方法 在这里同时加入了 第二个方法 在这句话 test test test testbbtests 非常 question 的最后面也加入了一句 let's think step by step 就迫使模型产生思维链 然后作者对比了一下这两种 prompting 的方式 对于 answer only 的 prompting 方式和人类的平均水平进行比较 大部分性能都不如人类的平均水平 但是一旦使用这种 chain of thought prompting 的形式 在大部分的任务上面 模型的性能是要超过人类平均水平的 这里作者还研究了一下在 cot 情况下的 scaling law 比较了三个不同的模型 IntructGPT 是当时比较先进的模型 Codex 是一个编程的模型 PaLM 是当时 google 的比较大的一个模型 随著模型逐渐的变大 加入 cot 之后 这个模型是展现了 scaling law 并且增大的幅度比没有 cot 的情况下要更大一些 比较难的任务上 比较 了这个模型 之后 的性能已经 达到了人类的现像水平？ curve 也就是我们前面提到了 在一些需要多步推理的数学问题上面 不加 cot 的模型就是这个灰色的线 随著模型的增大 它并没有展现出 scaling law 性质 但是一旦把 cot 这个方法加上去之后 模型就展现了 scaling law 并且还有一定的涌现能力 也就是当模型足够大的时候 才会有这种 scaling lawscaling law 的产生 同样的现 像 上面不同的任务。 objects 上面也观察得到 如果不加 cot 的话 模型的性能基本上和 random 差不多 加入 cot 这个方法之后 模型的性能才会出现 scaling law 的性质 Jason Wei 在 google 这段时期的 cot 的工作可以说是开创性的 因为 它打开了一扇窗户 让大家意识到通过产生这个重要的提高模型的性能 尤其是在那些需要的 工作 Wei 提出的这一系列的方法 本质是在做一些 prompt engineering 的工作 是在 test time 时候做的 也就是这系列的方法不会改变模型的架构 也不需要额外的训练 这就是 test time compute 的前身 基于这个后续 又有很多的工工作 有一些 工作开始把 cotpa.Spapaperkpapert . Wei 他们这里就用了一个 self consistency 的方法去进一步提高 cot 的效果 具体的做法就是这样的 首先给定一个问题 还是用之前提到的 cot prompting 的方法让模型产生解题的思路以及答案 但是与之前的方法不同的是 这里让模型不光产生一个答案 而是产生一个很多个 答案 因为 使用的思路 lanativeage cotmodelative gudelative prompting 的加持 模型就会产生推理的思路以及答案 然后作者就这里采用几个平均的方法 将这个答案给平均起来 那这篇 paper 里作者发现最好的方法其实就是做 majority voting 比如说有两个答案是十八 有一个答案是二十六 十八这个答案占绝大多数 最后就用这个]majority votesmajority 比如说有两个答案是十八有一个答案是二十六十八 这个 答案 占绝大多数最后一个探索 但 majority soprostaljority voting 的效果好 所以他们就用这种简单的 majority voting 去选取最终的答案 这里背后有一个简单的哲学思想 对于这个问题存在一个正确的答案 并且模型有能力回答正确的话 那么重复多次之后 模型平均来说应该是能够回答对的 所以作者 就 用了这种平均的思想去获得最后的答案 我们来看这个 性能 GSM8K 上面表现大概是五十六点五 但是如果使用了他们这种 self consistency 加上 majority vote 取平均的方法 GSM8K 就一下子能飙升到七十四点四了 所以这个效果是相当显著的 作者这里还展示了如果模型生成的备选答案越多 也就是这里的横座标从一个备选一直到现在直到一个备选答案一直到四十个答案 。 voting 之后 这个性能是不断的上升的 甚至有点类似于听取群众的声音 只要你的群众足够的多 经过平均之后你就能得到真相 在这个 self consistency 的想法之上 随后 google 和 uiuc 的 Han Jiawei 团队又提出了一个 self improve 的想法 前面提到的 cot prompting 的方法是在 inference 的时候透过 prompt 注入一些指令 让模型产生 cot 但是我们在训练的时候并没有教给模型 如何产生 cot 所以下面一个很直接的想法就是 我们能不能直接训练这个模型去产生 cot 呢 如果要训练这个模型产生 cot 的话 可以在两个步骤做 第一个就是在 supervised finetuning 这个步骤做 第二个就是在 reinforcement learning RLHF 的那一步做 这篇 paper 就选择的在 sft 阶段做 下面一个问题就是 如果我们要在 sft 阶段训练模型自己产生 cot 那就需要产生 cot 数据 所以这篇 paper 的核心思想就是利用 highself consistent 的想法 从模型自己产生的想法筛选中从模型自己产生的想法 confidence 的数据 然后使用这些数据来训练大语言模型 从而模型能够产生 cot 我们可以看到 在 GSM8K 上面 这个效果也是非常炸裂的 直接从七十四点四提高到了八十二点一 具体的看一下它是怎么做的 这一部分和前面讲到的 cot prompting 是一样的 也 就是给定一些答案 我们人为的插入一些 cot consistent 的方法 让模型产生很多不同的 cot 然后再做一个 majority voting 把那些正确的答案给选出来 同时保留这些答案所对应的 cot 然后这些结果就可以作为训练数据 然后把这些训练数据再扔给 language model 去做 sft 整个想法就是这么简单 这篇 paper 所提供的思路 跟 DeepSeek-R1 里面所使用的思路非常的相似 使用 DeepSeek-R1-Zero 产生了很多这种带 cot 的数据 然后经过一系列的后处理以及人为的筛选 产生了一些 cot 的 training data 然后把这些 training data 到 这篇本质到这篇文章。 improve 的思想 我们在使用 self consistency 的方法用到了 majority voting 但是如果一个结果占绝大多数的话 它不一定就是正确的 所以这里作者比较了一下 confidence score 和 accuracy 之间的关系 这里的 confidence 就是 majority voting 的结果 比如说零点八 也就是最终选的答案就发现百分之八十的这一点就 发现零点八也就是。 voting 的结果和最终的真正的 accuracy 是完全相关的 所以这就证明了这种 majority voting 的方法是可靠的 DeepSeek-R1 的 paper 也用到了 majority voting 的思想 在它的 figure 2 里面 他们使用了十六个结果的 majority voting 使得 DeepSeek-R1 的结果超过 了十六个结果的 majority voting 使得 DeepSeek-R1 的结果超过十六个结果的结果超过了 o1 的结果 有了这些背景之后 再当你去读 DeepSeek-R1 的 paper 的时候 你就会觉得这些想法都是那么的自然 那么的熟悉 这也是我经常强调的 科研领域类的想法基本都是连续的 没有石头里蹦猴子的那种创新 所以先理解前人的工作 对于理解 DeepSeek -R1 这篇 paper 本身是非常有帮助的 这里后面 OpenAI 其实又提出了一个流派的方法 在众多 cot 里面 选取哪个结果好的时候 你其实可以用一个外部的模型 OpenAI 叫做 verifier 通过这个 verifier 来选取 而不是用这种 majority voting 的方法来选择 我们来大致的看一下这个结果 LMSI 也就是这篇 paper 所提出的方法 我们就直接看一下 GSM8K 上面的结果 这篇 paper 发表的时候 当时最好的结果是八十二点三 如果用他们的方法产生了一些 cot 的数据 并且去 finetune 这个模型的话 然后在 inference 的时候再加上这些刚刚提到的 cot prompting 的方法的话 最好的结果能够达到八十二点一 也就是和当时最好的方法已经相当了 这篇 paper 还做了一个工作 跟 DeepSeek-R1 非常的像 就是当他们使用 self improve 的方法训练出来一个 PaLM 540B 的大模型之后 这个大模型是具有产生 cot 能力的 他们就使用这个大模型去蒸馏一个更小的模型 可以看到蒸馏出来的这个小模型 比如说 62B 它在 GSM8K 上的结果是五十七点四 甚至比这个大的模型 540B 但是不使用它们的方法结果都要好 这个结果只有五点十六点五 paper 里提到的 把一个具有推理能力的大模型蒸馏到一个小的模型上面 思路是非常一致的 我们可以看到 google 和 uiuc 的这篇 paper 已经从 Jason Wei 的早期的通过 prompt engineering 的方法 增强 cot 进化到了通过训练模型 也就是在 sft 阶段让模型产生 cot 并且 sft 的训练数据就是来自模型本身 那是否很自然的一个想法就是 我那是否 learning 的方法让模型产生 cot 呢 我就用这篇 paper 给大家来介绍一下用 reinforcement learning 去增强模型 cot 几个关键点 这篇 paper 是想从 reinforcement learning 的角度讨论如何复现 o1 的工作 这个图就对比了传统 的 reinforcement learning 和在大语言模型下 reinforcement learningreinforcement learningreinment learning 首先需要有一个 agent 和环境进行互动 这个 agent 会透过一个所谓的 policy model 产生一定的 actions 这个 policy model 一般就是用 neural network 来代替 所以给定一个当前的状态 输入到这个 policy model 里面 这个 policy modelcyaction 然后这个本质还让这个本质还想要给这个本质上 要使这个本质回传 ？ model 使得最终 cumulative reward 最大 如果我们放到大语言模型下面去讨论这个 agent 就可以把它换成一个大语言模型 这个大语言模型产生的 cot 的每一个步骤 比如说这里的 step one 也就是 cot 里面的第一步 一致到 cot 里面的 step t 就可以看成一个 policy modelcotrewards 的输出 我们希望这个大语言模型 透过这个 大语言模型透过 这个 最终去的 model 如何定义就是一个难点 reward model 有两种定义方式 一种叫 outcome reward model 也就是缩写成 orm 一种叫 process reward model 也 就是缩写成 prm orm model 其实又可以分成两种 一种叫 solution level 的 orm modelcottoken level 的 model 一样对每个步骤进行判断 process reward model 它是对于这个思维链的每一个步骤都去做验证 比如说看 step one 它是否正确 step two 是否正确 一直到最后一步 所以 prm model 在 reinforcementing 框架里面可以提供更丰富的监督信号 下面我想给大家讲讲领域类关于 cot reward model 的一些文章 在开始讲之前 我想先给大家用 top down 的方式介绍一个大概 reward model 本质是给定一个 cot 一个整体 或是每一步这个 框架 signal 的话 那么它就叫做 reward model 但是还有一种情况是在 test time compute 的时候使用 这种情况下就叫 verifier 这个 verifier 具体是一个什么意思呢 就比如说如果我们用最简单的 prompt engineering 的方法 在测试的时候可以产生一系列的 我们 这 每个圆圈表示 文件 consistent 的方式产生很多这种平行的 cot 例如这样 我们可以用 majority voting 的方式去选择最终的 solution 也可以用一个 verifier 去选择最好的答案 这个 verifier 本质就是一个大语言模型 经过训练之后能够判断哪个 cot 更好 当然 在这个基础之上 我们可以有很多的变种 比如说我们可以以树的形式 在每一步都用 verifier 去验证 从而 得到 最后的 solution. model 的讨论 而领域类几篇比较重要的 paper 是把 reward model 和 verifier 这两个概念混在一起 所以我就也把这两个概念混在一起给大家挑几篇 paper 讲解一下 但是 大家 在读 paper 的时候请稍微区分一下这两个概念 我们先来看 OpenAI 在 2021 年发表的一篇 test 文章 这篇 文章 compute 是发表 o1 时候提出来的 其实不是 在 2021 年发表的文章 OpenAI 就已经提出了 test time compu 的这个概念 所以我觉得这篇文章对于理解后续的工作是一个基石 这篇文章主要的想法是提出训练 verifier 去解决数学问题 它有几个主要的贡献点 第一个就是它提出了 GSM8K 这个数据集 这就是我们前面多次提到的那个数据集 这个数据集是一个小学水平的资料集 但与其他资料集不同的是 这个资料集提供了解决这些数学问题的解题思路 并且这些解题思路都是由人类所标注的 有了这个数据集 就极大地推动了 cot 这个领域的研究和发展 第二个我觉得比较重要的贡献就是这个 paper 提出了一种基于 orm 也就是 outcome reward model 的方式去验证模型最后 cot 是否正确 从而实现 test time compute 我觉得这三个贡献都非常的重要 我们就挨个儿的说一下 GSM8K 总共有八千五百个 grade school 也就是小学程度的数学题 这些数学题 里面的问题都是由人类去创建 这些问题所对应的答案也都是由人类去写 并且当时的 OpenAI 还把这整个数据集都给公开了 放到了 github 上面 然后 OpenAI 就提出了一种去训练 verifier 的方式 在这里有两个概念 一个叫 generator 一个叫 verifier 这都是这篇文章在 这篇文章 generator 就是我们想要训练的那个大语言模型 让它能够产生 cot verifier 就是给定一个 cot 判断这个 cot 是正确还是不正确 如何训练这个 verifier 呢 OpenAI 提出的方法分为三步 第一步先把训练数据中的 question 和 solution 人工针对每个 question 写出来的答案一起送给大一步语言模型进行训练 所以这本质本质就是在做 supervised 就是在做 supervised finetuning 当训练好这个大语言模型之后 给定一个 question 让这个大语言模型产生一百个不同的 solution 也就是这里的 s1 到 s100 然后让人类去标注这每个 solution 是否正确 所以针对每个 solution 都会有一个 label 也就是这里的 y1,y2 一直到 y100 所以这一块跟前面的 paper 思想非常像前面的 paper 思想非常像前面的 paper 思想非常像前面的 paper 思想非常像前面的 paper 思想非常像前面的 paper 思想非常 像我们用 cot 标注资料去训练一个大语言模型 然后让这个大语言模型再产生更多的 cot 数据 用人工的方法去标注一下这些 cot 数据 这样就产生了大量的 cot 训练数据 并且这些训练数据带有 label 这个 label 作用就是对应的 cot 是正确的还是错误的 然后把这产生的数据 也就是给定一个 question 一个 solution 以及它的 ground truth 这个 solution 是正确的还是不正确的这个 verifier 去做训练 所以这一步本质也是 supervised learning 通过这种方式训练之后 我们就得到了一个 verifier 这个 verifier 的作用就是给定一个 question 以及 cot 预测它的 label 也就是整个 cot 正确还是不正确 有了这个训练好的 verifier 在 inference 的时候 让这个问题 得到这个问题怎么定？ GPT3 产生一百个带思维链的答案 solution1 一直到 solution100 然后把这每个答案经过这个 verifier 并预测它是否正确 所以针对这每个 solution verifier 都可以给出一个 score 我就用 p 表示 然后再从中选出最好的那个 score 所对应的 solution 因为这大部分的计算 比如说一百个 testutionution 以及由论文 testution） 都发生在这个 时间 点 compute 这个方法跟我们前面提到的 self consistent 方法非常的像 唯一的区别就是这里引入了一个 verifier 而不是用 majority voting 的方法 作者在这里比较了一下 finetune 的方法和 verification 的方法哪个好 finetune 的方法 就是 直接 用 人类的 cotifier compute 左边是一个 6B 的 GPT3 模型 右边是一个 175B 的 GPT 模型 基于这种 verification 的方法都比 supervised finetuning 方法要好 这个图其实就显示了 test time compute 可以增强模型的性能 作者这里还做了一些有趣的实验 左边这个图的横加法就是在 inferencegenerator 产生多少个时间点从两个时间段的矩阵法 从二度序列数到二 值一时所产生。 在产生四百个候选的 solution 的时候 解题的成功率是最大的 为什么产生的候选 solution 大于 四百之后性能是在下降呢 OpenAI 给出的解释是候选答案太多的时候 因为这个 6B 的 verifier 并不是特别的完美 使得这 其中的某一些 solutionutionre . hacking 的例子 所以导致性能的下降 接著 OpenAI 又做了一个实验 用 verifier 选出了一定的结果之后 然后再做 majority voting 坐标表示的就是我用 verifier 选多少结果出来 做 majority voting 相比较直接用 verifier 选出最好的 solution 我们选出十个比较 好的 模型 voting 的想法 又结合起来 我们再来看一下 OpenAI 是怎么去训练这个 verifier OpenAI 的 verifier 就是一个大语言模型 和前面提到的 generator 是同样的架构 只不过它在这个大语言模型上面加入了一个 scalar head 这个 scalar head 具体是干什么的呢 OpenAIc​​percaialcary -bul 好吗 tospecial tobulity truth 对应的就是整个 solution 是正确还是错误 注意这里使用的 ground truth 是针对整个 solution 的 对于里面的每个步骤是否正确是没有 ground truth 所以这里所有的 token 对应的 ground truth 都是一样的 如果这个 solution 是正确的 那所有 token 的 ground truthground 如果 这个 solution spectruth 是表的所有对应错误 是 使用的对应物 也都是正确的。 prediction 有一个好处 就是它不影响这个大语言模型对于其他 token 的预测和使用 我们来看一下这个图 左边的这个图是在训练 generator 右边的这个图是在训练 verifier 左边这个图本质就是在做前面我们提到的 supervised finetuning 然后使用的 loss 就是 next token prediction 给定一个 question 以及人类标注的 solution 就是要预测它的下一个 token 所以这里有一个 shift 同时又是因为我们想模型学习如何产生这种带 cot 的 solution 所以 prompt 里面的 question 不应该进入 lost 的计算 所以这里灰掉了 当我们有了训练好的 generator 产生了大量的人工标注数据 并且人工标注之后 我们就来训练 verifier 这里的 label 就是来自于 human 的标注 所以给定一个 question 以及对应的 solutions 如果这个 solution 整体是正确的 那么这里的 label 就全是一 如果这个 solution 整体是错误的 那么这里的 label 就全是零 通过这种方式 我们就可以训练一个 verifier 而这里作者就设计了两个变种 solution level 的 verifier 另外一种就是叫 token level 的 verifier token level 的 verifier 比较容易理解 就是这个 verifier 它对于每一个 solution 的 token 它其实都可以有一个 prediction solution level 的 verifier 意思 就是只使用这最后一个 词 菜 level 的 prediction 作者在这个图中也比较了一下这两种 verified 差异 solution level 的 verifier 也就是这个橘色的线 在 训练的早期是比 token level verifier 结果要好 但是当训练到一定的 apple 之后 这个 token level 的 verifier level 的 verifier 其实对于模型来说 这个任务更难 因为模型需要在 cot 比较早期的那些 token 就要预测整个 solution 是否正确 这是非常难的一个事情 从另外一个角度 这个 token level 的 verifier 特别像 alphago 里面所使用的 value function 在围棋还没有下完的中途 这个 模型最终 是赢下的这个中途 站 level 的 verifier 这种 token level 的 verifier 还有一个好处 提供了可解释性 如果模型出错了 通过这种可视化 我们就能判断出来模型不确定的部分是哪里 OpenAI 在随后二零二三年的时候又发表了一篇文章 叫做 let's verify step by step 这是 OpenAI 最后一篇有技术细节的 paper 所以很多人反复研读这篇 paper 希望能找到 o1 的秘诀 我觉得这篇文章就是前面 OpenAI 那篇文章的继续 在前面那篇文章里面 OpenAI 提出了一个 GSM8K 的数据集 在这个数据集上面 他们提出了一个 orm model 也就是 outcome reward model 这个 model 是对整体的思维链 solution 进行打分 好还是不好 在这篇 paper 里 OpenAI 提出了一个更加细化的数据集 叫做 PRM800K 我们来对比一下 在 GSM8K 里面 给定每一个(问题)数据集 由大语言模型 当时是 GPT3 产生多个 solution s1 , s2 直到 solelsn n 然接著用这个资料 train 了一个 orm model 在 PRM800K 里面 对于每一个问题 同样的用大语言模型 这里用的是 GPT4 产生多个 solution 但不同的是 对于这每个 solution OpenAI 把它的 cot 分成了更细的 steps 也就是推理的步骤 我 这里 用 t 来表示 从 t1 到 tm 后面会有它的 cot 然后以此类推 有了这样的资料之后 OpenAI 就可以训练一个 process reward model 也就是这个模型可以对 solution 里面数列的每一个步骤进行判断是否正确 可以理解为 prm 的分辨率要比 orm 的分辨率更高 这里我想再跟大家强调一下这个 reward model 两种用法 以免 让大家 在这里 产生混淆 reward modeldel compute 的时候作为一个 verifier 去选择哪个答案最好 在 OpenAI 这篇 paper 里面 他们讨论的是 verifier 这种用法 换句话说说 在这篇 paper 里面还没有涉及到任何的 reinforcement learning 的部分 在后面我会给大家讲一篇 DeepMind 的 paper 在那篇 paper 里面 reward modelper 里面 reward testreinforcement lsignrewardreinforcement lsigning 我里面 的 那篇 compute 里面的 verifier 去选取最佳答案 在 DeepMind 那篇 paper 里面 它们的结论是使用 orm model 和使用 prm model 结果是差不多的 那 OpenAI 的这篇 paper 里面 它主​​要的结论是 prm model 其实要比 orm modell. 这两篇 paper 的结论是有一点矛盾的 Deepreindelin 它的工作阶段是这个工作阶段完全用 了 base 的方法 在 test time compute 的时候使用的是最简单的 majority voting 我们先来看一下 fig 1 这个 PRM800K 数据集是如何标注的 给定一个数学问题 这里有多个 GPT4 模型产生的推理步骤 这每一行就是 一个标注的 step 然后 OpenAI 让人类的标注者对每一步进行标注者 分别 是对 这一步骤 negative 比较容易理解 就是这步推理是错误的 positive 就是这步推理是对的 neutral 的意思就是这步推理从技术上讲没有任何的问题 但是对整个答案其实没有太大的帮助 对于这个例子 人类的标注者认为这前面所有的 推理 步骤都是正确的 直到最后一步解注方程的时候出了错误 所以有了这样的训练 资料 集 数 model 的作用就是给定每一个推理步骤 可以判断出它是对还是错 这个 prm model 的训练过程跟前面的 orm model 类似 因为这部分内容对帮助我们理解 DeepSeek-R1 没有太多的帮助 所以这里我就略过 这个图就展示了 当我们有了一个训练好的 prm model 之后 我们对模型输出的 cot solution 可以进行打分 并且这种评价是可以到每​​个推理步骤上面的 比如对于左边这个问题 模型生成了一系列的推理步骤 prm 认为所有的推理步骤都是正确的 但是对于右边这个问题 prm model 认为前面的这些推理是正确的 但是到这一步 也就是红色高亮的部分推理步骤是错误的 还有后面具体解方程的这些步骤 prm 也认为是错误的 这样的 prm 我们在 inference 的时候 对于 一个问题 我们可以产生很多个备选的 solution 然后用 prm 对这每一个 solution 进行打分 选出 得分最高的 那个 小号 OpenAI 采取的方法是把这个 proability 整个都乘起来 这个乘积作为这整个 solution 的打分 也就是用这个 proability 去筛选备选的答案 这里和前面提到的 self consistent 方法不同的地方就是 这里用的是 prm 在选答案 self consistent 是用的 majority voting 在选择答案 这里展示的这篇文章就是这篇研究了另一个 部分 的结论 。 voting 横座标是在 inference 时候让大语言模型产生多少个 cot 的备选答案 纵坐标是解题的成功率 可以看到这个 prm 这条局线明显是好于 orm 的 并且这两个方法都要好于 majority voting 的 从上面这个数值也能看出问题 是 要比 orm 好的 这就是 OpenuteAItest law 的结果 OpenAI 在这篇文章里面没有非常直接的讨论这个概念 但我觉得完全可以从这个角度去理解 因为我们在 inference 的时候 sample 越多的备选答案 其实就是在增加 test time compute 随著这个 test time compute 的增多 模型的性能 是 在不断的提高的 不论是 prm ormtestmtest compute 增加 它的增大幅度是比另外两个方法要好 所以衍生出来的一个问题就是很多人猜测 OpenAI 的 o1 模型会不会是一个在 test time 使用了 prm 这个方法的推理模型 我们再来看一下 DeepMind 在二零二二年发表的这篇 文章 这篇文章其实 在刚才那篇文章 之前 比较 base 的两种方法 也就是我们前面提到的 prm 和 orm 但是这篇文章和 OpenAI 前面两篇文章不同的是 他们把 orm prm 这个 reward model 既用到了 reinforcement learning 里面作为 reward signal 的来源 也作为了 test time compute 的时候去选取最好 的 词 compute 但是这两个其实说的都是一个东西 这片 paper 主要用的数据集是 GSM8K 和前面 OpenAI 的做法类似 对于一个问题 DeepMind 让大语言模型产生很多个不同的 cot solution 然后 对于 这 每个 solution 里面的推理步骤 DeepMindrmgwardre 的 标注 learning 里面去使模型产生思维链 我们先看第一个 final answer rl 这个其实就是 DeepSeek-R1 所采用的方法 也就是它不用任何的 reward model 只用数学问题最后的标准答案作为 reward 去训练大语言模型 ORM-RL 的意思就是这个时候我不使用这些标准答案作为 rewardreward 而是使用我们前面已经帮助训练所提供的 这里 orm 因为它是一个 outcome base model 所以它只会对整个 solution 整个思维链打分 同理我们也可以用这个 prm 进行 reinforcement 训练 prm 可以对每一部思维链进行打分 这个表示就是思维链当中的不同的步骤 对于第一步 prm 认为这个是最好 的 基于 这个 结果最终 可以再往下继续产生思维链 例如第一步 model 在 reinforcement 里面训练的基本思想 这个表格展示的是在 GSM8K 上面的 error rate DeepMind 作者这里用了两种衡量标准 一种叫 trace 一种叫 final answer final answer 比较容易理解 就是看模型最后的输出答案是否正确 注意这里用的是 error rate 所以这一列的值越小 理解就是看模型最后的 输出答案是否正确 注意它 每一步 rate 所以这个也是越小越好 我们来看一下这个结果 sft 加 final answer rl 和 majority voting 这个其实就是 DeepSeek-R1 所采用的方法 final answer error rate 是百分之二十点二 trace 的 error rate 是十二点一通同样的 我们再来看这两个结果 这两个结果是 在这两个结果下 的时候提出 compute 的时候使用 reward model 对答案进行 ranking 然后选出最好的答案 可以看到 使用了 reward model 之后 这个 error rate 从百分之二十左右降到了百分之十二左右 这个 trace error 从百分之十二左右降到了百分之三左右 所以这改进是非常显著的 另外如果我们比较 orm 和 prm 这两个结果的话 可以发现它们的结果是非常类似的话 是不管它们的结果 是非常类似的话 error 所以第一个结论就是使用 orm 还是使用 prm 这两个结果是差不多的 但是后来 OpenAI 发表的 paper 也就是我们刚刚介绍了一篇 paper 就推翻了这个结论 认为 prm 是要比 orm 好的 但是不管是 OpenAI 的结论还是 DeepMind 的结论 都是使用 reward model 的 不论是在 information 阶段还是在 test timeation compute 阶段做 ranking 但是很有趣的是 DeepSeek-R1 却没有沿著前人的这些思路继续去探索 reward model 的使用 而是反其道而行之 直接把整个 reward model 都拿掉了这篇 返璞归真 回到了最原始的方法 在这里就有一个矛盾了 为什么在 DeepMindward 的这篇返璞归真回到了最原始的方法在 这里就有一个矛盾了为什么在 DeepMindward 的 这篇返璞归真回到了最原始的方法在这里就有 一个矛盾 rate 是要大的 也就是结果要差的 我觉得这里可能有两个因素 第一个就是这个 reinforcement learning 的算法 DeepMind 在这片 paper 里面使用了一个叫 expert iteration 的方法 而 DeepSeek 在 R1 的 paper 里面使用的是叫 grpo 的方法 这是第一个不同的 第二个不同 方法我觉得可能在这个大小的 原因 里面 看 model 只有 70B 我们回忆一下 DeepSeek-R1 的 base model 用的是 DeepSeek-v3 有 671B 的参数量 所以比 DeepMind 这片 paper 所使用的 base model 大了我们前面有 671B 的参数量所以比 DeepMind 这片 paper 所使用的 base model 大了参数差不多有十倍 我们前面给大家讲了 scaling law emergent 能力 有些模型的能力只有 在这篇文章上的能力只有在这篇文章上升 到一定程度的程度。 model 比较小 所以他们得出的结论就是 这种最简单的方法训练出来的模型推理能力 反而是没有用 reward model 训练出来的模型推理能力要好的 看到这里我也有一个想法 就是在科研领域 有的时候这些大牛 比如说像 Google OpenAI 他们的结论也不一定完全正确 我们有的时候需要放到一定的 context 下去思考 不能盲目的听从或者是相信前人的一些结论 有些结论我们可能要自己去验证 更多的时候 一个结论的成立是有很多的前提条件的 比如说 这里的 base model 大小 rl 的 算法 所以 科研 思维 Shepherd 他们其实在 OpenAI 以及 DeepMind 的思路上面还是做了一些的探索 这篇 paper 主要在探索的是如何能够更好地使用 prm 这个方法 prm 这个方法最大的难点就是它需要人类去对推理步骤的每一步是正确还是错误进行人为的标注 这就极大的增加了这个方法的难度 所以 这篇文章 supercesperpise 时提出了一种自动的方法 data 这篇文章主体的部分和 DeepMind 的很像 它把这个 prm 用在了两个方面 一个是 verification 一个是 reinforcement learning DeepSeek 在这篇文章里 是如何自动地给思维链的每一个步骤进行标注的呢 我们来看一下这个图 给定一个问题 可以用大语言模型产生一个思维炼 也就是解题步骤 然后用人对每一个解题步骤进行标注 这种方法耗时耗力 dDeepSeek 就提出 我们可以利用蒙特卡罗树搜寻的想法 对每个步骤进行自动的标注 具体是什么意思呢 比如说对于这个问题 我们有了第一步解题思路 也就是 s1 然后这里让一个大语言模型基于第一步的思维过程继续往下生成剩下的思维过程 也就是这里的 s2 s3 当然我们不只是产生一种思维链 我们也产生好几种思维链 在这个图中是三种 这就好比在 alphago 里面下围棋 我们已经下了第一步 也就是这里的 s1 然后我们用模型去模拟接下来可能发生的事情 让模型自己去产生可能剩下来的走法 比如说这里就有一个走法 然后 再统计最后的输赢 第一种表达第 一种 胜利 是第二个胜利者 也就是算一个正确答案的百分比 这里有两个是正确的 所以是二除以三 第二种方法作者叫 hard estimate 也就是这三个答案里面只要有一个答案是正确的 那么这个 label 就是 1 当有了这样的一个 label 之后 我们就可以把这个 label 作为 s1 的这一步的 ground truth label s1 这一步可以思考这 一步 estimate 的角度来讲 s1 可以通往正确的答案 类似的我们也可以对 s2 做下面的这些计算 这样我就可以给思维链过程当中的每一步都打上了标签 然后再用这种每一步都有标签的思维链过程去训练 prm model 或者是 orm model 然后 类似于 DeepMindpapertesteded compute 里面作为一个 verifier 去选取最佳的答案 因为这一部分的过程和 DeepMind 的 paper 非常像 所以这里我就不仔细解释了 我们直接来看一下结果 左边这个图是在 GSM8K 上面的准确度 横坐标是不同的大语言模型 黄色的部分表示使用了他们的方法训练出来的模型 可以看到 在 GSM8K 上面 这个结果已经达到了 GPT4 的水平 而且 GSM8K 上面的结果已经达到了百分之九十多 所以这个数据集基本上已经被刷爆了 我们可以回想一下 OpenAI 最早提出这个 GSM8K 资料集的时候 最好的结果也才百分之四十左右 短短两三年的时间 随著技术的发展 这个资料集就已经被刷爆了 所以在后续的 paper 里面 很少有 paper 在用这个 GSM8K 数据集 转而用这个 MATH 资料集 小学程度的数学问题对大语言模型已经不是一个难事 在这篇 paper 之后 并没有沿著 prm 这个思路往下走 我的猜测是他们可能遇到了一些问题 像他 paper 里面提到的 prm 存在 reward hacking 的问题 所以经过一些研究之后 他们抛弃了 prm 这个思路 进而发展出来了后面的 DeepSeek-R1 希望给大家铺垫了这么多 能帮助大家对 DeepSeek-R1 有一个更好的理解 今天的视频差不多就到这里了 我来给大家做一个小结 这个视频里我主要给大家介绍 了 cotcot1 DeepSeek 的 R1 非常重要的一个基础 cot 技术让大语言模型具备了一定的 system two 的思考能力 为了使模型能够产生 cot 领域内目前有很多的方法 比如说早期的时候可以直接用 prompt engineering 的方法 let 's think step by step 或者是也可以收集有 cotinment ） learning 的方法让大语言模型产生更好的 cot 在这个视频里面 我们也花了很多的时间去讲 cot 里面的 reward model 和 verifier 因为这个跟 DeepSeek-R1 的工作直接相关 领域内早期像 OpenAI DeepMind 其实都是在研究如何去构建一个最后更好的 reward model DeepSeekeek 也在这方面有一些跟进的工作结果 直接用了这 方面 reward 我也给大家提了一下 test time compute 因为有了 cot 这个技术之后 我们可以让大语言模型在 inference 的时候产生很多的 cot 然后用 verifier 或者是用 majority voting 的方法 从这 众多的 cot 里面选一个最好的作为最后的答案 这种 类型 o1 背后所使用的技术 我们再来回顾一下之前我提到的 top down 的讲解计划 我已经给大家介绍了 reinforcement learning 的 background 以及 OpenAI GPT 整个系列的发展史 透过这个发展史介绍了训练范式 scaling law emergent ability 这个序列里面也给大家介绍了 cot 所以大家有了这些 background 再去看 DeepSeek-R1 这个 paper 的时候 你就会觉得所有的那些知识都是那么的自然 并没有很难理解 唯一可能比较难理解的就是这个 grpo 的部分 我会放到第三个阶段给大家讲解 通过这一系列的背景介绍视频 我希望对大家理解 DeepSeek-R1 这篇 paper 有一定的帮助 也感谢大家的支持 有任何问题 欢迎你留言 也希望你能订阅、 点赞、 转发 让更多的人能够看到
