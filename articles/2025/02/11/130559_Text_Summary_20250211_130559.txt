Timestamp: 2025-02-11T13:05:59.591599
Title: Text_Summary_20250211_130559
URL: Direct text input
Status: success
Duration: 0:00

Description:
由于内容包含中文，以下为简体中文总结：

**1. 概要结构：**

*   **核心发现：** 知识蒸馏的有效性 - 大型模型的推理模式可以有效地转移到较小的模型中，使其性能超越通过强化学习（RL）在小型模型上发现的推理模式。

*   **实验验证：**
    *   使用 DeepSeek-R1 生成的推理数据，微调多个常用的稠密模型。
    *   蒸馏后的小型模型在多个基准测试中表现出色（例如，AIME 2024, MATH-500, LiveCodeBench）。
    *   开源了多个基于 Qwen2.5 和 Llama3 系列的蒸馏模型检查点（1.5B, 7B, 8B, 14B, 32B, 70B）。

*   **DeepSeek-R1 的性能：**
    *   **推理任务：** 在 AIME 2024 和 MATH-500 上表现出色，与或优于 OpenAI-o1-1217。在 Codeforces 上的 Elo 评分超过 96.3% 的人类参赛者。
    *   **知识任务：** 在 MMLU, MMLU-Pro, GPQA Diamond 和 SimpleQA 等基准测试中表现优秀，超过 DeepSeek-V3。
    *   **其他任务：** 在创意写作、一般问答、编辑、摘要等方面表现出色（AlpacaEval 2.0, ArenaHard）。在长文本理解任务上优于 DeepSeek-V3。

**2. 核心结论：**

大型模型的推理能力可以成功蒸馏到小型模型中，显著提升小型模型的性能，使其在多项任务中达到或超越大型模型和现有开源模型的水平。

**3. 总体框架：**

该内容展示了一个研究项目，核心是探索和验证知识蒸馏技术在模型推理能力提升方面的应用。项目通过训练大型模型（DeepSeek-R1），利用其生成的推理数据训练小型模型，并最终开源这些小型模型，以促进研究社区的发展。

**4. Mermaid 概念图：**

<Mermaid_Diagram>
graph LR
    subgraph "知识蒸馏流程"
        A[大型模型 DeepSeek-R1] --> B(生成推理数据);
        B --> C[微调小型模型];
        C --> D{评估};
    end

    style A fill:#f9f,stroke:#333,stroke-width:2px
      style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#aaf,stroke:#333,stroke-width:2px

    subgraph "DeepSeek-R1 性能"
      D --> E[推理任务: AIME 2024, MATH-500, Codeforces];
        D --> F[知识任务: MMLU, MMLU-Pro, GPQA, SimpleQA];
        D --> G[其他任务: 创意写作, 问答, 编辑, 摘要, 长文本理解];
    end

      style E fill:#bbf,stroke:#333,stroke-width:2px
      style F fill:#ddf,stroke:#333,stroke-width:2px
    style G fill:#eef,stroke:#333,stroke-width:2px
    subgraph "开源"
         C --> H[开源蒸馏模型: 1.5B, 7B, 8B, 14B, 32B, 70B (Qwen2.5, Llama3)];
    end
      style H fill:#99f,stroke:#333,stroke-width:2px

    I([结论: 知识蒸馏有效提升小型模型性能])
    D --> I;

  style I fill:#ffa,stroke:#f33,stroke-width:3px
</Mermaid_Diagram>


Content:
Distillation: Smaller Models Can Be Powerful Too
  
 
We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.
  
 
Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks.
DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini.
We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.

1.2 Summary of Evaluation Results
  
 
Reasoning tasks:
(1)
DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-o1-1217 and significantly outperforming other models.
(2)
On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition.
For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks.
  
 
Knowledge:
On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.
  
 
Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks.
