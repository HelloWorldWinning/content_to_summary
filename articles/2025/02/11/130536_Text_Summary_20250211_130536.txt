Timestamp: 2025-02-11T13:05:36.749691
Title: Text_Summary_20250211_130536
URL: Direct text input
Status: success
Duration: 0:00

Description:
本文核心观点是：通过纯强化学习（RL）和结合少量监督数据的多阶段训练流程，可以显著提升大型语言模型（LLM）的推理能力，甚至超越传统监督学习方法。

**核心总结：**

1.  **核心观点：** 纯强化学习结合少量监督数据的多阶段训练流程能有效提升大型语言模型的推理能力。
2.  **总体框架：** 本文提出了一个多阶段训练流程，包括纯强化学习阶段（DeepSeek-R1-Zero）、结合冷启动数据的多阶段训练（DeepSeek-R1），以及知识蒸馏到更小模型。每个阶段都有特定目标，最终实现推理能力的提升。
3.  **概念图：**

<Mermaid_Diagram>
graph LR
    subgraph "Base Model & Initial RL"
        A[DeepSeek-V3-Base] --> B(GRPO (RL Framework))
        B --> C{DeepSeek-R1-Zero}
        C --> D[Reasoning Benchmarks]
    end

    subgraph "DeepSeek-R1: Multi-Stage Training"
        A --> E[Cold-Start Data]
        E --> F[Fine-tuning]
        F --> G(Reasoning-Oriented RL)
        G --> H{Rejection Sampling}
        H --> I[New SFT Data]
        I --> J[Supervised Data (Writing, Factual QA, Self-Cognition)]
        J --> K[Fine-tuning]
        K --> L(Additional RL (All Scenarios))
        L --> M{DeepSeek-R1}
         M --> N[Reasoning Benchmarks]
    end

     subgraph "Distillation"
       M --> O[Qwen2.5-32B]
        O --> P{Distilled Models (Qwen, Llama)}
       P --> Q[Reasoning Benchmarks]
    end

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
     style C fill:#aaf,stroke:#333,stroke-width:2px,color:black
     style D fill:#bbf,stroke:#333,stroke-width:2px
    style E fill:#ddf,stroke:#333,stroke-width:2px
    style F fill:#eef,stroke:#333,stroke-width:2px
    style G fill:#ccf,stroke:#333,stroke-width:2px
    style H fill:#ddf,stroke:#333,stroke-width:2px
    style I fill:#eef,stroke:#333,stroke-width:2px
    style J fill:#ddf,stroke:#333,stroke-width:2px
    style K fill:#eef,stroke:#333,stroke-width:2px
     style L fill:#ccf,stroke:#333,stroke-width:2px
    style M fill:#aaf,stroke:#333,stroke-width:2px,color:black
     style N fill:#bbf,stroke:#333,stroke-width:2px
      style O fill:#f9f,stroke:#333,stroke-width:2px
    style P fill:#aaf,stroke:#333,stroke-width:2px,color:black
 style Q fill:#bbf,stroke:#333,stroke-width:2px
    C -->|Emerges with Reasoning Behaviors| C
    M -->|Outperforms SFT| M
     P -->|Outperforms Open-Source Models|P
</Mermaid_Diagram>


Content:
In this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL).
Our goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process.
Specifically, we use DeepSeek-V3-Base as the base model and employ GRPOÂ (Shao etÂ al., 2024) as the RL framework to improve model performance in reasoning.
During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance on reasoning benchmarks.
For instance, the pass@1 score on AIME 2024 increases from 15.6% to 71.0%, and with majority voting, the score further improves to 86.7%, matching the performance of OpenAI-o1-0912.
However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training pipeline.
Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.
We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-32BÂ (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities. We open-source the distilled Qwen and LlamaÂ (Dubey etÂ al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-PreviewÂ (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models.
1.1 Contributions
Post-Training: Large-Scale Reinforcement Learning on the Base Model
  
 
We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.
  
 
We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the modelâ s reasoning and non-reasoning capabilities.
We believe the pipeline will benefit the industry by creating better models.
