Timestamp: 2025-02-11T13:05:52.525908
Title: Text_Summary_20250211_130552
URL: Direct text input
Status: success
Duration: 0:00

Description:
由于内容包含中文，所有回复将使用简体中文。

1.  **核心结论：** 通过将大型模型的推理模式提炼到较小的模型中，可以显著提升小模型的性能，甚至超越通过强化学习在小模型上发现的推理模式。

2.  **总体框架：** 该内容围绕模型蒸馏（Model Distillation）展开，具体是通过DeepSeek-R1（大型模型）生成推理数据，然后用这些数据微调一系列较小的、开源的模型（如基于Qwen2.5和Llama3系列的模型）。评估结果展示了蒸馏后的小模型在推理、编码、知识等多个任务上都取得了显著的性能提升。

3.  **概念图：**

<Mermaid_Diagram>
graph LR
    subgraph "模型蒸馏流程"
        A[DeepSeek-R1 (大型模型)] --> B(生成推理数据);
        B --> C{微调小模型};
        C --> D[Qwen-7B, Qwen-32B等];
        C --> E[Llama3系列];
        D --> F(性能评估);
        E --> F;
    end

    subgraph "评估任务"
    F --> G[推理任务 (AIME 2024, MATH-500)];
      F --> H[编码任务 (Codeforces, LiveCodeBench)];
      F --> I[知识任务 (MMLU, MMLU-Pro, GPQA Diamond, SimpleQA)];
       F --> J[其他任务 (AlpacaEval 2.0, ArenaHard, 长文本理解)];
    end
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#aaf,stroke:#333,stroke-width:2px
    style D fill:#ddf,stroke:#333,stroke-width:2px
     style E fill:#ddf,stroke:#333,stroke-width:2px
    style F fill:#eef,stroke:#333,stroke-width:4px
     style G fill:#ccf,stroke:#333,stroke-width:1px
     style H fill:#ccf,stroke:#333,stroke-width:1px
    style I fill:#ccf,stroke:#333,stroke-width:1px
     style J fill:#ccf,stroke:#333,stroke-width:1px

</Mermaid_Diagram>


Content:
Distillation: Smaller Models Can Be Powerful Too
  
 
We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.
  
 
Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks.
DeepSeek-R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini.
We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.

1.2 Summary of Evaluation Results
  
 
Reasoning tasks:
(1)
DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-o1-1217 and significantly outperforming other models.
(2)
On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition.
For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks.
  
 
Knowledge:
On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.
  
 
Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks.
