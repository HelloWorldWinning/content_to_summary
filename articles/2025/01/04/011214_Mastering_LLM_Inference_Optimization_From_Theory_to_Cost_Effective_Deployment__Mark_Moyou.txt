Timestamp: 2025-01-04T01:12:14.015882
Title: Mastering LLM Inference Optimization From Theory to Cost Effective Deployment: Mark Moyou
URL: https://youtube.com/watch?v=9tvJ_GYJA-o&si=t0Daiq1N3aT7WI_6
Status: success
Duration: 33:39

Description:
好的，这是对您提供的文本的总结，包含核心思想，并以最大的清晰度和简洁度进行概述：

**核心思想概述：**

1.  **介绍与背景：**
    *   演讲者来自英伟达，主要与零售客户合作，处理数据、计算机视觉和LLM推理。
    *   本次演讲旨在帮助听众更好地理解LLM推理的工作原理，如何调整规模，选择GPU，并控制部署成本。
    *   强调了混合部署模式（部分请求走API，部分走开源模型或微调模型）。
    *   建议使用Nvidia的免费推理请求资源进行基准测试。

2.  **LLM推理工作负载：**
    *   **核心概念：** LLM推理的核心是逐个生成token，并且每次生成token都需要将之前的token（包括原始prompt）保留在GPU上。
    *   **KV缓存：** 强调了KV缓存的重要性，它是LLM记忆的关键，也是影响成本的关键因素，所有模型（无论是API还是本地部署）都使用相同的机制。
    *   **步骤分解：**
        *   文本到token的转换（Tokenization），模型使用自己的词汇表。
        *   初始prompt处理（Prefill），计算整个prompt的注意力机制。
        *   逐个生成token，并将生成的token存储在GPU上。
        *   Detokenize，将token转换回人类可读的文本。

3.  **GPU上的数据可视化：**
    *   **Token表示：** 一个token约等于四个字符，使用数字索引表示。
    *   **嵌入向量：** 每个token都有对应的嵌入向量，LLM使用这些向量进行比较和数学计算。
    *   **矩阵运算：** 所有文本和图像最终都转换为矩阵，GPU擅长快速处理矩阵运算。

4.  **注意力机制：**
    *   **核心思想：** 理解如何区分句子中哪些token重要，以及如何利用之前的token生成下一个token。
    *   **Query, Key, Value：** 模型权重以Query, Key, Value矩阵的形式存储，矩阵乘法用于生成token的表示。
    *   **KV缓存的作用：** 保存生成的token的Key和Value，避免重复计算，提高生成速度。

5.  **性能测量：**
    *   **关键指标：**
        *   **Time to First Token (TTFT):** 处理prompt并生成第一个token的时间。
        *   **Token Latency:** 生成后续token的延迟。
        *   **Time to Total Generation:** 从开始到完整答案的生成时间。
        *   **ISL/OSL:** 输入序列长度和输出序列长度。
    *   **查询模式：**
        *   长输入短输出、长输入长输出、短输入长输出，以及随机混合。
        *   强调了需要测量生产系统中不同查询模式，因为它会影响成本和性能。
    *   **数据分析：** 使用2D直方图分析输入输出序列长度，并根据分析结果优化引擎和资源分配。

6.  **软件和工具：**
    *   **TRT LLM:** 英伟达GPU上的LLM模型编译包，实现最佳性能。
    *   **Triton:** 开源推理服务器，支持各种模型部署。
    *   **NVIDIA Inference Microservice:** 英伟达的企业级产品，简化了引擎构建和部署。
    *   **模型优化：** 包括量化KV缓存，使用FP8等技术减小模型大小，提高推理速度。
    *   **未来趋势：** FP4 和混合精度计算（如 INT8，FP8），可以进一步提高性能。

**核心结论（Core Point）：**

理解LLM推理的内在工作机制，特别是KV缓存和注意力机制，对于优化部署成本和提高性能至关重要。

**根本结论（Fundamental Point）：**

通过仔细测量和分析不同查询模式下的性能指标，可以选择最合适的优化策略，构建高效且成本可控的LLM推理引擎。

**总体框架（Overarching Framework）：**

1.  **理解（Understanding）:** LLM推理的基本原理和工作流程，包括 tokenization, embedding，注意力机制和KV缓存。
2.  **测量（Measurement）:**  对不同查询模式，包括TTFT、token latency和总生成时间进行精细的性能指标的监控和分析。
3.  **优化（Optimization）:** 使用各种软件工具和技术，如TRT LLM、Triton和量化技术，来优化模型和推理过程。
4.  **部署（Deployment）：**  根据测量结果和分析，构建和部署适应实际查询模式的推理引擎。

希望这个总结对您有所帮助！


Content:
[Music] it's very difficult to teach um extremely technical material in about 20 20 minutes initially I had planned for at least a 45 minute session so I left some reading material for you at the end and um all of the resources you could download slides and everything so feel free to take screenshots or not and so I work at Nvidia I'm a Architects I work primarily with retail clients and it's my job to essentially work with those clients understand sort of what their main challenges are this is data processing computer vision um across all of the different use cases and then now I'm focused on llm inference so my hope today is that um you get a better intuition of exactly what's happening with this particular workload and how you go about uh to some degree sizing things choosing different gpus etc etc and more importantly controlling the cost of a deployment cuz that's often times the thing that's going to really prevent you from taking this taking this to any meaningful scale is that overall cost of a deployment most folks that I've seen are doing some kind of hybrid so you choose a big box API you have some set of queries that go there in addition to you have some set of queries that go to some open- Source you know hosted model or some fine tune model that you have internally so just reference um if you go to build. nvidia.com or ai. nvidia.com everyone can get a thousand inference requests for free so I typically recommend this to folks who are benchmarking um different types of op Source models we have all of those models hosted it's optimized if you're teaching a course um and you are trying to evaluate all of the different LMS that are out there for your business there are also multimodal llms um speech llms every model that Nidia accelerates will be available there for you and that's sort of a path to you um to either go optimize them yourselves or to work with us um you'll see things about uh Nvidia inference microservice and all of those things that you can take to Enterprise so we have sometimes the uh I'll call it the rocky road and then there's smooth roads whatever path you want to take we're here to support you uh in terms of agenda very simple I I want you to understand the llm inference workload and then we'll move to how you go about measuring a production imployment and some of the things you need to be watching it's a little more than um let's say you know the total time to generate and really understanding what's happening on the gpus as you sort of scale out even if you have a single GPU I think it's very important for you to just have the intuition and then lastly I'll show you some software that you can use um some open source packages that you can use and then point to uh some paid offerings okay we're going to get into the llm inference workload itself so the first part is is really understanding what happens when you send a prompt onto the GPU so I have this example here I'm saying okay write me a presentation so I sound smart I come to the a engineer conference and you guys are maybe going to like to talk and essentially what I'm going to do is I'm going to put that on the GPU so the moment that I send that prompt on the GPU it stays on the GPU so think about that and then from there I'm going to generate one token at a time so I'm generating the tokens llm inference is hard and I put the time stamps T1 through T4 so in every single deployment no matter how fast anyone claims they they're doing things um it's typically one token that's generated at a time that's very important to understand the next thing is that in order for an llm to give you a coherent answer just like how you speak you have to remember every single thing that you said before and that you'll understand the mechanism of um how that how llms are able to do that so that's why I'm putting llm inference is in red and putting that back onto the GPU so every token that I generate gets locked onto the GPU and then you'll actually see what that looks like um in terms of vectors how many of you have heard of KV cach before okay some of you um typically I don't see maybe many leaders here about this thing called KV cach KV cach is this thing that really drives to some degree the cost uh so whether or not you use some big box API or um you're using a single GPU it's all the same sort of mechanisms the same algorithm that everyone is trying to solve so in terms of steps here the the I I like to as I said sharpen your intuition so the first thing if we move from the left my first job is to convert these text whatever text that you send we're going to focus on llm inference into some words that the model understands so the model will have its own vocabulary and it's my job to translate that I'll give you the technical uh terms coming up after that and the first thing that happens is I do some initial prompt processing so I have to compute the attention mechanism on the entire prompt I repeat that I have to compute the attention mechanism on the entire prompt per user so if I have a million people hit my service and a million people send 10,000 tokens that's a million time 10,000 attention mechanisms that I need to compute also while generating tokens for other people so that's it's good for you to appreciate sort of that complexity that's happening and once I finish processing that prompt then I'm going to start generating one token at a time and that typically happens very fast and then from there every token that gets generated that's in the lm's vocabulary I need to now DET toonize that back into your language so here's the technical terms that you'll see when you read the literature or you read uh super technical documents first is tokenization each model will have its own tokenizer and um the thing to think about when you think of tokenizers when they did pre-training they had they downloaded the internet and some right and they cleaned it up etc etc so tokenizer and as you start thinking of the complexity across languages coding languages uh regions etc etc they Tred to get what is the minimal set of character groups that can represent this entire training data set efficiently CU it's really all about efficiency so for instance the Llama tokenizer has 128,000 tokens all right and I I'll talk a bit more about that so here's what it actually kind of looks like on a GPU so I tokenize the llm understands it I go into this thing called prefill prefill is a stage where you compute the attention mechanism and many people are doing advancements with attention mechanisms um I'll talk bit more about that so there tons of different schemes people leverage all of the different types of memory hierarchies in gpus to really accelerate this type of workload and then I start generating tokens one at a time the red and the green just signify hey I'm storing those tokens on the GPU the green is the latest one that I sent out so hopefully that makes sort of intuitive sense all right so the other thing I want you to visualize um I I think it's nice to visualize what what is the actual data that sits on the GPU so first a token is approximately four characters that's a nice way for you to think about it um so from here I have two vectors so the first Vector is just showing token one through token v v is the total number of tokens that I have in my tokenizer and the second Vector below is just I have some numeric index I don't want to keep using the token to reference itself I just use use the number as a lookup so my job when a prompt comes in is to convert that text into those token what I'm going to call token IDs okay so I I have make me sound smart I make me sound smarter you see two vectors sets of tokens and the key thing I want you to walk away with from from that distinction is that an l&m's token is not a human word sometimes it is sometimes it's not it's typically some subpar of words you'll see we symbols when you look at tokenizers from different models um but you want that first framing so now we have text we hit to a vector right so from there each one of those llm tokens had a corresponding embedding Vector so embedding Vector is everything we embed uh videos we embed images we embed text tokens think of it as a representation that uh an llm can use to compare things and do math on so that's why we always want to com excuse me convert into some Vector representation right because some Vector representation is just some high dimensional coordinate space and we're just rearranging objects that's to some degree what you're doing okay so from those token IDs I went to the actual um embedding vectors themselves so if you look make me sound smart now becomes a matrix all right make me sound smarter becomes a matrix with an extra column so in reality what you're doing every time you submit a prompt I don't care what llm you submitted to who you submitted to this is what you're doing right you are converting your text now images as well they get converted to some Ida tokens or something like that that'll be another interesting talk to do diffusion models etc etc but you're really putting this large Matrix on the GPU so why that the next question you should ask is okay um why are gpus is good for this workload because they process matrices really really fast so that's sort of the the advantage and the thing hopefully that that makes a little more sense to you now the next thing I want to talk about is how the llm is going to process these tokens and I'll keep in mind if any well I'm not even going to ask you to raise your hand I'm 100% sure each of you is used it an LM if you have not I'm not sure what's happening uh uh the other one is the attention mechanism I I truly think um it's one of the things that you should understand if we ever drift away from it that's fine but the fundamentals of that mechanism and and seeing sort of the Innovations around that I think can help anyone any business leader etc etc just because you are able to speak a different kind of language um in this generative future so as you think of the attention mechanism the intuition that you should have is just that mechanism of relating tokens how do I distinguish in a sentence what is important all right and then for the next token that's going to be generated hey what tokens that I said before were really important for me to make a next good decision for that next token so that's the intuition and now we're going to won't necessar touch too much of the math but I want you to see sort of what's happening on the GPU so once again The Prompt comes in I'm just going to do a short one make me sound smart I'm going to generate this token called llm all right uh we saw the same matrices that I said before so remember my text now turns into a matrix hitting onto the GPU and and the main thing I want you to understand or visualize here is actually how an llm memory works so now when you're speaking you've recorded everything that I've said for the last uh 10 minutes in your brain somewhere it's it's stored so now you're going to see how the L is storing what it is that you just said so from there um a lot of folks will hear about these query key and value matrices this is what the actual model weights look like so when you look at a model weights file if you go on hugging face there's typically a Json file that will show you all of the different pieces of model files and you'll see this thing called qk andv so I have these model weights so now I've went from text to a matrix I'm going to Matrix multiply against the weights of the models so now I get these three output models so think of these weight matrices that I showed here think as um when you're doing a projection what you're doing is you're taking um some coordinates and you're putting it into a different space that's really what you're doing when you do Vector Matrix math so now when I do this Matrix multipication this query key and value Matrix so if you look at different tutorials on attention you'll see these things pop up a lot so hopefully that'll help you to read it a lot more this is now the lm's interpretation of each of those tokens that you sent in right and now the job is how do I now take uh these query key and value matrices and sort of interpret it to try to generate the next best token and this is just happening constantly over and over every single token that's happening but the key thing I want you to walk away on the slide is where I drew the key and the value right when people talk about KV cach optimization every llm performance engineer is just literally trying to make that thing as fast and small as possible and I'll I'll that it'll make a little more sense as to what that does to your cost But ultimately these key and value matrices this is like your llms memory so it'll make a little more sense coming up I know I didn't show a ton of the math I show some tutorials afterwards so you can go read more about that um my intention here is for you to visualize key and value so every time you see a prompt I just want you thinking crap key and value is on my on my GPU okay the next so here's the real value of the KV cache so remember we said said that whenever I generate a token I'm going to push it back into the GPU right so every token I generate it goes back into the GPU and then I have to compute an attention mechanism so this is what's happening this new token I generated llm I get its Vector representation as you see in blue but now I I do that Vector Matrix math now so before I did Matrix Matrix math that's my first prompt first comes in I generated my first token now I'm doing Vector Matrix math you know people will batch this across all requests but I'm just showing you a single request so you can see it now the value of the KV cache is if I were to if I didn't have the KV cache I would have to reprocess all of that work I did on the prompt that I did before so this is the benefit of your KV caches to now I'm just going to compute attention on this newest token how does this new token relate to everything that I said before that's the thing that's really happening intuitively so so if I have this KV cache my Generations can be fast okay and it's really up to the the What's called the batch manager on the GPU to make sure that I'm just pushing out as many tokens as possible okay so if you look at uh an llm these groups of three matrices are calling attention head there more matrices than not but these are the main ones um llama has 32 attention heads so I just kind of want you to appreciate what an llm really looks like right so I have 32 sets of these matrices I'll have 32 of those KV caches happening at the same time and now I have to combine all of that to then generate the next tokens so there's an incredible amount of work that happens in a very short space of time to give you a coherent token okay a good mental model for you to keep in your head I'm going to speed up a little bit is to um if you see the number of parameters multiply that by two and that is your fp16 gigabyte memory on the GPU so if you have let's say an L4 I think is 20 gigs um and I have a llama 8B that's automatically 16 gigs fp16 so I only have 4 gigs left for my KB cache so on the GPU it's either the model weights or tokens that's it there's nothing else on the GPU and I I have a thing to read on that this is a really good blog it shows you all of the different um optimizations that you can do and okay now let's talk about measuring so if you ever see this thing called ISL or I have ITA oh sorry ISL or SL that's input sequence link output sequence link so now I want you to see what some Advanced monitoring might look like if any of you are devops folks these are things that you want to record the first thing that we measure is time to First token so how long does it take me to generate um the process the prompt and then generate my first token and that's typically a measurement of how good your attention mechanism processing is that's really what you're trying to sus out so that's time to First token into token latencies so after I've generated my first token every single token after that I'm looking at those individual spaces so everything that's going to happen there U think about when the system is on the load I have you know a thousand requests coming into my system I'm generating a thousand sets of different tokens and the more memory I occupy typically that slows down processing so if you start to see drift in this metric then so I'll show you some plots that you can look at and then time to Total generation how long did it take me to initially get the prompt fully finish the answer right super intuitive like I I said ISL OSL that's all that means when you see them on the plots coming up okay uh this is a very important Paradigm for you to understand in your mind uh so I I worked with a lot of folks on you know maybe uh rexus deployments or deployments of other types of models so on the GPU if you're only deploying one model on a GPU outside of llm in France uh in my opinion I think you're wasting the GPU you can put multiple models on the GPU to actually increase your throughput that's why it was really created um so this is a slide is excuse me this figure is just showing I can have multiple models I have some space for data and that's how I increase my throughput per unit Hardware however on the llm INF front side it's very different I have one model you know folks can fit multiple models on a GPU that's cool but that's not a real production use case you'll typically have a single model the remaining space that you have is all for KV cash and generating all those tokens so I just put four different requests and I I just kind of want you to see the boxes that are happening okay uh I would say this is the most important slide in the entire presentation because this is the thing that will determine both your cost and performance so there are four different querying patterns that happen and this is something that you must measure in your deployment because often times you might read benchmarks and they'll just say all right they'll cherry pick one or two of these but in reality in your production system you might have several of these different patterns that are occurring so let's take a look at the first one long input short output so a long input means it's going to take me technically longer to compute the attention mechanism so my pre-fill stage will be longer I occupies more memory from my prompt does that make sense intuitively hopefully it it's grabbing you but then on the generation side I don't generate much tokens so there's not much those tokens are not picking up a lot of memory and they will tend to finish Fast so the second the second one or the maybe the most costly use cases um so I have clients that will message me and say hey my data scientists are putting two bigger prompts on my GPU so now they're killing my deployment cuz if everyone went and put the maximum context length I can only fit so many requests on the GPU so that's something for you to think about you'll have to manage that internally with your deployments so that's why I'm putting you know okay the GP is really full because a long text uh excuse me long input uh long output the next one short long you know your time to first to them be really fast I don't have much to compute the attention mechanism on but hey I'm generating a ton of tokens that's really really fast so hopefully as you start measuring these types of different query patterns um you'll see different results I just put you know what a random sampling set might actually look like on the GPU because not everyone will send the same length of input and output um so that will it'll be good for you to just sort of visualize and track these statistics more importantly why we're doing that uh internally I'm I'm going to steal the time here Peter uh more importantly why we're doing that or why we're tracking these things is that the whole goal is to build I have a big model my goal is to shrink it as much as I can but to keep it as accurate as possible so the more that I shrink the faster it runs the more GPU memory I have for what tokens all right so that's how you really try to improve your cost this is why I'm I'm sort of proposing to you to build inference engines so what all I'm showing here is a 2d histogram of input sequence length versus output sequence length cuz the question that you'll have to answer is hey how long are my actual prompts someone might say okay here's the max prompt length that you can ingest and the max prompt you can out you know excuse me get on the output and all of The Big Box model providers have to estimate this when they go into costing or providing a service to you right because they have to host all of that Machinery under the hood now that you understand what's happening so we use this to statistically determine what is the max input sequence length and the max output sequence length across all of my users and this would give you a really good indication of um how you can size your engines we use that to actually build more optimized engines in addition um it'll just give you good viewers to maybe uh what you call it scaling out and things like that the next one is time to First token analysis remember time to First token is measuring my performance of the attention mechanism under load so someone might show attention mechanism at one query woohoo show me attention mechanism underload when this thing is fully maxed out 24/7 that's when you really need to start um measuring these types of things so this is something you can look at these are sort of experimental plots um there's a package called gen perf that will be released open source it's out already I have a link to it there this is where you can it'll generate these plots for you but I I'm just showing you what the engineers uh looking at internally to measure the performance of the compute platform next time to completion analysis how long did it take me to go from start to finish across every single request naturally The Wider that box plot you have have to intuitively ask what's happening why did this person's prompt take longer than another so you can investigate either batching issues scheduling issues different things like that I I'll take questions in here why I have to move really fast sorry there okay I'm going to speed up here I to to token latency Peter how much time I got oh you're fine definely have time for questions okay cool I'm going to Ste I'm definitely so sorry I just want to I realize I may have gone a little too fast so forgive me for that got five minutes cool all right time uh token to token latency so that is I'm generating token so I'm looking at that spacing versus token position so the longer a sequence gets remember my memory grows so typically that means that system is under more load it has more throttling that might happen under high load of requests so if I see a large variation in token to token latency as the sequence gets longer when I'm generating that means I'm not very performant right so we look at that to see I try to make make sure that that's constant no matter how much tokens I'm generating that means I'm I'm really proficient okay uh last one would be time to First token versus number of input tokens so time to First token remember is Computing the attention mechanism okay versus number of input tokens so if I have a bigger prompt my attentions will take longer but I if that plot goes up like from your perspective it goes up like this in terms of sequence length that's not really good performance we really look at that slope and we try to get that slope almost you know as as low as possible so if you send me this long sequence I can get that thing done really fast okay okay uh in terms of software uh you'll see this thing called trt llm uh Triton is an open source inference server so you can deploy models on CPU on GPU computer vision rexus python pytorch tensorflow it's uh it'll host all of the different types of models so there's one way that your deployment team deploys all your data scientists are happy because they don't have to do conversion you're happy as a deployment person because you don't have to manage a torch serve versus TF serve and uh flask and all of it is done through one it's written in C++ blazingly fast and then the other thing you'll see Nvidia you'll see a lot more coming out of Nvidia is NVIDIA inference micros service cuz building these engines getting them deployed optimiz the scale it's not easy so we've sort of made that easy for you as an Enterprise offering but you guys can try it out uh for free okay so trtm let me just give you high L lots of stuff on the slide but the main thing I want you to walk away with um uh is this is the model compilation package for llms on Nvidia gpus this if you want to get best performance from Nvidia gpus please make sure you use trt llm um and naturally uh once we're investing more in name you'll see some more things come out so you'll see performances on a100 and h100 really focus on fp8 gpus so fp8 will be Hopper and AD love lace okay so fp8 I'll I'll talk a bit more about that what the advantage there is but mainly is if I go from fp16 fp8 is this half my memory almost the same accuracy and so we measure the accuracy and we publish the accuracy so now I have this much more space for tokens but more importantly this model is that much faster okay so I want you to understand where the sort of industry is going this is why Hopper the will ate hopper for breakfast and lunch and dinner because of fp8 it gave folks that cost um benefit to do this thing a lot faster okay um in Flight batching it just means I don't have to wait for all the requests to finish to start a new request the moment your request finishes I can inject a new request While others are going okay tons of features here I put the features um so some wants to to focus on are quantize KV cach so I can actually represent my KV cach in different uh excuse me Precision so that means I'm actively shrinking that memory making it more performant um you have phkv cach that's just you managing uh your gpus is a lot better in terms of all of that memory so there are tons of things you can do tenser parallelism the thing to remember about tenser parallelism if you want to increase latency you use tza parallelism split the model up across multiple gpus that's typically done within a node I repeat that that's typically done within a node you don't like to do tza parallelism across a node you'll see pipeline parallelism go across a pipeline parallelism is more sequential so I process this chunk so in a multi- node model like huge models this box will finish and pass off to the next box but most folks will typically just work most models will work within a single node like um nvidia's 340b model that they just released it was designed to do inference on a single h100 node but that's an fb8 okay so those are some of the things in terms of models that you have access to um we optimize those models and we give you a lot of scripts for you can go do that on your own or you can sort of take our software um and take a easy path you either way we support you so here are some of the models that are there all of the llamas mistol mixs we work with all those teams behind the scenes so typically before any foundation model comes out we we work with those teams to to get them deployed okay what does it mean for tenz RT so you might have seen tenz RT before which was a deep learning compilation package for NVIDIA gpus lots of folks in computer vision etc etc I've used that we took the best practices from there and added all of the extra things that need to happen in the llm inference Loop so that's what trt llm is really about um so mainly focus on llm inference uh here's a good visual an engine that's built to a specific GPU cannot be moved to another GPU so you always have to compile to that GPU that's why it's that performant because we really leverage um all of the the actual Hardware on that system to rewrite the algorithm rewrite that model to that specific piece of Hardware okay um trt LM and Triton so trtm will give me an inference engine I need something to host that inference engine and accept requests batching etc etc so we have Triton Triton works very simply it's literally a folder where you specify it it works on tenser in and tens are out so it will tell you what are my inputs coming in and out and then it'll basically understand how to interpret that file or you can host any other different models that's one that's a thing I do a lot with folks just two more slides this is where the future of inference is going so a lot of folks do fp16 in France today um a lot of folks are moving towards fp8 just because hey I know half the model size almost twice the speed more space for tokens it just makes more sense from a cost perspective that's why folks like that and then uh you saw Blackwell was announced that's the major Innovation I get fp4 so that's where things are really going to get interesting I'll end with um Nvidia inference microservice so we've made this thing really easy we've gone and actually found the best configurations for all of these models on each piece of GPU and we're slowly rolling out all of the models cuz it you know it'll just take some time to optimize the world essentially and yeah you can use this to download all the slides I put papers um tons of other things you for you to read so yeah hopefully your intuition has sharpened shall we just conclude with the because there was someone had a question sure yeah I think where was the question yeah oh hang on I'm going to come over and point my mic at you thank you ome hi um sorry my question is actually on the heat map that you shared you mind um walking through the heat map and how to interpret it it was a little small yeah sorry about that yeah so the heat map all I'm looking at is um so when you go to build an engine you build an engine to the max input sequence length and the max output sequence length so we actually change how that Matrix math is happening under the hood based on those settings so you might say all right uh my users are only going to send 4,000 tokens but in reality they might have been sending 1,300 over the past week that you measured so now you can stay with statistical certainty that hey the majority of people that were serving um during this time these were their querying pattern so I can rebuild an engine for that period of time what gets super interesting this is a topic I'm very interested in is seasonal engines so during the day you have different querying patterns so you'll scale down you'll scale up and so you might have different engines built for different types of querying patterns based on traffic and stuff like that so hopefully that that may have answered your question yeah but it's just saying you know looking at the bounds of what's the minimum number of tokens that came in the Max uh Min Min out and max out and just looking at that over the entire distribution yes it oh yeah right when it comes to those uh infant strategies you talk about like Lio and lyso um how do you what kind of strategies do you have to manage like which ones are used at like because obviously each session is going to be pretty generic you don't know which one to use at first correct um do you split those between gpus or do you stick with one and do it switch between so typically we'll we'll go to you try to find what's one configuration that will manage the the plora of types of requests that you have coming in so we we're typically at a a one engine per all the different querying types and I think you'll start seeing I'm giving you a little bit of future ways to think about it on the devop side because that's something you'll have to test right if I look at this quering pattern that came into my system with this engine if I switch the engine does it still satisfy the querying pattern and how much cost does it save how much faster is it so that's more of a an engineering exercise that you'll have to deploy sorry I I didn't have a yeah yeah so I I just I'm very interested in the seasonal side just because okay querying patterns will change um especially when agents come it'll just be that's going to get super interesting when agents are just throwing stuff yes yeah so so what people do in order to scale the attention mechanism is here's another interesting fact that uh why folks don't train huge context models because it's actually now you've seen the bigger my uh prompt the more memory I need so imagine what that does to a huge I don't know 10,000 100,000 GPU deployment it might make it a million gpus just to do that context thing so people will train to a small context length and then interpolate in that value to to give you that length of context length and then you're sort of bound to what attention mechanism you were using designed there there's things like flash attention that will just do everything in the L1 cache really really fast so it depends on the speed of some of the different it also depends on the GPU as well so that's why um if you look at Blackwell that was announced by by Jensen they literally connected I think 72 different gpus on one EnV link so NV link connects gpus together that's how we can move data insanely fast so now we' connected like 72 gpus on one that's that's just to show you um like mixture of experts trying to computer attenion across all of these different things but that's a actually a really good question no I I don't necessarily think so like the entire industry is you know going after that problem that's why everybody wants to maybe see something other than attention and ah you know there's so much excitement there yeah unfortunately have to call time on but that's been fantastic thank you Mo um [Applause] [Music]
