Timestamp: 2025-01-02T04:17:47.292631
Title: 【博士Vlog】2024最新模型Mamba详解，Transformer已死，你想知道的都在这里了！ Fc3zzOfFjsE
URL: https://youtube.com/watch?v=Fc3zzOfFjsE&si=rLHutNGFfUI192nd
Status: success
Duration: 26:14

Description:
**核心要点总结：**

1.  **Mamba 模型的核心创新:**
    *   Mamba 是一种新型的线性时间复杂度序列模型，旨在超越 Transformer。
    *   它基于状态空间模型（SSM），使用动态参数和选择性扫描机制，解决 RNN 和 Transformer 的问题。
    *   它通过将矩阵运算转化为一维，实现了线性的时间复杂度，速度更快，且内存占用更少。
    *   通过硬件感知算法，Mamba 优化了在 GPU 上的内存使用和计算效率，尤其是在 SRAM 上进行计算。
2.  **Mamba 相较于传统模型的优势:**
    *   **RNN (循环神经网络):** 克服了梯度消失/爆炸的问题，能进行更长时间的训练。
    *   **LSTM (长短期记忆网络):** 解决了RNN训练慢的问题, 并提升了长期记忆能力, 但训练仍旧很慢且长序列仍然会遗忘。
    *   **Transformer:** Mamba 解决了 Transformer 的二次时间复杂度和巨大的内存需求问题，训练更快，推理也更快，且内存占用更少。
    *   **SSM (状态空间模型):** Mamba改进了SSM中不能解决的问题。
3.  **Mamba 的硬件优化:**
    *   Mamba 优化了在 GPU 中的数据读取和计算，尽量让所有操作都在高速缓存（SRAM）中完成，从而避免了慢速内存（HBM2）的延迟。
    *   采用并行计算技术，实现更快的计算速度。
4.  **Mamba 的应用前景:**
    *   可以替换 Transformer 用于各种任务，如文本处理、图像识别和视频处理。
    *   具有更长的序列处理能力，适用于处理长文档、视频等。
    *   提升自动驾驶系统，可能用于实时处理，提高安全性。
    *   有望用于生成视频等任务。
5.  **现有深度学习应用中的替换和替代潜力:**
   *  传统Transformer架构可以被Mamba架构快速替代.
    *  现有基于Transformer的模型，如OpenAI的GPT，都存在被Mamba替换的潜力。
6. **微调版 Mamba (Vision Mamba):**
   *  视觉应用中, Vision Mamba 在图像分类和目标检测等任务中展现出比 Transformer 更高的准确率和更快的速度, 且内存占用更少。
   *   它可以通过切分图像的方式进行输入和处理，与 ViT 架构类似。
   *   Vision Mamba 也被应用于一些更高级的视觉任务, 例如骨骼图像分割等医学图像处理领域。
7. **总结:**
    *  **核心总结:** Mamba 模型是一种新型的、高效的序列模型，它通过其独特的状态空间机制和硬件优化，解决了现有深度学习模型（如Transformer）存在的计算效率和内存占用问题。
    *  **根本总结:** Mamba 模型预示着深度学习模型架构的新方向，其优越的性能和效率有望在文本、视觉和视频等多个领域带来革命性的变革。


Content:
大家好我是PHDVL之前的视频给大家讲mamba的时候呢讲的不是很清楚然后呢我最近呢就和一个大牢讨论的一下他的一个模型的具体细节现在呢对这个模型的具体细节还有一个更深入的了解所以给大家的重新去讲一下mamba这篇文章那mamba这篇文章的讲的是什么呢叫做Linux Time Cycling Smodoling为sss这样的一个模型就说他是使用了一个现性的时间这样的一个须点去够前的债用模型那么他的这个目的呢就是去打败这个Transformer那Transformer他是一个二次时间复杂度的这样一个模型那么呢我们首先先来回顾一下就说我们之前有一个什么样的work呢rn这个模型叫做这个循环神经网络这个网络呢他其实是1986年就发明的这样一个网络我去查一下自料发现这个网络真的是很久以前就发明出来了他是怎么做的呢就是他像一个拉锁一样在一个拉他像一个拉链一样的拉锁上面去这个滑洞那么他就把这个这个每一次呢就皮状态机一个状态机这样就把他们都给他装到自己这个模型的进去了所以如果他展开的话他会变成这样的一个很长这样的一个训练吧所以说的就是每次放进去一个就像一个什么带白质的一个每一架的DNA上面去滑洞的这样的一个锁链一样的感觉他的每一步呢都是一个成性的就是都是一个用程法来做的所以说他会产生一个问题就是他的一望速度比较快他的会因为他是成性的嘛他后面的时候就会发生规定的外粒或者规定的Expo定这样的问题就是他的因为他成来成去成成成成成成成他成成成成成的结果就是他越成越大和越成越小后面就后面就没发训练了所以说的他能只能做一个非常短的这样的一个状态还有一个事他训练的时候是这样一个Aso去训练的但是TES的时候他是可以一个一个这样并行去TES的所以他训练的时候是相对比较慢的但是他TES的时候是相对比较快的那我来看一看在他之后有什么改进的就是改进就是RSTEN这个东西其实也是一个很久以前就出来的模型1997年就出来的这样的一个模型那么我们看RN那他就是一个成性的这样的一个方法就是他都是成法那RSTEN的它就是一个加法的这样一个过程所以说他有一个好处就是他不会产生一个很强烈的这样一个Gridden的Weinert或者Gridden Exploding的这样一个问题所以他到后面的时候还能继续续练下去他可以训练的比RN更久一点然后这样有这个模型他其实就是RSTEN只他稍微改进了还有门的这个排队方式让这个器刷量变得更小的一点他们俩的其实是完全一样的这样的一个东西那么这个RSTEN它有引入了比如说这个一些门这些门的话就可以让这个模型就有一个更长期的记忆但是RSTEN还是有同样的问题和RN是完全一样的它的训练的速度非常的慢它太性的速度比较快因为它就可以展开了它也有一个问题就是说当它去很长的时候它会忘掉它以前的东西所以说它也不能搞得太大RSTEN和RN都是这样那么在2017年的时候一个新的方法《狠空驻适》这方法叫做《川普方法》那么这个方法它是使用这样一个多头注意力的机制然后多头注意力这样一个机制它是怎么去进行计算的呢它就是算出一个这样二围的一个矩战然后在二围的矩战里面去计算这样的一个注意力所以在它春天的时候它是很快的因为它只要算买一个点的注意力就可以了但太性的时候它要到这个矩战中去进行一个队比所以说太性的时候它的速度是非常非常慢的这是它不仅它速度慢而且它存储的空间也要大因为你要把这个矩战存下来所以它需要一个欧尔方的这样的一个巨大的这样的一个时间复杂度所以看到它不仅复杂度高它的这个层储的需求也非常大那么你看就是这个地方是这样的一个情况所以说它是一个非常恐怖的战用过什么办法是一个线线的所以说你能看到它们俩之间的这样的一个队比是什么样子那么我们就来看看这个传统外面有一个什么样的结果它的春天是比较快的它还讲过它太性的是比较慢的它们俩正好反过然后它的时间复杂度不仅时间复杂度它的空间速度还动了都是欧尔方这样一个级别那么我们就要来讨论一下就说在这PO中出去是怎么样去工作的呢那么这个就是一个纽纪的应该是A100的这样的一个这PO那么这个这PO是什么样的呢就是这些都是它的计算内容对啊这头它计算内容一堆一堆一堆的这样的一个计算线上那么这个就是它的这个流处理单元对吧一个一个的流处理单元哦那刚刚去收的都快递我们计算一件就说在这PO之中它有很多的这样的机串和这是它的小的机串和所以这PO可以听一个高强度的一个并行处理因为它有这么多的这样的一个机串和对吧这些机串和可以把他们都计算出来然后呢这个是它的IOTOCASHIOTOCASH就是它在这个芯片上面去自己做的这样的一个机程的这样的一个纯组单元所以这个IOTOCASH的速度呢是非常非常快的然后呢这边呢是它的内存控制器对吧内存控制器之后呢它接着一个叫HBM2的这样的一个东西那这边也有这边接着HBM2HBM2那就是它的片材的这个内存就是说它的L就是说纯组的时候是一个金子塔价格它越是这个小的这个就是越快的这个方式呢它越贵对吧所以它做的呢只能越小然后越大的这个纯组方式呢它越便宜但是它呢也就是速度比较慢所以说呢IOTOCASH就是它金子塔必要靠加尔的地方它的速度呢非常的快但是它的大小的比较的小HBM2呢就是它的这个内存对吧就像咱们那些条样的内存普通的内存它这个内存呢它呢比较的便宜但是它的就是说怎么说的速度呢比较的慢毕竟它还需要通过MIME的砍错了它就会有很高的延迟啊这一个事情那么呢传说嘛有一个什么问题呢它的战斗内存太大了所以它只能在HBM2之中去运行它刚刚就不可能在L2开始上进行一个比较有效的预算所以说它的速度呢就比较的慢它每次的时候呢它都没发系的所以说它只能跑到HBM2你再去算一遍那么呢这个就是我们传说嘛的这样一个问题想想就说好的一几年之后呢大家就想办法就说去改进船舱门或者是改进RSTM去造一个新的模型这个模型呢它的目标是什么呢就是它的春景的时候呢非常的快太稀了时候呢它也非常的快然后呢它这个有没有这个额外的问题呢它没有额外的问题所以我希望它的RAM和TIME的时间复合度都能达到欧温这样一个级别那这样的话就非常好的解决了我们现在这个遇到的一些平景的问题那么呢有什么样的方法去解决这样的问题呢有一个方法呢它叫做SHM就是一个状态机的这样的一个方法那状态机呢就是像我们去做的这样的一个地规你看我们在抓地扣的时候这个提到地规对不对就说这是上一个时间的上一个时间部的这个情况这是下一个时间部的情况下一个时间部等于上一个时间部的成立一个A再加上另外一个韩数对吧这个呢就是一个状态机的这样的一个过程它就是一个地规的这样一个过程对吧那么这个传错贷呢它是机船那个二围的这样的一个曲战那机船一个二围的曲战呢是非常慢的因为它把所有的时间都保存下了状态机呢只需要保存当前的这样的一个状态时间就可以了它不需要去保存太多的这样的一个内容那么这个使用的它自己的地方是一个A呢这说如果你要是就是随便取一个职A那么它的结果呢不好所以A呢最好是一个动态的这样的一个东西那么A呢使用一个叫T-POP-MITRICS的方法去计算那么这样的话这会儿会有一个更好的一个效果然后呢T-POP-MITRICS你发现它也是个二围的所以说它就跟传错贷区别的它也是个二围的曲战那怎么办呢就是我们能不能把A给它变成一个一围的是可以的通过一个像SVD一样的算法就可以把A呢给它直接压成一个一围的只是说相当于体育它的一个主程犯然后把它压成一个一围的通过这样的一个方法那么这个就是Mamba的主要的使用的方式当然Mamba并不是第一个做这个的它之前还有SSM就是它之前的所以Mamba是改进的SSM解决SSM里面一些不能解决的问题那么它Mamba能不能进行这个拍照Compute 也是可以的也就是说以前的时候大家都是这种现性的这种一个一个去计算的这样一个模式但在显卡之中我们要想算就是说这个叫做哪家合的话呢它就可以通过这样的一个方法含叫做Pairer LawReduckish这样一个方法去计算这个还不是Reduckish另外一个词但是我忘了就是说它可以通过这样的一个方法去计算我们之前来视频中也讲过就是它计算的方法是你看它是一个它是一个这样的一个像叠形形算的一个远算就可以把每一步的哪家合都算出来比如说上面是一堆数对不对这个就是前两个数的家合法和这个就是前三个数的家合这个就是前四个数的家合这样意思的一推下去然后还有一个事儿就是说我们刚才说的就是说我们的带如果在我们这一批欧之中它有Cache 还有内存如果是我们省Cache的话它速度比较快如果省内存的话它速度就比较慢那么怎么去避免就是说在这两个东西今天来后来去的靠批或者说就是它产生一个很长的一个延迟那么最开始的方法就是说我们在第一转和SRAM之间来后的导藤对吧这就是传说们的方法因为它没有办法它的内存就用太多了它就换码的非常多慢那怎么办呢我就是能不能在DRIM里面去算在从DRIM里面把数据的靠批到SRAM里在SRAM里面把它全部都算完算完之后再给它直接靠批回DRIM像那一只有一次过程就可以解决这个问题那么这个慢慢就把这个问题给它解决掉了那么在SRAM之中其实还有一个问题就是有的时候它这个数据还是会产生的比较大成真比较大呢在SRAM中就没办法都存下来怎么办呢这一批哦它的计算其实是比它的内存那样快的所以说怎么办呢我就在SRAM中再给它重新算一遍把它重新算一遍这样我就可以保证所有的数据还一定要保试在SRAM里面那些巨大的数据我们并不需要再去移取出一遍了但这样的话就可以继续增加它的数那么就是这边就是这个mama这样一个模型mama这个模型中它有些地方是重新去进行这个计算是它进行一些反复的计算它这样做虽然会增加工号虽然会可能会增加时间但是它总比从内存里独一遍可能要败一点所以它是使用这样的一个方法什么这就是说的mama这样的一个方法叫硬件感知算法那么mama就成功地把问题解决掉了也就是说我们该在HTBM2的东西还在HBM2的但是当我们计算计算的时候它都是在高速的这个IO2开始的它就和这个和尽量一个超高速的一个交互那么它总比在HBM2的你们今天交互的方法要好所以说它这就是所说的这个mama这样的一个核心算法这是买码的一个算法如果把它直接画合法是这样这是它内文中的算法所以说这个是金字塔比较价格的地方它的Size比较小等下速度比较快这个是它的金塔比较下面的地方它Size比较大但是它速度比较慢所以说我们就是尽可能的把我们所有的核心训算都给它放上Sramb里面这样的话就可以让这个系统的变得更快那么也是因为这个SSM这个方法这个方法它非常的SSM这个方法它非常的省内存所以说它可以做这样的一个优化那么传说嘛它根本就不可能做这样的优化因为它根本就不省内存OK那我们来看一看这个mama还有一些其他的算法什么呢就是说在状态机在这个连讯状态机里面靠毕一个东西它靠毕的时候整个是靠毕过去是比较容易的但是当它中间有一些空档的时候我们不想要这些空档我们想靠毕的时候把中间今天有一个有选择性的一个靠毕的话要不必要难mama它就设计这个算法把这个问题解写掉了以前那是SSM这个不炸发明的对吧它设计的它发明的东西是那个SLICESN的部分这是它的发明的这个部分那么我们现在就把这个表都填完了对吧mama我们也不需要特别去了解它到底是怎么做因为它即使和穿专门的是完全一样的它的这个这个可以完全进行一个替代对吧所以说呢我们并不需要去知道mama它就有这个最后一个什么样的一个结果我们有时候看看mama的效果是怎么样还有它速度是怎么样的那mama它的春顶的frecy就非常快它的Type-to-the-the-thrace也非常快他们它的没有额外的问题它的这个软模和time的时间都是欧恩这样的一个时间那么所以说呢type-to-the-thrace以前会卡对吧如果它用上这样的星机的话可能它就不会卡因为它Type的部分非常慢但是现在的Type-to速度可以非常快可以更结盛它的这样一个计算资源那么就来看一看mama这样的一个情况是怎么样的呢这个是一个穿专门上的一个模型对吧它的大小呢也是2.8gb对于是mama它也是2.8gb他们的参数量是完全一样的时候会怎么样呢它在训练的时候训练在一个2k的大小的样本上面所以这个样本的大小呢就是2k就是从这个地方结断了到后面就没有东西了对吧所以你看tensh这个这个这个Transformer的这个Selftensh在超过它的这个这个范围之外呢很快的就进入到了一个不收点这样一个状态了但是mama你看它因为它是一个状态机马上是一步导一步一步导一步这样一个地归的一个方程它就可以一直持续一个很长的时间直到这个后面它只是进行了一个略微的一个上升所以说mama它可以去进行一个更长的有效的这样一个文本处理它可能以后也能进行一个视频的处理因为文本和视频其实很相似的那么我们在分享这篇文章之后再给大家讲一讲微占mama微占mama就是说mama里面它没有透太多的这样的一个对比它没有画出图来所以说我们有时候看的呢就不是很清楚但是微占mama就把它这个图给它画出来进行了一个比较好的对比你看这个速度非常快这个文章一个月就出来了那么我们来看就说你看它在这个上面是VAM对啊这个是它的微占mama这个是Transformer能看到的就是说它做这个ClassworkK线的时候效果那么好对啊这个是它的就是说做那个框那个车比如说框个车对吧框个人对吧就是把这个人从这个图片里标出来IMU呢就是它说你原来有一个Ground数Ground数就是那个真实的那个纸那真实的纸的和你新的那个纸它中间呢对啊它能够久多的重合面积那重合面积越高的说明它的效果越好对吧所以IMU是做这样的点事你看它效果也更好但是这个图有点误导性为什么呢它是从71开始到7所以它其实是提升了几个点你看这好像很厉害其实它就提升了一点对吧这个也是提升了一点提升了2%对吧这个提升了2%这个人可能提升了一点但是你知道Transformer已经是个很牛的模型了所以说提升2%呢也是一个非常大的一个改进吧只能这么说那么呢我还看这个是它的速度的这样一个对比它速度对比的话你可以看到就是说Mama的速度呢是比这个在Transformer上的这样模型的叫快了2.8倍对吧越大的就是越快对吧所以说当以后的软速度身边的更大的说它变得更快还一件事就是软速度身就是说当它的这个分辨越高的时候的你站的内存也是越大的因为你跳嘛你做视频也是这样的你分辨越高它的速度越慢对吧站的时间速度越慢然后它的这个按这种资源的也越多那么你看Mama的它的资源是一个现象的占用的这样一个状况但是呢这个Transformer它是一个平方占用所以到后面就已经Otel for Memorade就是它的 Memorade直接就爆炸了对吧所以这个地方它可以减掉了8.6%的 Memorade所以它呢可以去处理更大的这样的一个数据比如说前段时候Chai GPT可能只能处理一个比如说5000大小的一个数据那么现在呢它可以处理500大小的处理那么Chai GPT就可以绝生成一本小说看到就脱离的之前那个就说生成一本小说到后面它有点忘了前面显什么都这样的一个仰境对吧它就可以到后面一直绝续的生成下去所以如果这个技术被后盆开用上的话应该是一个非常爆炸性的一个结果那么我们就来看就说微卷版网的这个其实呢是和这个VIT的是非常像的只是微卷穿梭门微卷穿梭门它就把一个图片切成一块一块然后塞到你们去然后呢就用它的这个塞伏拳准的价格即使呢得到一个最终的输出微卷版网的其实是一样的它也是把一个图片对吧切成一块一块的然后呢放了这个这个一个模型里面然后之后呢做出一个这样的结果然后呢这个是穿梭门的这也就是说呢就是说从穿梭门到微卷穿梭门那么穿梭门是2017年出的微卷穿梭门是2020年出的它用了三年的时间但是MIMA是2024年应该是1月份出的然后这个东西是2024年2月份出的所以它呢只用了大概1月的时间就把它给搞出来了这样做呢这样对比呢其实也不是特别公平因为这个是一个新哪一地它是从来没有人去做过的微卷穿梭门是一个Aneo的一个ID是一个完全新的一个ID但是呢这个MIMA呢它的就是怎么说呢是和上面这个呢是一种灯地增也有关系它是基于这样一个工作做一个新的模型的这样一个工作所以说呢它呢其实并没有发明出什么太多新的东西不像是微卷穿梭门它是整个从头发明的一个新的东西所以说它这个时间对比呢也不是一个特别特别有这个地的一个对比我们就是大概知道就是它这个Work可能确实出来的很快如果现在CV有多卷呢你看当时2017年的时候三年才卷穿一个东西对吧那现在的话呢你要是不能用一个月两个月的时间把一个东西转出来的话你就被人超越了所以这个现在做微卷呢真的是非常难做的一件事讲也兴趣大家不要入CV这个坑这个坑的话非常的恐怖那我们来看看它效果是怎么样的呢可以看到它Rystein这样的一个模型我们之前讲我Rystein的对吧Rystein这样一个模型去进行对比就是扛墓升的这样的一个就是转机时间往落的这样一个模型然后呢还有去查它和传续外界的对比就是和BRG的一个模型进行对比那就是传续外的这个模型然后这个底下就是它的满满的这个模型是SXM的这样一个模型对吧这是可能是别的SXM这是这个满满是它自己的一个SXM然后你就能看到就说准地厌上的我们要看因为这SX多少呢对吧你看这因为SX大部分都是R24平方对吧都是R24乘以R24这样一个SX然后这个VIT可能用了一个更高的这样的一个纸对吧一个它是一个更大的一个模型那R14的话也是R24但R16好的就变得更大一点然后你看它的参数量呢你看这些参数量呢其实CN的参数量呢还是挺省的对吧就是不是很大的一个参数量然后呢到了这个传说面这边它的参数量就变得非常的红步了然后呢到了这个满满这个地方它的参数量是比较小的你看这个7照的话可能就是大概它比这个CN的所有的都小对吧那它可能和这个这个差不多是一个SX但你要去对比就是7照和6照的这个模型呢你看它的模型呢有78.3%的这样一个准确率那这本是72.2%的这样一个准确率所以它的准确率其实是比较高的然后呢你看这26照的这样一个模型对于它来说的话26照的模型它就低高了一点对吧它会提高大概1.1%的这样一个效果但是它的参数量呢但是你不能去只对比它的参数量因为呢它的使用机制也不一样它可以用更多的这样的一个高速的这样一个缓存所以说它的速度呢会比它快很多你看这点它快了3倍的速度大概是这样的一个情况这是它的内存的这样的一个消耗那么呢我们来看一看它最后做出来解我是什么样的呢这个是MASKRC这样一个方法MASKRC它可以把这个图片之中的飙舱但是你去看这个飙舱这个地方应该是这地方应该是它的飙舱的机翼可以把它的飙舱给它比较有效的给它飙舱对吧那么MASKRC也可以把飙舱的飙舱但是你看这个机翼的部分呢它就没有飙舱对吧它只知道飙舱的主体表舱可能跟它的天神机制是有关的但是呢对于这个模型来说呢这个图片太大了它根本连飙舱飙不动对吧这个根本就是做得做不了所以就直接把原子图给你输出出来了根本就没有标出来它飞机的部分所以说慢慢的可以比较有效的把这个可以去有效的去处理一个比较大的这样的一个Science你看它可以去KAPCHR YR LARGEOPJECK的一枚只可以去图片中找的一个非常大的这样的一个物体那么我们介绍了这个微卷版马之后呢给大家来稍微看一眼优伴马这样的一个工作优伴马这样的一个工作呢是坐在一学上面的这样的一个工作那么一学上面的工作呢它就是说它省的一个像优耐特一样的东西对吧这个东西很像一个优耐特优耐特是做什么的呢就是比如说我输入一张图片对吧输出一张图片输入的图片是卫星的图片输出的图片是一个地图的图片我就可以把这个卫星的图片给它转面成一个地图的图片所以说呢我们就比如说这个大家这个使用的地图比如Google地图或者百度地图它肯定是使用一些这样的技术它不会去人工的去把这个路都画一边那画一边画太累了所以它可能就使用一些这种机器标准的这种方法的去把这个路都给它升成出来这样就可以了这样的话非常方便那么它要做什么事呢比如说我们输入一张图片是一张有物的图片然后输出的图片是一张没有物的图片那么我们就可以做一个区无境对吧我还可以输出一个人的图片输出一个人的图片输出一个人美严的图片做一个美严的效果那么它做出来它做的一个什么事就是它输入的时候是一个骨头的这样的照片这可能是个腹部的这样的照片它输出的时候把这个腹部底的器官都给你进行了一个标柱那么它输入的时候是这样的输出的时候这样这样去训练那么你在塞进行一个新的图片它做Tiles的时候它也会变成这样的一个形化就是Unet这样一个大概的一个思路那么你就可以看到就说在不同的方法上这是光处就是这是真实的一个结果就是说人去标处出来的结果这个是使用的不同的这样的Nice去做的这样的结果那么你会看到就说它拿剪条纸出来的地方是空Nice就说可能会不好的这样一个位置你看这个地方就把干的这个位置大家给大家装到的位置对吧所以它就完成了一个不太好的这样的一个图形你看有地方它就不能正确的进行了一个分类但是你看你看Unet网上可以进行了一个有票的分类你看这个图也是这样这个地方确的一块这个地方是完成的而这个地方也是这个地方本来应该没有东西对吧它就多了一个奇怪那么Unet网也没有问题像这个地方也是这样的也是就是错误的分类但是Unet网可以比较正确的去进行了一个分类好我们就来讨论一下就是这个东西对我们的以后的科研会有一个什么样的一个影响呢那么首先第一件事就是说传说门和漫码的他们其实实际上是几乎完全一样的东西他们的东西的几乎是一样的所以说他们可以进行一个直接的替代就是说把漫码的直接就替代成传说门就可以了唯一的不同就是模型内部的这样的一个东西不同它的输入和输出都是一样所以说他们之间的是主义一个可以互相快速企业的这样的过程所以说以前的时候我们用传说门来做的任何的东西都可以用漫码来代替比如说这个是传说门做的这个模型对吧它的左边的边马器边上的Bort然后它的节码器的边上的GBT那么GBT以后可能就变成GPM因为它可以使用漫码来代替它里面那一些过程然后我们来做的这个Grip这样一个模型我也在之前讲过它有Type-in code这是一个Wi-i-T或者是一个RyceNet但是是一个Wi-i-T这是一个传说门这是传说门因为它是文本然后你现在可以用漫码来代替这个地方它可以是RyceNet或者是一个传说门现在可以用漫码来代替看看传说门的传说门的效果比RyceNet好漫码的效果比传说门可能要好速度主要是速度要快所以说当时的时候这个Open-i-T可能用了几百块钱可能来训练这个东西现在可能还用几百块钱卡它能够多到一个比以前更好的效果所以说可能以后可地不能会出现一个新的版本可能会出现一个Wi-i-T而Wi-i-T错的话可能就是用漫码来代替里面的东西可能会得到一个比现在更好的效果然后Open-i这个公司你也知道它这个公司比较疯狂对吧做什么东西都做个顶楼做个视频生成的做个骚软做个生成的时候以前是用盖它用的一个DPU整然后以前的时候大家都是去用类来进行类比的话它直接给你整的一个成对比学习所以说Open-i做什么东西的做得比较疯狂这个东西咱们是做不了因为它用了400个1.0的Size也就是说四个亿这样的图片来做的咱们拿地去搞四个亿这样的图片本本对的咱们都存不下对不对那他们可以把这东西都存在内存里所以说他们的资本是非常丰富的那么他们如果现在如果是使用满把这样的模型拿化使用的这个显存资源的更少所以说也许他们会在之后做一个更大Size的这样的东西尤其现在COMECRAW现在过了其实现在已经有100就是以前是40个密令可能有100个密令或者是200个密令的这样的对调了所以他们可以用更多的这两个数据去说明一个更好的一个结果那么我们就期待Open-i给我们带了一个什么新的大伙吧然后呢这个呢就是微镇参丑门可以改成满把参丑门所以说呢所有的用丑门做的事情都可以改成满把并且呢以前呢说丑门的他有个问题他赛爱的比较大所以说呢他在处理图片的时候可能就已经到他的一个比较极限的一个状态了那么呢因为他的这个转物呢还有他的这个SPEED都发生一个变化所以说呢在满把上面也许我们就可以去直接去处理一个视频你看这个地方呢他是丑门去处理一个视频那这个模型呢他没有太活起来的原因呢就是他的机板量呢实在是太大但是呢用丑门的话这件就要改成满把的话呢他以后生成这个视频啊或者是什么样的这个过程呢就可以直接去做那么呢大概一些新的可能性吧因为他的速度那变得更快了所以说呢这个处理视频呢可能就变成了一个未来的一个可能的一件事并且呢还有一件事儿呢就是说他可能能做到一个实施处理的这样的过程因为他的速度变快了但我们使用多坏先看的时候可能那个视频的就不用变了实施处理那么这样的话呢我们就可以有更多的这样的一个应用了对吧还有一个事儿呢就是说比如说我们这个现有这个汽车汽车上面呢会有一个激光雷达激光雷达可以拍到周围的这样的一个物体那么有些车的现有一个激光雷达然后有的车呢是通过摄像头的方法那么不管你是摄像头还是激光雷达他呢都需要去跟别周围的这样的一个物体的这样一个情况那么呢使用一些简单的算法有时候呢会有一部好效果或者说呢他产生一些安全问题那么可能使用这样的一个传说卖这样一个模型呢再加上一个可解释性的因为娜麦可解释性也会比传说卖好一点因为他呢会有一个更好的可解释性的这样的一个结果那么如果他有可解释性比较好的话呢也许在自动驾驶的上面呢以后会用啊这样的一个东西并且呢这个娜麦这个模型呢以后可能会彻底改变这颗的架构因为如果大家要是都去使用这样的东西的话那么以后因为达公司或者是什么告可这样的公司呢他可能会涉及一个架架我专门的就是去 fit 娜麦这样的一个模型那么呢后来速度咱也会变得更坏一点我说的买码可以直接去出的一个视频这样的一个情况说的如果啊能直接去出的一个视频呢那么呢也许以后会有更多的可能性因为没有知道在 deep learning中我们去精神认为干预越多呢有时候他效果呢就越能越好会变得越差所以说有时候呢我们就是无微而至用一个这种人工干预比较少的这样的一个方式他也许都会有更好的一个效果所以我们如果是去人工的切成一个 fremen一个 fremen这样的结果我们用一个什么流啊就是双流网罗啊或者是用一个像cn的这样一个3D的cn这样一个网罗啊他有时候效果会变得比我们去一个一个一个出的 fremen更好因为他能加强他 fremen之间的这样的理解吗所以如果是买码能够去整个出的一个视频的话可能会比我们人工去切成一整一件的这个效果呢可能要好一些那么这就是我今天所讲的这个所有的内容了那感谢大家观看我们今天呢就晚晚了给大家听听那个完整的处理想这样慢慢还有他的一些应用对吧比如说微责慢慢还有幽慢慢没有讲太多的这个慢慢的这个相关的应用还有现在已经有限说了很多新的这个应用如果以后呢我们也绝对有必要的话那会等一下去深入的探讨想慢慢的这些模型说那也感谢大家的观看我们下期再见
