Timestamp: 2025-01-02T00:36:16.823196
Title: Aider + Deepseek 3 vs Claude 3.5 Sonnet
URL: https://youtube.com/watch?v=EUXISw6wtuo&si=qOTu5rBXjBXB9W4w
Status: success
Duration: 24:02

Description:
* **Core Ideas:**
    *   **Performance Comparison:** DeepSeek 3 generally outperforms Claude 3.5 Sonnet in coding tasks, particularly in speed and code execution accuracy, across multiple benchmarks.
    *   **Cost-Effectiveness:** DeepSeek 3 is significantly cheaper than Claude 3.5 Sonnet, making it a more affordable option.
    *   **Task-Specific Strengths and Weaknesses:** While DeepSeek 3 showed greater success in most tasks, it had notable failures in certain areas (e.g., Tetris game), highlighting task-specific variation in performance.
    *   **Inference Speed:** DeepSeek 3 is noted as being significantly faster than Claude. 3.5
    *   **Real-world Coding Capabilities:** The comparison tested real-world scenarios like creating a game, an API, a web scraper, and modifying a full stack application.

* **Core Point:** DeepSeek 3 is a more cost-effective and often more performant option than Claude 3.5 Sonnet for various coding tasks.
    
* **Fundamental Point:** While DeepSeek 3 generally excelled in the tests, no model is perfect, and task-specific performance variations highlight the complexities of AI coding capabilities.


Content:
today we'll be comparing deep seek 3 and clae 3.5 son deep seek is the first fast model to beat clae 3.5 Sonet on The ater Benchmark what makes it more impressive is that it beat Claude on the more difficult polyglot Benchmark which now tests llms across multiple languages not only python deep seat 3 also be claw on the code forces in the Live code bench Benchmark deep seek is also extremely cheap at 14 cents per per million input tokens and 28 cents per million output tokens this will change to 27 cents and A110 on the 8th of February still great value for money if it's as good as they say it's also updated to be faster now producing 60 tokens per second three times faster than version two clog 3.5 Sonet is a lot more expensive at $3 per million input tokens and $15 per million output tokens let's test their coding abilities head-to-head remember to subscribe to the channel to get the latest AI reviews like these also comment with the option you think won in the battles deep seek 3 on the left caud 3.5 Sonet on the right first task write a game of Super Mario and python using pyate let's paste The Prompt in both deep seek wins The Prompt to First token race tokens per second seem to be close to equal de seek finishes first both want to add more features suspiciously using similar words let's allow it to create the files let's create files for Sonet as well I see images we'd have to download some for it to use let's start the Deep seek version okay we downloaded large image files at Sams we'll get to that later let's see the cloud version okay the character can move but cannot jump let's ask both to fix their issues deep seek to not use images and quad to fix jumping and screen size let's try both again the Deep seek version works great now I can jump and interact with the environment let's try the claw version okay I still can't jump which is a bummer it did mention that we use the space bar to jump here's how they look side by side next task we need an application which searches the internet for llm benchmarks like the ader and LM Arena benchmarks then consolidate them into one place let's clear the terminal in both past the prompt in both let's go they seem to have started at the same time their speeds look [Music] similar deep seek finished first again let's accept the changes and create the file let's install the dependencies for deep sea [Music] okay now let's install its dependencies let's add the serper API key for both deep seek hardcoded it while Claude rightly used the environment variables file let's try them out side by side okay deep seek only returned URLs of The Benchmark websites while Claud returned an error both didn't do what is expected let's give them a follow-up prompt deep seek gave the first token this time and looks a bit faster let's try them out again okay still no llm Benchmark data let's tell them to wait a bit longer for the site to load still nothing finally let's tell them to use playright [Music] no that's a fail for both this is a difficult task for an llm though they should be able to pull it off by asking relevant questions next task let's ask them to create an asp.net API which returns a random AI coding joke the API should use the Gemini 2 flash model copy The Prompt clear both terminals paste the prompt and both now begin the fight now CLA looks a bit faster the fluctuations in inference speeds is probably because of server load okay plot is [Music] done for deep seek let's create the API with net new then run it with net run navigate to the URL let's see if if it modified the template code it didn't let's make sure it created files in the correct directory yep it didn't let's move them to the correct directory I'll explain why llms do this with AER in another video it has to do with creating new directories and assuming we are working from the new directory now building the project gives us an error let's see how Claud did firstly it also put the files in the incorrect directory let's fix that let's build it we run into an error as well let's give deep seek it's Error okay it fixed the error let's try it again great we have a Swagger page let's try it out execute oh user error we didn't put the EnV file in the correct directory let's fix that now if we retry voila it works the jokes are random and unique not the funniest of jokes so we'll call them dad joke let's also give Claude its error let's try it again it still gives an error it's hallucinating it made up a new get package name that doesn't exist it should have made a fallback to calling the Google API directly like deep seek did deep seek clearly wins this round next task we have a Merin stack application with replaced by sqlite repository is public and URL can be found in the description Please Subscribe while you're there it means a lot and goes a long way let's clone it in both folders test it out it works basic Tailwind with login functionality clear the terminal in both paste The Prompt in both go okay they both want to add files to the chat so they can modify the files let's allow that e e e Claude wants to add some packages deep seek also wants to edit the front end let's instruct it to proceed a [Music] [Music] it's done let's allow it to create the files let's run the front end and back-end servers for deep seek this is a very goodlooking Pomodoro app I have never gotten one that looks this good good in any of the previous reviews let's start it that works let's add a task that works as well complete a task that works let's add a new session and some tasks to it Let's test brakes let's check the data is persisted to the database the sessions table looks exactly as expected the task table also looks exactly as expected this is the best version of this in one shot that I've ever seen let's try out Claude looks good let's start the Pomodoro that works let's add a task that does not work strange it seems to not create a session beforehand but attempts to create tasks Claude used to Ace this task I wonder what happened might be load balancing or it just got worse let's get the error and retry it created another error okay okay we'll revert the commit using aers undo commit command let's compare them side by side they both look good I prefer the Deep seek version e for comparison with other LM models we reviewed before let's ask them to both create a Tetris game in Python using pame Claude looks faster in this way let's run the Deep seek version it fails that's a shock let's now try the claw version it works changes shapes doesn't move on its own though navigation works clear's a line let's give deep seek its error and see if it can fix it no same error it couldn't fix it that's a fail let's assess Claude with the tetris criteria changes shapes clear's line deep seek gets 0% Claude gets 71% so surprisingly deep seek is the winner it got 60% on average while Claude got 46% if we put everything in perspective we see that Gemini 2 flash is the best overall with the sample tasks we used followed by Deep seek 3 we Ed the more difficult questions for deep seek 3 so smash the Subscribe button for a Gemini 2 Flash versus deep seek 3 video
