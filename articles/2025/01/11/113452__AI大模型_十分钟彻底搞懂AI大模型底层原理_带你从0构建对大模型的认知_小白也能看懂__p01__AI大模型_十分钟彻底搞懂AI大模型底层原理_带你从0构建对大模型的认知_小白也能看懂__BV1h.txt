Timestamp: 2025-01-11T11:34:52.000565
Title: 【AI大模型】十分钟彻底搞懂AI大模型底层原理！带你从0构建对大模型的认知！小白也能看懂！ p01 【AI大模型】十分钟彻底搞懂AI大模型底层原理！带你从0构建对大模型的认知！小白也能看懂！ BV1hfkMY9ERM
URL: https://b23.tv/3BORcu7
Status: success
Duration: 25:54

Description:
好的，下面是对您提供的文本进行的总结，包含大纲结构，核心观点，以及概念图。

**Outline and Structured Summary:**

1.  **AI 大模型背景与发展：**
    *   **CHATGPT 的出现：** 2022 年 10 月 OpenAI 发布 CHATGPT，迅速引发全球关注，用户量破亿，成为增长最快的应用。
    *   **行业变革：** CHATGPT 的强大能力推动 AI 行业从决策式分析 AI 向深层式 AI 演化。
    *   **市场趋势：** 国内外企业纷纷投入大模型赛道，广泛应用于教育、汽车、金融等领域，对话类应用层出不穷。
2.  **决策式 AI vs. 深层式 AI：**
    *   **决策式 AI：** 基于历史数据分析和预测，如房价预测、智能推荐等，主要进行判断和决策。
    *   **深层式 AI：** 通过学习规则和数据进行创造，能够生成全新内容，更符合人类理想的自动化形态，类似 CHATGPT。
3.  **AI 大模型的工作原理：**
    *   **核心机制：** 根据上下文推理下一个文字，通过不断迭代生成长文本。
    *   **技术名词：** 涉及到深度学习、强化学习、监督微调等，但应用层不需要深入了解。
4. **AI 大模型的训练流程：**
    *   **三个阶段：**
        *   **预训练：**
            *   采用监督学习，基于海量文本语料库进行自主学习，如 GPT-3 使用网络爬虫、公开书籍等。
            *   文本被分割成 “Token”，Token 是模型处理文本的基本单元。
            *   通过无监督学习，模型学习人类语言的语法、语义、表达结构。
        *   **微调：**
            *   在预训练基础上，用对话数据进行监督微调，使模型更擅长对话。
            *   通过强化学习进一步提升模型能力，使其在环境中采取最优行动。
        *   **推理:**
            * 根据概率生成新的文本。
5. **Tokens:**
    *  模型处理和理解文本的基本单元。
    *  单词可能被分割成多个tokens，空格也可能被视为一个token.
    *  不同模型token的结果可能不同，因为它们使用不同的分词器。
6.  **无监督学习 vs. 监督学习：**
    *   **监督学习：** 已知输入和输出结果的关系，如猫的图片和“猫”的标签，让机器学习特征和标签的关联。
    *   **无监督学习：** 不知道数据和特征之间的关系，通过模型找到数据之间的隐藏结构，如区分猫和老鼠的特征。
7. **概率模型和幻觉问题:**
    * 模型通过训练得出下一个词的出现概率，从而生成新的文本。
    *  早期模型会一本正经的胡说八道（幻觉）因为模型无法真正理解文本含义，只能根据概率生成。
    *  随着模型参数增加，智能程度提高，低级幻觉问题会减少。
8. **文本相似度与向量化**
    *  为了体现不同词语在语义和语法上的相似性，引入 “词向量”（Word Embeddings）的概念。
    *  文本先转换为 Token ID，再通过向量化转换为向量，用一串数字来表示，以捕捉语义和语法信息。
    *  相似的词，其对应的向量在向量空间中的距离更接近。
    *  通过向量的距离计算，可以判断文本之间的相似度。
9.  **Attention机制：**
    *   通过注意力机制，模型可以关注文本中不同词之间的关系，从而决定生成什么内容。
    *   模型通过计算每一个词和其它词的相关性来确定权重。
    *   多层注意力机制可以使得模型理解文本更加深入。
10. **Transformer:**
     * 注意力机制是Transformer 的核心。
     * 通过注意力机制，可以高效地补充文本中各个元素之间的关系。

**Core Point:**

AI 大模型的核心在于通过海量数据训练，建立概率模型，并运用注意力机制理解文本关系，从而实现文本生成和智能对话。

**Fundamental Point:**

AI大模型本质上是一个根据上下文进行概率性预测，并辅以注意力机制去理解和生成文本的复杂系统。

**Overarching Framework:**

The overarching framework of the content is based on the evolution of AI, from decision-based analysis to a deeper learning approach, highlighting the underlying mechanisms of AI large models (LLMs), including the process of training, text understanding, and text generation, to give the reader a high-level understanding.

<Mermaid_Diagram>
graph LR
    A[AI发展历程] --> B(决策式 AI);
    A --> C(深层式 AI);
    C --> D[AI大模型];
    D --> E(核心原理:根据上下文推理);
    D --> F(训练流程);
        F --> G(预训练);
           G --> H(海量文本语料库);
           H --> I(无监督学习);
           I --> J(tokens);
        F --> K(微调);
          K --> L(对话数据);
            L --> M(监督微调);
              M -->N(强化学习);
        F --> O(推理);
        O --> P(根据概率生成文本);
    D --> Q(Tokens);
     Q --> R(分词机制);
    D --> S(无监督学习);
        S --> T(特征);
    D --> U(监督学习);
         U --> V(标签);
    D --> W(词向量);
        W --> X(语义相似度);
        W --> Y(语法相似度);
    D --> Z(Attention机制);
        Z --> AA(Transformer);
        AA --> AB(上下文关系);
        AA --> AC(权重分配);
    AB --> P
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#cfc,stroke:#333,stroke-width:2px
    style Z fill:#fcc,stroke:#333,stroke-width:2px
    linkStyle 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23 stroke:#333,stroke-width:1px
</Mermaid_Diagram>


Content:
各位小伙伴大家好我是郭报科技的联合创始 麦然后今天我想通过十分钟的时间去给大家分享一下关于AI大模型的底层原理2022年的10月3日上OPAI发布了一款产品叫CHATGPD这工具已经问是就引发整个后面网络的政策它只用了两个月不到的时间就日后用户量过亿打破了原来的TikTok保持9月的纪录成为了历史上增长最快的一个应用CHATGP强大的内容深层能力引起了业界的一个普遍关注也加速人工资的行业从决策式分析AI到深层式的演化国内外各大企业像风冷一样全部拥护到这样一个大模型赛道短两年时间国内63%的企业都在做大模型应用大俗之的一些像盘谷通易文心混淫延期这样的一些叫通用模型的一个基础以及对于大模型行业的应用将世界上教育汽车金融领域以及到我们的一些行业的大模型产品就是整个生态在国内外已经非常成熟在用层面在基于AI聊天住所的应用也向语后纯顺般的拥护在整个市场里面比如说像Alsa Rawpick推出的Called以及OPENI的CHATGP1以及像并就微软推出并AI还有就是谷歌推出的Brought和国内的像文心一言统一千万以及像腾讯的元宝就是在CHATGP火了之后这两年时间里面国内的对话类的应用非常非常多这是AI大模型所带来的一个技术变革或者AI大模型所带来的一个产业变革在AI大模型出现之前市面上的AI解决法案更多的都是决策式和分析式AI大模型出现以后加速了人工智能行业基于深层是AI的一个发展我再解释一下决策式AI就是说根据原来的一些数据做分析来做一个判断和预测比如说房价预测我们可以拿到过去一年到两年的房价数据然后去判断影响到房价涨幅的一些因素通过大量的积缺其的算法去进行一个分析最后得到一个模型我们可以摸著模型的预测房价的渐跌我们可以摸训练算模型的进行预测这就是我们所谓的决策式AI决策式AI在企业中的场景用得比较多的像智能推荐封共辅助决策等等而甚至AI是学习规则以有数据以后进行一个创造它有一个叫创造过程一般是几历史的数据进行一个模仿和学习然后通过符合式的创作可以深层全新的一个内容也能做出一定的判断的决策甚至是AI更加符合人类理想中的自动化形态也使得AI能力踏入到一个新的理成被解点那到底什么是深层是AI呢很多时候大家在初步使用欠了GP的时候发现提过问题回答的时候它是什么它是一个词一个词的往外控对吧感觉像是有个人在输入一个内容给你反馈显得很高级其实呢这个一个词一个词往外奉并不是为了高级感而是它的原力决定的我们在这里面可以用一个扣子的平台的一个模型叫豆瓣模型如果大家可以用工具的话可以看到它在深层内容的时候它是一个词一个词或者一个词组去给我们反回信息然后在前段平阶的这就是AI当模型的工作模式大原模型的原理其实很简单就是根据上下文来推理下一个文字是什么比如说我给ARM一个句子ARM就可以根据上下文具推理出下一个文字是在这句话里面它的今天的天气很后面可以推出一个好字那么甚至这个文字有什么用呢注意ARM会把深层的文字再丢到这个上下文语境中继续深层更多的文字以此类推我们就可以深层更多的长文文内容比如今天今天结成好啊今天天气很好啊请老今天天气很好啊请老细温今天天气很好啊请老细温室中拖站了一个古蹲的内容深层就可以得到我们所需要的一个长文文内容了这就是从表诚的现象去了解到了AI大模型深层文本的一个基本原理那么大模型是如何做到这个能力的呢接下来我们了解一下AI大模型背货的技术原理说到这个原理呢大家可能第一时间会想到很多技术名词比如说深层是AI强化学习 积极学习 深度学习监督微调 讲立模型 多摩泰 拖坑签物相量 确实访问制度力机制 编辑马器防心抠临 四位链Agent 以及解锁增强深层等等但是作为普通的AI大模型应用来说我们并不需要了解这种深只要有一个粗暴认识你就能够理解AI模型的一个表现更深度的去实现AI大模型的应用和调教AI模型比论也会知道AI大模型为什么会成为换决为什么输出的打压和预期相差比较远以及如何对模型阶级优化调教是更加个性化的模型等等所以要理解AI大模型的原理那么我们首先需要去知道从红官层面AI大模型从训练到推的整个流程它整个过程包括三个阶段预训练微调和推理在第一步的预确中中我们采用的是我们监督学习它会基于大家的支持库去进行一个自主学习我们举个例子比如说以GP3这个机动模型来说有多个互联文本做一个舆辣库覆盖书籍新闻文章科学论文回击百科社交媒体体制等等然后通过对这次要进行一个无监督学习得到一个机动模型在GP3的模型里面我们可以看到在官方公布的在一个数据里面可以看到它里面包括的事项Carmen Core就是网路爬充公开书籍以及像论文的网路文本库盖网书籍比较库整个机文为机百科支持库这里面已经描述了每一个舆辣库它的一个拖坑的数量这叫CoreliteToken 410B脸就是拖坑的数量那么这里面那么这里300B是什么意思呢它是说采用3千亿的拖坑进行训练那么这拖坑其实是我们经常听到的那什么是拖坑的在大国行中拖坑是模型处理和理解文本的基本单元模型通过把用户所有的文本分隔成拖坑然后在这些拖坑的技术上进行一个计算和推理简单理解我们给大家看一个例子比如说我们在这里面有一段文本我们需要把这个文本进行一个训练那么需要先转换拖坑我们可以利用在工具把这个文字出进去以后选择GP4O它会得到在一个文本这个文本是染色过的这里面它会告诉你有46个拖坑而且大家还会发现拖坑并不是严格按照英文单词的一度性划分的比如TALKING ANSYSION它会被分隔为拖坑和IZATION两个拖坑NUERO会被分隔成NU和ER两个拖坑另外单词前面的控格与单词一起会被划分了一个拖坑但如果有兴趣的话可以在BORBENI的一个网站上去输入对的文本然后去演示一下它是如何去生成拖坑的在这个过程中我们显示的文本表示拖坑单词并不是直接用这些拖坑来进行一个训练和学习的单词在推理计算的时候会基于这个拖坑的唯一ID进行一些的转化然后通过切换Tax拖坑ID的标签可以看到它会进行拖坑的唯一ID进行一些转化通这个菜单我们可以看到拖坑文本和ID的因素关系这里需要做一个点事跟单词使用的分子器是不一样的所以同样一句话在不同模型中的拖坑化的结果可能是有差异的这个差异是因为分子器会根据一个算法来把文本切割成一个个小片段可能是一个字符或者一个符号一个单词或者半个词甚至是一个经常同时出现的字符或者语句构成的训练等等有了大量的可以用来训练的文本数据以后我们接下来会采用一种监督学习的方式来训练模型那么很多没有学过AI同学对于无监督可以有监督这个学习方式有一些不理解那么接下来我给大家讲一下监督学习和无监督学习的一个差异所以监督学习就是我们根据一二的书籍知道输入和输出资金的结果关系也就是说在监督学中训练书籍有特征又有标签我们在这里面可以看到的特征是什么特征是我们的猫和老鼠特征而标签是表示我知道这个猫叫他们这个老鼠叫结督然后通过训练让机器可以找到这个特征和标签之间的关系在面对只有特征没有标签的数据的时候可以判断标签比如说当通过整个训练过程中得到一个模型之后我再一次把这个猫的图片丢过去他能告诉我这只猫叫他们同样的道理我把老鼠的图片丢过去他能告诉我这只猫这叫有监督学习所以无监督学习就是我们并不知道数据的数据和特征之间关系而是根据去内或者一定的模型得到数据之间的关系换句话说就是在未加标签的碎中试出找到他的隐藏结构在这里一个叫无监督学习过程中我们并不知道这个猫叫他们这个老鼠叫结督但是通过基地学习我们可以知道老鼠的特征是同一个类型猫的特征是同一个类型我们可以对他第一集一个分类哪些特征是属于Jerry哪些特征是属于Thom当然最后结果我们并不知道这个是老鼠这个是猫所以在无监督的学中我们只能去知道数据集中的特征之间一些关系我们在以及B3为例在训练过程中会基于大量输入的数据自行学习人类语言中的语法、语意了解表达结构和模式当通过当年文本的无监督学习以后他就能够基于上下文来进行下一个文本的预测居然来说模型会看到一部分的文本然后基于上下文的长时间去预测下一个坡坑然后再去比较正确答案和他的预测模型会更新全重除了去根据上下文来深圳合理的下文并且随著进步的文本越来越多他深深的文本也会越来越好预训的结果就是得到一个基础模型基础模型和确疾病背后能够合人类对话的模型还是有很大差别的虽然只是基础模型能够根据文本去预测下一个坡坑他会根据上下文去补充下一个文本内容但是他并不擅长对话比如说在基础模型里面活能省会是后面会深层一个藏沙这是他基础模型确认的能力但是要想达到确疾病内的对话效果还需要对基础模型进行一个进一步的微调所谓的微调就是在一有的基础模型上做进一步的训练那么这个步骤也会去改变模型内部的参数让模型更加适合特定的一个任务换句话说如果想要去年出擅长对话的模型需要给基础模型看到更多的对话数据在这个节奏中模型不需要从海量文本去学习的而是从人类企业的专业且高至两对话中去学习正的话相对于寄给了模型问题又给了模型的参考回答而这个过程就相对于前面说的监督学习也叫监督微调通过微调是否的模型相比基础模型他更擅长于对话类的问题的一个回答但是为了让模型的能力继续提升我们还可以让微调之后的模型进行强化学习而所谓的强化学习就是让模型在环境中采取行动并获得结果反馈从反馈中继续学习从能够在特定的情况下采取最佳的行动反馈通过对基础模型的一个训练微调之后我们可模型之间就可以进行正常对话了从整个内容反馈来看身上的文本质量还是比较不错的而在一个内容回答过程中比如说长沙式一个充满活力的程式历史文化与现代都是风情相结可这个内容其实就是大模型在根据大量文本训练之后所得到的一个推理的数据结果这个推理其实我们用一个非常简单的图片给安希解释就是我们可以在回击百科学到中华人民共和国在一个文字所以当我们通过训练之后得到一个全中模型之后我们在模型上输入中华人民他就可以推出共和国成立于他会不断地去把这个文字丢到上面的内容不断去生成一个更新的内容这就是我们所谓的大模型的整个内容文本生成的一个基本流程所以通过再一个分析大对大模型的原理在红湾上是有一个比较清晰的认识但是大家应该还有一些疑问就是我们本在生成的时候我们知道是通过训练来得到下一个词的出现概率的从而去生成新的文本那么这个训练和推的过程是否做到的绝利者在战具化中下一个词是本来的还是Iquers是根据训练过程中学习到的关系得到了一个概率模型假设本来的词概率是80%那么它所生成的文本就是Iline本来的词那么这个概率是怎么得到的呢假设我们下面有这样的三组数据训练级然后接下来这些数据会被转换回Token由于训练的任务是预测跟随其他Token的一个Token那么训练模型的一个简单方法就是获取训练数据集中所有连续的Token对并使用他们构建一个概率比如说在一个例子我们可以构建一个五成五的表格在每个单元格中写上代表单元格行的Token被代表单元格列的Token跟随的次数自己根据前面的最集中我们会构建在一个表格比如说Ilike你看IlikeAprusIlikeBananas这边有两个Like所以当我们在I后面跟随的Token是什么呢是Like有几次呢两次同样You likeYou like正面跟了一次所以这边会记录一次那么LikeAprus证明LikeAprus会跟一个LikeBananas会跟两个所以BananasAprus是1现在我们知道了每对Token在训练数据集中出现了多少次我们就可以计算每个Token跟随另外一个Token的概率于是我们把每个行中的数字转换的概率比如TokenLike被Aprus跟随了一次被Bananas跟随了两次因为这Aprus跟随Like出现的概率是33.3%而Bananas跟随他的概率是66.7%这可能会有些同学看到说Aprus这Bananas后面没有什么TokenLike概率出现是因为这里面我们觉得Like只有3个对话文问实际上在大魔性训练的时候我们会有大量的数据文问去进行一个学习所以不会出现像竟然在一个漏洞当然我们可以发现其实早期的大魔型在深层内容的时候它会一本进正经的胡说八道其实这个幻觉的产生是因为大魔型它本身无法理解滚本它所代表的在人类世界的一个行业所以它只能根据上下文的概率来深层它会按照合理的概率模型来深层对内容在整个内容上独起来的与法格式是满足我们的需求但是这个与法格式给我当前的历史背景和问题之间的相关性到底是什么样的大魔性是无法去做出战的一个判断这就是为什么我们在使用大魔性的时候那些幻觉的问题和一本正经胡说八道的问题会在早期的模型里面产生当然随著模型去年参数越来越多它越来越智能那些沈低级的问题慢慢的也会不会出现有了再一个机制我们在文本身上的时候就可以根据概率得到I like the balance的一个内容了但这个没有有另外一个问题在人类原作文本表现的渔事会更加丰富的那么如何去体现不同词语在语意和语法上的相似的问题举例子当我们当魔型我经常吃的水果有哪些大魔性是如何知道Apple and Bannanas都是随意的甚至如果还有其他的训练数据比如说Orig如何在吃的语言下来去实现联想和替换这里就需要说到另外一个很重要的概念叫《相当的潜步》根据前面说的所谓内容我们知道原始文本在这一训练之前需要先偷恳化最后转换了偷恳ID然后每个偷恳都会被一个整数数字表示也就是偷恳ID这是因为进程机没有办法存出文字任何自服最终多的用数字来表示使用机能够理解和处理文本出去有的数字表示以文妹以后我们需要把数字通过相量化潜入转换为相量结过数据相量本上就是一串数字为了演示效果我把这个相量长度用三个长度来表示实际上的相机长度可能更长在这里面你们可以看到我们的文本甚至偷恳ID通过相量化潜入之后它会得到一个三个长度的数字那么为什么要把偷恳ID的一个数字通过相量化的方式转换为一串数字呢有两个原因第一个相量化表示数据维度会更丰富因为一串数字它能表示喊应远远到一个数字将对于文本描述的信息它就会更多比如说语法、语意等信息举个例子man和women这两个词他们都在描述人类但是性别是完全相反如果只用一个数字来表示那这两个数字之类的大小是距离很大还是距离很小呢距离很大那我们之前相似度就很小了但距离很小那么他们的性别不一样在这种情况下单个数字所表现喊应就非常有限但是如果有多个数字我们就可以用更多微的细节表示那么对于个文本的描述和理解就会更清晰同时相似的词对应的相量作标在相量空气中的距离也更接近而且没有什么关系的词和词之间距离就更远这样模式就可以利用数学计算相量空间的距离去捕捉不同词在与以后与法之间的差异性以及词语词间的负责关系这对我们为了简单让大家理解只用到一个三围的相量来表示在GPD3的模型中相量的长度是12288所以大家可想知道它能保存性企业多大第二位原因是相量是一个既有方向又有大小的数字可以在空间中用来表示跌或者对象之间的关系在数学和物理中相量表示大小和方向的物理量螺速度加速度等在自然圆的最终相量表示的是词语距止或者门档的离性型使得机器能够理解和处理自然圆数学上面相量可以表示为一组有序的数字列表通常写成一个叫列向量形式比如一个三围空间中的相量可以表示为0,4.8和3.5这样一个围度而一个二围向量它只能表示两个围度所以相量化的表示有两个特点第一个是高围度相量给通过更高的围度去表示更加复杂的特征在机器学系和生的学中常用一组相量来表示训练级的特征比如说一个词相量可能包含300围或者更多每一个围补助不同的余特征第二个相似度量可以通过计算相量之间的距离比如说余显相似度或者欧基里的距离来度量对象之间的相似性余显相似度很量的视量各向之间的角度而余里的距离则是两个相量的空间中的直线距离比如说在这一个图中我们通过假设一个三围的结构去表示不同对象它的空间作标我们就可以看到像水果它的作标是比较先进的而动物的作标是比较先进的这样的话我们可以通过这样的一个距离的计算公式去判断我们多个目标对象之间的一个纹本相似度所以总结一下大魔形在对纹本训练问中需要了解纹本之间的与法结构和与关系也就是说在基于大量纹本与量库的训练过程中需要能够变别不同纹字所属的类别所以其实你可以看到像苹果和这个苹果他们其实属于两个不同的物体但是他们在作标上是完全不同的两个对焦点这样我们再进行一个判断的时候我们知道死苹果非底苹果因此这个以往的相量化的数据表示方式把输入的纹本转为多伟多的相量数据基于相量通天作标的相似度来做计算比如说这个例子里面这个多少钱这个什么价格陪我报个价这都是属于温价格的一个问题那么他们所得到一个相量化的数据在作标上它的相似度是比较近的同样我想要这个给我他们两个在空间作标上相似度也是比较近的但是这个问题和这个问题是属于两类不同的问题他们在作标上的相似度是比较远的这样就可以成为了解决纹本相似度的判断问题到这里其实大家就基本上有了一个更全面的理解就是所谓的预训练到微调到推理的整个流程以及双在预训练过程中我们到底有什么去训练的怎么要夸诚的话怎么要监督学生怎么要夸诫怎么要微调对吧然后我们也详细的分析了一下整个训练的本质就是一个概率性问题我们怎么去得到这个概率那么这个概率过程中我们会有以存另外一个问题就是我们怎么知道纹本的相似度问题我们怎么知道对于一个吃的场景Apple本来都是可以吃的所以这里面就会设计到纹本的分类纹本的一个相似度所以用了下量化到这里面其实大魔性还没有解决所有问题为什么呢比如说举个例子我们在这两个化中我在京东上买了最新款苹果非常好我在京东上买了阿克苏的苹果非常好那么这里面两个都是苹果但是作为人的事件来说他并不是同一个问题但是我们做生存的内容里面你们如果当大魔性就会发现他得到的结果是满足我们上下的要求的比如说大魔性能够向人一样去理解纹本的含义然后生存符合语意的回答这里是怎么做到的呢就会说叫另外一个非常重要的技术叫全是方法全是方法是2017年在论文里面叫Attention is all your内的里面提出来了他提出来以后迅速成为了自然原处理领域的一个主流架构广阔应用一个种人物比如说机器反应纹本生存问大系统等等全是问模型的核心创作显示的自主义机制这使得模型能够在初里训练宿旧的时候有效地补充训练内各元素之间的关系无论他们在训练中的位置如何远定比如说我在京东上买了最新款的苹果体验非常好那么他要去关注的是什么那么全是方法需要关注的是在整个文本里面的每一个文字之间的迟和指尖的一个关系全做从而去决定我要生存符合上下文的内容因此全是方法有能力知道当前这个词和其他指尖的一个相关性然后去专注在输入中的最重要的部分自主义机制通过计算每一个词和指尖的相关性来决定做一粒权重如果两个指尖的相关性跟弱那么他们之间的做一粒权重就更高这边有一个简单的粒子可以让大家看一下这就我们可以通过一个动画来看一下其实跟前面说的五成五的表格是类似的比如说则下面的权重比较大的是animal你看animal红色的标志开头部分animal则animal后面跟animal对不对DedentCross the StreamBecouse it was too tight这就是通过注意机制去判断一句话中每一个文本之间的关系和与关系好那么我们再通过一个比较清晰的粒子给大家讲的像比如说我们来看这道内容就是刚刚我们所表示内容DedentCross the StreamBecouse it was too tight在这粒子里面我们的Eat可能则是Animal也可能则是Jid那么Transfer网要知道Eat它指的是哪一个就需要有一个注意力机制那么我们可以通过Transfer网作为在一个眼睛去通过在那个眼睛去看一下在整段内容中上个下文中的每一个文字机的关系和与与关系然后全是包括在一个眼睛持遍到Eat这个代表的是什么是Animal那么它会把Animal在一个隐藏的状态在这个项目中进行一个表示所以GPD3里面的项目是有12288的程度Eat这两个单词用1万多个程度去表示是完全够的而且是远远远超过表示的一个容量所以这么大的一个项量空间可以存储的信息太多了到我们通过Transfer网这个眼睛去滴完以后就发现这个Eat表示Animal那我就可以把Animal这两个隐藏状态写著个项量的空灵之风那么就可以记录在Eat所在的项量空间中这样我们就可以达到所谓的理解的一个目的实际上Transfer网布局一层可能有数十层甚至更多每一层都有它自己的注意力机制每一层所关心的东西可能不一样比如说有的会关动词有的会关动词有的会关动情感等等所以它的层数越多那么纹本的内容理解就会越深刻而且大家注意的是它并不是串形化的一层先去做一个计算而是并且计算的实际上在LLM里面在GPD的3的版本里面它有多少个层呢有96个层我们看一个动画就是GPD3里面有96层层层的当本那么当我们把一个内容丢进去的时候每一层的那个Transfer网的一个注意力机制都可以去盯这个纹本里面不同的信息比如说前几层可以关注理解军事与法和解决企业后续层可以致力于对整个断落进行高层的理解比如说当LLM阅读短片小说的时候它会跟踪性别年龄关系 位置 个性和目标的信息96层的LLM目标是输出包含所有必要信息的最终词的隐藏状态从而去精准的预测下一个词这样模型通过多层确实通过有去实现对句子和断落的更深层次的一个理解并通过大围堵的词下的基督每个单词的双下文信息从而能够达到像今天能够跟人去自由对话的这样一个智能化程度这就是关于整个大模型的工作原理的一个理解
