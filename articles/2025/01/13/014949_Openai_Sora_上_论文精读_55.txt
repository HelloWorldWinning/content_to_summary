Timestamp: 2025-01-13T01:49:49.133099
Title: Openai Sora 上【论文精读·55】
URL: https://youtu.be/5MGq7dSOghY?si=kUomciQFoMkBYgOZ
Status: success
Duration: 1:04:18

Description:
好的，下面是针对您提供的文本内容的总结，我将按照您的要求进行结构化呈现。

**总结:**

1.  **Sora 的发布与更新:**
    *   OpenAI 的 Sora 视频生成模型在 2 月份首次亮相，但仅限于早期测试者。
    *   12 月 9 日，OpenAI 发布 Sora Turbo，速度更快，更适合产品化。
    *   OpenAI 采用连续 12 天的发布方式，吊足了胃口。
    *   Sam Altman 亲自发布 Sora，强调其创造性和 AGI 意义。

2.  **Sora 的核心功能 (Features):**
    *   **Remix:** 多模态输入 (文本、图像、视频) 支持，可替换、移除或添加元素。
    *   **Recut:** 重新生成和编辑视频，基于好的部分扩展。
    *   **Story Board:** 提供视频时间轴编辑界面。
    *   **Loop:** 创建无限循环视频。
    *   **Blend:** 合并两个视频，可实现物体或风格的融合。
    *   **Style Preset:** 定义风格并应用于多个视频。

3.  **Sora 的定价:**
    *   Sora 包含在 ChatGPT Plus 和 ChatGPT Pro 计划中。
    *   ChatGPT Plus 用户每月约可生成 50 个视频 (取决于积分消耗)。
    *   ChatGPT Pro 用户每月约可生成 500 个视频，并提供不限速的 relaxed video 生成。

4.  **Sora 的市场反响与竞争:**
    *   用户对 Sora 的期望较高，但新功能与早期预览相似，引发了心理落差。
    *   快手的可灵和 Google 的 Veo 2 等竞品在效果上不逊色于 Sora。
    *   用户评测显示，Kling 表现突出，Sora Turbo 相对较弱。
    *   Sora 在动作幅度方面表现优秀，更注重为艺术家和电影工作者服务。
    *   视频生成领域竞争激烈，预计 2025 年将有显著进步。

5.  **数据驱动的视频生成:**
    *   Sora 没有发布论文，但其技术报告 (world simulator) 强调了向语言模型学习的思想，使用 visual patches 作为视觉数据的 token。
    *   数据来自公开、合作伙伴和内部资源，存在数据来源的争议。
    *   Meta 的 MovieGen 论文详细阐述了数据处理流程。

6.  **MovieGen 数据处理流程:**
    *   **数据规模：**  视频文本对约 1 亿，图像文本对数十亿。
    *   **数据过滤：** 包括视觉、运动和内容过滤，包括场景边界检测、去重、重采样。
        *    **视觉过滤**: 分辨率, 长宽比, 无文字, 单场景, 美学质量，无边框。
        *   **运动过滤**:  移除无运动, 缓慢运动，抖动和不合理的运动。
        *   **内容过滤**: 去重, 重新采样，内容平衡。
    *   **标注：**  采用 Dalle3 类似的生成方式，为视频提供长文本描述，并添加镜头运动控制信息。
    *   **多阶段清洗：**  通过三个阶段逐步增加过滤严格性，从低分辨率到高分辨率，并添加高清视频数据。
    *   **人工标注数据**: 针对高清视频，人工标注 smooth 运动， 美观自然，覆盖范围广和高质量caption。
    *   **SFT 数据**: 手动挑选，10.6-16秒，长视频。
    *   **Multi-stage training**: text to image pretraining, 低清joint training, 高清joint training。
    *  **Model Averaging**: 模型平均提升性能。
7.  **Hunyuan Video 数据处理流程:**
    *   **多子集训练:** 利用图像和视频联合训练，并分为五个数据子集。
    *   **数据清洗和分类:**  包括场景分割、视频清晰度选择、去重、重采样等，同样使用多层过滤。
    *   **结构化标注:**  使用 JSON 格式标注多维度信息，包括短描述、详细描述、背景、风格、镜头类型、灯光和氛围等。
    *  **Dropout机制:**  增加prompt 的多样性。
    *   **人工标注数据**: 类似 Moviegen, 视频+长文本, metadata
    *   **Camera movement classifier:** 14种运动类型

8. **核心观点:** 视频生成模型的发展突飞猛进，但数据质量和处理流程至关重要，直接影响模型的性能。

9. **根本观点:** 数据清洗和标注对于视频生成模型至关重要，决定了模型的上限，其重要性甚至高于模型架构本身。

10. **Overarching Framework:**  The content describes a progression in the field of AI video generation, starting with the release and features of Sora, moving into its market impact and the data pipelines that support this technology, with a focus on high quality data curation and processing techniques.

<Mermaid_Diagram>
graph LR
    A[Sora 发布与更新] --> B(功能);
    A --> C(定价);
    A --> D(市场反响与竞争);
    B --> E[Remix];
    B --> F[Recut];
    B --> G[Story Board];
    B --> H[Loop];
    B --> I[Blend];
    B --> J[Style Preset];
    D --> K[用户期望落差];
    D --> L[竞品对比];
     D -->M[市场竞争激烈];
    N[数据驱动视频生成] --> O[Sora数据];
    N --> P[MovieGen数据处理流程];
    N --> Q[Hunyuan Video数据处理流程];
    P --> R[数据规模];
    P --> S[数据过滤];
    S --> T[视觉过滤];
    S --> U[运动过滤];
    S --> V[内容过滤];
    P --> W[标注];
     P --> X[多阶段清洗];
     P --> Y[人工标注数据];
     P -->Z[Multi-stage training]
      P --> AA[Model Averaging]
   Q --> AB[多子集训练];
    Q --> AC[数据清洗和分类];
    Q --> AD[结构化标注];
      Q --> AE[Dropout机制];
    Q --> AF[人工标注数据]
     Q --> AG[Camera movement classifier]

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style N fill:#ccf,stroke:#333,stroke-width:2px
</Mermaid_Diagram>


Content:
Hello 大家好 好久不见 今天我们就来聊聊 Sora 聊聊视频生成 很多人呢 去年都说 2023 是 the year of language model 然后预测 2024 呢是 the year of video model 那今年视频生成呢确实不负众望 效果突飞猛进 无疑呢是 2024 年 Machine Learning 这条街里最亮的仔 或者呢至少是最亮的仔之一 毕竟还有 O1 model 在 那 Sora 呢 其实今年 2 月份就已经推出了 不过当时大家呢就只能看看 除了一些早期测试者谁也玩不了 毕竟推理成本那么高 各种优化 安全性和合规性的测试 可能还也还没有大规模的做 所以呢 OpenAI 就是给大家过了个眼瘾 那时隔 10 个月 终于在 12 月 9 号 OpenAI 正式发布了 Sora Turbo 这里他们也说了 这个 Sora Turbo 是比他们 2 月份的这模型 要快得非常多的 这样产品才能落地嘛 所以呢终于可以把 video generation model 从这个 实验室里的 demo 搬出来搞成产品变现了 然后 OpenAI 今年的发布也很有意思 之前大厂的发布会呢 一般就是一天或者几天 非常紧凑 OpenAI 这次呢搞了个连续 12 天的发布 再排除掉周末时间 整个发布长达半个多月 算是吊足了全世界的胃口 Sora 呢这次是在第三天 一个周一 release 的 而且 Sam 亲自坐镇 所以重视程度还是相当可以 Sam 在视频里 还穿着个带 Sora logo 的 hoodie 挺好看的 OpenAI 卖点周边也挺好 那在 Sora 这个发布里呢 Sam 说了一下 就这个 OpenAI 为什么做 Sora 因为除了说这个视觉有吸引力之外呢 第一点就是 OpenAI 喜欢做这种 带有创造力的生产力工具 那确实如此 ChatGPT 也是这样的一个工具 那第二点呢 就是他们希望这个世界不仅仅是有文本 而更应该像我们人一样 用更多的感官去感知这个世界 这样才能改变我们最终 跟电脑或者以后的这个 AI 机器人 交互的方式 那第三点呢 Sam 强调了就是 视频生成是通向 AGI 道路的一个 非常关键的部分 这一点我也是非常赞同的 那我们先回来看看最新的这次 Sora release 有什么 feature 那首先呢 就是像 Sam 说的一样 这个 Sora 的目的呢 就是把你的想象变成现实 做创造的这个生产力工具 这里面像说这个 text image video 说的其实就是用户的输入 就是说不光可以从文本出发 也可以从一张图片出发 或者一个视频出发 然后再生成视频 这个多输入 在实际应用中还是非常关键的 因为咱们大家都知道 很多东西呢是很难表述出来的 如果你只用文字呢 你想描述一个特别的一个什么光影效果呀 或者描述一个你脑海中的一个物体 你可能 不论写多少字都是描述不出来的 那视频模型自然而言也不可能生成的很好 很满足你的心意 那这个时候呢 如果你能给一张参考图片或者视频 那对于创作者来说 那做一个他心目中 想要的这个视频就简单多了 那这次第一个 feature 呢叫 remix 其实是一种 video editing 他支持一些替换移除 或者说直接想象出来的一些元素的 这个操作 那么先来看一下 OpenAI 这里给的他的一个例子 比如他这里先给了一个 prompt 说打开一扇这个通向图书馆的门 那确实呢 右边这个视频 是打开一扇通向图书馆的门 但这个门开的真的是很诡异 正常人他不这么开门 那接下来呢 就像多轮对话一样 把这个 prompt 改成把这个门 换成 french doors 那你可以看到这个 generated 的这个视频里 确实换成 french doors 然后第三个呢 他说把这个图书馆换成 space shape 这回这个门开的是正常的了 然后接下来呢又说把这个 space shape 换掉加上一个丛林效果也不错 那最后呢是说把这个丛林换成这个 从月球上往外看的一个景象 那看到这个地球倒是挺 make sense 的 那第二个 feature 呢叫 recut 就是重新生成重新剪辑 因为我们在日常生活中 不论是真实拍摄还是用 AI 模型 得到的视频呢 这肯定有很惊艳的部分 也有很不行的部分 那这个时候呢 recut 就是说你可以把你想要的东西 部分选出来 然后把你不想要的部分呢删掉 然后根据已有的这些好的部分 sora 再去往前或者往后做扩展 那其实这个 suppose 就是说呢 如果你的这个起点很好 比如说你清晰度也非常高 style 就是你想要的 这个 motion 呢也非常的 smooth 那这个时候 你再往前或者往后延伸个 5 秒 10 秒 应该也不会差到哪里去 你就能得到一个非常好的一段视频了 那第三个 feature 呢其实就更有用 叫 story board 因为 openAI 相当于直接给一个产品 重新做了一个 interface 他直接整个 video editor 就像 final cut pro 或者这个剪映一样 他给你一个视频的 timeline 你可以细化到每一帧 去剪辑和生成新的视频 比如像这里的话 他其实就有三个这种很短的这个视频片段 那在第一个里头呢 其实是对应的第 0 到 114 帧 那如果按 30 帧每秒的话 这可能就是四五秒的一个视频 那接下来呢是第二个视视频片段 从 114 帧到 324 帧 讲的是这个场景 然后再往后推 那第四个 feature 呢是 loop 就是说把一个视频呢 这个头和尾稍微剪一剪 然后再把它拼起来变成一个无限循环 这个呢我个人倒不常用 但是呢 据说是 artist 的最需要的 feature 之一 看这几个例子呢确实效果不错 比如说这个是楼梯的 还有接下来这是 好像冲浪一样的 那第五个 feature 呢叫 Blend 就顾名思义就是把两个视频合一下 合成一个 那有可能是这种像物体的合并 比如说这个雪花变成那个 黄的花朵了 也有可能是这种就是 style 的合并 那这个估计看你怎么写 prompt 的 和抽卡的概率了 估计能搞出一些 非常有意思的视频 那么来看最后一个 feature 叫 style preset 这个呢就是说你可以先定义一个 style 然后接下来他就能把你的这个视频 都变成这个 style 那比如说 openAI 里给的一个例子 这个一只大象和一个犀牛 在雪地上走路 如果我把它变成这种这这这种 paper cut 的形式 就都变成纸板做的这种象 这个效果还是挺有意思的 或者呢 就是可以变成这种黑白的这种特殊的 电影风格 那我觉得 这个 style preset 呢 其实跟 之前 比如说五六年前这种 image 和 video style transfer 是一个意思 只不过呢 之前你是需要给每一个 style 去专门训练一个模型 或者说给几个 style 训练一个模型 那现在呢我们只要动动嘴皮子 写个 prompt 就可以了 大大简化了这个视频制作的难度 那最后呢我们来看看价格 openai 这里说 sora 是包含在这个 chatgpt 的这个 plan 里的 这里面呢有两个 tier 分别是 chatgpt plus 和 chatgpt pro 如果你已经是 chatgpt plus 的用户呢 你每个月大概可以生成 50 个视频 但这里呢其实他都是按这个积分来算 他就给你 1000 个积分 你去花这个积分去生成视频 所以呢这 50 个 其实是用默认参数来算的 如果你把这个视频的清晰度改高 或者把它的长度增加 或者每次多出几个视频你想抽卡 那估计生成 十几个甚至几个视频就用完了 那另外一个 tier 呢 就是这个价格非常感人的 chatgpt pro 了 一个月 200 刀 真是很敢要 每个月呢他给你 1 万个积分 大概就能生成 500 个视频 同样的如果你去改一些参数的话 而且很有可能你是要改那些参数的 最后你生成的视频呢 大概率肯定是比这个要少的 不过 OpenAI 这里比较良心的是呢 对于这个 pro 的用户 他是 unlimited relaxed videos 意思就是说如果你不 care 时间的话 他可以给你排到队列里去 不论你有多少个要生成的视频 他都给你排到队列里去 你可能等个几分钟 十几分钟甚至几个小时 他再返回给你结果 当然呢我只是 plus 用户 并没有升级成 pro 所以说具体的限速是多少呢 我也不知道了 那看完了这些不错的 feature 和感人的价格之后 我其实也就理解了为什么这次 sora 的发布 评论区不像二月份那样 刚见到 sora 的时候一样 一边倒的赞美了 因为对于很多人来说 他们都对 sora 有着非常大的期待 感觉 expectation 呢都还在等着 就当时 promise 的那个真正的 world simulator 结果最后出来的 feature 呢 跟二月份的 preview 也差不多 做的好的呢还是好 该 fail 的 case 呢还是会 fail 那自然就会有一定的心理落差 而且产品效果好不好 有时候也要看同行衬托 比如 23 年年底 或者 24 年年初的时候呢 各大视频生成平台 都还在 3-5 秒之内疯狂 pk 有些认识的研究者 当时能刚做到 10 秒钟左右吧 我觉得其实已经非常厉害了 结果呢大年初五 sora 一来 直接整 了个这个一分钟的视频 一镜到底又高清又 consistent 这个 motion 呢也足够大 确实给人一种甩开一个身位的感觉 而且呢从 research 角度上 openai 又搞了个这个 world simulator 的概念 直指 AGI 所以让人非常激动 甚至当时有消息 导演 Tyler Perry 8 个亿美金的摄影棚扩建项目都停摆了 原因呢是说他之前 也听说过 ai 能做很多事情 但是直到看到了 sora 的发布 才发现是非常 mind-blowing 他觉得呢 电影拍摄里很多工作 可能都可以被简化或者取代了 于是无限期停止他的这个摄影棚扩建 但随着时间推移呢 sora 的产品迟迟没有发布 反而是快手的可灵 在 6 月份先上线 而且大家反响都非常不错 那从 research 角度来说呢 meta 10 月份也放了 Movie Gen 的论文 腾讯 12 月呢出了混元 video 论文细节非常详实 效果也都很不错 我们今天的论文精读呢 其实就是围绕这两篇论文 来展开的 那直到 12 月 9 号呢 sora 才姗姗来迟 但紧接着 12 月 16 号 就一周之后 google 呢就出了自家的视频生成模型 veo 2 用户反响呢也是异常的好 具体的 veo 2 的博文呢 我们就不仔细看了 我们可以来看一下他的这个 benchmark 那 benchmark 里呢这里就有两张图 是来看模型之间的 pk 的 这个 x 轴呢就是这几个这个视频生成模型 有 meta 的 Movie Gen 快手的 Kling 还有 minimax sora turbo 然后呢 y 轴 是这个用户整体的这个喜好度 也就是说 这个用户对不同公司生成视频之间的 这种偏好 那粉红色呢是说 用户喜欢其他家生成的视频 不喜欢 google 的 Veo 2 那绿色呢就是说用户非常喜欢 Veo 2 但是不喜欢其他家的生成结果 白色呢就是平局 用户看不出来 可能很好也可能很烂 反正没有 preference 了 那如果我们觉得 google 的这张图呢 是比较可信的话呢 那我们就会 看到这 Kling 是非常能打的 他的这个 preference 有 32%点多 是这里面最好的一个 就是属于仅次于 Veo 2 然后呢 sora turbo 呢是最差 所以呢真是给人一种 openai 起了个大早 赶了个晚集的感觉 但有一说一 我个人觉得 sora 还是挺不错的 我自己也试了很多例子 从动作的幅度还有高清度 还有这个遵循指令的能力上来看 都是非常不错的 尤其是动作幅度 或者说这个 motion 的这个幅度 是几个竞品里面最大的 那可能 openai 更多的是为艺术家 或者电影工作者 在这个 tune 他的模型吧 而且 openai 这个产品化做的是真可以 从 sora 现到现在的这个 sora turbo 能让全世界这么多人用起来 还做了个视频编辑器 还要考虑安全还要考虑合规 这是挺不容易的 从零到一难 从一到一万也很难 以 openai 的 infra 的能力和经验 sora release 之后竟然服务器能崩 而且五天之内都不让新用户注册 可见流量之大 那接下来呢随着这个用户增多 还有模型迭代 我觉得不论是可灵 sora Pika runway 即梦 minimax 其实只要有用户数据飞轮 应该都能有非常大的提升 而且呢有这么多有实力的玩家 百家争鸣的情况下呢 2025 年 视频生成应该还能看到阶梯式的进步 那 sora 这个产品介绍的差不多 接下来呢就该论文精读了 但可惜 sora 至今没有 paper 虽然他自己 claim 说 有个 technical report 其实就是之前这个 world simulator 的博文 但没什么具体细节 至少是没法精读的 然后最新发布的 sora 呢 有一个 system card 这个现在都是标配 一般模型发布的时候都有 有时候呢数据也会专门有一个 dataset card 不过 sora 这个里面 基本都是讲这个怎么 deployment 和 safety 的 或者就刚开始做了一下这个 sora 的 overview 对于模型 数据还有训练呢基本是只字未提 但好在最近 meta 10 月份的时候呢 放出来了 Movie Gen 这篇论文 整整有 96 页 内容非常详实 而且效果也不错 然后 12 月 腾讯又开放了混元 video 这篇论文 twitter 上呢也是收到一众好评 甚至有人在 twitter 上直接就开始公开测评 说这个 sora out of game on day one 发布及过时 他去跟这个 Hunyuan 做了一下对比 说这 open source 的 model 已经做的很好了 而且说老实话呢 说如果说要跟这种商用的这个闭源呢 Hailuo AI 或者 Keling ai 去比呢 sora 应该看起来更差 这个呢就不好评论了 那来看一看发布的人 也有 2 万个 follower 所以内容应该还是比较可信的 主要是做 film director ai educator 所以应该玩了不少 ai 的工具 那我们也可以看一看他这里比的一些例子 第一个例子就直接看后面 第一个例子是说 一个女的在拥挤的在街道上边走边哭 sora 这个就是 脸有点皱啊 这个不知道是为什么 确实 Hunyuan 生成的会 更自然一些 这个哭的还是比较真实的 第二个是说一个 fighter 在打拳击 然后 sora 直接就拒绝生成了 这个就属于 safety 了 这 这说明混元还没有卡的那么严 而且确实生成的不错 第三个这种静态 Hunyuan 确实这 emotion 也不错 不过这两个都还行 都比较真实 这个是说 baby 在水下游泳 这个 sora 是有点逗 b 动作很神奇 那 Hunyuan 是挺可爱的 然后还有就是 一只狗和猫在水里打架 这就没在一起嘛 这个而且这个也看着也不像猫 那 Hunyuan 呢 哈 这个最后这只猫直接凌空微步了 这个 差不多都一般吧 但确实这个狗 挺可爱的 那接下来是说 两个武士在一个日本传统花园里 这个 fight 这个其实就是著名的这个体操问题嘛 这个 你看这个动作 直接手和剑都不知道飞到哪里去了 是吧手和脚一一顿乱飞 这边 Hunyuan 的话 就还好 但是动作幅度和真实性都相对差一些 还有这个又是拒绝生成的 那总之估计还有很多例子吧 我们就不一一看了 我们今天呢就从技术角度 说说视频生成 先说数据再说模型 最后说训练 主体上呢 呃我会跟着 Movie Gen 的论文走 但也会穿插一些 Hunyuan video 和其他一些论文 或者博文之类的 多铺开一些细节 那我们呢先开始说数据 首先呢就是预训练数据和这个预训练 数据的收集 在 sora 的这个 system card 这博文里呢 有一节非常模糊 说了跟没说一样的 这个 model data 的 section openai 说 在我们的这个 二月份的 technical report 里 也就是那个 world simulator 的博文 sora 是向这个 language model 学习的 主要学的呢 就是说在这个 internet-scale data 上 去做这种 scaling 从而获得了这种这种通用的能力 那这个 language model 的这种成功 其实他说 部分原因是因为用了这个 token 也就是说不论你是这种自然语言 还是 code 还是这种数学 他能把不同的这种 text modality 全都能融合到 token 的表示 那这样呢 modeling 做起来就容易多了 因为你所有的东西都全都先打成 token 然后直接 token 进 transformer 就行了 那 language model 有这个 text token sora 有什么呢 他有这个 visual patches 这个 visual patches 我们之前在 vit 的那个视频里 也讲的很详细了 其实就是把一张图片打成 16x16 或者各种 size 的这种 patch 然后这种 patch 呢在之前的这种实验中 或者其实已经是一种 共识了 就是说对于这个视觉数据 或者视觉模型来说 它是一个非常有效 且能够 scale 的一个表征 那最后呢 openai 说 at a high level 他们是把这个视频 先通过一个 video compression network 压成了这个 low dimension 的 latent space 然后在这个 latent space 里面呢 他们继续 去把这个 representation 打成这种时空的这种 patches 那接下来 openai 说呢 这个 sora 是在 diverse 的这个数据集上训练 他大概罗列了三种 类型的数据 就是说一个呢是这个就是公开的数据 一个呢是通过这种合作伙伴关系 得到的这种私有数据 还有呢就是这个 in-house 内部自己搞的一些数据 那对于公开数据呢 说的很少 他就说大概呢就是这个 标准的一些 ML 的数据集 还有这个网上爬取的数据 那这个网上爬取就很有意思了 那在 reddit 这个 chatgpt 板块呢 10 个月前就有一个网友把一个 45 秒的视频 放上来 是这个 open ai cto Mira 之前 其实现在已经是前 cto 了 跟一个记者的一个采访 然后在这个采访里呢 这个记者还是比较咄咄逼人的 就一个就问这个你们 训 sora 到底用的是什么数据呢 到底是怎么 train 的呢 然后这个记者就问是不是 youtube 那个 Mira 的表情就很僵硬 就说我不知道 然后接下来记者就说 instagram 还是这个 facebook Mira 就来一个 我不知道这个 呃如果是公开的 就是公开了 但最后又来了个说哎我我不清楚 我我一点也不 confident 就作为一个公司的 cto 来说呢 这个答案显然是不能让大家满意的 所以说很快呢 这个 reddit 上又有另外一个 这个好事网友 直接就把这个表情直接截图放在这 说为什么这个 cto 会 make 这个 face 底下这网友呢说的也很直白 这个永远不要问一个女人的年龄 一个男人他的工资 也不要问一个 ai 公司 这个他的训练数据 到底哪来的 那当然了不光是 openai 这个 sora 还是说其他任何的一个这个 视频生成的公司 比如说这个 runway 在今年这个 7 月份的时候 就被爆出说他们这个内部 leak 出来的一个 document 这个被放在这个 404 media 的这个网站上 上面呢就详细罗列了就是他们这个 数据准备过程中 可能用了哪些这个 youtube channel 用具体用了哪些 youtube 视频 还是一个很详细的一个 google sheet 的 当然了这都是没有 confirm 过的信息 也不一定就是真的 那我们刚说完了这个网上爬取的数据 接下来呢 那就是跟这个合作伙伴关系 去拿到的这些私有数据 比如说呢 openai 就说 他们和这个 shutterstock 的这个 合作伙伴关系 那其实这个呢是在 23 年 7 月份的时候就发出了一个消息 就是 shutterstock 呢 会和 openai 去有这么一个 6 年的一个 合作伙伴关系 这 6 年的 agreement 还是挺长的 说明他们双方对这个 未来的这个图像生成 视频生成 整个 generative ai 的这个 roadmap 还是比较 aligned 那 openai 还提到 他们和很多别人 去做这种 commission 去创建一些数据集 那就是说他可能需要哪些具体的 task 他就去造一些数据集 就跟这个 language model 那边 造 sft 数据一样 那最后呢就到了这个 human data 那就是说从他们这个 ai trainer 里 还有这个帮他们测试的 red teamer 里 employee 还有就是 全世界各地的那些艺术家 给他们提供的 feedback 那在 sora release 的这个网页呢 最下面呢 也提到了就是说一些 这个艺术家 film maker 怎么去用 sora 做他想做的事情 这几个视频挺短的 可能都两分钟吧 也挺有意思 大家如果有兴趣也可以看一看 那这个 system card 呢 接下来还说到这个 pre-training filtering 和这个 data preprocessing 但是就短短的六行 其实没有任何信息量了 那整个 report 后面还有什么 risk identification mitigation stack 然后 risk area 但是基本都是 怎么 deployment 相关的东西 那接下来呢 我们就去看一下这个 movie gen 的论文 那 movie gen 呢 是 meta 在今年 10 月份 放出来的一篇论文 当然又配套了这个项目主页和这个博文了 那论文的题目呢是 moviegen A cast of media foundation models 之所以是 cast 呢 是因为他是很多个模型 他不是光一个 然后他不说 video 说的是 media 是因为他这里除了这个视频之外 他也有这个图像 音频 然后视频 所以呢是 a cast of media foundation models 那摘要里说呢 moviegen 是一系列这个底座模型 他可以做什么呢 他可以生成这种高质量的 1080p 的这种高清视频 同时呢这个视频 他是可以有各种各样的这个 aspect ratio 同时还能和这个音频呢去同步的 然后在这个底座模型基础之上呢 论文里还 show 了几个 use case 那其中呢就有这种 instruction based 的这种 video editing 还有呢就是基于一个用户上传的这个图片 他可以做这种 personalized video 然后论文说这个他们的模型 在多个任务上取得了这种 sota 的结果 那其实是一系列模型不是一个模型 那他们最大的这个视频生成模型呢 是一个 30 billion parameter 的 transformer 然后这个最大的上下文长度呢 是这个 73,000 个 video tokens 大概换算过来呢 就是一段 16 秒的这个视频 然后每一秒呢是这个 16 帧 一会在论文里我们也会仔细讲 这个 73,000 到底是怎么来的 然后他说 在论文里 他们展示了多个这种就是技术上的创新 然后还有这种简化 包括呢在这个模型结构 latent space 呀 还有这个训练 objective 上 就是各个方面上的这些优化 从而让他们能够享受到这种 不光是数据 还有这个模型的大小 以及这个 training compute 的这个 scaling 所带来的效果 其实对应的就是 language model 的这个 scaling law 那作者最后说呢 希望这篇这么详尽的论文 能够对整个 research community 有所帮助 能够加速 这个整个 community 在这个整个 media generation models 领域的这个进展 那左边呢就是这个论文大概的骨架了 上来就是先是这个简介 然后呢是一个 overview 主要就是讲这个 moviegen 大概做了一些什么事 是很简单的一个 overview 然后接下来这个第三章是重中之重 基本上就把所有我们关心的东西呢 就全讲了 包括这个模型长什么样 然后 pre training 怎么做的 里面其实就包括有这个数据和训练 然后接下来呢就是 fun-tuning 是怎么做的 然后这个模型训好之后呢 这个推理该怎么做 然后 evaluation section 呢 就讲的是说 大概他用了哪些数据集 然后用的是什么 metric 然后最后呢到 3.6 节吧 然后就讲的是 跟别的各种模型之间的对比的这种结果 然后最后 3.7 大概讲一下这个 text to image generation 前面呢都是这个 text to video 然后讲完第三章之后呢 接下来几个就是一些 use case 和一些其他的这个模型了 包括就是把这个底座模型 变成这个 video personalization 的 这个任务上该怎么做 然后后面呢这个是 video editing 该怎么做 然后最后这个是 这个 sound effect 和这个音乐生成怎么做 那这些所有的都讲完之后呢 最后有一个 related work 然后就是 conclusion reference appendix 那按照我们之前说的 我们上来先讲数据 所以我们先去一下这个 3.2 节 这个 pre training 里 看一下这个预训练数据 是怎么收集怎么清理的 那 3.2 节呢 论文上来先说 他们的这个预训练数据集的大小 就是这个量级 对于 video text pair 来说呢 大概是 big o 一个亿左右的这个量级 那对于 image text pair 来说呢 肯定是更多的 所以是这个 big o billion 级别的 可能有好几个 billion 那对于 image text pair 的这个 data 数据的处理呢 他们是 follow 之前呢 这篇 Emu 的论文 这也是他们 meta 自己的 那个 text image generation 的论文 所以这篇论文里呢 他们说他们主要是 focus on 这个 视频数据到底该怎么处理 那接下来作者说呢 就假设这个数据已经 都爬好了 全都已经准备好了 那他们原始的这个数据池子里呢 呃所有的视频大概呢 都是从这个 4 秒 到两分钟这是长度不等 视频中涉及的这个概念呢 也是非常多么多样的 就是有这种人 然后大自然 动物和各种各样的这个物体 而数据肯定是越 diverse 越好嘛 但经历过他们的这个 data curation pipeline 之后 就是模型最后见到的这个 clip prompt pairs 呢 其实长度都已经是从 4 秒 到 16 秒这么长了 就是 基本不会有比 16 秒 更长的这种训练数据 因为他们想确保呢 就是每一个这个视频 就是个 clip prompt pair 里的这个 clip 呢 尽可能都是描述的一个概念 所以是 single shot camera and non-trivial motion 这样呢主要就是对这个模型训练 比较友好 因为如果你上来就整那种特别复杂的 那种视频 10 秒钟就换了无数个场景 然后无数个人物 那这个对于模型的预训练来说呢 可能就难度比较大 这种呢可能就属于 hard example 就算要做 也是放到后面的 一些训练的 stage 里去做 一般不会放到预训练里来做 那其实我也看了很多别的这个 视频生成的论文 大概呢他们都是类似这么处理的 视频长度呢都是十几秒 或或者 30 秒之内吧 所以这大概也就说明了为什么 就是包括 sora moviegen 然后就是大部分的这个视频生成 如果一把出的话呢 其实视频还是在这个 10 秒左右的长度的 部分原因呢也就在这里 因为你训练数据就是这个 distribution 嘛 那你这个输出的 最好也是在这个 distribution 之内 效果就会很好 如果你训练都是十几秒 结果你硬要让他生成几分钟的视频 那后面的视频呢 大概率就质量就不能保证了 很有可能就糊了 或者这个概念就飞了 那最后呢作者总结了一下 他们的这个 data curation pipeline 画到了图 9 里 大概呢是包括了三个 filtering stage 和一个这个 captioning stage 那我们先来大家看看这个图 9 那这个数据清洗的 pipeline 还是做的相当好的 而且呢也是相当复杂的 从最开始的就是所有的这个 原始 视频开始 第一步呢是先做这种视觉上的这个清洗 包括比如说 resolution aspect ratio 这些我们一会会去正文里细讲 然后这个 visual filtering 做完以后呢 就去卡 motion 这个比如说不要那种完全没有 motion 的 或者太 slow 的 motion 的 不要那种断断续续跳的 或者那种特殊的 motion motion filtering 完了以后呢 就到了具体的内容的 filtering 这里面有去重 跟 language model 一样肯定得去重 然后 resampling 确保这个概念的这个 balance 那这三步呢全都是清洗 那最后一步呢是 captioning 这个 captioning 其实就是这个 Dalle3 论文里主要讲的东西 他的目的呢就是说原来你这个 image 或者 video text pair 里的那个 text 是直接从网上爬下来的 那个呢是非常短 而且经常出错或者说 根本就文题不符 那现在呢如果我能借用一些模型 然后重新把这些图像和视频呢 去进行一遍 annotation 然后把他们的这个 caption 变成这种 generated 就生成的 新的 caption 那每一个 caption 我尽量让他保持非常长 就把整个图像或者视频都描述清楚 包括这个人怎么动的背景是什么呀 可能甚至把故事情节都描述在里面 把各种这个天上地上的这种细节 也都全都描述清楚 那像这篇论文里说 基本上他们的数据集里 平均每一个这个图像的 caption 呢 都有至少 100 个字 所以还是很长的 那我们先来看这个第一阶段 这个 visual filtering 那在第一阶段里呢 作者就说 他们用了 6 个 filter 这 6 个 filter 其实很多都已经是 model based 了 所以不是那种 简单的调调包 或者就是那种 cpu based heuristic based 的那种 filtering 很多都已经是要用这个 gpu based 就 model based 的方式去 把这些 low visual quality 的这个视频移掉 或者去进行一些处理 所以现在 gen ai 的这个数据处理流程 还是相当复杂的 很多时候呢 是需要一个非常有经验的这个 machine learning engineer 去做 那比如说一开始呢 作者这里 写的这个 resolution 和这个 aspect ratio 具体指的就是说 他们把这个视频的 size 不论是这个宽还是高 只要比这个 720 个 pixel 小 那他就把这个视频移掉了 那对于这个 aspect ratio 来说呢 他们是保持了一个比例 就是 60%的是这种横屏的视频 然后 40%的是这种竖屏视频 虽然现在这种短视频很火 竖屏视频也很火 但是对于视频生成来说呢 作者说他们还是 prefer 这种 横屏的这种视频 那因为呢横屏视频更多的 是比如说 youtube 或者这种长视频 有的这个更好的这个美学 而且呢这个 motion 也更 stable 不像你手持设备拍出来的 肯定很抖 那接下来的 filtering 呢就是 不要文字 其实意思就是说 不要文字特别多的那种视频 哎因为有的时候 那个视频可能是别人在给你讲一本书 或者说在念一段什么东西 那这个时候视频生成就没什么意思嘛 视频生成或者至少在现阶段来说呢 他们是想生成这种 比较大自然的这种 scene 或者人脸 或者这种数字人之类的 就是还不太关注这个文字 所以说呢就会先把这些东西都删掉 那第三个呢就像我们刚才说的那样 他希望这个训练数据都是 single shot 都是一个场景 最好不要有多个场景之间的切换 否则会对模型训练造成难度 那这个时候呢就需要做一个操作 就是 scene boundary detection 就是一段长视频里 怎么把它切成不同的场景 然后每个场景呢是讲的一个故事 最后呢从而达到就是说 从一个 长视频里 能抽出来多个 4-16 个秒的这种 小的短的这种视频片段 那接下来呢就是从美观上来说 他们也训练了一些简单的 visual models 去做这种美观上的这种 filtering 那这个 ocr 模型 然后还有这个 scene boundary detection 和这个 美观的模型 其实在这个 appendix 里都有提到 他们具体用的是哪个模型 感兴趣的同学可以去这个 附录里看一下 都是调用的一些其他性能比较好 而且比较成熟的这个模型 那最后呢就是这个 no border 就是没有这个边界 因为很多时候呢 为了避免什么版权 或者说一些竖屏的视频之类的 这个视频就会有很大的这个边框 它要么是黑边框 要么是别的那种模糊的边框 这种就很影响这个视频生成的效果 所以说最好是训练一个这个 classifier 或者说用一些方式 去把这些视频找出来删掉 那这几步呢就是这个图 9 里 visual filtering 大概包含的一些步骤了 那 说完了 visual filtering 那接下来看这个第二个 stage 叫 motion filtering 作者说呢他们是 follow 之前的这个工作 其实也是卖他自己的工作 就是 Emu video 里面呢 其实已经详细的讲过这个 motion filtering 怎么做了 第一步呢 就是说他们内部有一个这个 static video detection model 专门去检测那些哪些视频 根本是没有 motion 的 那这些肯定就删掉了吧 因为没有 motion 的 video 呢 生成起来其实就跟 copy paste 的一样 不是一个好的训练数据 那接下来呢 作者说他们用这个 motion score 和这个 motion vectors 也就是这个 ffmpeg 抽出来的 motion vector 呢 去 identify 这个有 比较合理的 motion 的视频 那这里合理呢他打了这个引号 因为其实你也没法很具体的定义 什么叫合理的 motion 什么叫不合理的 motion 什么叫大 motion 什么叫小 motion 其实都是很难去描述的 然后接下来呢作者又发现 这个 reasonable motion 里面呢 还有一种 motion 是这个 jittery camera movements 就是跳来跳去的这种 camera movement 这个情况呢 就没法用这个 motion vector 去 detect 到了 然后呢他们发现之前 做这个 short boundary detection 的模型呢 反而可以拿过来复用 因为当你这个视频跳来跳去 有这种 jittery effect 的时候呢 这个 short boundary detection 就会把一个视频 切成无数个小的片段 然后每个片段呢都特别特别短 所以你就很容易设置一个 threshold 那就比如说当一个一秒钟的视频 还被切成了多个片段的时候 你大概率就知道 这个不是说真的 一秒钟里有多个场景 而是说因为他有这种 jittery 的 camera movements 从而导致呢 他被切成了多个这种特别碎的片段 那我们就会把这些视频呢都移掉 那最后呢 就是说把一些有这种特殊的这种 motion effect 呢都移掉 比如说呢就那种幻灯片的视频 那种呢就是每隔多长时间 可能翻个页 或者说即使在同一页里有一些 animation 但那个呢很多时候这个 action 是人为的 它不是一个就是物理空间上的这种 因果关系吧 所以你如果硬要让一个 video generation model 去学它 其实也不是很合理 而且一般也没有这个 use case 的需要 那说完了这个 motion filtering 那最后一步呢就是这个内容 filtering 内容 filtering 呢首先就是说要去重 那去重这里呢 肯定首先上来第一步都是抽 embedding 那抽完 embedding 之后呢 一般因为这个数据量太大 所以说你肯定还得先做一次这个 clustering 然后把这种常见的这种 concept 呢 全都抽出来 当抽出来之后呢 那根据每个 cluster 这个大小和这个相似度呢 我们也可以把这个 cluster 给 merge 起来 然后从每个这个 cluster 呢 再去比较均匀的抽一些这个视频数据 那这个比较均匀是怎么抽呢 一般是根据这个 inverse square root of the cluster size 去抽的 那就大概保证每个 concept 都有一定的数据量 确保这个训练呢不会 bias 那这个 content filter 呢 别看只有短短的五行 但其实这个 llama 3 里面这个 vision model 的训练数据的准备 那也是用的这一套流程 他们也是用的这个 copy detection embedding 当然不光是因为这个 embedding 效果好 因为他也是 meta 自己的工作 而且这个 content filter 非常关键 尤其对面试来说 这个呢 其实就涉及到之前这种 machine learning system design 比如我们来看一个叫 bytebytego 的网站 就是那个写 system design 书的那 Alex Xu 搞的一个公司 他之前写的这个 system design interview an insider guide 算是上一波 SDE 面试的这个必备宝典 然后 他们这里呢 也有一门叫这个 machine learning system design interview 的课 我们可以看到一共呢 其实就是从 02 到 11 一共就十个 use case 然后里面呢 大概至少五个 都多多少少跟刚才说的这个 pipeline 是有关系的 比如说这个什么 visual search youtube video search 什么 video recommendation system similar listing 之类的 因为这里都会涉及到抽 embedding 这样才能做 similarity search 嘛 然后呢 clustering 或者 ranking 还有一些 feature engineering 然后最后拼出来一个 solution 那如果我们来看另外一个这个 machine learning 面试准备的网站 就是什么 www.aiofferly.com 看起来有点像 machine learning 的 leetcode 就会发现除了比较经典的这种什么 self attention transformer 之外的 还有一些很火的题目 比如说什么 beam search 这都考的很多了 还有就是我们刚才说的这种 比如说这种 language model 里的这数据去重 这还有两道 呃然后还有就是比如说 这种图像的去重 还有就这种 visual based product search 所以这一整套 pipeline 考的 真的是非常高频的 那对于正在找工作的同学来说呢 这个还是比较关键的 建议大家去把 llama 3 的 tech report 也好好看一下 然后梳理一下 可以说能得到一套答题的模板吧 那所有的清理阶段完成之后呢 最后一步就是这个 captioning 就是把这个 prompt 变得更长 那这里面呢 他们就用 llama3-video 就再次用自己内部的模型 去做这个任务 就是说给定一个 video clip 重新这个给这个 video 做 annotation 提供这种 accurate 和 detailed 的 text prompt 他们呢是在这个底座模型的基础之上 重新又 真专门针对这个 video captioning 的 task 去做了一定的这个 funtune 他们 tune 了这个 8b 的模型 也 tune 了 70b 的模型 完事了之后呢 用这两个模型 对所有的这个数据呢重新做了一遍 annotation 那他们的数据集里呢 就包含了百分之七十 用这个 8b 的小模型做的这个 caption 和百分之三十用这个 70b 做的模型 那为什么要用两个模型 为什么不用这个 70b 全做呢 那其实还是太贵了 那对于现在的大模型来说 大家都觉得这个训练很贵 其实这个 inference 呢也不便宜 如果你整个数据集合大小有几亿个 这个视频片段的话呢 如果你用这个 70b 的模型 这个 llama 3 video 去做这种 inference 的话 可能也是需要很多卡去跑好几周的 所以这里呢 即使 豪 如 meta 可能也不得不 根据这个时间上的这个 constraint 去做一些取舍 百分之七十用了 8b 百分之三十用了 70b 当然这里呢可能还有一个考量 就是说需要有一些这个 diversity 就是说即使 70b 的模型很好 但如果你全用 70b 的话呢 可能会有一些这种内在的 bias 那这个时候呢如果你换一个模型 这个数据的多样性呢就会稍微好一些 所以可能也不光是 cost 的问题 然后对于这个 captioning 来说呢 除了提供这个 caption 就文本的描述 那 movie gen 这里还做了一个事 就是他想给这个 caption 里 提供一下这个 camera motion 是怎么 control 的 比如说一些常见的运镜 这个 zoom in zoom out 向左移向右移 那他们这里呢就是首先先搞 搞个小数据集 然后训练一个 camera motion classifier 能够判断 16 种 这种常见的 camera motion 那具体的 16 种呢 大家回头可以去这个附录里去看 然后呢他们把这个就是说 prediction 里比较 high confident 可能大于 0.9 添加到这个生成的这个 caption 里去 那这样呢 当这个 video generation model 经过很多这种训练 sample 之后 那在 inference 的时候呢 如果用户提供了一个这个 运镜的 instruction 就说我现在要让你向左平移 那生成的视频呢 大概率就会 follow 这个用户的这种 instruction 从而达到很好的这种这个 camera control 用户呢可能就会比较满意 那之前呢也看过网上很多 post 和这种 youtube 视频 里面会介绍一些什么 magic prompt 能够让这个生成的视频效果 特别特别好 但其实这里呢 往往是有两个东西比较重要 一个呢就是这个 camera control 怎么运镜 那另外一个呢就是这个 artistic style 那说完了一整个 pipeline 接下来呢 Movie Gen 又说了一个 multi-stage data curation 意思呢就是说 刚才那个 pipeline 呢其实只是一个 stage 他们呢其实还有三个 stage 每个 stage 里呢都会有越来越严格的 这个 visual motion and content 的这个 filtering 的 threshold 从而可以进行多个阶段的这个预训练 那对于这个 multi-stage 来说呢 其实对应的呢就是这个 curriculum learning 从简单开始然后再到难的数据 所以一开始呢 我们就是先从这种 low resolution 的这个 视频开始 就是有样学样 rgb 和 motion 这些都先学点意思意思就行了 先给模型训练来个 warm up 这里面 720 pixel 可能是个 typo 因为下一页里说的第一个 stage 呢 是 256 pixel 接下来呢 这个 filtering 就更严格了 从低清转高清 意味着呢就是说你这个最短的边 不论是高还是宽 至少你得有 768 pixel 然后最后一个 stage 呢 就是加一些更多的这个视频 而且都是高清的这种视频 那 Movie Gen 的论文里说呢 他们的这个高清的这个数据集里 80%都是这个 横屏视频 20%呢是竖屏的 而且其中呢把至少 60%的数据 里面都是有人的 可能之前他们也统计过一些 statistic 这个 user 的 query 一般都是说根据人来展开的 然后具体他们是怎么去把这些 有人的视频选出来的呢 然后他们说他们先是建立了一个 这个分词系统 里面有 600 个常用的这个 动词和这种 expression 然后呢他们做了一下这种 就是 zero shot text to video retrieval 可能也用的是他们内部一些 video model 就能把对应的视频 也就是说视频里有人的这些呢 retrieve 回来了 然后针对这些分词系统呢 他们也做了这个 resampling 就跟刚才做去重的那个 clustering 一样 通过做 resampling 来保证 每个 concept 也就是这里的这 600 个 human words and expressions 能够比较 balance 的进入这个训练数据 然后这里面比较有意思呢 就是 附录 b.1 里其实是有一个表的 我们现在来看一下 这个表 39 就是说 那这个 moviegen 在创建他们这个 高清的数据集的时候 会有多步的这个清洗过程 那每一步这个清洗完之后呢 这个数据量 肯定都会减少吧 从刚开始的可能 百分之百然后降到百分之八十 百分之七十 一直到最后 如果你的这个越严格 那洗掉的数据就越多 那这里面我们来看一下 从最开始的这种 visual 上 这个 duration resolution no text no border aesthetics 这是刚才的 visual filtering 然后到这个 slow motion jittery motion 还有到这个 content 的这个 filtering content duplicate content resampling 我们会发现呢这个留下来的数据呢 是越来越少 而且是非常非常 aggressive 比如说这个 duration 一下就从这个百分之百掉到 25% 那就 75%的数据在第一步就已经没有了 然后接下来呢 这 aspect ratio 也拿掉了很多 然后再往后走呢 其实每一个都在拿掉拿掉 一直到最后这个 concept resampling 拿完以后 相对于最原始的那个数据池子来说呢 最后剩的数据已经不足 1%了 所以洗数据不光是一个苦力活 而且也是一门艺术 那说完了 3.2.1 节的这个预训练数据 接下来就看一下 3.2.2 节的这个预训练 作者这里说呢 接下来的这些训练细节都是针对这个 300 亿参数的模型的 然后为了让这个训练更快更稳定 他采取了这个多阶段训练的策略 其实就跟之前他们自己的这个 Emu video 一样 那现在呢其实对于大模型 或者 gen AI 来说 一般都是这种多阶段的训练 procedure 那包括 language model 也是如此 那这里面呢作者说 这个过程就包括三个主要的步骤 第一个步骤呢 就是说他先用一个 text to image 的 task 去预训练这个模型 去做一些 warm up 这样呢他下面也说 会让整个这个 converge 的速度变得更快 更有效 然后接下来呢再是这个 joint training 就是说 text to image 也做 然后 text to video 也做 然后第二个阶段呢就是做这个 resolution scaling 从这个低清的 256 pixel 然后到这种高清的 768 然后最后呢 就是不停的去洗数据 得到更好的这个数据集 或者加更多的 task 然后还有呢就是这个把炼丹的丹方 这个越来越优化 然后作者呢基本把训练的这个 recipe 全都 summarize 在 table 3 里了 我们马上上去看一下 但这句话其实很关键 作者说 他们呢会保持一个 validation set 这里面的这些视频呢就是 从来模型没有见过的 所以可以拿来的测试模型的性能 然后他们发现呢 就是只用这个 validation loss 呢 是跟这个 human evaluation 呢是 偶合的非常好的 那虽然现在对于 language model 来说 很多时候大家做这种 automatic evaluation 或者拿另外一个 language model 过来当这种 judger 去裁判你这个到底做的好不好 但是呢对于这种 enterprise 或者对于一些非常 critical 的 use case 很多时候还是去靠这个 human evaluation 那对于视频来说呢 或者图像视频来说的话 现在也没有什么特别好的 metric 去衡量 很多时候也还是用 human evaluation 包括我们刚才看 google deepmind 放的那个 veo 2 那他们那个 benchmark 其实也是用人去 evaluate 但这里我想强调呢 其实对于 language model 来说 还有就是说这种模型来说 大家现在普遍的这个 practice 呢 都是去看这个 validation loss 还是挺管用的 那我们往上来看一下这个 table 3 这里面就主要讲一下这个 multi stage 训练这个 strategy 跟刚才说的一样 这里面分了这个三个 stage 第一个 stage 呢 就是这个 text to image 的 warm up 这里面呢 数据集就是刚才提到的 这个量级在这个 10 亿个图片 text image pair 的上面去训练的 下面第二个阶段呢 虽然还是低清 256pixel 但是呢已经是这个 joint training 有 image 也有 video 所以用的呢 就是那个一个亿左右的这个 video clip text pair 然后再往后呢就是高清的这个就是 joint training 大概也是这么多量级的这个数据 那作者这里有两个 note 第一个 note 就是说在这个 joint training 的时候呢 不光是有这个 video 数据 这个之前的这个 t2i 训练时候用的这个 image 数据呢 其实也是会混进来用的 而且呢是一个 1 比 10 的比例 那第二点呢作者就是说 通过不同的数据来源来的这个视频数据 最后采样的时候呢 也是根据他原有这个量级的大小来采样 那比如说 youtube 可能就是最大的量 那其实采样过后呢 他肯定也还是占最大的这个 percentage 那说完了 3.2 节的这个 pre-training 接下来我们快速说一下这个 3.3 节的 fine-tuning 其实这一节本身也比较短 之所以现在说 主要就是先想说一下这个 fine-tuning 的 video data 一口气先把这个数据说完 我们再回去说模型 作者这里说呢 为了提升最终模型的这个效果 所以我们要在接上做一下这个 fine-tuning 然后 fine-tuning 的数据的量级呢可以小 但是呢它的质量肯定得非常高 所以说 这个 fine-tuning 的这个数据 集 包括这个视频和这个他的 caption 全都是 manually curated 全是靠人的 所以这里呢作者叫这个 stage 呢 叫 supervised fine-tuning 就是 language model 里的 sft stage 因为最开始呢在 RLHF 的论文里 沐神之前也讲过 这个 sft 的 stage 呢就是靠人去写的 当然现在呢很多这个 sft 的数据 都是合成的 那我们接下来看一下这个 fine-tuning 的 video data 到底 movie gen 是怎么做的 能够得到这么非常 high quality 的视频 论文里说呢 就是这个 high quality 该如何定义 那就是说需要有这个 good motion 就是动作也很 smooth 或者看起来很美观很自然 很真实 而且呢这个概念 cover 的这个要广 这样用户在用的时候 不论用户写什么 prompt 模型都能够生成 还有呢就是 high quality caption 其实对应的就是 language model 那边的 instruction 需要更详尽更准确 那为了找到这些视频呢 作者这里说他们有四个步骤 而且呢是 run in sequence 就是一步接一步的 就是上一步的这个输出呢 拿来当下一步的这个输入 所以又是洗掉了一大批数据 那首先第一步呢 就是我得有一个视频的池子嘛 这个呢一般肯定是自动化来做 因为这个池子本身也比较大 可能也是大几百万个视频 所以靠人来说还是太慢了 作者这里说呢 他们就是设置了更严格的这个阈值 在这个美观 motion scene change 上去卡视频 然后额外的呢 他们还用一个目标检测的模型 把凡是这个视频中有特别小的这个 物体的视频也都干掉了 然后作者说呢这一步干完之后 就留了这个小几百万个视频 但是呢这小几百万个视频 虽然说整体来说 quality 还行 但他们的这个概念的这个分布呢 是非常不均匀的 可能就像 imagenet 那样 一千类 但是里面有好多好多类全都是狗 各种各样的狗 这分布是很不均匀的 所以第二步呢 就跟之前这个 预训练阶段做那个 content filtering 一样 他们需要把这个 concept distribution 弄的这个更 balance 一点 那具体的做法呢 就是沿用之前的这个 human verbs expression 的这个分词表 再做一次这个 video retrieval 从而去把相关概念的那些视频呢 这个抽出来 那这里面呢还提到了 knn 而且呢是既有 text 的 knn 也有 video 的 knn 那 video knn 这里还比较有意思的 他们呢是每个概念里呢 去人工的选了几个 就他们认为非常好看的视频 然后把这个呢当成这种种子视频 然后根据这个种子视频呢 去做这个 video 上的 knn 从而去拿到这个 concept 的 balance 的一个子集 那说完了这个前两步 前两步还都是 automatic 去做的 接下来第三步呢就是人工去做了 他们是人工的去找出这些 cinematic videos 总是就是好看的 适合这种 video generation 的这个训练样本 虽然到这一步呢 视频确实也已经被删掉很多了 可能就剩一两百万了 但还是很多 之所以呢要去 manually label 主要还是因为这个 high quality 的 funtune data 是无法被这种 automatic filter 可靠的抓出来的 这里面呢因为牵扯到很多细节 而且是人的这种主观感受 比如说这里的 angle 还有这种打光 还有就是色彩艳丽度 但是呢又不能过饱和 还有 no clutter 还有刚才说的就这种 non-trivial motion 到底啥叫 trivial 啥叫不 trivial 这个模型呢暂时还是难以拿捏的 而且除了上述说的这些之外 在这个 stage 标注人员 还要把一个视频里头 最好看的那一个片段挑出来 那这个任务呢 对于模型来说就更不可能完成了 所以说这个整个第三步呢 就是 manually 去做 那第三步呢其实说的是视频 第四步呢就到了这个文本 所以呢就是 manually captioning 这些视频 具体来说呢 这些 caption 其实都已经生成好了 因为如果你让这个 annotator 从头去写一个这个 caption 的话呢 不仅不够详细会出错 而且这个耗时实在是太长 所以肯定是先用模型给他生成一个 底稿 然后他去改是比较快速的 而且 在这个改的过程中呢 他们还要再加一些这种 metadata 这种 metadata 呢肯定是越多越好 因为你加的越多 到时候用户的这个 prompt 里头呢 就越有可能跟这些训练数据呢有偶合 那一旦偶合上了 这个 instruction following 肯定就会更好 出的图或者视频呢 肯定也就会相对更好 那这些 metadata 呢就包括 camera control human expression subject background information 和这个 detailed motion information 和 lighting 然后这里面的这个 camera control 呢 已经不是刚才呢说的 16 种了 那在 fine tuning stage 里呢 这个 human annotator 还要标注额外的 6 种 camera motion 在 appendix 里有写 那 captioning 这里呢 hunyuan video 其实说的比较详细 我们一会可以去看看那个 tech report 最后呢作者强调了一下这个视频长度 那这个 fine tuning 的数据集呢 所有的视频 在这个长度都是在 10.6 秒到 16 秒之间的 实际上呢 大概 50%的视频呢就是都是 16 秒 然后剩下的 50%呢 是在 10.6 秒到 16 秒之间 所以都是 长视频 因为这种三五秒的视频是 相对比较好生成的 所以意思就是说 pretraining 的 data 就已经足够能 cover 这个 fine tuning 呢 主要就是针对这种高清动作大 时间长 这种语意信息比较丰富的 这种视频生成而设计的 那说完了 fine tuning video data 作者这里也简单说一下 sft 的这个 训练的 recipe 我这里就不照着读了 最后呢作者简单提了一下 model averaging 就是说在 fine tuning 这里呢 他训练了多个 checkpoint 然后把这些 checkpoint 呢 直接通过 model averaging 的方法 得到最终的一个模型 那 model averaging 呢 其实已经是个 standard practice 了 这 llama3 里也是这么做的 那为啥要做 model averaging 呢 因为这个模型的性能 除了很大程度上 被这个算法或者模型决定之外呢 还有相当大一部分是由数据决定 毕竟你所有的这个 knowledge 你的这个语意信息都是 直接从数据里来的嘛 那这个时候呢你的数据的这个分布 这个配比就很关键 所以 像作者这里说的一样 这个 fine tuning data 的这个 不同的这个数据集的配比 还有这个各种超参数的设计 就会导致呢最后训出来的这个模型 它们之间相差还是非常远的 有的模型呢可能运镜做的比较好 有的模型呢可能动画做的比较好 有的模型可能人做的比较好 有的可能自然做的比较好 那这个时候呢 是很难寻找一套统一的超参数 去把所有的部分都做很好的 那 model averaging 呢 就是一个不错的替代方式 他把这些 各有所长的这个模型呢 直接 model averaging 拼到一起 相当于 10 年前的 machine learning 时代 我们做了一个 model ensemble 那一般 model ensemble 呢 总是会带来性能的提升的 无非是这个性能提升大小的问题 那讲到这里呢 movie gen 的数据部分就说的差不多了 从 pre-training 到 fine tuning 这个数据哪来的 怎么准备一步一步的 filtering 我们对这个流程应该是比较清晰了 接下来呢 我们快速去看一下 Hunyuan video 的这个数据部分 就两页 看看有哪些相同的 哪些不同的 查漏补缺再巩固一下 那 Hunyuan video 这篇论文呢 主要是在这个第三章 讲了这个数据的处理 这里面他分了两节 第一节这个 3.1 讲的是这个 data filtering 然后第二节 3.2 讲的是这个 data annotation 每一步骤里 他做的事情是相当之多 一会我们看这个图 还有这个文字就能看出来 但是具体的细节呢介绍的不多 所以这也是为什么选择 moviegen 这篇论文来精读的原因 那开始呢作者先介绍了一下 说他们 也是用这个图像和视频 一起 joint training 的这个 strategy 然后视频呢分了这个五个 subset 然后模型的训练呢 也是针对这五个 subset 去进行的 那跟刚才 moviegen 那边也差不多 moviegen 那边呢也是 multistage multisubset 的 然后作者这里还提了提 GDPR 什么 privacy computing data synthesis 但是也就是提了一下 具体怎么做的也并没有说 接下来说呢 就是他们的这个最原始的这个数据 池子里的 diversity 也非常高 然后视频选择的 这个条件呢也是非常苛刻 他跟 moviegen 也一样 其实都是比如说 size 呀 然后 duration 呀 然后 special quality 呀 就是从各个方面来筛选这个视频 最后呢说一句大白话就是 通过实验发现这个数据质量越高 最后的这个模型的性能越好 这个应该是所有做 ML 人的一个共识了 那接下来呢我们看 3.1 data filtering 他具体做了什么 如果我们看接下来这几段文字的话呢 会发现跟 moviegen 的这个整个数据处理流程 是非常非常类似的 我们甚至能看到非常非常多 一样的这个关键词 那作者上来说呢 因为这个最原始的这个数据呢 来自于不同的这个来源 而且呢有各自有各自的一些特性 对数据肯定是要清洗和分类的 基于此 所以他们用了一系列的方法 去预处理这个数据 那首先呢他们就用这个 PySceneDetect 去做这个 short boundary detection 从而把这个长视频 切成一个一个的这个短视频 那其实 moviegen 那边也有这一步 我们也说过 就是训练数据呢 最好是一个 single shot 这样方便模型训练 而且呢他也不会太长 不会给模型输入 造成太大的这个 token 的压力 那接下来呢 作者说用这个 opencv 里的这个 拉普拉斯算子 去找一下这个视频里哪一帧是最 clear 的 然后就把这一帧呢 当成这个 starting frame 然后去训练模型 这个呢倒是个新东西 至少 moviegen 的论文里是没有说这步操作的 然后 Hunyuan 这边说呢 接下来呢是用一个内部的这个 video clip 的模型 其实就是一个视频理解的模型 然后把每个 video 的这个 embedding 就抽了一下 那这些 embedding 主要是干两件事 第一件事就是去重 第二个这呢就做 resumpling 和 balancing 那其实这两步就对应的是 moviegen 的那边的 content filtering 然后作者接下来说呢 这个 Hunyuan video 是做了一个 hierarchical 的这个 data filtering 那具体的过程呢是画到图 4 里了 我们马上讲完这一段 就去看看图 4 长什么样 但其实这个图 4 呢 就跟刚才 moviegen 这个 附录里的那个 table 是差不多的 就是一步一步往下走 然后每一步都筛掉很多数据 像一个大漏斗一样 一点一点漏到最后就剩一点点 这个最高质量的数据了 那接下来这一段呢 主要就是说一些具体的这个模型操作了 比如说对于美观来说 他们用的是 dover 这个模型 去选更美观的视频 然后又单独训练了一个 clarity 检测的一个模型 就把那些非常 blurry 就非常模糊的视频都删掉了 因为有 motion 的存在 或者拍摄的这个原因 所以很多视频都会很糊 那就这些很糊的这个视频 还是移掉比较好 然后接下来呢 就是怎么去判断这个 motion 幅度的大小 把那些完全静止 或者只有 slow motion 的视频呢拿掉 那 hunyuan video 这里用的是 optical flow 这个我们之前讲 视频理解 action recognition 的时候 提过很多次了 那 moviegen 那边用的是 ffmpeg 自带的这个 motion vector 但是效果都差不多 然后接下来作者说 就是继续用这个 pyscenedetect 里的这个 shot boundary detector 然后去拿到这个 scene boundary 的信息 那跟 moviegen 是一样的 用的包和模型也都是一样的 然后呢就是 ocr 模型 去把那些带有很多字的视频呢拿掉 这一步 moviegen 也是有的 然后呢就是有一个这个 detection 模型 去把这个 watermark borders logos 这些 比较 sensitive information 呢拿掉 就一个 object detection 那 moviegen 那边也是有的 那作者最后说呢 因为我们这里做了太多的这个 filtering step 那到底这个数据是移掉要好 还是不移掉好 其实真不好说 因为从个人感观上来讲 你觉得可能清晰的视频更好 然后或者说这个横屏视频更好 然后 motion 幅度大的视频更好 但是呢 当你把这么多这么多 呃视频数据扔给模型的时候 他其实跟 language model 一样 有时候 diversity 更关键 而且呢他越接近这个真实世界的 这个数据分布 他有可能反而这个效果会更好 所以 还真不知道 就是哪些 filtering 是好的 哪些 filtering 是对模型训练有害的 那这个时候呢 呃 Hunyuan Video 就说 他们呢是用一个小规模的 这个 Hunyuan Video 的模型 去做了大规模的这种实验 相当于就是做了很多很多这种 ablation study 从而验证就是这里的每一步 每一个 filter step 到底是有用还是没用 如果有用 再去大规模的跑把这个数据处理了 那万一没用甚至是有害呢那就别跑了 那接下来呢我们就看到这个图四 就是 Hunyuan Video 所谓的这个 阶梯式的 data filtering pipeline 看了这张图以后呢 就一下明白了 他们说的这个五个 subset 是什么意思 其实就是呢 就从原始的这个数据库里 他会做五次这个操作 第一次呢就是把大量的数据选出来 这种 256 pixel 的这个低清的视频 然后逐步呢去做各种各样的这个 filter 同时提高的这个 resolution 就跟 movie gen 那边一样 也是一步一步的提高这个 resolution 从而让这个视频越生成的越好 越 生成越清晰 然后 resolution 这边呢相当于有四步 然后最后呢还有一个 sft stage 也对应 movie gen 那边的这个 sft 而且呢也是 manually filter 出来的 当然在具体的操作上 还有就是说这个 filter 到底是放在前面一步还是后面一步 我觉得 Hunyuan Video 和 movie gen 可能做的是 不那么一样的 但是整体思路和整体的 pipeline 呢 都是高度类似的 我觉得 Hunyuan Video 这里的 pipeline 呢 还是比较合理的 比如说把 deduplication 和这个 motion filter 放在最前面 因为这是最能删掉大量数据的 可能一下子就把 50% 甚至 80%的数据都干掉 那对接下来的这个算力的要求 就会轻松很多 而且确实你静止的这个视频 或者说这个重复的视频就没什么用 本身就应该在最早的 stage 被干掉 但像这种美观的 filter 或者这种 source filter 呢 就会放到后面一点 那因为像美观呢 肯定就是针对比较高清的视频 才有美观一说嘛 而且美观这个东西也比较主观 就如果这个视频本身内容 很好 semantic concept 的各方面都很好 那有可能我们也是想 比较保留到后面的 然后 Hunyuan Video 这里也说了 在每一个这个 stage 里 都会有大量的数据被移掉 就跟当时 movie gen 后面那个 table 里写的一样 每次呢都是 少则一半 多则可能 80%的数据都被干掉了 那经历了这几层的这个 filter 之后呢 其实最后剩下的数据 跟原始的数据比 肯定是非常小的一个百分比了 那至于 fine tuning data 就是这个 sft stage 呢作者这里也简单描述了一下 说最后的这个 fine tuning data set 大概呢是有 100 万个这个数据吧 而且呢这些数据 全都是通过这个人来标注 或者 filter 的 但标注的方式呢 基本跟 movie gen 是非常类似的 所以这里就不多说了 我们直接来看一下 3.2 这个 data annotation 尤其是这个 structural captioning 跟 movie gen 还是不太一样的 而且比 movie gen 写的更详细一些 所以这里我们来看 Hunyuan Video 是怎么做的 那这里这个 structural captioning 呢 其实就对应的是 movie gen 那边的这个 captioning 那个 section 然后比较有意思呢 是他这里用的这个 json format 去把 multi-dimensional 其实就是多个方面的这个信息 全都写到这个 json 里 那最基础呢 我们肯定是得有一个这个 short description 就是你大概有一个短的一个摘要 这个图片里到底有什么 然后第二个呢就是来个加强版 这个 dense description 就是能长则长写的越详细越好 把各种各样的信息 甚至包括后面这几个 比如说 style shot-type 也都写进去也无所谓 反正就是非常 dense 非常详尽的这个 description 然后第三个 json 的 key 呢 就是 background 到底这个 subject 的 所处的背景是什么 和背景有什么 object 也都可以写进去 还有呢就是 style 这个我刚才也说了 是非常关键的一个 video generation 里的东西 很大程度上决定了用户到底喜不喜欢 然后第五个 key 呢是这个 shot type 比如说是不是一个这个无人机的这个视角 还是一个 close up 的视角 这个对于创作者来说非常重要 那第六个 key 呢就是 lighting 第七个 key 呢就是 atmosphere 然后除了这七个 key 之外 作者说我们还加入了很多别的东西 就是如果这个 text video pair 他本身爬下来的时候 就有更多的 metadata 或者也有可能是用这个 language model 或者 vision language model 生成了一些额外的 metadata 也会被包括进来 但如果说借用这种 json structure 去当做这个 prompt 呢 多样性上呢可能就会有一些问题 因为格式可能都太像了嘛 所以作者这里说呢 他们非常小心的设计了一个 dropout 的 mechanism 去合成了一些这种非常 diverse 的 这个 caption 这样呢就会生成各种长度和各种 pattern 的这个 prompt 从而去更好的 match 到这个用户 提交上来这种 query 而且呢也避免了一定程度上的 overfitting 同时呢跟 movie gen 一样 Hunyuan Video 也做了 camera movement 的这个一个 classifier 只不过他们这里呢只 predict 了 14 个这个 camera movement types 而在 movie gen 里呢一开始就用了 16 个 而且在 finetuning data 里呢又加了 6 个 但整体思路和做法都是一样的 那数据准备这里呢算是讲的差不多了 之所以我们花了将近 50 分钟的时间 来精读这个数据的 section 也是因为在 generate ai 甚至整个 machine learning 领域 数据呢都是最关键的一环 很多时候呢数据的 quality 数据的好坏 决定了最后模型的性能 那在 linkedin 上呢之前也看到一个这么个梗图 对于传统个 machine learning 来说 如果你给他一个比较垃圾的数据 那他给你的就是一坨屎 那对于 ai 来说呢是一样的 只不过是带点星星的屎 那对于 generative ai 来说呢 不论他多 fancy 如果数据质量很差的话呢 他最后给你的呢 无非也就是一个像 unicorn 一样的 一坨垃圾 那最后呢就是大家吹的很厉害的 包括今年 2025 年大家都说的这个 agent 元年这个 agentic ai 同样的道理呢 你如果这个原始原料给进去呢 你如果这个原始原料给进去呢 你如果这个原始原料给进去呢 都是垃圾 那你就会得到一系列各种各样的垃圾 所以这也就是为什么我会花这么多时间 去精读这个论文的数据部分 因为确实里面内容很多 而且也是值得大家深挖的地方 而至于模型和训练 我们现在会回到 movie gen 的论文 再很快速的过一下
