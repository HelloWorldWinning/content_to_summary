Timestamp: 2025-01-14T12:33:43.293580
Title: 【时序预测】PatchTST模型精讲，通俗易懂，一学就会 BV11LrTY3E2P
URL: https://b23.tv/eqbhpg9
Status: success
Duration: 1:07:32

Description:
好的，我将按照您的要求，对这篇长篇内容进行总结。

**Summary:**

**I. 核心思想**

*   **背景:** 现有基于 Transformer 的时间序列预测模型，在长序列预测任务中存在计算量大、未能有效捕捉局部和序列信息的问题，以及通道融合可能引入干扰。
*   **创新点:**
    *   **Patching (子序列):** 将时间序列分割成子序列（patches），作为基本学习单元，而不是单个时间点，以捕捉局部性特征。
    *   **Channel Independence (通道独立):** 将多变量时间序列的不同变量（节点）视为独立的通道进行处理，而不是建模它们之间的相互关系，只建模时间维度上的依赖，简化模型，避免节点间信息融合可能带来的干扰。
*   **关键改进:** 通过 patching 和通道独立，模型降低了计算复杂度，提升了长序列预测能力，并增强了模型的泛化能力。

**II. 详细解释**

*   **输入数据结构:**
    *   交通预测问题通常使用四维张量(B, N, L, C)，其中 B 是批次大小，N 是节点数，L 是输入序列长度，C 是特征数。
    *   多变量通常指的是 N（多个节点）或 C（多个特征）。交通数据中，多变量通常指多节点。
*   **通道独立:**
    *   在交通预测中，通道通常指节点(N)。
    *   传统方法会建模节点间的关系，而该论文提出的通道独立方法，认为每个节点都是独立的，只建模时间上的依赖，放弃空间上的依赖。
    *   通过通道独立，模型参数量减少，且同一时间序列的不同变量之间共享参数。
    *   通道独立提高了模型的泛化能力，可用于不同节点数的其他数据集。
*   **Patching:**
    *   传统方法以单个时间点为单位建模，丢失了局部性信息。
    *   Patching 将相邻的时间点组合成子序列（patch），能保留局部性信息，同时捕捉长序列依赖。
    *   类似卷积操作，通过共享参数的 MLP 将 patch 转换为特征向量。
*   **模型结构:**
    *   输入：单变量时间序列，经过 Instance Normalization (实例标准化)。
    *   Patching：将时间序列分割成 patch，可重叠或不重叠。
    *   特征映射：通过线性层（MLP）将 patch 转化为特征向量。
    *   Transformer Encoder：标准的 Transformer 编码器，学习 patch 之间的关系。
    *   输出：预测的时间序列值。
*   **自监督学习 (Self-Supervised Learning):**
    *   使用 Masked Autoencoder (掩码自编码器) 进行预训练，通过随机掩盖部分 patch，然后预测被掩盖的部分。
    *   预训练后，可以通过两种方法微调：固定预训练权重（线性探测）或微调所有参数。
*   **实验结果:**
    *   在多个数据集上，PatchTST 超过了 Transfer-based 的方法和 D-LIN 模型。
    *   通过自监督预训练，模型的预测性能进一步提升。
    *   消融实验表明，patching 和通道独立均对性能提升有贡献。
    *   模型对回望窗口的长度更敏感，更长的回望窗口可提升性能。

**III. 结论**

*   **Core Point (核心点):** PatchTST 通过子序列分割和通道独立设计，有效提高了时间序列的预测精度和泛化能力，尤其适用于长序列预测任务。
*   **Fundamental Point (根本点):** 该研究颠覆了传统建模时空依赖的范式，提出了只建模时间依赖，反而能提升性能的思路，这种独立处理不同时间序列的方法在某些情况下是有效的，表明对数据特性的深入理解是设计模型的基础。

**IV. Overarching Framework (总体框架)**

该论文的核心框架可以概括为：

1.  **问题定义:** 长序列时间序列预测挑战以及现有 Transformer 模型的局限性。
2.  **创新方法:**
    *   Patching: 分割时间序列为子序列。
    *   通道独立: 独立处理多个时间序列变量。
3.  **模型架构:** 结合 Patching 和通道独立改进 Transformer 模型。
4.  **自监督学习:** 利用 Masked Autoencoder 进行预训练。
5.  **实验验证:** 在多个数据集上进行实验并与现有方法对比。
6.  **结果分析:** 分析实验结果，并进行消融实验和参数敏感性分析。
7.  **结论:** 总结方法贡献，指明未来研究方向。

**V. Conceptual Map (概念图)**
<Mermaid_Diagram>
```mermaid
graph LR
    A[时间序列数据] --> B(预处理);
    B --> C{Instance Normalization};
    C --> D[Patching (子序列)];
    D --> E{特征映射 (MLP)};
    E --> F[位置编码];
    F --> G[Transformer Encoder];
    G --> H{预测层};
    H --> I[时间序列预测结果];
    
    subgraph Channel Independence
    G
    end
    
     subgraph Self-Supervised Learning
      style J fill:#f9f,stroke:#333,stroke-width:2px
        J[预训练数据] --> K(Masking);
        K --> L{Encoder};
          L -->M[预测被遮盖部分];
    
    end
    
    
    J -.-> D
   
    
    
    I -- 自监督训练--> J
    
    
    
    style E fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style G fill:#ccf,stroke:#333,stroke-width:2px
    
   
    
    
    
    
```
</Mermaid_Diagram>


Content:
大家好 我是打架的萧勇然后今天的话我们带来一篇长续续徐侧离比较经典的那种是排制TNT那这边那种的话是发表在SR223的体积会议上的首先我们看一下它的摘要首先它就介绍我排制TNT它是一个船方Base的一个模型它是基于船方法架构的主要用在的就是多边量持续一次问题以及资奠度的这个本能学习然后它两个核心的创新点的话就是说首先它的基本的一个学习的一个基本单元它是一个什么样它是一个子训列或者叫子时序而不是说像以前一样就是说我的基本单元就是一个时间创对 然后第二个比较核心的创新点的话就是这个差点到in-damping的意思也就是说是这个通道独立其实读到这的话如果说对这个时序于测或者说这个时工于测的就是不是特别了解说就有点梦什么时候通道独立呢那我们这里的话就基于这个交通这个领域然后我们继续前辈子去讲一下我们知道对于这个交通预测问题来讲的话就是我的这个模型的输入一般来讲它是一个四位的一个张量那就是B然后N然后L然后这个C对B的话直带的就是FascistSans对吧然后N的话它直带的就是我这个路网上的一个接量数对吧然后这个R的话就是我输入训练的一个长度对 比如说我96个时间创去预测为大的24个时间创那么R我直带的就是什么样就是96对吧 输入的R2就是96对然后C的话一般来讲直带的其实就是所谓的每个节点上它的一个特征比如说它这个上面机有速度也有流量也有保合力也有占绿对吧那它的这个特征特征数量或者叫通道数量就是什么呀 就是4对就是说它是一个什么样所谓的一个多特征的一个东西对 多特征那在交通里头一般我们去说交通的所谓的一个左边亮它的这个多边亮直带的是什么意思呢这个在大部分的时候所谓多面亮它直带是其实这个N也有说直带的其实就是所谓的多节点多节点对也有说其实所谓的多边亮多节点其实这俩是等效的在交通的这种数据机上对 那当然有些数据机比如说像这个污水数据机对 污水数据有时候它污水可能就一个传感器它就一个传感器或者说它只有一个设施但是这个传感器的话它可能就它能采集非常多的多样的数据比如说它能采用污水的各种指标那每一个指标其实都有一个曲线其实它这种曲线的话它如果去提这个多边亮的时候它直带的这个变亮其实更多的是直带的这个C对 因为它这个N这个位它是1它这个C反而是很多的对 就它特征会很多的如果它去说多边亮它更多的去直带的是这个C或者说是IF叫Filter对 所以说大家去看这个一系个就是都会说我是一个什么多边亮实际预测或者说我是一个多边亮的这个时空预测这说你需要具体去看一下它的选的Dead Sett比如说它选的这个数据集到底是一个什么东西对比如说如果说比如像交通它的这个多边亮其它更多的是直的这个节点的多或者直带是多个节点多个设施对 但是比如说如果说像一些个数据集它可能就只有一个设施但是它可能采集非常多的Filter它的多边亮直带的更多的是这个Filter对 所以这里需要注意它这个排着TST它所谓的这个拆脑到底是个什么我们知道通道的话一般是取自CV的领域对吧就是说一个图像对吧 一个图像它上面有这个像素一般的话是三通道对 三通道就是三个通道那一般来讲的话我们经常会理解这个通道是不是就是特征的意思如果说以理解这个那你可能再看这个排着TST你可能就会很废弃因为排着TST的话它所谓的通道并不是值得这个东西它更多的它直带的是什么呢它直带的就是比如说在交通给它直带的更多的是个多边亮或者叫多节点其它直带的是N这个伟渡就N这个伟渡那什么意思呢这里的话我去解它的一张图解它的论文里的一张图找一下就这里那我们可以看到的话它输入的话就是你看它这里有非常多的这个训练对那这里的训练我刚才讲的对于交通来讲的话其实它每一条训练代表就是一个比如说这里代表就录口1录口2录口3录口5它其实代表的是N条的训练其实值带的是上面的N的这个伟渡对吧这带上面是这个N的伟渡对那它指出什么意思呢就是在原来的时候我们是需要什么呢我们是需要把这个什么样我们认为每个节点之间其实它是有相互关系的比如说我录口1和录口2它的流量就是有一个关系也说我去建模那时候我应该去建模它的什么样建模它的一个所谓的叫什么样这个节点间的关系对吧那以建模节点的关系其实说白的就是你会做各个录口之间的一个所谓的交通什么样信息融合对比如说大部分的这个时空模型你看它都说了我时空模型需要建模两个艺来第一个艺来的话我需要建模每一个这个时间上的时间走上的这个什么样它的一个演变的一个趋势它的一个变化这叫时间艺来那我同时还要建模纵走上就是各个节点之间它的一个艺来我称之为空间艺来对不对所以说时空本质上来讲的话它的这个空它建模的就是什么样所谓的辩量和辩量之间的一个关系对那在排txt里头它它把这种的话它称之为交通什么呢它称之为交通道融合通道融合比如说通道非独立嘛因为它的通道我刚才讲了它的通道其实更多的是指带的是这个N就是通道独立比如说是N的独立比如说我每条训练相当于我不去建模什么样这个节点和前点之间的关系比如说我指建模这个航轴也就是我指建模实序之间的一个艺来关系一看它是不是它都独立的送到这个网络里头去然后独立的输出对不对然后就你会发现任意的两个节点之间一看中间有什么交互吗它是没有任何交互的对也就是说我的输入是bntxt也说实际上我主要指建模它的什么样替这个围度我这个n这个围度我实际上是不进行任何建模的也就说说白了它抛弃的是空灵玉里头的空间艺来它就认为我只需要建模这个时间上的艺来这个模型就可以取得比较好的效果它反而指出如果你去建模这个空间上的艺来它反而有时候这个节点和节点的这个信息一融合反而会给当前的这个节点带来一些什么样带来一些干扰对带来一些个不好的信息导致着你可能我自己学自己的反而学的会更好对这个就是所谓的通道独立对当站这里有一个同学就位它这里的通道独立指的是我每一条训练或者说我每一个节点都要学一组传送模的参数吗那并不是它就认为我所有的这个什么样我所有的通道或者说我所有的事实它在时间上的一旦它是有什么样是有共通性的也就是说它这里会有一个叫做什么样参数共享的一个操作也就是说它就认为我这个时间上的这个什么样它的这个相互的一个关系每一个训练都是什么样共享的也就是说中间的这个部分它是什么样参数共享的对那你这样的话请你会发现是不是参数量就减少了很多对吧我通过通道独立这种设计我的参数量就减少因为这里它认为这个整体的一个东西它是什么样它是共享的我只需要学习一个就可以第二个的话我共享一个有一个第二个好处就什么样我可以有非常多的训练去干什么呀去更新这个你这里这个网络里头的一个传众嘛对吧然后当然还有第三个好处有大家就是如果去思考这个问题你能想到第三个好处是啥呢其实第三个好处就是说这个模型的签议性是非常好的那什么意思什么叫模型的签议性呢就我举个最简单的例子比如说我们交通里头有好几个数据节比如说有PEMS03然后还有个PEMS04对吧那如果说按照我们原始的比如说实空的那种健康饭式我去建模PEMS03的时候是不是你需要指PMS03有一个对应的N比如说N1对不对那我基于这个PEMS03那个数据集去训练出来的这个模型能直接用于PEMS04这个数据节上的这个推理或者预测吗实际上是不能的为什么呢是因为PEMS04和PEMS03它俩的什么样节点数它是不一样的对吧有说它模型的一头实际上它里头的很多权重很多这个模块它实际上把N1已经签到这个模型里头了那我直接去在这个数据间上去都推理的时候它会告诉我会报一个shake不匹匹的错误也有说基本是所有基于实空的这种饭式的这种它是原子上是没有任何签议的性能的比如说我在一个数据集上shake那好的一个东西我是没办法直接拿到其他的数据集上去直接跑推理的跑预测的对这个其实是实空流域或者叫实空模型它在这个签议上它是有很明显的一个历史对那当然它这里就签上归比了因为它价格它不见过这个N这个违度上的任何一代比如说它其上是N和接两树是没有任何关系的那我去做签议的时候实际上我只要在一个数据间去签议只要你这个数据集上这个T的违度上又相似性原子上我都可以进行任何签议从一个场景签议到另外一个场景也就是说通过这种通道独立这种设计带来了第三个好处就是什么呢它的签议性大大的一个增加了对也就是它后面为什么一直在强调它去做所谓的就是它这种这种也提到了一个吗就提到了一个我得除了能用于多辆量的实蓄的认为预测我还可以做这种自奸都的这种表证学习对它为什么就是说因为我们知道就是这种自奸都表证学习其实一般来讲的表证学习表证学习其实更多它其他的就是一种编法作用它把我们的输入去编法成一个签议的一个张量然后我下游再去基于这个签议的张量然后我相当于在什么样去匹配下温的任务对那我匹配其实本身有时候它其实就有点含的这种签议这种感觉对再搭配它这种通道独立其实你这种表证学习它在这种签议学习在很多人多任务学习上它能够更加的灵火能够更加的去大方一彩对那这里去这里大家就理解了就是什么叫这个通道独立对至少就是在交通问题里或者说在大部分这种多设施的问题里它这个通道指带的是N对指带的设施所谓的独立它就认为就是双国NN和N之间有时候这个设设和设施之间我不去见谋它单独的去什么样指学习时间上的依赖对也就是实际上派制体其他的自己更多了它就是一个纯实续的一个模型它不去见谋任何的所谓的空间依赖或者说它不去见谋多边亮之间边亮和边亮之间的这种依赖对那当然我这里的边亮之间是这个N因为我们包括这个它用的好多数义机其他它那种所谓的多边亮之间都是一个N对所以这里大家去理解这个所谓的什么是通道独立对那当然就是反之任那什么是通道非独立或者叫通道融合也很容易理解的就是我这个模型既见谋N之间的依赖我又见过这个RO之间的依赖那这种的话就是叫做通道非独立的对通道非独立的对那这样的话我们就把它这个论文的两个出发点给大家简单的介绍一下然后这个介绍一下看那介绍一下看它就是首先介绍了一个这个Introduction就是反正就是基本就介绍一下为什么我要用船的方法去做这个东西它就说哎我船的方法在这个ARP在这个CV领域以及在这个什么云它就是它都取得了非常好的一个成果对然后所以说后面的话有些人就开始把这个船的方法这个架构去引入了引入到我们这个实区预测这个领域那么在之前也出现了一些比较好的一个成果比如像什么infoam or out什么 outfoam or pdfoam or反正就是一对的foam对然后他们在这个常识去这个任务下也取得的也不错的一个成果但是后面的话出现了一篇那个出现了一篇动尔出现了一篇啥动尔这篇动尔的话其实就是应该是就是D-Linux对D-Linux这篇动尔的话它就是一个很简单的一个只剧于这种现性的一个现性层就打败了重多的什么呀重多的这些方法对所以当时它的论文的题目就是说这个船的方法是否适合做这种常识去预测的任务因为它就剧于很简单的一个什么呀这个全年间的这种架构就打败了这些所谓的这些一对的方法它就得出一个推论就是说其实这些方法根本就不适合去见过这种实际测试问题它价格可能就是它天然的因为我们知道川普方法里头的话它是直接去见过这个实际各个时刻的一个什么的它的一个相关性或者叫注意力对它其实天然的抛弃了什么呀我们时间预测里头比较尽点就是局部性和顺序性对我们知道这个时间这个时间的事件的一些演化它是有一个很明显的一个顺序性的对包含有时候它还会有些很明显的一些局部的一个特征比如早高峰平峰交通里头的早晃峰平峰它局部还会有一些一个这种有一些比较强的语意但是可能如果我继续注意力的这种时刻和时刻或者叫时间窗和时间窗之间直接去见过可能这部分语意完全被丢失掉了这个顺序性也被丢失掉了当然还有人说你的船方不是人也有位置编碁吗但是其实位置编碁它对于这个东西它的什么呀这个顺序性它这个保留其实还是比较薄弱的对这也就是这个D-LIN的就是这个人就是这个就这片动物吧2.8发的这片动物它只出来的就是说这些方法都不行对所以当时其实也给给这些演决这些穿的方法演决这个方法呢其实也给大家带来一些带来一些挑战对所以当时其实叶劫叶一直对这个东西有中文其实这片动物它其实也是在回应这片就是它就认为其实方法是适合去见过这种常识序这种依赖的只是说原来这些犯事它们没有什么样做很好的一些处理对那我需要对这个方法进行一些优化优化就是怎么优化呢对优化前面的两点就是说我第一个就是说我要去见过什么呢这个词序列对我让这个词序列去保留我这个什么样时间序列里头它的一些局部性的一些特征对而不是只是见过时间双时间因为时间创比如说我某一个五分钟的流量和另外一个五分钟的流量只是两个点之间的这种相关性货依赖它其实在很多领域它实际上它没有很强的一个相关性货说没有一些很强的那种狱意对反而我去一个块一个块之间的它反而既保留的这种什么样既保留的这种这种序列的局部局部性又能够什么呢又能够去见见过这个过程中我就能够见过很长的那种序列对就是长期性和局部性或者叫全局性和局部性都能得到很好的一个保留和融合它是认为这个是非常OK的非常棒的对然后第二个的话就是像刚才讲的就是说我可能就是原来的很多饭是他们都是通道融合的对它里头记忆记住了这个便当和便当之间的一些剑膜也做到什么也做了这个这个时间上的一个这就是一个混合可能反而会带了一些干扰它就指出我们应该用这种通道独立让每一个序列都是这样只剑膜它实际上的一些意大空间上我们不去设计对它只说这样改进非常的棒非常好然后之后就是我经过测试的话发现这两个非常的吃笑对就打败了这里它的这个提临的对传为这个传字方式被死的这个模型给我争下一局对不至于说全军覆没对当然下面的话它就是说我这两个比较核心的设计第一个的话就刚才讲的就分块就是也是这偏论文它的这个派尺TSD派尺就是它的第一个床心就是分块分块它这里说就是说这个实区预测的话它重点的话去理解和剑膜这个什么样它的每个十年部十年的一个项目关系然而的话就是一个这个这个十年部的话它是没有什么样它不像这个我们的这个语言系统比如说自语NLP里头因为NLP里头它的一个基本的话就单词嘛我们每个单词实际上它本身就还有很丰富的一个语语对那我去见过单词和单词之间的这种关系那实际上是有是有作用的对但是在这个实区里头你某一个时刻的一个指或者说某一个时间创造指实际上它的语语性是非常差的对因此的话我如何去提取什么样这个叫做局部或者说Local或者说这个本地的这种什么样语语的信息实际上是非常重要的对它只说我怎么去做呢那我其实就是把原来这种Pointerways就说基于点和点之间的这种这种这种关系我把它抽象中什么样块和块之间对就说我聚合几个点成成为一个块对那这样的话我每个块实际上它包含的语语性息会更多一点然后去见谋这种块和块之间的种注意力才更容易对然后这个是它去分一下它这个派手其实这个也很容易理解确实是我们就是我们再聊点前片就是大家有没有思考过就是为什么就是说比如像我们现在比较活着这种大模型它率先是在自语言这个流域活着这种突破它没有在其他流域大家有没有思考成问题对其实这个也跟它这种说的是类似的就因为在自语言领域我的基本的一个单元就最细力度的它就是一个单词那其实每一个单词的话它就本身就有拥有非常丰富的一个语语对所以说很多模型在这上面去学习的时候我当我的这个语言的体系比如说我的语料增大的时候我的模型的差畅量增大的时候可能会获得一个能力的用线因为它信息的密度会更高对那么四字的是什么是CV领域就是图像比如像现在很多人我为什么图像会这种图片的这种自动生成案其实后来也是有些突破了那图片稍微好也有因为我们图片里头基本的单位当然是像素对吧像素那图片我们像素比如说我一个局部其实是一个自动它本身也有很丰富的一个语语的表达对然后刺着的就是其他的一些领域比如像语言还有像这种实趣因为它表震出来就是一个点对它能带来的这种信息是非常孤立的我举个简单的例子比如说我告诉你一个点它是它的交通流量是60量它能代表什么含义其实你没有任何含义为什么这个60量它放在城市因为它可能是代表这个路口非常的堵这个城市它可能就是一个很小的一个城市但是当这个60这个数字放到了一个什么样放到了一个比较大的城市或者什么的它可能有一个60所带来的语语它就发生变化了对所以单独的这种数字它其实不能反而你比如说我拿一个块它反映的是这个局部的它一个变化趋势它反而就能带来一些语言信息了比如说它到底是一个商生还是局部在下降还是在震荡它至少能带来一些信息了对所以说其实很多领域其实很多时候它本身就是就是互通的包含其实我们的那个很多去研究更挺层设计它们去从信息学的角度去研究我怎么去更好的去设计这个模型对它可能设计出来模型它能重一些基地上或者说什么样能够让你的模型更好的去工作然后达到搜塔的一个表现那当然这就是有点成员了对然后第二个的话它就又继续说了一下在这个拆道里你就是说这个通道独立是怎么 怎么搞的对不对是不是通道独立的话就是说它其实也是也是接见挑发现的几片动物吧就是一个是这个片动物在CNN上另外一片动物就是那个DLintern它是证明的就是这种通道独立实际上相比于这个通道绒合它是有优势的所以它就至少哎我我们去做这个的时候用这种通道独立的这种思想对那当然的话就后面的话就开始开始说了一下它是这种优势吧对这个优势我们简单过一下但是优势的话就是说第一条的话就是说首先的话它能够带来这个时间和空间的这个复达度的一个下降对这个很容易理解就是我原始的这个传播的话因为它是有什么呀N方的这个复达度的对无论是时间和空间都是有N方的那那个N的话值代的就是我这个输入的这个Token子的这个数量对那如果说我们没有任何的这种预处理那其实N的话就是什么样它就是你这个输入序电的一个长度对因为它建访的就是这个时间上的一代我输入的是R那个长那原则上它的这个注意力举例它一定就是R方嘛对吧打这个的话在这种长时续预测的这种问题上因为你的输入的R越长你这个注意力的这个举例就会越大对比如说当R输入的1000那一个注意力的人就是1000的平方对吧就是100万对那无论是对于这种运算的时间还是这个对于这个显存或者到内存的占用其实都是比较大的一个消耗对那我们假设我们引入了这个太沉弄思想那么我比如说我输入的是966对我派是我刚才说了派是说白的就把它签了一个块一块的假设我10个为一块那966是不是假设我不重点的话那966出一时就变成了只有96个那你这个注意力的人是不是就从966成一966缩断为了什么样96成一96了对所以它这个整体的一个运算的一个什么样时间和空间的这种占用或者是复达度就会下降为原来的什么样就是R出眼S的平方倍嘛对吧因为它有个平方的一个方达作用对然后它当大人这里的话它也是举一个例子就是说在一个数级以上我去设置这个派是赛字的大小是16然后我这不常设这为8的话那我这个L就我输入的原始的讯战长度是336那我设置完之后实际上你的这个真正的讯战长度应该是后面会建设下怎么去算会缩断为应该是42从336会缩断为42那带来的整体的这个讯战时间也会什么样变回原来的什么样就是差不多是原来的22分之一对 就是这里你看到这里举的几个例子嘛比如说这个TRIPETER这个数级我有这个派池只需要464秒然后如果说不进行这个派池的话就需要1万多秒是不是22倍那这个电力数级的话也类似19倍然后这个Y的这个数级的话是这个4倍那失败的就是说你这个数级越大或者说你的输入讯恋的这个长度越长你采用这种派池的这种思想你整体的这个讯战时间整体的这个计算时间带来的提升越明显对然后第二个的话就是这个表证学习的不是第二个是这里第二个他说的就是说这个他有利率就是我能够用更长的这种回望窗口什么叫回望窗口就是说白了就是我的输入是输入历史段子去段时间窗对 因为他经过派池之后的话其实这个计算压力和这个存储的压力小然后这个存储的压力然后这个存储的压力就是说我这个他觉得这个力的就是我这个这包一就是这个表一的话他展示了就是我通过这个把这个回望窗口的这个长度重96加到这个336我这个MSE的话能够重0.58下降为0.397你看的话你看就是这种这个表一的他展示了就是我通过这个把这个回望窗口的这个长度重96加到这个336你看96预测96的时候他的MSE的话是0.58当我变成这个什么的336去预测这个336的时候都变成了这个什么样这个0.3970.397对吧对3.397那么然而的话就是我只是单纯的去这个把这个L去增长的话就可能带了一些问题吧就像刚才讲其实说白了就主要是这个Memberer就是内存或者想存占用以及这个训练时间训练时间的太长对 因为他是平方背的一个增长吧对那你看他就说了就是说但是的话这个实序的话他经常能够带来非常多的这种什么样这个实序实际上的一些个这种信息原来上我们输入更长的训练其实能够对于我们那个模型带了一些有注意的对那既然当我这个长实序用标准能够处理去计算的时候比较困难所以就困难其实就是内存和这个虽然算太慢了对所以后来有些个人做的一些改进那么第一个改进我们其实最容易想到的就是说我去做价材料对你不是他是说比如说说是三百九三百三十六我什么样我先对李头做一些个价材料就像他一把他的训练长度缩短对比如说最简单的一个价材方式就是抽样比如说我每次我只取这个每次不是理头的之后一个点作为作为这个训练的一步那这样的话我就变为了价值就会变来变成一样的四分之一对不对是不是这样的话其实因为发现就是我只需要这种简单抽样太能带来一些性能的一个一个提升对吧是不是对但是这个抽样的话就会带来一个问题就是一毕竟是从这个四个训练里头是什么样只抽样其中的一个或抽样其中的几个对吧他可能没被抽中的那那部分其实信息就被损失掉了对他会带来一些个信息损失对所以所以后来就起发我们能不能用另外一种方式我技能保留全部的信息又能够什么呢又能够这个把这个训练给缩短对所以他就指出那其实就我就可以同我这种派驰的一个思想你看这派驰也得估计要是推特对吧他能够去保留这个局部的这种持续然后也能不损失任何的一些信息对如果他测下来的话就是这个经过这个派驰以后你看经过派这以后他整体的这个性能的话就可以什么样提升到这个0.367对是不是当然这里他说的就是说白的就是就是证明这个派驰非常合理的而且前两年都是为了证明他这个派驰非常合理的对当第三年的话其实就是说他的别人学习对 别人学习他这里就举个例子就是我加入这种自家做这种预讯念以后再去做同样的一个任务因为发现从0.367提升到的0.349又指出我这种自监度的这种预讯念也能够带来这个对于下游任务的这个预测性的那个提升对这个是他3.1的一个试明对那整体看完他这个前面的这个眼眼或者音车大陈其实他翻来覆去的就一直在在产生出他这个派驰以及他这个通道独立他的一个有消息对但其实你从本质上来讲他所谓的派驰其实不就是这种就说白的就是我这里哎呦呦呦对对对有非常多的这个书过当书物非常长的时候我基于这个注意力去算的时候他的伏达都是N方或者叫R方对吧大R非常大的时候可能我这个运算的这个速度会非常慢同时这个显存的站位非常高但其实他重点去缩的这个书物这个传在方法的这个力同时这个R的一个长度那缩在这方式那第一种的话很简单就我抽样比如说我每四个我只保留一个点每四个我只保留一个点这的话他就抽样我原来4分之1对吧那整体的伏达对于4分之1的平方就变成原来的这个16分之1对那这是一种但这种的话刚才也讲了就是有无论抽哪一个他都会什么样带来星星上的损失对那所姓的话他就什么呢比如说还是用4个去做我所姓把这3个什么样把这4个通过一个什么样全年间的网络把它变成1个点有时这里类似于就是一个MRP然后那那其实这里就他所谓的一个派试吗对吧所谓的一个派试然后把他从4个点聚合成1个点那那这样的话相当于是我这我这一个点的信息实际上他已经什么样把这个4个点的信息都拿到了其实这里就是我去学习的就是局部的一个数据对吧然后同样的往后走往后走的方式有有什么样有几种一种的话是重点的一种是不重点的那什么意思呢比如说我的这个派试我甚至的是4假如我不重点那其实就是我后面都是4的4的4对也说实际上就有点类似于卷击对吧我的这个这个卷击和的大条甚至这个4我的不常也是4那这样的话我每一个派试就是不重点的那比如说我的派试4但是我的这个我的这个不常是2那实际上就会有个重点嘛对不对那第一个派试的话就这样那第二个派试应该是这样的对吧就是相联的两个派试之间就会重点两个对也不说他这个派试TXT里头也说了就是我那个派试的派试之间可以重点也可以不重点就这个也可以进行调整但你这样一看其实他本质上一会发现这个东西就是啥呢他本质上一会发现什么的一看他说了我这个派试去进行这个聚合的是我他这个这个就是这里的MLP他其实是共享的全都是共享的那你看这个东西我不就是在这个训练上然后聚合一个然后这些权中就都是共享那不就是类似于Cinema对吧那这个权中不就是我的这个卷机盒的参数嘛对吧因为对于对于这个卷机盒他参数是共享的然后去扫对吧所以说你这个模型本质上他这个派词的过程不就是想到于是我先进行一次的Cinema或者叫Tin我把这个L的这个长度把它缩减为RP对吧缩减为缩减为RP然后呢我带把这个RP再送到什么样传细方式里头就学习对所以本质上他整体的价格就有点像这个什么样Cinema加传细方式怎么改变的当然你去变成实际上可以用去去去行他这个派词的一个过程对吧那当然他这种派词的思想的话其实也不是说他简介提出来的其实在其他领域已经有很多人去用了就当然他这种人也提到了这个问题就认为在他这里提到的这个提到的话应该在哪提到的这款系的应该是比如说在相关工作里他其实也提到了比如说在这个NLP里头那我这个BortBort的话其实知道的话他去见某预言和语言这些外面他是基于这个什么样自服和自服的对 就是每个单词和每个单词之间到后来他会发现我用一些自主也是单词的集合对 去见了这种能够有些人性能那其实类似于就是派词了一个派词单词那CV的话其实也类似这个模型叫VIT对 他是见某的肉我不去见某整个图像每个相速和相速之间的一个关系而是我拿了一张图我先把它切成一个16成一16的一个什么样就像它是切成一个小的紫图我去见某这个紫图和紫图之间的这个种一代对啊 像它是相速的一个派词对 是不是 是其实放到我们这个时续浴测里头就是我去派词这个时间创对然后的话这里继续说那其实本质它其实就是像这个派词的过程我是可以G-CNG实现那当然这个什么的前半的这几个比如像这个VIT连继续放在其他派词它原来玩的也是继续什么样G-CNG来实现的对吧 G-CNG来实现的对 你这样一看其实也发现有的时候你换一个巧度去讲如果说你讲的故事讲的话我是前面只是讲了一个CNG后面接着就穿的方法然后和继续派词去讲你会发现两种讲故的态度就决定了以这个论为你说我说用CNG加传方法这种区域去发可能不一定能发到很顶的会议上对 但是你从这个派词的这个故事上去讲它就能发现一区发在CTIA会议上对吧 当然这个SR的话它应该是好像好像目前好像没有进行CTIA但是大家也会供电它是一个一个联工制装的一个挺会嘛对 这里其实也给大家一会发动的是这个提供了一个起发吧对 接着往下看然后第二个的话它就是又说的一些个这个船的方法的这些个研究那前面竟不当然也提了就是各种的什么方法嘛这个老个船的方法呀 音方嘛呀这个Auto方法 PD方法但是这些方法刚才也说了都被一个叫D-Linux被激败了对 所以说这个拍着TR的记得猜提出来然后去什么给这个船的方法在在那边站下来就是我船方不行 原来可能它们这些方法可能在剑馆的时候可能有些个细节没有可预到所以说我我才搞出来这个东西反正就是说白了就是这部分其实主要也出于后面的它的这个Bitsline基本就是这几个方法对 一级D-Linux嘛对然后同时这相当功能也说的一下这个表证学习对 那其实什么叫表证学习呢表证学习啥意思呢其实表证学习有的时候它其实就是给这个给这个我们经常理解的这种欠入或者叫Bammer 其实一定程度上是等效的就是我给你的一对数据对吧 我可能这个数据它就是一个这原式的形式表达比如说实距它就是一个时间创5分钟的流量但是这个本身直接的信息我放到下午任务的时候我是很拿去学习的它就只中我可以把你这个输入的那个时序我做我通一些欠入的方法我把它欠入成一些野狭障的张量做一些类似于就身为降为这种操作之后我在把它拿去接一个下午任务比如下午就简单一个建议外MLP它就能取得非常好的效果对 这是欠入那当然Bammer也是类似的所以有的时候很多人会想Bammer欠入还有这个表证学习其实它只带的意思大致相当那当然有些那种人可能在这个行为上会做一些区别而已那当然这咱们这边的文化也简单说了一下什么这个时序的那种表证学习这里的话我们就不过多说我们重点下面的话讲一下它具体的一个结构它这个结构的话因为前面其实我已经讲完了这个派池以及它这个通道独立其他这个结构再讲起来就非常简单了比如说因为它是通道独立的相当于是它每一个每一条训练其实走的都是共享的一个传统的这个Bammer的白暴所以说它的这个输入它就不是N成雅尔了它就是一个一成雅尔而这把这个图结一下就这个区别就带你看看它这是一成雅尔刚才讲了为什么一成雅尔是因为它这个亚根不去见过N之间的这个议单所以说它其实它输入就是训练每一个训练所以它输入是一个一成雅尔的一个训练然后的话我先进行什么大家先进行这个Instance Normal什么意思呢就是实力标准化我们知道标准化这一块的话就是我们其实经常会讲标准化就是一般来讲我去做训练体验证体和测试体购件的时候我去做标准化就是我会把这个训练体就是把这个吹这个训练体它继续它的数据算一个比如说假如我用的是This Call这个标准化我去去算出它的一个均值和这个标准差然后我把这个训练体验证体和测试体都继续训练体的标准差和这个均值然后去做标准化对正式就是传统的一个标准化它这里指出我可以就是我标准化我那个上次你当然去说我标准化之后或者说我不标准化也无所谓它这里会有一个叫实力标准化那实力标准化啥意思呢就是我们知道每一次前向传播你传过来的是以白志赛子来组织的它的意思就是说我怎么去做这个标准化我直接去算每一个白志赛子里头这些样没呢均值和编辑差然后我去之前编辑化之前完之后我在最后输出的时候可以再把它什么样你一编辑化回的就ok了这种的话它就是基于什么类似于我的每次前向传播的一个实力来进行的这个标准化所以这个叫实力标准化它有什么好处呢它的好处就是说它能够应对一些个分布能够缓解一些分布漂浴的问题那什么意思呢今后刚才讲的就是传统的就是我去做的是我可能用的比如说我微而 out test我这三个数据级其实我做标准化的时候都是基于这个吹就是基于这个训练级上的这个均值和编辑差去做的但是有的是我的数据可能它会发生一些分布比如说可能训练级里头的分布和艳证级这次训继的分布是不同的比如有些个数据级它可能训练级的话它可能这些天它可能这个流量不是很高但是到艳证级和这个次训继的时候它可能这个流量变高了那也就是说这三个数据级的分布不一样那以如果说用这个训练级的这个上面的这个这个得到的这个均值和方程去做这两个不能让它可能会带来一些问题对像他人数会有一些所谓的叫分布飘义那这种它就不存在这种问题因为像它是基于实力的其实来到了未来它分布变化了那其实它也是基于它只是基于一个白色色的它是很细密度的所以它能够缓解一定从上能缓解这种所谓的分布飘义的一个问题那当然如果说你的数据不存在这种分布飘义其实你用不用这种实力编辑的话其实影响都不会特别的大那做完标准化之后的话下面一步的话就是派出对那下面我们看这个派出怎么做呢那派出我刚才也讲的其实就是切块了对那当然切块有很多讲究那这片论文它是怎么切的呢我们可以看一下那这里这一件那切块的话在这里有个公式就是说它最后切成的块的课数的话有个N它对R件P出于S加2那这个R是什么的R就是你输入的这个训练的一个长度对于是训练的一个长度P的话就是PatchS就是你这个Patch就是你每一块我到里包含多少个时间创S的话就是说是不长对加2那为什么这里有这个公式怎么得来的S怎么怎么算的那我们这里就是可以简单帮大家理一下那我输入的是是这个R嘛对吧那这个长度是R那那讲是我第一个第一个切出来的矿是P对不对是这样是P然后我开始切第二块第二块的话就是我前切的话就是说因为取代一个问题就是有重叠和无重叠比如说其实跟这个不长有关那当我这个P顿1S的时候其实这个无重叠的状态就像这我这里第一刀切下来了在我前走P因为走S嘛因为S顿P现在现在走P在切第二刀然后再走S向了也是切第三刀P因为S顿P那这种切出来的就是什么呀就是无重叠对那第二种的话就是有重叠那有重叠的话就是我第一刀切完是P那第二刀的话我是往前走什么呀我走S然后切第二刀然后去就这一块是P嘛然后从这一刀切下来的位置往前取啊去P作为第二块然后接着三往前走S切第三刀然后我先去P是不是啊那这样的话它一共能够切出刀的P呢就是能切出刀的P呢其实很简单那怎么的当然太太说了就是我这里的话是首先微胖的S就是会把最后一个值往后延长什么样就是像它一攀领就像一个攀领就是类似于复制S分那也说它其实数的学校长度应该是S加上S就是往后续的一个S嘛对吧所以整体的长度的话就是Ir加上S对不对那下面我们就其实就是去去算这个P的个数了嘛对吧算这个P的个数那其实就看第一刀然后第二刀第三刀然后一张好第四刀然后第五刀然后还在在在多少S其实就是像它们是这里有多少个S再加上前面的这个一刀就是整个这个个数那这个长度整体的长度是多少呢不就是Ir加上S减去这个前面的这个P嘛就是这样然后处理里头有多少个小S就是这边起了多少刀吧对吧再加上前面的这一刀这加E对不对所以整体的N的话就是对于Ir加S减P处于S加E那画点一下的话S那这两个还是加E挪出去呢不就是Ir加P处于S那如果处不进的话我们是往下去嘛所以它这个伏法是这样嘛往下去因为有可能会处不进嘛所以往下去整然后再那个E挪出去把S出S11加E2随时就它的这个公射对对对对但是其实因为这个排RTXT的话它发的比较早但其实在很多后续就继续它这个任务去做的时候大部分其实认为它这个都是做的是无重跌的这种设置那当然它自己做实验的时候它又是有重跌的就是它的P和S是不相等的对我刚才也讲的P和S相等的时候就是任务重跌的那当S小EP的时候它就是什么呀就是有重跌的那当然一般来讲不会出现这个P然后这个小ES这种情况对 基本就前两种情况有重跌的当然这个公射大家就理解了吧所以其实就是类似于前新挂器切成的对 所以这可能应该是比较简单的那切完之后就是一个个小块了那这个块的大小就是排RTXT就一个块有块的对吧一个块有块的那当然它仰视它是切的8块对但是它是一个一个对 切完之后后面其实就是标准的这个就是硬射了嘛就是我把切成的块我要做一次什么呀做一次特征的硬射对 因为切完块之后的话就不是它就是切成了N成一P的一个长度的嘛原来是RR会变成P成一NN的话就是派尺的这个个数了这个P的话就是派尺的这个长度嘛对 我先基于一般是MLP就是圈链间对 或者叫先行层把这个什么样把这个P转化成D那说白了就是我切成了一个块的一个字许链比如说我这里包含了几个时间点我实际上我也不是直接基于这个时间点去做后续的学习我先做一个简单的线硬射把它硬射成P硬射成D对吧 成P硬射成D对它主要是一般来讲我们经常会做这样一个处理就把它做成这个从直接的一个变量做成这么厉害的特征有利于它后续的学习对然后后面当然也会加一个这个什么样这个位置编码对 那当然位置编码的话因为这里是只有N个这个派射所以它位置编码也是相当于编码的是N对 会不会编码N个这个这个这个这个这个位置对 那后续的话就是标准的一个传方法 encoder就是传方我们知道它拔两部分那这个是 encoder后面还有一个 decoder那它就说我不需要后面的 decoder我只是去这个传方法的 encoder就编码的结构去进行这个学习对 学习完之后的话它出来的就是一个地成N那学习完之后这个地成N然后我会把它什么样进行这个占平以及这个什么样这个现形层的操作之后再把它还原成从地成N再后最后还原成什么样一成也这个T对 一成一T那这样的话我就之后重一个一成T变成一成一T那这样的话我就之后就完成了我一个整个的一个预测的一个过程了对 那当然这里的话我一个许臂一个许臂的速度去学就ok了对 那当然那当然我们去真正去写代码的时候一般来讲因为它送进来的可能是这个BNL对吧 那当然之后那个c呢因为一般因为是只有一个特征一般都会强成BNL对吧BNL它一般来讲怎么去做的就非常地因为它是输了是一成L它其实就可以BNL然后把这个BN和A一起那就是BNL其实就是B和N它俩共同做一本质赛子L就是输入了嘛 对然后去许臂这个网络就ok了那当然另外一种方式就是说我输入的是我提前我输了就是一个个的什么样BL就是我在购业许臂的时候把这个N都融到这个许臂的击力头我在这个头去做杀否我每次还是一个个的去白日菜的一个白日菜子的去绑后走那这种好处就待于对显存的占用非常的低那这种的话它还是要包含因为白日菜的线都被放到了嘛放到了N倍 对吧你这个N越大这个上回这里走到白日菜子越大他们对显存会有一定的压力 对那这里的话就就把它这个这个监督的这个 对吧监督学习这一块就预测这个量我应该就讲明白了吧对然后下面的话它还有一个这个这个传方这个扣的这就不说了这个公式都是很飘着的你看看这个位置边嘛你看它是一个N成一地的位置边嘛就是它一定是N个位置嘛它是基于派是嘛它每个派是它是都只有一个位置所以它就是一个N成一地的位置的话就欠入存的照料嘛因为这个每个派是它也会通过这个WP就是通一个线形召唤它从这个原来它就披个点嘛派是再的个点它会转成地位对吧这里其实学习的就是局部性 对吧局部性 局部的一个特征议单那当然我这里就要其实就完全可以基于那个什么样基于那个CNN去实现或者叫失去点地区实现对 那当然你也可以就是一个个的去做这个线形召唤对吧 线形召唤也是可以的后面的话这个船方式这一块就是这个我建议大家就是如果说对船方不理解的就建议还是去补一下船方式的基础的效果比如说它这个NCO的什么叫多头注意力对吧 对吧这些东西这些都是很简单或者说很那个啥的这个我在这个论文竟然就不重复的去讲了对 它基本的话就是三个嘛就QKW然后Code进来的时候就像做成功召唤当然这个K和YL也是一样做完成功召唤现在算注意力举例虽然注意力举例几任之后我去在和YL去做这个聚合对 然后多头其实就是我比如说M个头我就是做M次然后在什么叫做融合对 这就是基本就是船方模针这里它没有做这个合的优化或改进就是用的标准的船方模的这个NCO的这个架构对然后船上航数的话它这里也是用的就MSE嘛那当然我们去做船上航数就是对于侧文提供虽然航数的话又有两种就用的比较多的MSE也是一种对当然如果说你更关注的是这个MSE那也可以用MSE来做你的船上航数对 就是或者说你两种都试试看当中效果会更好对然后这里当然也简单提了一下这个实力标准化刚才也讲了实力标准化和我们传统的这种标准化它的一个意当然我当时也说了就是它主要是去应对这个什么呀去规避或者说怀疑这个它这些说了叫分不飘移嘛对吧 不用飘移这手维分不飘移飘的就是这个训练级上的分布和可能和艳尔吉特尔吉可能有些不同对接着我们去讲一下这个表中学习表中学习其实也很简单其实所谓的表中学习其实工作的其实它走的是这种预训练的架构就是说我可能是基于海量的数据或者说其他领域的数据或者基于自己的一些数据我先做一些预训练然后去提取某些过为度上的一些个特征的一个枪表式然后我拿到这些枪表式我后下游再去接一个简单的网络然后去做一个端端端的一个学习就是所谓的一般来讲就是称为两阶段对那当然我们现在常用的这个做预训练的方式就是就是这个Mascara Automatic Code的就是自编码就是这个就是这个研码的这种什么样自编码器对去做那什么是研码的就说的你反正不用看这些东西那我们直接看这个图就很容易理解它图什么意思呢就是我前面都一样的就我送了一来一个东西送一来还是原始的那个异成也二了然后我下面就看是一样先进行这个Intense Normal就是先进行式一边的话然后把它切片切完之后呢我要干什么呢我随机的去Mascara就Mascara就是研码嘛它就是把比如说原来正式又直到我直接把他们都搞成零都搞成零就所谓把它给折闭住了我研码住了对那我后面也是一样的就是特征转换什么这个加这个这个位置边大家位置边和前面是一致的呀就不是说我掩住了之后我重新重新标这个位置比如说零一二三四五六七就是我进行的位置边嘛的时候我一定编的还是原来的这个位置具体的位置边嘛而不是说我掩完之后它变成零它变成一样这里是需要注意的就是这个掩不掩它不影响后面的位置边嘛其实说我在后宋的时候我把这个对应的这一块它的位置边嘛掩住就ok了那后面当然也是直接给它一个传方文code对然后指头在后面的话我会经过线子那我的任务师干什么呢我的任务变成了我去补其这一块我把这个眼调的东西去补点补出来对所以我发现它整个里头它就没有标签嘛对它输入的是X其实输出的也是X因为因为像它用我输入的X我本身就有标签因为这个这个真直我是知道的只是我把它是真直给它掩住了而已所以它这种方式它称之为叫做自点度对吧所以叫自点度对 监督的是它都是有标签的我输的是X输出的是外对吧它这里输入的是X其实输出的原来它也是X我在损这行处就尽可能让什么样我补出来和我掩掉的真实的它的这个什么样差值越小那我这个模型就越好对我不断地去训练这个那实际上它就能够学习我你总能把这个东西重点的非常好了但证明你对这个输入的这个实蓄的依赖学习的就非常好那我怎么去和下我认为去做呢一般来讲我们就会把这个预训家的厚厚办部分把这个切掉换成我们这个任务头什么意思呢就我只保留这一块只保留这一块那这一块一般来讲的话我们在这个OtomCode就称之为Incode就叫Bamma曾它起到的就是Bamma或者叫欠弱或者叫表证学习的目的拿到这一块就这部分全弄我们就其实就复用这部分全弄然后把这部分全弄和什么样和这个任务头任务头就这一块吧比如说我家做预测了我就把这个任务头和这一块还给拼起来拼起来之后我就可以有几种方式就是说我可以把这个这个这个全弄把它给动结住我只训家厚办部分对吧那这种方式叫做向行探测对吧那当然对于很多大模型的心理人仗做的比如说我预训家一个大模型我的大模型的这个全弄其实上是动结住的就是RM是动结住的我只是在下游的这个任务头我去进行这个参数的这个训念和更新对吧因为相当于我这部分我认为它主要是起到去进行这个特成欠入和特成学习或者叫表证学习的作用下游主要是会的匹匹军机的任务去做的对那当然还有一种方式就是说我预训家完车我接到任务头我现在任务头上像什么样像跑机吧训念机吧然后接着我再把这个底下的这个表征的这个模块我也把它解开也把它训念更新几次对那当然这片的伍头也提到了就是预训念的两种犯事就微调了对吧那这个的话就是它的这个表证学习对应该也比较容易理解吧对那下面的话就说一下它的这个实验的这个情况就是实验的话它这里选取的数据级的话就是基本就是在常试区域策比较主流的一些数据级比如像这个天气交通电力对吧包括这个ET.DH应该是也是类似于这个能源和电力这种情况的数据级对它底下也说了它的这个时间不熟就多少那这里它飞翼串死或者高台也讲了这个飞翼串死在不同的这个数据级里它指带是不同比如说如果说你的须得就一个设施它这飞翼串指带的就是什么样它理想实际的一个特征那如果说你这里头可能你的特征比如说像Tryvig它本身可能就只有一个流量速度它这个飞翼串指带的就是多个路口或者多个路段多个事实对这个也是就是刚玩这个时续与色和生续大家容易缓的地方就是说飞翼串死就是特征变量通道它在不同的论文里可能表震的是不同的含义大家一定要自己去运开然后下面的话就是北斯坦和这个时间一些事儿的当然它也说了对于如果说配置相同的情况它直接引用了这个这片弄文的跑出来的这些结果当然在这片弄文有些配置的情况下因为有时候输入的讯战场和输输的讯战场它没有央队企这种的话它就在自己跑一遍这个实验对然后之后跑出来这个实验结果是这样然后之后我的指标去算MX1和M1这里有需要注意的就是在几乎是所有的MX1和M1就是这种长时间须连预测的这种认识里头它的MX1和M1它都是什么样它去计算的时候都是什么样就是因为你标准化之后因为比如说我这里去预测完了吗不是我做完我去送过网络的时候会有标准化的过程原来我应该是预测结果出来之后你标准化之后我再和真指去算MX1或者是MX1但是在它部分的长时间须预测的弄文里你会发现它们好像都会在你标准化之前就把MX1和MX1算出来了就它好处当然很明显就是说这个数值会显得比较小会比较好看一点第二个还有一个好处就是说因为它用的数据机比较多不同数据机的它的亮钢不大一致如果说你把这个把它逆标准化之后再去算有时候不同的数据机做对比的时候你可能很难去看因为有的数据就是它亮钢很大它的M1算是很大MX1对那边的那在逆标准化的人去算因为它都是统一到类似有点便宜一点到一那种它整体都是小数可能就是会跨数以及去对比的时候可能会更个方面一点但是我的感觉就是可能就是还是你以标准化之后你再去算这个M1和MX1才更容易因为毕竟你这个没有我根本就看不出来到底这个预测的准确都到底是个什么水平然后第二个的话就是这个长时间续在预测我会发现它们好像都不会去算M1P这个指标就是不会去算那个相对的指标那当然大家也可以自己去跑一下这个MVP其实就理解为什么这些个论文它们不去算MVP了对这个我就不好听说对自己去跑一下可以试一下比如说我去强行去算一把这个逆标准化之后的MX1M1以及MVP当然MVP因为它是相对指标所以说你这个你不逆标准去算了MVP其实影响都不大但是你可以去算一下就是为什么这些它们都不去算MVP然后这个Model的一些个变量嘛对它这里说了就是说它去提供的200模型那第一版就要拍TRD64对就是说我这派式的各处是64的那当然它对应的这个回望称呼的种程度就是512它怎么算的呢就刚才那种公式嘛就512然后拍这赛的16所以是512然后减去16然后再出一S的出一B然后再加2这好就等于64那头上第二版本就是412的它对应的长度就336对然后这里的话就开始考了一些结果结果的话它就说它基于相比于这种Transfer Base的这种模型的话它MX1下降的21%然后M1下降的10.7%然后这个42%的话它这个MX1是下降了20%然后MX1是在那些当然前一条件它是和Transfer Base的这些模型对比然后和DDN的去对比的话它相应就没有这个下降幅度了对当然它也说了我是Transfer Base的所以说我肯定就是先跟这个这些Transfer Base的这些模型去对比然后当然我也超过了这个DDN它这也说了就相比于DDN的模型的话我拍的TX1就是Sota1就是最佳的尤其是在这个大的数据集从Wire Traffic和这个电力的数据集然后下面可以看见它这个结果对吧它结果的话就是64和412当然基本都是终于Sota的一个性能但是在部分的数据集和任务上这个DLN依旧是最佳的是最佳的对它也不是说全线都超有纸说大部分它取得了Sota的一个效果对然后后的话它这里还说了一下它这个表证学习的一个性能对 表证学习刚才讲的就是说它是用的是输入1512然后排Sides的话甚至的是12那当然这里它说了就是说它采用的是一种不重点就是拍这个派之前不重点的这种策略这样的话512出一12相像取一整应该是有42个派池对然后42个派池的话就因为它味道和前面的这个什么样前面这个412对起嘛都是412个派池嘛对起这样的话我就可以把这个对应的对应的对应的这个预讯念东西后面的这个任务统去接起来嘛然后它就是说的然后前面不是会释疑到这个Mask的一个过程就是会把这个随机去演掉一起个这个派池然后它演掉的这个概率的话是40会把40%的这个派池可以演掉然后去进行这个学习演的方式也非常简单就是通过这种Zer路的方式就是把它直接制零对然后的话大家首先的话会把这个自监度的这种预讯念在这种在一些数据机上然后去预讯念一个100个Input对然后一大预讯念的模型完成了以后然后下面的话就是会接上这个任务头然后去做它有两种方式地中我刚才讲的就相信探测就是我这样任务头是洞节住这个预讯念就洞节住它的这个身子只做这个任务头上参数的更新和训练第二种的话叫做端端的微调端端的微调就是说我我也是先做这个线探测我做10个Input可做完10个Input可之后我把这个任务头和我前面的那个整个身子就是整个模型的所以那个洞节内部分我也把它放开这样的话整个模型直接做端端的微调对做端端的微调然后微调个20个Input可对它认为就是这种当然在其他的这种两阶段的这种策略就是线探测然后再做这种微调的方式在其他的线里有一个其实取得了一些个比较好的一个成果所以它还直接拿我来用对然后它选取了一些结果来说明了一下它这个东西你看它这说的还是TNT对吧然后这里的话这个就是这个第二种策略然后这个的话是第一种策略就是像探测这个是做微调然后这个的话是做这个字眼睛字眼睛就说我不去做表证学习直接采用这个就这种方式对吧端端的这种学习然后它就发现的话就是这个表它就可以出来就是我通过这种这种这种什么样微调的饭式的话实际上它在某些个书记和领域上其实还是取得了这个领下的一个效果对对像这个效果当然的话在这个书记上有发现其实反而是字眼睛更好可能这种这种表证学习并没有取得更好的一个结果对其实就是说在某些书记上这种表证学习就运行占这种饭式可能能起到增强的作用对这个可以做一下测试然后下面的话它又做了这个做和这个对比就是因为它是做表证学习的所以它还和其他的一些做这个签入的比如说TX2VQaBTSLFaTCTSTC它这个怎么做呢它也说了就是说我数十年训练之后数十年训练之后然后后的话我把他们用这些方法我都去做签入比如拍着TXTBTST和这些都做同样的一个签入但是我下游的话我只是接一个什么呢我只是接一个线探设就是做一个简单的那个线用转换直接就是匹配下游任务然后我去做对比然后它这里就是说明就是在这种资奸度的这种学习下它这种拍着TXT也就取得非常好的那这也得还有一个Transfer的是什么意思呢它这里也说了就Transfer的就是意思就是说我的这个预讯练是在这个ETTH1上去做的对但是我测试的话可能就是说在其他数以下你去做了测试对吧我看它说Bus的福人母预讯练它自己的ETTH1上去做的预讯练它自己的ETTH1上去做的就是我的这个预讯练是ETTH1上去练的然后我测试也是在ETTH1上然后Transfer的它就是意思就是说我是在这个交通数以上去做的预讯练但是我测试是在ETTH1上但是无论是哪种方式无论是千亿范实下还是说这个字颜度那种范实下我都是优于这些了其他的一些的这种做表情学习或签入学习的方法的对接着S3的话它是做了一下消除实验消除实验因为它的合音创一点只有两个所以说它的消除实验的话就做了四组了那第1个就是说我这个PatchPatch和这个通道独立都加上对第二个就是我指加通道独立但是我不加Patch那第三个的话就是我我指加Patch不加通道独立然后另外一种的话就是我两者都不加对那两者都不加的情况下这里看它发现有很多都空的制度因为我们刚才也讲了因为两者都不加的时候你这个PatchPatch的话没有Patch的话就是你通道越长你这个训练就干啥就那个那个那个那个注意力的那个器算的复杂就非常高对于显存或者说这个运算这个战友都非常高那这两个书以及恰好它这个你看在训练比较长的时候它确实算不太好第二个就是说它没有采用这种通道独立那可能它会见抹这个训练和训练之间那这几个书以及的话它也会就训练比较多嘛前面也看了就是Y的那几个你看它是这个训练的各书训练的各书的话训练的各书你看看这几个什么Y的Crafting 这几个都862 321它训练的各书也比较多所以说它整体的这个什么样就会出现它就说它在这个它这个单卡上去训练就是这个因为大AA0就48G上面它会出现那个内锤移出或者说这个7大时间太久所以说它就没有把这个天上对 这的话偏偏这么地就我PatchCM的时候效果是最好的相比于这个O2方法相比于这个方法里头这个比较搜它们的房型PD方法的话也是也是点下很多的无论是MS1还没还是这个M1对 当然你还看到想反映这个M1和M1它这个值都是小于一的就是说它们都是在你一段时间之前就把这辆值去算出来算掉对 然后下面的话就是做这个参数敏感性的一些实验就是取不同的这个狂状床口去做然后去探究它这个变化就是D-Linux它就输就是很多船方法的这个Best模型的话其实它对这个回望窗口的长度其实不怎么敏感甚至你把这个输入的长度增加了它反而性能还下降了对 比如像这里在电视书上你比如像这个E-former你看 你这个给它的越长它反而这个M1越长越长 当然有限模型它是基本没什么变化有限模型就是可能先下降后上升也有说其实其他模型对于这个回望窗口其实不怎么敏感或者它不能够有效的去学习这种长式续的一来但是拍着TNT也会发现随着这个回望窗口的增加它都是有一个什么样下降的这种趋势对 原来都能带来一个性能的一个提升对 然后这个最后的话就做了一下整体的一个总结对 那整体的一个总结整体的一个总结整体的一个总结整体的一个总结整体的一个总结整体的一个总结整体的一个总结整体的一个总结整体的一个总结整体的一个总结整体的一个总结整体的一个总结原来就是不去见过空间一来对 但是人家从这个发生上去讲就能讲中很多优点来对 但是从我个人的理解上来讲的话就是对于一心的比如说明显一个续节和续节之间它有很多这种一来关系的时候如果说采用这种通道独立我觉得它的上线肯定是不如那种我时间和空间都去考虑的对 只是恰好可能在有些数据以上可能确实通道之间的这种相关性反而不强所以说它可能拍着TXT会占用也就是说其实是否这个独立师傅就是说对我的这个任务有这个有提升其实我觉得更多的是取缘于数据级的特点对 而不是说某种价格它一定是优的比如说拍TXT只出通道独立师优的就它就通杀所有数据级不会的有些数据级它价格人家可能就是空间一来非常明显你如果不用空间一来它肯定是你的模型的上线会被拉低对 但有些模型天然就它虚烈和虚烈之间价格人不存在太多的依赖你反而你如果强行去做这个通道的之间的融合反而会带来当它也讲的就是会带来一些信息的一个混杂反而会也好到你当前虚烈的一个预测了或者说见过了对然后第二个的话就是说它所谓的这个派池刚才也讲了其实派池这种方式的话其实更多的有点类似于这种声音就是我先做一地路的声音把它这个虚烈的传络给它拉短然后再做到什么方法然后以及在其他领域比如像这个Bort它就是自于颜领域和这个WIT就是CV领域很多领域其实这种派池的思想也都是用它很多它只是签议到这个上面所以整体这个论文它是一种组合式的创新对 但是确实整体的效果还是不错的对整个故事讲的也比较的完整所以它也是叶叶就是后续很多的论文也都是基于它来做的那最多可能是在派池上最先文章比如说这个派池TNT它可能它做的就是说它的派池是固定的吗对 比如说是这个是16它整个网络就是16但有些人又会觉得我这个派池我是不是可以多样化不同数义级它应该是不一致的或者说我一个数义级我应该是这多个派池那多个派池它是去提取这个不同力度上的局部特征然后再去做融合或者做这个路由就MOE的思想对 这是做优化的当然还有一个在自在这个预讯量或者叫表诺选行这样去做 去做文章的就是也是基于这种饭式我去做一些任务的蹭墙但是我可能不是简单的只是计划这个任务头可能我这个架过我是用来做其他一些蹭墙比如说我用来做了一个空间信息的一个蹭墙然后用于做时间信息的一个蹭墙然后我再和主体的这个预测的这个白个帮去结合然后去做的 当然现在的尤其是今年24年挺多论文其实都是基于这个评论学习或者叫签入学习或者叫预讯量墙这个上面去做的当然整体的论文我觉得鞋单还是很OK的然后带板然后带铁也可以自己去看一下也写的是比较清晰的整片论文大家还是知道大家去进入一下我们今天的话就讲到这里吧那时间也挺久了然后大家如果说感觉我讲的还可以的话就是大家三点支持一下好 谢谢大家
