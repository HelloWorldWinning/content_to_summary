Timestamp: 2025-01-24T14:14:33.269281
Title: Titans: Learning to Memorize at Test Time
URL: https://arxiv.org/html/2501.00663v1
Status: success
Duration: 0:00

Description:
好的，这是对您提供的文本的总结，包括核心观点、基本观点、总体框架和一个概念图：

**总结：**

1. **核心观点 (Core Point):**  Titans模型通过结合一个可学习的、长期的神经记忆模块和注意力机制，在多种任务中超越了传统Transformer和线性循环模型，特别是在处理长序列时表现出色。

2. **基本观点 (Fundamental Point):** 该论文提出了一种新的神经长期记忆模块，它通过学习在测试时记忆和遗忘信息，结合人类的记忆机制，来增强序列模型的性能，并提出三种使用此记忆模块的架构变体。

3. **总体框架 (Overarching Framework):**
    *   **背景:** 探讨了Transformer和线性循环模型的局限性，以及人类记忆机制对模型设计的启发。
    *   **神经记忆模块:** 介绍了可学习的神经长期记忆，包括如何测量“惊讶”、记忆更新和遗忘机制。
    *   **Titans架构:** 提出了三种使用神经记忆模块的架构：记忆作为上下文 (MAC)、记忆作为门控 (MAG) 和记忆作为层 (MAL)。
    *   **实验评估:** 在语言建模、常识推理、针在干草堆、基因组学和时间序列预测任务中评估了Titans的性能。
    *   **结论:** Titans模型在多个任务上表现优于现有模型，特别是在处理长序列时。

**概念图 (Conceptual Map):**

<Mermaid_Diagram>
  graph LR
    subgraph Memory_Module [神经记忆模块]
        direction TB
        A[长期记忆] --> B(学习记忆/遗忘)
        B --> C{惊讶度测量}
        C --> D[梯度计算]
        D --> E[记忆更新]
        E --> F[衰减机制]
    end

    subgraph Titans_Architecture [Titans架构]
        direction TB
        G(核心模块) --> H{短时记忆 (注意力)}
        I[长期记忆模块] --> J(存储/记忆)
        K[持久记忆模块] --> L(任务知识)
    end

     subgraph Titans_Variants [Titans变体]
       direction LR
       M[记忆作为上下文 (MAC)] -- 连接 --> H
       N[记忆作为门控 (MAG)] -- 门控 --> H
       O[记忆作为层 (MAL)] -- 层叠 --> H
     end

    Memory_Module --> Titans_Architecture
    Titans_Architecture --> Titans_Variants
     H --> P[实验评估]
    Titans_Variants --> P
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#9f9,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#99f,stroke:#333,stroke-width:2px
     style F fill:#ccf,stroke:#333,stroke-width:2px

     style G fill:#eee,stroke:#333,stroke-width:2px
     style H fill:#cdf,stroke:#333,stroke-width:2px
     style I fill:#acf,stroke:#333,stroke-width:2px
    style J fill:#aaf,stroke:#333,stroke-width:2px
     style K fill:#fcc,stroke:#333,stroke-width:2px
     style L fill:#faa,stroke:#333,stroke-width:2px
        
    style M fill:#bbf,stroke:#333,stroke-width:2px
    style N fill:#bbd,stroke:#333,stroke-width:2px
    style O fill:#ccb,stroke:#333,stroke-width:2px

     style P fill:#ada,stroke:#333,stroke-width:2px

</Mermaid_Diagram>


Content:
\addbibresource main.bib Titans: Learning to Memorize at Test Time Ali Behrouz Peilin Zhong Vahab Mirrokni Abstract Over more than a decade there has been an extensive research effort of how effectively utilize recurrent models and attentions. While recurrent models aim to compress the data into a fixed-size memory (called hidden state), attention allows attending to the entire context window, capturing the direct dependencies of all tokens. This more accurate modeling of dependencies, however, comes with a quadratic cost, limiting the model to a fixed-length context. We present a new neural long-term memory module that learns to memorize historical context and helps an attention to attend to the current context while utilizing long past information. We show that this neural memory has the advantage of a fast parallelizable training while maintaining a fast inference. From a memory perspective, we argue that attention due to its limited context but accurate dependency modeling performs as a short-term memory, while neural memory due to its ability to memorize the data, acts as a long-term, more persistent, memory. Based on these two modules, we introduce a new family of architectures, called Titans, and present three variants to address how one can effectively incorporate memory into this architecture. Our experimental results on language modeling, common-sense reasoning, genomics, and time series tasks show that Titans are more effective than Transformers and recent modern linear recurrent models. They further can effectively scale to larger than 2M context window size with higher accuracy in needle-in-haystack tasks compared to baselines. 1 Introduction “The true art of memory is the art of attention!" — Samuel Johnson, 1787 \lettrine [lines=3]Transformers, pure attention-based architectures \parencite transformers, have been firmly established as state-of-the-art models in sequence modeling, mainly due to their in-context learning and ability to learn at scale \parencite kaplan2020scaling. The primary building blocks of Transformers–attention modules—function as associative memory blocks \parencite bietti2024birth, where they learn to store key-value associations and retrieve them by computing pairwise similarity between queries (i.e., search signals) and keys (i.e., contexts). Accordingly, by design, the output of a Transformer is exclusively conditioned on the direct dependencies of tokens in the current context window. This accurate modeling of dependencies, however, comes with quadratic time and memory complexity in terms of the context length. In complex real-world tasks (e.g., language modeling \parencite liu2024lost, video understanding \parencite wu2019long, long-term time series forecasting \parencite zhou2021informer), the context window can become extremely large, making the applicability of Transformers challenging in these downstream tasks. To overcome the scalability issue of Transformers, recent studies aim to design different variants of linear Transformers \parencite katharopoulos2020transformers, kacham2024polysketchformer, yang2024gatedattn, where softmax is replaced by a kernel function in the attention ( see § 2.1 for details ), resulting in a significant drop in memory consumption. Despite efficiency and the ability to scale to longer context, linear Transformers do not show competitive performance compared to Transformers as the kernel trick makes the model a linear recurrent network, in which the data is compressed into a matrix-valued states \parencite katharopoulos2020transformers. This, however, brings a contradictory fact about linear recurrent (or linear Transformers) models: On one hand, we use these linear models to enhance scalability and efficiency (linear vs. quadratic complexity), whose advantages is appeared for very long context; On the other hand, a very long context cannot be properly compressed in a small vector-valued or matrix-valued states \parencite wang2024longssm. Furthermore, beyond efficiency, most existing architectures–ranging from Hopfield Networks \parencite hopfield1982neural to LSTMs \parencite LSTM and Transformers \parencite transformers–face challenges when dealing with generalization, length extrapolation, and/or reasoning \parencite anil2022exploring, qin2024exploring, all of which are inseparable parts of many hard real-world tasks. Although these architectures draw inspiration from the human brain, each of which are missing: (1) a crucial component for learning process—such as short-term memory, long-term memory, meta-memory, attending to current context, etc. \parencite cowan2008differences; (2) how these components are interconnected systems that can operate independently; and/or (3) the ability to actively learn from data and memorize the abstraction of past history. We argue that in an effective learning paradigm, similar to human brain, there are distinct yet interconnected modules, each of which is responsible for a component crucial to the learning process. Memory Perspective Memory is a fundamental mental process and is an inseparable component of human learning \parencite terry2017learning. Without a properly functioning memory system, humans and animals would be restricted to basic reflexes and stereotyped behaviors. Accordingly, memory has been the inspiration for many seminal research in machine learning literature; e.g., Hopfield Networks \parencite hopfield1982neural, LSTMs \parencite LSTM, and Transformers \parencite transformers. Taking inspiration from the common definitions of memory and learning in neuropsychology literature \parencite okano2000learning, most existing architectures consider memory as a neural update caused by an input, and define learning as a process for acquiring effective and useful memory, given an objective. In this perspective, Recurrent Neural Networks (RNNs) \parencite williams1989learning can be defined as models with a vector-valued memory module ℳ ℳ \mathcal{M} caligraphic_M (also called hidden state) with two main steps: Given a new input x t subscript 𝑥 𝑡 x_{t} italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at time t 𝑡 t italic_t , the model (1) updates the memory using a function f ⁢ ( ℳ t − 1 , x t ) 𝑓 subscript ℳ 𝑡 1 subscript 𝑥 𝑡 f(\mathcal{M}_{t-1},x_{t}) italic_f ( caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) (with compression); and (2) retrieves the corresponding memory of input using a function g ⁢ ( ℳ t , x t ) 𝑔 subscript ℳ 𝑡 subscript 𝑥 𝑡 g(\mathcal{M}_{t},x_{t}) italic_g ( caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ( see § 2.1 for details ). Similarly, Transformers can be seen as architectures with a growing memory and two similar steps. That is, the pair of key and value matrices acts as the model’s memory, and the model: (1) updates the memory by appending the key and value to the memory (without compression), and (2) retrieves query vectors’ corresponding memory by finding the similarity of query and key vectors, which is then used to weight the value vectors for the output. This perspective, can help us better understand existing paradigms, their critical differences, and design more effective architectures. For example, the main difference between Transformers \parencite transformers and linear Transformers \parencite katharopoulos2020transformers is the memory structure as well as the memory updating step, in which linear Transformers compress the historical data into a fixed-size matrix-valued memory while Transformers keep all historical data (within the context length) without any compression. While both linear Transformers and linear RNNs (including state space models) compress the information in memory update step, the critical difference lies in the structure of the memory, where linear RNNs (vs. linear Transformers) use a vector-valued memory (vs. matrix-valued memory). Therefore, this perspective motivates us to ask: (Q1) What constitute a good structure for the memory? (Q2) What is a proper memory update mechanism? and (Q3) What is a good memory retrieval process? Revisiting our understanding of human memory, it is neither a unitary process nor it serves a single function \parencite cowan2008differences. In fact, memory is a confederation of systems–e.g., short-term, working, and long-term memory–each serving a different function with different neural structures, and each capable of operating independently \parencite willingham1997systems. This fact motivates us to ask: (Q4) How to design an efficient architecture that incorporates different interconnected memory modules. Finally, storing a memory is a neural process that requires to encode and store the abstraction of the past. It can be over-simplification to assume a single vector or a matrix, whose parameters are encoding the data in a linear manner, are enough for storing long-term history. (Q5) Is a deep memory module needed to effectively store/remember long past? Contributions and Roadmap In this paper, we aim to answer the above five questions by designing a long-term neural memory module, that can efficiently and effectively learn to memorize at test time. Building upon its design, we discuss how it can be incorporated into an architecture. Neural Memory (§ 3 ) .
We present a (deep) neural long-term memory that (as a meta in-context model) learns how to memorize/store the data into its parameters at test time. Inspired by human long-term memory system \parencite mandler2014structure, we design this memory module so an event that violates the expectations (being surprising) is more memorable. To this end, we measure the surprise of an input with the gradient of the neural network with respect to the input in associative memory loss ( see § 3.1 for details ). To better handle the limited memory, we present a decaying mechanism that consider the proportion of memory size and the amount of data surprise, resulting in better memory management. We show that this decay mechanism is in fact the generalization of forgetting mechanism in modern recurrent models \parencite dao2024transformers, yang2024gated, gu2024mamba. Interestingly, we find that this mechanism is equivalent to optimizing a meta neural network with mini-batch gradient descent, momentum, and weight decay. Building upon tensorizing mini-batch gradient descent to use more matmul operations \parencite sun2024learning, we present a fast and parallelizable algorithm to train our deep neural long-term memory. Titans Architectures (§ 4 ). After designing the long-term neural memory, an important remaining question is how to effectively and efficiently incorporate memory into a deep learning architecture. We present Titans, a family of deep models that consists of three hyper-heads: (1) Core: this module consists of the short-term memory, and is responsible for the main flow of processing the data (we use attention with limited window size); (2) Long-term Memory: this branch is our neural long-term memory module that is responsible to store/remember long past; (3) Persistent Memory: this is a set of learnable but date-independent parameters that encodes the knowledge about a task. Finally, as a proof of concept, we present three variants of Titans, in which we incorporate memory as: (i) a context, (ii) a layer, and (iii) a gated branch. Experimental Results (§ 5 ) .
We perform experimental evaluations on language modeling, commonsense reasoning, recall-intensive, needle in haystack, time series forecasting, and DNA modeling tasks. We observe that our Titan architecture outperforms all modern recurrent models as well as their hybrid variants (combining with sliding-window attention) across a comprehensive set of benchmarks. Furthermore, Titans outperforms Transformers with the same context window, and show competitive performance with Transformers that use the entire context. This results are achieved while, contrary to Transformers, Titans scale to larger than 2M context window size. 2 Preliminaries \lettrine [lines=3]In this section, we discuss the notation and some background concepts that we use though the paper. We let x ∈ ℝ N × d in 𝑥 superscript ℝ 𝑁 subscript 𝑑 in x\in\mathbb{R}^{N\times d_{\text{in}}} italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT end_POSTSUPERSCRIPT be the input, ℳ ℳ \mathcal{M} caligraphic_M be a neural network (neural memory module), 𝐐 , 𝐊 , 𝐕 𝐐 𝐊 𝐕 \mathbf{Q},\mathbf{K},\mathbf{V} bold_Q , bold_K , bold_V be the query, key and value of the attention mechanism, and 𝐌 𝐌 \mathbf{M} bold_M be the attention mask. When segmenting the sequence, we use S ( i ) superscript S 𝑖 \texttt{S}^{(i)} S start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT to refer to the i 𝑖 i italic_i -th segment. Through the paper, we abuse the notation and use subscripts to refer to a specific element of a matrix, vector, or segments. For example, we let S j ( i ) subscript superscript S 𝑖 𝑗 \texttt{S}^{(i)}_{j} S start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT be the j 𝑗 j italic_j -th token in the i 𝑖 i italic_i -th segment. The only exception is subscripts with t 𝑡 t italic_t , which we reserved to index recurrence over time, or the state of a neural network at time t 𝑡 t italic_t . Given a neural network 𝒩 𝒩 \mathcal{N} caligraphic_N and a data sample x 𝑥 x italic_x , we use 𝒩 ⁢ ( x ) 𝒩 𝑥 \mathcal{N}(x) caligraphic_N ( italic_x ) (resp. 𝒩 ∗ ⁢ ( x ) superscript 𝒩 𝑥 \mathcal{N}^{*}(x) caligraphic_N start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_x ) ) to refer to the forward pass with (resp. without) weight adjustment. Also, we abuse the notation and use 𝒩 ( k ) superscript 𝒩 𝑘 \mathcal{N}^{(k)} caligraphic_N start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT to refer to the k 𝑘 k italic_k -th layer of the neural network. In the following, we first, discuss the backgrounds for attention and its efficient variants followed by a review of modern linear RNNs. Finally, we discuss a memory perspective of these architectures that motivates us to design Titans. 2.1 Backgrounds Attention. Transformers \parencite transformers as the de facto backbone for many deep learning models are based on attention mechanism. Given input x ∈ ℝ N × d in 𝑥 superscript ℝ 𝑁 subscript 𝑑 in x\in\mathbb{R}^{N\times d_{\text{in}}} italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , causal attention computes output 𝐲 ∈ ℝ N × d in 𝐲 superscript ℝ 𝑁 subscript 𝑑 in \mathbf{y}\in\mathbb{R}^{N\times d_{\text{in}}} bold_y ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT end_POSTSUPERSCRIPT based on softmax over input dependent key, value, and query matrices: 𝐐 = x ⁢ 𝐖 𝐐 , 𝐊 = x ⁢ 𝐖 𝐊 , 𝐕 = x ⁢ 𝐖 𝐕 , formulae-sequence 𝐐 𝑥 subscript 𝐖 𝐐 formulae-sequence 𝐊 𝑥 subscript 𝐖 𝐊 𝐕 𝑥 subscript 𝐖 𝐕 \displaystyle\mathbf{Q}=x\mathbf{W}_{\mathbf{Q}},\qquad\mathbf{K}=x\mathbf{W}_%
{\mathbf{K}},\qquad\mathbf{V}=x\mathbf{W}_{\mathbf{V}}, bold_Q = italic_x bold_W start_POSTSUBSCRIPT bold_Q end_POSTSUBSCRIPT , bold_K = italic_x bold_W start_POSTSUBSCRIPT bold_K end_POSTSUBSCRIPT , bold_V = italic_x bold_W start_POSTSUBSCRIPT bold_V end_POSTSUBSCRIPT , (1) 𝐲 i = ∑ j = 1 i exp ⁡ ( 𝐐 i ⊤ ⁢ 𝐊 j / d in ) ⁢ 𝐕 j ∑ ℓ = 1 i exp ⁡ ( 𝐐 i ⊤ ⁢ 𝐊 ℓ / d in ) , subscript 𝐲 𝑖 superscript subscript 𝑗 1 𝑖 superscript subscript 𝐐 𝑖 top subscript 𝐊 𝑗 subscript 𝑑 in subscript 𝐕 𝑗 superscript subscript ℓ 1 𝑖 superscript subscript 𝐐 𝑖 top subscript 𝐊 ℓ subscript 𝑑 in \displaystyle\mathbf{y}_{i}=\sum_{j=1}^{i}\frac{\exp\left(\mathbf{Q}_{i}^{\top%
}\mathbf{K}_{j}/\sqrt{d_{\text{in}}}\right)\mathbf{V}_{j}}{\sum_{\ell=1}^{i}%
\exp\left(\mathbf{Q}_{i}^{\top}\mathbf{K}_{\ell}/\sqrt{d_{\text{in}}}\right)}, bold_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT divide start_ARG roman_exp ( bold_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_K start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT / square-root start_ARG italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT end_ARG ) bold_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG start_ARG ∑ start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT roman_exp ( bold_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_K start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT / square-root start_ARG italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT end_ARG ) end_ARG , (2) where 𝐖 𝐐 , 𝐖 𝐊 , subscript 𝐖 𝐐 subscript 𝐖 𝐊 \mathbf{W}_{\mathbf{Q}},\mathbf{W}_{\mathbf{K}}, bold_W start_POSTSUBSCRIPT bold_Q end_POSTSUBSCRIPT , bold_W start_POSTSUBSCRIPT bold_K end_POSTSUBSCRIPT , and 𝐖 𝐕 ∈ ℝ d in × d in subscript 𝐖 𝐕 superscript ℝ subscript 𝑑 in subscript 𝑑 in \mathbf{W}_{\mathbf{V}}\in\mathbb{R}^{d_{\text{in}}\times d_{\text{in}}} bold_W start_POSTSUBSCRIPT bold_V end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT end_POSTSUPERSCRIPT are learnable parameters. Despite the power and effectiveness in recall, transformers need at least N × d 𝑁 𝑑 N\times d italic_N × italic_d operators to calculate the output, resulting in larger memory consumption and lower-throughput for longer sequences. Efficient Attentions. To improve the memory consumption and throughput of softmax attention for longer sequences, various studies focused on I/O aware implementations of attention \parencite flashattention-1, dao2024flashattention, designing more efficient attention mechanisms by sparsifying the attention matrix \parencite choromanski2021rethinking, dai2019transformerxl, chen2021scatterbrain, approximating the softmax \parencite arora2024simple, or developing kernel-based (linear) attentions \parencite kacham2024polysketchformer, schlag2021linear, yang2024gatedattn, aksenov2024linear. In this part, we focus on the later, i.e., linear attentions, where the softmax in standard attention is replaced with an alternative kernel function ϕ ( . , . ) \phi(.,.) italic_ϕ ( . , . ) , such that ϕ ⁢ ( x , y ) = ϕ ⁢ ( x ) ⁢ ϕ ⁢ ( y ) italic-ϕ 𝑥 𝑦 italic-ϕ 𝑥 italic-ϕ 𝑦 \phi(x,y)=\phi(x)\phi(y) italic_ϕ ( italic_x , italic_y ) = italic_ϕ ( italic_x ) italic_ϕ ( italic_y ) . Accordingly, the attention can be written as: 𝐲 i = ∑ j = 1 i ϕ ⁢ ( Q i ⊤ ⁢ K j ) ∑ ℓ = 1 i ϕ ⁢ ( Q i ⊤ ⁢ K ℓ ) ⁢ V j = ∑ j = 1 i ϕ ⁢ ( Q i ) ⊤ ⁢ ϕ ⁢ ( K j ) ∑ ℓ = 1 i ϕ ⁢ ( Q i ) ⊤ ⁢ ϕ ⁢ ( K ℓ ) ⁢ V j = ϕ ⁢ ( Q i ) ⊤ ⁢ ∑ j = 1 i ϕ ⁢ ( K j ) ⁢ V j ϕ ⁢ ( Q i ) ⊤ ⁢ ∑ ℓ = 1 i ϕ ⁢ ( K ℓ ) , subscript 𝐲 𝑖 superscript subscript 𝑗 1 𝑖 italic-ϕ superscript subscript 𝑄 𝑖 top subscript 𝐾 𝑗 superscript subscript ℓ 1 𝑖 italic-ϕ superscript subscript 𝑄 𝑖 top subscript 𝐾 ℓ subscript 𝑉 𝑗 superscript subscript 𝑗 1 𝑖 italic-ϕ superscript subscript 𝑄 𝑖 top italic-ϕ subscript 𝐾 𝑗 superscript subscript ℓ 1 𝑖 italic-ϕ superscript subscript 𝑄 𝑖 top italic-ϕ subscript 𝐾 ℓ subscript 𝑉 𝑗 italic-ϕ superscript subscript 𝑄 𝑖 top superscript subscript 𝑗 1 𝑖 italic-ϕ subscript 𝐾 𝑗 subscript 𝑉 𝑗 italic-ϕ superscript subscript 𝑄 𝑖 top superscript subscript ℓ 1 𝑖 italic-ϕ subscript 𝐾 ℓ \displaystyle\mathbf{y}_{i}=\sum_{j=1}^{i}\frac{\phi(Q_{i}^{\top}K_{j})}{\sum_%
{\ell=1}^{i}\phi(Q_{i}^{\top}K_{\ell})}\>V_{j}=\sum_{j=1}^{i}\frac{\phi(Q_{i})%
^{\top}\phi(K_{j})}{\sum_{\ell=1}^{i}\phi(Q_{i})^{\top}\phi(K_{\ell})}\>V_{j}=%
\frac{\phi(Q_{i})^{\top}\sum_{j=1}^{i}\phi(K_{j})V_{j}}{\phi(Q_{i})^{\top}\sum%
_{\ell=1}^{i}\phi(K_{\ell})}, bold_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT divide start_ARG italic_ϕ ( italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_K start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_ϕ ( italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_K start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT ) end_ARG italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT divide start_ARG italic_ϕ ( italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_ϕ ( italic_K start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_ϕ ( italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_ϕ ( italic_K start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT ) end_ARG italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = divide start_ARG italic_ϕ ( italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_ϕ ( italic_K start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG start_ARG italic_ϕ ( italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_ϕ ( italic_K start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT ) end_ARG , (3) resulting in a higher-throughput as terms ∑ j = 1 i ϕ ⁢ ( K j ) superscript subscript 𝑗 1 𝑖 italic-ϕ subscript 𝐾 𝑗 \sum_{j=1}^{i}\phi(K_{j}) ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_ϕ ( italic_K start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) and ∑ ℓ = 1 i ϕ ⁢ ( K ℓ ) superscript subscript ℓ 1 𝑖 italic-ϕ subscript 𝐾 ℓ \sum_{\ell=1}^{i}\phi(K_{\ell}) ∑ start_POSTSUBSCRIPT roman_ℓ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT italic_ϕ ( italic_K start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT ) are re-using in each step. When choosing the kernel as identity matrix \parencite sun2023retentive, the above formulation can also be written in a recurrent format: ℳ t = ℳ t − 1 + K t ⊤ ⁢ V t , subscript ℳ 𝑡 subscript ℳ 𝑡 1 superscript subscript 𝐾 𝑡 top subscript 𝑉 𝑡 \displaystyle\mathcal{M}_{t}=\mathcal{M}_{t-1}+K_{t}^{\top}V_{t}\>, caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT + italic_K start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_V start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , (4) 𝐲 t = Q t ⁢ ℳ t , subscript 𝐲 𝑡 subscript 𝑄 𝑡 subscript ℳ 𝑡 \displaystyle\mathbf{y}_{t}=Q_{t}\mathcal{M}_{t}\>, bold_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_Q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , (5) which allows efficient inference for linear attentions. Modern Linear Models and Their Memory Perspective. As discussed earlier, one can define learning as a process for acquiring effective and useful memory. Building upon this, one can see the hidden state of Recurrent Neural Networks (RNNs) as a memory unit, which the model aims to compress the information into. Accordingly, in a general form of recurrent neural network, the hidden state can be treated as a memory unit and the recurrence process can be split into the read and write operations in the memory unit. That is, we let x ∈ ℝ N × d in 𝑥 superscript ℝ 𝑁 subscript 𝑑 in x\in\mathbb{R}^{N\times d_{\text{in}}} italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT end_POSTSUPERSCRIPT be the input, ℳ ∈ ℝ d ℳ superscript ℝ 𝑑 \mathcal{M}\in\mathbb{R}^{d} caligraphic_M ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT is the memory unit, and 𝐲 ∈ ℝ d in 𝐲 superscript ℝ subscript 𝑑 in \mathbf{y}\in\mathbb{R}^{d_{\text{in}}} bold_y ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT end_POSTSUPERSCRIPT is the output, then the general form of the recurrent neural network is defined as: ℳ t = f ⁢ ( ℳ t − 1 , x t ) , subscript ℳ 𝑡 𝑓 subscript ℳ 𝑡 1 subscript 𝑥 𝑡 \displaystyle\qquad\qquad\qquad\qquad\mathcal{M}_{t}=f(\mathcal{M}_{t-1},x_{t}%
),\qquad\qquad caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_f ( caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , Write Operation (6) 𝐲 t = g ⁢ ( ℳ t , x t ) , subscript 𝐲 𝑡 𝑔 subscript ℳ 𝑡 subscript 𝑥 𝑡 \displaystyle\qquad\qquad\qquad\qquad\mathbf{y}_{t}=g(\mathcal{M}_{t},x_{t}),\qquad\qquad bold_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_g ( caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , Read Operation (7) where f ( . , . ) f(.,.) italic_f ( . , . ) is the read and g ( . , . ) g(.,.) italic_g ( . , . ) is the write corresponding functions. Note that here the subscript of ℳ t subscript ℳ 𝑡 \mathcal{M}_{t} caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT shows the state of the memory at time t 𝑡 t italic_t . In this perspective, the recurrence formula of linear Transformers (see Equation 4 ) is equivalent to additively compress and write keys and values, ( K t , V t ) subscript 𝐾 𝑡 subscript 𝑉 𝑡 (K_{t},V_{t}) ( italic_K start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , into a matrix-valued memory unit ℳ t subscript ℳ 𝑡 \mathcal{M}_{t} caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . Therefore, when dealing with long context data, this additive nature of the process results in memory overflow, significantly damaging the performance of the model. To address this, studies have focused on two promising directions: (1) Adding forget mechanism: several studies have presented adaptive (data-dependent) forgetting gate mechanisms for linear models, where it can erase the memory when it is needed. As examples of such models, we refer to GLA \parencite yang2024gatedattn, LRU \parencite orvieto2023resurrecting, Griffin \parencite de2024griffin, xLSTM \parencite beck2024xlstm, and Mamba2 \parencite dao2024transformers, which the later is also connected to the discretized version of traditional state space models \parencite gu2024mamba.(2) Improving the write operation: To overcome the additive nature of memory write operation in traditional recurrent models, \textcite widrow1988adaptive presented Delta Rule, in which before adding a memory (i.e., a pair of key and value), the model first removes its past value. To enhance the parallelizable training and scaling, \textcite yang2024parallelizing present a fast paralellizable algorithm. Finally, very recently, \textcite yang2024gated improved the DeltaNets by adding a forget gate. Memory Modules. Memory has always been one of the core parts of the neural network designs \parencite schmidhuber1992learning, LSTM, graves2014neuralturingmachines, zhang2024memory. The idea of seeing linear layers as the key-value (associative) memory system backs to fast weight programs, in which dynamic fast programs are incorporated into recurrent neural networks to serve as writable memory \parencite schmidhuber1992learning. The two learning rules of Hebbian \parencite hebb2005organization and delta \parencite prados1989neural are the most popular learning rules for fast weight programs, which have been extensively explored in various studies \parencite munkhdalai2017neural, schmidhuber1992learning, munkhdalai2019metalearned, schlag2021linear, irie2021going, yang2024parallelizing, yang2024gated. All these models, however, are based on momentary surprise, missing the token flow in the sequences (see Section 3.1 ), and most of them lacks a forgetting gate, resulting in a poor memory management. We further discuss the connection of our architectures with recent models in Appendix C . Additional related work are discussed in Appendix A . 3 Learning to Memorize at Test Time \lettrine [lines=3]To overcome the lack of long-term memory and to enable the model to learn, forget, and retrieve information, in this section, we present a neural long-term memory module, which is a meta models that learns to memorize at test time. In Section 3.1 , we first discuss the motivation and the design of the neural memory. In Section 3.2 , we discuss how our architecture design can benefit from a fast and parallelizable training. Finally, in Section 3.3 , we augment our architecture using persistent memory module, in which we use learnable but data-independent parameters to learn meta information about the task. 3.1 Long-term Memory To design a neural long-term memory module, we need a model that can encode the abstraction of the past history into its parameters. An example of this can be LLMs that are shown to be memorizing their training data \parencite staab2024beyond, schwarzschild2024rethinking, leybzon2024learning. Therefore, a simple idea is to train a neural network and expect it to memorize its training data. Memorization, however, has almost always been known as an undesirable phenomena in neural networks as it limits the model generalization \parencite bayat2024pitfalls, causes privacy concerns \parencite staab2024beyond, and so results in poor performance at test time. Moreover, the memorization of the training data might not be helpful at test time, in which the data might be out-of-distribution. We argue that, we need an online meta-model that learns how to memorize/forget the data at test time. In this setup, the model is learning a function that is capable of memorization, but it is not overfitting to the training data, resulting in a better generalization at test time. Learning Process and Surprise Metric. The key idea to train a long-term memory is to treat its training as an online learning problem, in which we aim to compress the past information x 1 , … , x t − 1 subscript 𝑥 1 … subscript 𝑥 𝑡 1 x_{1},\dots,x_{t-1} italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT into the parameters of our long-term neural memory module ℳ t subscript ℳ 𝑡 \mathcal{M}_{t} caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . As discussed earlier, an event that violates the expectations (i.e., is surprising) is more memorable for humans \parencite mandler2014structure. Inspired by this, a simple definition of surprise for a model can be its gradient with respect to the input. The larger the gradient is, the more different the input data is from the past data. Accordingly, using this surprise score, we can update the memory as: ℳ t = ℳ t − 1 − θ t ⁢ ∇ ℓ ⁢ ( ℳ t − 1 ; x t ) ⏟ Surprise . subscript ℳ 𝑡 subscript ℳ 𝑡 1 subscript 𝜃 𝑡 Surprise ⏟ ∇ ℓ subscript ℳ 𝑡 1 subscript 𝑥 𝑡 \displaystyle\mathcal{M}_{t}=\mathcal{M}_{t-1}-\theta_{t}\>\underset{\text{%
Surprise}}{\underbrace{{\color[rgb]{0.1640625,0.2890625,0.40234375}%
\definecolor[named]{pgfstrokecolor}{rgb}{0.1640625,0.2890625,0.40234375}\nabla%
\ell(\mathcal{M}_{t-1};x_{t})}}}. caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT - italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT underSurprise start_ARG under⏟ start_ARG ∇ roman_ℓ ( caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ; italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG end_ARG . (8) This surprise metric, however, can result in missing important information that comes after a big surprising moment. That is, the gradient can become extremely small after several surprising steps, leading to stocking in a flat area (i.e., local minima), and missing information about some parts of the sequence. From the human memory perspective, an event might not consistently surprise us through a long-period of time although it is memorable. The reason is that the initial moment is surprising enough to get our attention through a long time frame, leading to memorizing the entire time frame. To improve the above surprise metric ( Equation 8 ), we break the surprise metric into (1) past surprise , which measures the surprise amount of a very recent past; and (2) momentary surprise , which measures the surprise of incoming data: ℳ t = ℳ t − 1 + S t , subscript ℳ 𝑡 subscript ℳ 𝑡 1 subscript 𝑆 𝑡 \displaystyle\mathcal{M}_{t}=\mathcal{M}_{t-1}+{\color[rgb]{%
0.1640625,0.2890625,0.40234375}\definecolor[named]{pgfstrokecolor}{rgb}{%
0.1640625,0.2890625,0.40234375}S_{t}}, caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT + italic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , (9) S t = η t ⁢ S t − 1 ⏟ Past Surprise − θ t ⁢ ∇ ℓ ⁢ ( M t − 1 ; x t ) ⏟ Momentary Surprise . subscript 𝑆 𝑡 subscript 𝜂 𝑡 Past Surprise ⏟ subscript 𝑆 𝑡 1 subscript 𝜃 𝑡 Momentary Surprise ⏟ ∇ ℓ subscript 𝑀 𝑡 1 subscript 𝑥 𝑡 \displaystyle S_{t}=\eta_{t}\underset{\text{Past Surprise}}{\underbrace{{%
\color[rgb]{0.1640625,0.2890625,0.40234375}\definecolor[named]{pgfstrokecolor}%
{rgb}{0.1640625,0.2890625,0.40234375}S_{t-1}}}}-\theta_{t}\>\underset{\text{%
Momentary Surprise}}{\underbrace{{\color[rgb]{0.1640625,0.2890625,0.40234375}%
\definecolor[named]{pgfstrokecolor}{rgb}{0.1640625,0.2890625,0.40234375}\nabla%
\ell\left(M_{t-1};x_{t}\right)}}}. italic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_η start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT underPast Surprise start_ARG under⏟ start_ARG italic_S start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT end_ARG end_ARG - italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT underMomentary Surprise start_ARG under⏟ start_ARG ∇ roman_ℓ ( italic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ; italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG end_ARG . (10) Interestingly, this formulation is similar to gradient descent with momentum, where S t subscript 𝑆 𝑡 S_{t} italic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the momentum element. Therefore, the momentum here act as a memory of surprise across time (sequence length). In this formulation, the term η t subscript 𝜂 𝑡 \eta_{t} italic_η start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is a data-dependent surprise decay (a function of x t subscript 𝑥 𝑡 x_{t} italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ), controlling how surprise decays over time, and the term θ t subscript 𝜃 𝑡 \theta_{t} italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is controlling how much of momentary surprise should be incorporated into the final surprise metric in a data-dependent manner. This data-dependency is particularly important in this design: While surprise of previous tokens might be needed to affect the surprise of the next token, it is mostly valid if all tokens are relevant and are in the same context. Accordingly, a data-dependent η 𝜂 \eta italic_η can control if memory needs to: (1) ignore the last surprise by setting η t → 0 → subscript 𝜂 𝑡 0 \eta_{t}\rightarrow 0 italic_η start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT → 0 (possibly due to the change of context), or (2) fully incorporate the last surprise by setting η t → 1 → subscript 𝜂 𝑡 1 \eta_{t}\rightarrow 1 italic_η start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT → 1 (possibly as the token is highly relevant to its recent past tokens). Objective. Our above surprise metric is based on a loss function ℓ ( . ; . ) \ell(.;.) roman_ℓ ( . ; . ) , which is the objective that our memory is learning to act as it at test time. That is, our memory module is a meta model that learns a function based on the loss function ℓ ( . ; . ) \ell(.;.) roman_ℓ ( . ; . ) . In this work, we focus on associative memory , in which we aim to store the past data as the pairs of keys and values. Given x t subscript 𝑥 𝑡 x_{t} italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , similar to Transformers \parencite transformers, we use two linear layers to project x t subscript 𝑥 𝑡 x_{t} italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT into a key and value: 𝐤 t = x t ⁢ W K , 𝐯 t = x t ⁢ W V , formulae-sequence subscript 𝐤 𝑡 subscript 𝑥 𝑡 subscript 𝑊 𝐾 subscript 𝐯 𝑡 subscript 𝑥 𝑡 subscript 𝑊 𝑉 \displaystyle\mathbf{k}_{t}=x_{t}W_{K},\qquad\qquad\mathbf{v}_{t}=x_{t}W_{V}, bold_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT , bold_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT , (11) where W K subscript 𝑊 𝐾 W_{K} italic_W start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT and W V ∈ ℝ d in × d in subscript 𝑊 𝑉 superscript ℝ subscript 𝑑 in subscript 𝑑 in W_{V}\in\mathbb{R}^{d_{\text{in}}\times d_{\text{in}}} italic_W start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT end_POSTSUPERSCRIPT . Next, we expect our memory module to learn the associations between keys and values. To this end, we define the loss as follows: ℓ ⁢ ( ℳ t − 1 ; x t ) = ‖ ℳ t − 1 ⁢ ( 𝐤 t ) − 𝐯 t ‖ 2 2 ℓ subscript ℳ 𝑡 1 subscript 𝑥 𝑡 superscript subscript norm subscript ℳ 𝑡 1 subscript 𝐤 𝑡 subscript 𝐯 𝑡 2 2 \displaystyle\ell(\mathcal{M}_{t-1};x_{t})=\left\|\mathcal{M}_{t-1}\left(%
\mathbf{k}_{t}\right)-\mathbf{v}_{t}\right\|_{2}^{2} roman_ℓ ( caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ; italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ∥ caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ( bold_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - bold_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (12) By optimizing the above loss function in the inner-loop of our meta model (memory), the model learns how to memorize the mapping between keys and values at test time. Note that, similar to meta-learning models \parencite zintgraf2019fast, nichol2018first, training of the memory is in the inner-loop, and so parameters W K subscript 𝑊 𝐾 W_{K} italic_W start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT and W V subscript 𝑊 𝑉 W_{V} italic_W start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT are hyperparameters in the above loss function. Accordingly, in the inner loop, we optimize ℳ ℳ \mathcal{M} caligraphic_M ’s weights, while in the outer-loop, we optimize other parameters of the entire architecture. Forgetting Mechanism. When dealing with very large sequences (e.g., millions of tokens), it is crucial to manage which past information should be forgotten–even with a deep or a very large matrix-valued memory. To this end, we use an adaptive forgetting mechanism that allows the memory to forget the information that is not needed anymore, resulting in better managing the memory’s limited capacity. That is, given the next token x t subscript 𝑥 𝑡 x_{t} italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , we modify the update rule as: ℳ t = ( 1 − α t ) ⁢ ℳ t − 1 + S t , subscript ℳ 𝑡 1 subscript 𝛼 𝑡 subscript ℳ 𝑡 1 subscript 𝑆 𝑡 \displaystyle\mathcal{M}_{t}=(1-\alpha_{t})\mathcal{M}_{t-1}+{\color[rgb]{%
0.1640625,0.2890625,0.40234375}\definecolor[named]{pgfstrokecolor}{rgb}{%
0.1640625,0.2890625,0.40234375}S_{t}}, caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ( 1 - italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT + italic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , (13) S t = η t ⁢ S t − 1 − θ t ⁢ ∇ ℓ ⁢ ( M t − 1 ; x t ) , subscript 𝑆 𝑡 subscript 𝜂 𝑡 subscript 𝑆 𝑡 1 subscript 𝜃 𝑡 ∇ ℓ subscript 𝑀 𝑡 1 subscript 𝑥 𝑡 \displaystyle S_{t}=\eta_{t}{{\color[rgb]{0.1640625,0.2890625,0.40234375}%
\definecolor[named]{pgfstrokecolor}{rgb}{0.1640625,0.2890625,0.40234375}S_{t-1%
}}}-\theta_{t}\>{{\color[rgb]{0.1640625,0.2890625,0.40234375}\definecolor[%
named]{pgfstrokecolor}{rgb}{0.1640625,0.2890625,0.40234375}\nabla\ell\left(M_{%
t-1};x_{t}\right)}}, italic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_η start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT - italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∇ roman_ℓ ( italic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ; italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , (14) where α t ∈ [ 0 , 1 ] subscript 𝛼 𝑡 0 1 \alpha_{t}\in[0,1] italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ [ 0 , 1 ] is the gating mechanism that flexibly controls the memory; i.e., decides how much information should be forgotten. For example, it can update the memory without affecting the past abstraction by letting α t → 0 → subscript 𝛼 𝑡 0 \alpha_{t}\rightarrow 0 italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT → 0 , and can clear the entire memory by letting α t → 1 → subscript 𝛼 𝑡 1 \alpha_{t}\rightarrow 1 italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT → 1 . Later in this section, we show that this weight decay mechanism is closely related to the gating mechanism in modern RNNs \parencite dao2024transformers, orvieto2023resurrecting. Memory Architecture. In this paper, we focus on simple MLPs with L ℳ ≥ 1 subscript 𝐿 ℳ 1 L_{\mathcal{M}}\geq 1 italic_L start_POSTSUBSCRIPT caligraphic_M end_POSTSUBSCRIPT ≥ 1 layers as the architecture of our long-term memory. The main reason behind this choice is that we want to focus on better motivating the design of the long-term memory and ways that it can be incorporated into an architecture. However, our formulation and architectural design opens a new research direction to design neural architectures that are more effective and efficient in memorization of data. Recently, there has been a promising line of work to design such architectures \parencite zhang2024memory, cetin2024evolved, berges2024memory, which incorporating them into our framework (i.e., replacing simple MLPs with such architectures) can be an interesting future work. When using vector-valued or matrix-valued memory \parencite yang2024gatedattn, orvieto2023resurrecting, de2024griffin, the memory module is compressing the past data and fit it into a line. That is, from the meta learning or online learning perspective \parencite sun2024learning, using a matrix-valued memory ℳ = W ∈ ℝ d in × d in ℳ 𝑊 superscript ℝ subscript 𝑑 in subscript 𝑑 in \mathcal{M}=W\in\mathbb{R}^{d_{\text{in}}\times d_{\text{in}}} caligraphic_M = italic_W ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT end_POSTSUPERSCRIPT is equivalent to optimize ℓ ⁢ ( W t − 1 ; x t ) = ‖ W t − 1 ⁢ 𝐤 t − 𝐯 t ‖ 2 2 ℓ subscript 𝑊 𝑡 1 subscript 𝑥 𝑡 superscript subscript norm subscript 𝑊 𝑡 1 subscript 𝐤 𝑡 subscript 𝐯 𝑡 2 2 \ell(W_{t-1};x_{t})=\left\|W_{t-1}\mathbf{k}_{t}-\mathbf{v}_{t}\right\|_{2}^{2} roman_ℓ ( italic_W start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ; italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ∥ italic_W start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT bold_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , which is an online linear regression objective and so the optimal solution assumes the underlying dependency of historical data is linear. On the other hand, we argue that deep memory modules (i.e., L ℳ ≥ 2 subscript 𝐿 ℳ 2 L_{\mathcal{M}}\geq 2 italic_L start_POSTSUBSCRIPT caligraphic_M end_POSTSUBSCRIPT ≥ 2 ) . Aligning with the theoretical results that MLPs with at least two layers are strictly more expressive than linear models \parencite hornik1989multilayer, in Section 5.5 , we show that deep memory modules are more effective in practice. Retrieving a Memory. In the above, we discuss how one can design and train a long-term memory module that learns to memorize at test time. A key remaining question is: How one can retrieve information from the memory? We simply use the forward pass without weight update (i.e., inference) to retrieve a memory correspond to a query. Formally, given an input x t subscript 𝑥 𝑡 x_{t} italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , we use a linear layer W Q subscript 𝑊 𝑄 W_{Q} italic_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT to project the input, i.e., 𝐪 t = x t ⁢ W Q subscript 𝐪 𝑡 subscript 𝑥 𝑡 subscript 𝑊 𝑄 \mathbf{q}_{t}=x_{t}W_{Q} bold_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT and retrieve the corresponding (or useful) information from the memory y t subscript 𝑦 𝑡 {y}_{t} italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT by: y t = ℳ ∗ ⁢ ( 𝐪 t ) . subscript 𝑦 𝑡 superscript ℳ subscript 𝐪 𝑡 \displaystyle y_{t}=\mathcal{M}^{*}(\mathbf{q}_{t}). italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = caligraphic_M start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( bold_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . (15) Figure 1 : The illustration of how the training of neural memory can be done in parallel and using matmul s. 3.2 How to Parallelize the Long-term Memory Training As discussed above, the design of our long-term memory module is equivalent to training a meta model by optimizing associative memory loss function ℓ ⁢ ( ℳ t − 1 ; x t ) = ‖ ℳ t − 1 ⁢ ( 𝐤 t ) − 𝐯 t ‖ 2 2 ℓ subscript ℳ 𝑡 1 subscript 𝑥 𝑡 superscript subscript norm subscript ℳ 𝑡 1 subscript 𝐤 𝑡 subscript 𝐯 𝑡 2 2 \ell(\mathcal{M}_{t-1};x_{t})=\left\|\mathcal{M}_{t-1}\left(\mathbf{k}_{t}%
\right)-\mathbf{v}_{t}\right\|_{2}^{2} roman_ℓ ( caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ; italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ∥ caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ( bold_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - bold_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT using gradient descent with momentum and weight decay. Therefore, in theory, the training of long-term memory module requires 𝒪 ⁢ ( N ) 𝒪 𝑁 \mathcal{O}\left(N\right) caligraphic_O ( italic_N ) FLOPs, where N 𝑁 N italic_N is the sequence length. However, in practice, we need to parallelize the training process and to fully take advantage of hardware accelerators (e.g., TPUs, GPUs), we need to tensorize the process and use more matmul s. Next, we show that calculating the weights in the inner loop with mini-batch gradient descent, data-dependent learning rate, and weight decay can be reformulated so that it uses only matmul s and sum. We build upon the work of \textcite sun2024learning that shows forward pass of a model optimizing with the mini-batch gradient descent (with constant learning rate) can be calculated using matmul s. We can split the sequence into chunks of size b ≥ 1 𝑏 1 b\geq 1 italic_b ≥ 1 , and write the mini-batch gradient descent as: ℳ t = ( 1 − α t ) ⁢ ℳ t − 1 − θ t ⁢ ∇ ℓ ⁢ ( ℳ t − 1 ; x t ) = β t ⁢ ℳ 0 − ∑ i = 1 t θ i ⁢ β t β i ⁢ ∇ ℓ ⁢ ( ℳ t ′ ; x i ) , subscript ℳ 𝑡 1 subscript 𝛼 𝑡 subscript ℳ 𝑡 1 subscript 𝜃 𝑡 ∇ ℓ subscript ℳ 𝑡 1 subscript 𝑥 𝑡 subscript 𝛽 𝑡 subscript ℳ 0 superscript subscript 𝑖 1 𝑡 subscript 𝜃 𝑖 subscript 𝛽 𝑡 subscript 𝛽 𝑖 ∇ ℓ subscript ℳ superscript 𝑡 ′ subscript 𝑥 𝑖 \displaystyle\mathcal{M}_{t}=(1-\alpha_{t})\mathcal{M}_{t-1}-\theta_{t}\nabla%
\ell(\mathcal{M}_{t-1};x_{t})=\beta_{t}\mathcal{M}_{0}-\sum_{i=1}^{t}\theta_{i%
}\frac{\beta_{t}}{\beta_{i}}\nabla\ell(\mathcal{M}_{t^{\prime}};x_{i}), caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ( 1 - italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT - italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∇ roman_ℓ ( caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ; italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = italic_β start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT caligraphic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT divide start_ARG italic_β start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_β start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ∇ roman_ℓ ( caligraphic_M start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ; italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , (16) where t ′ = t − mod ⁢ ( t , b ) superscript 𝑡 ′ 𝑡 mod 𝑡 𝑏 t^{\prime}=t-\texttt{mod}(t,b) italic_t start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = italic_t - mod ( italic_t , italic_b ) , and β i = ∏ j = 1 i ( 1 − α j ) subscript 𝛽 𝑖 superscript subscript product 𝑗 1 𝑖 1 subscript 𝛼 𝑗 \beta_{i}=\prod_{j=1}^{i}(1-\alpha_{j}) italic_β start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ∏ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( 1 - italic_α start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) . For the sake of simplicity, we focus on the first chunk, i.e., t = b 𝑡 𝑏 t=b italic_t = italic_b and so t ′ = 0 superscript 𝑡 ′ 0 t^{\prime}=0 italic_t start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = 0 . Also, we explain the process for the case that ℳ t = W t subscript ℳ 𝑡 subscript 𝑊 𝑡 \mathcal{M}_{t}=W_{t} caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_W start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is linear. The process for MLPs with N p ≥ 2 subscript 𝑁 𝑝 2 N_{p}\geq 2 italic_N start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ≥ 2 is similar. Using our loss function, we have: ∇ ℓ ⁢ ( W 0 ; x t ) = ( W 0 ⁢ x t − x t ) ⁢ x t ⊤ ⇒ ∑ i = 1 b θ i ⁢ β b β i ⁢ ∇ ℓ ⁢ ( W 0 ; x i ) = Θ b ⁢ 𝐁 b ⁢ ( W 0 ⁢ X − X ) ⁢ X ⊤ , ∇ ℓ subscript 𝑊 0 subscript 𝑥 𝑡 subscript 𝑊 0 subscript 𝑥 𝑡 subscript 𝑥 𝑡 superscript subscript 𝑥 𝑡 top ⇒ superscript subscript 𝑖 1 𝑏 subscript 𝜃 𝑖 subscript 𝛽 𝑏 subscript 𝛽 𝑖 ∇ ℓ subscript 𝑊 0 subscript 𝑥 𝑖 subscript Θ 𝑏 subscript 𝐁 𝑏 subscript 𝑊 0 𝑋 𝑋 superscript 𝑋 top \displaystyle\nabla\ell(W_{0};x_{t})=(W_{0}x_{t}-x_{t})x_{t}^{\top}\Rightarrow%
\sum_{i=1}^{b}\theta_{i}\frac{\beta_{b}}{\beta_{i}}\nabla\ell(W_{0};x_{i})=%
\Theta_{b}\mathbf{B}_{b}(W_{0}X-X)X^{\top}, ∇ roman_ℓ ( italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ; italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ( italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ⇒ ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT divide start_ARG italic_β start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT end_ARG start_ARG italic_β start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ∇ roman_ℓ ( italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ; italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = roman_Θ start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT bold_B start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ( italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_X - italic_X ) italic_X start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT , (17) where Θ b = diag ⁢ ( [ θ 1 θ 2 … θ b ] ) subscript Θ 𝑏 diag matrix subscript 𝜃 1 subscript 𝜃 2 … subscript 𝜃 𝑏 \Theta_{b}=\texttt{diag}\left(\begin{bmatrix}\theta_{1}&\theta_{2}&\dots&%
\theta_{b}\end{bmatrix}\right) roman_Θ start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT = diag ( [ start_ARG start_ROW start_CELL italic_θ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL start_CELL italic_θ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL start_CELL … end_CELL start_CELL italic_θ start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ) and 𝐁 b subscript 𝐁 𝑏 \mathbf{B}_{b} bold_B start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT is defined analogously on β b β i subscript 𝛽 𝑏 subscript 𝛽 𝑖 \frac{\beta_{b}}{\beta_{i}} divide start_ARG italic_β start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT end_ARG start_ARG italic_β start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG s. Note that, we do not need to store all Θ k ⁢ b subscript Θ 𝑘 𝑏 \Theta_{kb} roman_Θ start_POSTSUBSCRIPT italic_k italic_b end_POSTSUBSCRIPT and 𝐁 k ⁢ b subscript 𝐁 𝑘 𝑏 \mathbf{B}_{kb} bold_B start_POSTSUBSCRIPT italic_k italic_b end_POSTSUBSCRIPT for k = 1 , … , N / b 𝑘 1 … 𝑁 𝑏 k=1,\dots,N/b italic_k = 1 , … , italic_N / italic_b , instead, we store these matrices for each chunk, resulting in using less memory. Next, we extend this representation so we can also incorporate the momentum term. In a chunk wise gradient descent with momentum, if we look at the momentum term, we have: S t = η t ⁢ S t − 1 − θ t ⁢ u t , subscript 𝑆 𝑡 subscript 𝜂 𝑡 subscript 𝑆 𝑡 1 subscript 𝜃 𝑡 subscript 𝑢 𝑡 \displaystyle S_{t}=\eta_{t}{S_{t-1}}-\theta_{t}\>u_{t}, italic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_η start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT - italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , (18) where u t = ∇ ℓ ⁢ ( M t ′ ; x t ) subscript 𝑢 𝑡 ∇ ℓ subscript 𝑀 superscript 𝑡 ′ subscript 𝑥 𝑡 u_{t}=\nabla\ell\left(M_{t^{\prime}};x_{t}\right) italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ∇ roman_ℓ ( italic_M start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ; italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . Note that, we can compute all u t subscript 𝑢 𝑡 u_{t} italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at the same time, and so Equation 18 is a linear recurrence with u t subscript 𝑢 𝑡 u_{t} italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT as an input, S t subscript 𝑆 𝑡 S_{t} italic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT as the hidden state, and η t subscript 𝜂 𝑡 \eta_{t} italic_η start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT as input-dependent transition value. Accordingly, we can use parallel associative scan \parencite smith2023simplified to calculate S t subscript 𝑆 𝑡 S_{t} italic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT s in this chunk. Parameters as the Function of Chunks. Instead of making parameters like α t , θ t subscript 𝛼 𝑡 subscript 𝜃 𝑡 \alpha_{t},\theta_{t} italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , and η t subscript 𝜂 𝑡 \eta_{t} italic_η start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT input-dependent (i.e., a function of token x t subscript 𝑥 𝑡 x_{t} italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ), we can make them functions of their chunk. Despite losing expressive power, this formulation can help to make the training even faster. In this case, we are using the same value for each of α 𝛼 \alpha italic_α , θ 𝜃 \theta italic_θ , and η 𝜂 \eta italic_η in each chunk. Accordingly, in Equation 17 , we can store Θ Θ \Theta roman_Θ using a single scaler. Similarly we can make Equation 18 faster. That is, when η 𝜂 \eta italic_η and θ 𝜃 \theta italic_θ are learnable but time-invariant inside each chunk, this equation becomes a linear time-invariant system (LTI), which can be computed by a global convolution \parencite gu2022efficiently. In our experiments, we make these parameters as the functions of tokens. However, such simplifications (i.e., as the function of chunks) can be the interest of future work to training larger models in more efficient manner. 3.3 Persistent Memory Our long-term memory can also be seen as a contextual memory, meaning that the output is fully depend on the context. Therefore, in addition to our long-term memory, we also use a set of learnable but input-independent parameters to act as task-related memory. This type of memory has been referred to as persistent or meta-memory in the literature \parencite sukhbaatar2019augmenting, dong2024hymba. Given N p ≥ 1 subscript 𝑁 𝑝 1 N_{p}\geq 1 italic_N start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ≥ 1 , we use learnable parameters P = [ p 1 p 2 … p N p ] 𝑃 matrix subscript 𝑝 1 subscript 𝑝 2 … subscript 𝑝 subscript 𝑁 𝑝 P=\begin{bmatrix}p_{1}&p_{2}&\dots&p_{N_{p}}\end{bmatrix} italic_P = [ start_ARG start_ROW start_CELL italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL start_CELL italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL start_CELL … end_CELL start_CELL italic_p start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] and append it to the start of our sequence: i.e., given a context window size of N 𝑁 N italic_N , we modify the input as: x new = [ p 1 p 2 … p N p ] | | x , \displaystyle x_{\text{new}}=\begin{bmatrix}p_{1}&p_{2}&\dots&p_{N_{p}}\end{%
bmatrix}||\>\>x, italic_x start_POSTSUBSCRIPT new end_POSTSUBSCRIPT = [ start_ARG start_ROW start_CELL italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL start_CELL italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL start_CELL … end_CELL start_CELL italic_p start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] | | italic_x , (19) where | | || | | is concatenation. Next, we discuss the motivation of persistent memory from three perspective: Memory Perspective. As discussed earlier, our neural long-term memory is a contextual memory, in which all parameters are input-dependent. An effective memory system, however, also needs input-independent parameters to store the abstraction of the task knowledge. That is, mastering a task requires the memorization of the knowledge that how the task can be done, and these parameters are responsible for storing such knowledge. Feedforward Network Perspective. In the Transformer architectures, there are fully connected layers after the attention module, which are shown to be similar to attention weights but with data-independent parameters. That is, \textcite sukhbaatar2019augmenting showed that replacing the ReLU in fully connected layers with Softmax can results in an attention-like weights, in which weights are data-independent: F ⁢ F ⁢ N ⁢ ( x ) = W V ⁢ Softmax ⁢ ( W K ⁢ x ) . 𝐹 𝐹 𝑁 𝑥 subscript 𝑊 𝑉 Softmax subscript 𝑊 𝐾 𝑥 \displaystyle FFN(x)=W_{V}\>\texttt{Softmax}\left(W_{K}x\right). italic_F italic_F italic_N ( italic_x ) = italic_W start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT Softmax ( italic_W start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT italic_x ) . (20) In fact, W K subscript 𝑊 𝐾 W_{K} italic_W start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT and W V subscript 𝑊 𝑉 W_{V} italic_W start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT are acting similar to K 𝐾 K italic_K and V 𝑉 V italic_V matrices in attention module when they are input-independent. The persistent memory weights are expected to have the same functionality, meaning that using them in the first part of the sequence leads to having input-independent attention weights \parencite sukhbaatar2019augmenting. Technical Perspective. Attention with causal mask has implicit bias toward initial tokens in the sequence, and so attention weights are almost always highly active for initial tokens, resulting in performance damage. From the technical perspective, these learnable parameters at the start of the sequence can mitigate such effect by redistributing the attention weights more effectively \parencite xiao2024efficient, hanLMInfinite2024. Figure 2 : Memory as a Context (MAC) Architecture. This architecture includes three branches of (1) core, (2) contextual (long-term) memory, and (3) persistent memory. The core branch concatenates the corresponding long-term and persistent memories with the input sequence. Next, attention performs on the sequence and decides what part of the information should store in the long-term memory. At the test time, parameters corresponds to contextual memory are still learning, parameters corresponds to the core branch are responsible for in-context learning, and parameters of persistent memory are responsible to store the knowledge about tasks and so are fixed. 4 How to Incorporate Memory? \lettrine [lines=3]An important question that remained unanswered is: How one can effectively and efficiently incorporate the designed neural memory into a deep learning architecture? As discussed earlier, from a memory perspective, the pair of K and V matrices in transformers can be interpreted as an associative memory block. Due to their accurate modeling of dependencies and so their limited context window, we interpret them as short-term memory modules, attending to the current context window size. On the other hand, our neural memory with the ability to continuously learn from data and store it in its weights can play the role of a a long-term memory. In this section, we aim to answer the above question by proposing three different variants of Titans. Later in our experiments, we show that each of these variants has its own advantages/disadvantages and also can show a trade-off between the efficiency and effectiveness in very long-contexts. 4.1 Memory as a Context In the first architecture design (see Figure 2 ), we treat the memory as a context to the current information. That is, given a long sequence x ∈ ℝ N × d in 𝑥 superscript ℝ 𝑁 subscript 𝑑 in x\in\mathbb{R}^{N\times d_{\text{in}}} italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , we first chunk the sequence into fixed-size segments S ( i ) superscript S 𝑖 \texttt{S}^{(i)} S start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT for i = 1 , … , N / C 𝑖 1 … 𝑁 𝐶 i=1,\dots,N/C italic_i = 1 , … , italic_N / italic_C . Given the incoming segment S ( t ) superscript S 𝑡 \texttt{S}^{(t)} S start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT , we consider it as the current context and its past segment as the historical information. Therefore, let ℳ t − 1 subscript ℳ 𝑡 1 \mathcal{M}_{t-1} caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT be the state of long-term memory before segment S ( t ) superscript S 𝑡 \texttt{S}^{(t)} S start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT , we use the input context as the query to the memory ℳ t − 1 subscript ℳ 𝑡 1 \mathcal{M}_{t-1} caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT to retrieve the corresponding information from the long-term memory. That is, we retrieve the past information that corresponds to S ( t ) superscript S 𝑡 \texttt{S}^{(t)} S start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT as: h t = ℳ t − 1 ∗ ⁢ ( 𝐪 t ) , subscript ℎ 𝑡 subscript superscript ℳ 𝑡 1 subscript 𝐪 𝑡 \displaystyle h_{t}=\mathcal{M}^{*}_{t-1}(\mathbf{q}_{t}), italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = caligraphic_M start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ( bold_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , (21) where 𝐪 t = S ( t ) ⁢ W Q subscript 𝐪 𝑡 superscript S 𝑡 subscript 𝑊 𝑄 \mathbf{q}_{t}=\texttt{S}^{(t)}W_{Q} bold_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = S start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT italic_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT . Next, we use this historical information along with our persistent memory parameters as the input sequence to the attention module: S ~ ( t ) = [ p 1 p 2 … p N p ] ⁢ ‖ h t ‖ ⁢ S ( t ) , superscript ~ S 𝑡 matrix subscript 𝑝 1 subscript 𝑝 2 … subscript 𝑝 subscript 𝑁 𝑝 norm subscript ℎ 𝑡 superscript S 𝑡 \displaystyle\tilde{\texttt{S}}^{(t)}=\begin{bmatrix}p_{1}&p_{2}&\dots&p_{N_{p%
}}\end{bmatrix}\>\>||\>\>h_{t}\>\>||\>\>\texttt{S}^{(t)}, over~ start_ARG S end_ARG start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT = [ start_ARG start_ROW start_CELL italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL start_CELL italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL start_CELL … end_CELL start_CELL italic_p start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] | | italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | | S start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT , (22) y t = Attn ⁢ ( S ~ ( t ) ) . subscript 𝑦 𝑡 Attn superscript ~ S 𝑡 \displaystyle y_{t}=\texttt{Attn}\left(\tilde{\texttt{S}}^{(t)}\right). italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = Attn ( over~ start_ARG S end_ARG start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ) . (23) The structure of the attention map over the entire sequence is shown in 3(a) . We then use y t subscript 𝑦 𝑡 y_{t} italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT to update the long-term memory module for the next segment and the final output: ℳ t = ℳ t − 1 ⁢ ( y t ) , subscript ℳ 𝑡 subscript ℳ 𝑡 1 subscript 𝑦 𝑡 \displaystyle\mathcal{M}_{t}=\mathcal{M}_{t-1}\left(y_{t}\right), caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , (24) o t = y t ⊗ ℳ t ∗ ⁢ ( y t ) . subscript 𝑜 𝑡 tensor-product subscript 𝑦 𝑡 superscript subscript ℳ 𝑡 subscript 𝑦 𝑡 \displaystyle o_{t}=y_{t}\otimes\mathcal{M}_{t}^{*}\left(y_{t}\right). italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ⊗ caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . (25) Note that, in the above, we are updating the weight of ℳ t − 1 subscript ℳ 𝑡 1 \mathcal{M}_{t-1} caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT through forward pass. (a) Memory as a Context (MAC). We segment the sequence and use full causal attention in each window. Again, the first N p subscript 𝑁 𝑝 N_{p} italic_N start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT tokens are persistent memory and the next N l subscript 𝑁 𝑙 N_{l} italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT are long-term memory tokens (b) Memory as Gating (MAG). We use sliding window attention (SWA) as a short-term memory and our neural memory module as a long-term memory, combining by a gating. Figure 3 : Attention masks for different variants of Titans. This architecture has two key advantages: (1) Attention by having both historical and current context, has the ability to decides whether given the current data, the long-term memory information is needed. (2) The attention module helps the long-term memory to store only useful information from the current context. That is, not all tokens in each segment are useful and memorizing all of them can result in memory overflow. Therefore, attention is helping the memory to understand which information is useful, better managing the memory capacity. (3) At test time: (i) persistent memory parameters are fixed as they encodes the knowledge about the task, which should not be changed; (ii) the attention module weights are in-context learner; and (iii) the long-term memory module is still learning (memorizing) the information at test time. That is, we update the weights of the neural memory even at test time as weights are encoding the abstraction of long past. Figure 4 : Memory as a Gate (MAG) Architecture. This architecture, similarly, has the three branches of (1) core, (2) contextual memory, and (3) persistent memory. It, however, incorporates only persistent memory into the context and combine memory with the core branch using a gating mechanism. At test time, the behavior is the same as Figure 2 . 4.2 Gated Memory In the next variant (see Figure 4 ), in one branch, we directly use the input data to update the long-term memory, and in the second branch, we use a sliding window attention (SWA): x ~ = [ p 1 p 2 … p N p ] | | x , \displaystyle\tilde{x}=\begin{bmatrix}p_{1}&p_{2}&\dots&p_{N_{p}}\end{bmatrix}%
\>\>||\>\>x, over~ start_ARG italic_x end_ARG = [ start_ARG start_ROW start_CELL italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL start_CELL italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL start_CELL … end_CELL start_CELL italic_p start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] | | italic_x , (26) y = SW-Attn ∗ ⁢ ( x ~ ) , 𝑦 superscript SW-Attn ~ 𝑥 \displaystyle y=\texttt{SW-Attn}^{*}\left(\tilde{x}\right), italic_y = SW-Attn start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( over~ start_ARG italic_x end_ARG ) , (27) o = y ⊗ ℳ ⁢ ( x ~ ) , 𝑜 tensor-product 𝑦 ℳ ~ 𝑥 \displaystyle o=y\otimes\mathcal{M}(\tilde{x}), italic_o = italic_y ⊗ caligraphic_M ( over~ start_ARG italic_x end_ARG ) , (28) where SW-Attn ∗ superscript SW-Attn \texttt{SW-Attn}^{*} SW-Attn start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT is sliding window attention with prefix (see 3(b) ). Note that, contrary to the previous design, we are not segmenting the input data. Also, we abuse the notation and use ℳ ⁢ ( x ) ℳ 𝑥 \mathcal{M}(x) caligraphic_M ( italic_x ) to refer to the final output of the memory after all recursion over the tokens of the sequence. In the above equation, ⊗ tensor-product \otimes ⊗ can be any non-linear gating. In our experiments, we normalize the outputs y 𝑦 y italic_y and ℳ ⁢ ( x ~ ) ℳ ~ 𝑥 \mathcal{M}(\tilde{x}) caligraphic_M ( over~ start_ARG italic_x end_ARG ) using learnable vector-valued weights, followed by a non-linearity σ ( . ) \sigma(.) italic_σ ( . ) . The overall attention mask of this design is shown in 3(b) . In this design, sliding window attention is act as a precise short-term memory, while the neural memory module is acting as a fading memory for the model. This architecture design can also be seen as a multi-head architecture where the structure of heads are different \parencite dong2024hymba. 4.3 Memory as a Layer The last variant uses the neural Memory As a Layer (MAL) of a deep neural network (see Figure 5 ). This architecture design is more common in the literature, where the hybrid models stack recurrent models with full or sliding window attentions. Given input x 𝑥 x italic_x , we have: x ~ = [ p 1 p 2 … p N p ] | | x , \displaystyle\tilde{x}=\begin{bmatrix}p_{1}&p_{2}&\dots&p_{N_{p}}\end{bmatrix}%
\>\>||\>\>x, over~ start_ARG italic_x end_ARG = [ start_ARG start_ROW start_CELL italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL start_CELL italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL start_CELL … end_CELL start_CELL italic_p start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] | | italic_x , (29) y = ℳ ⁢ ( x ~ ) , 𝑦 ℳ ~ 𝑥 \displaystyle y=\mathcal{M}(\tilde{x}), italic_y = caligraphic_M ( over~ start_ARG italic_x end_ARG ) , (30) o = SW-Attn ⁢ ( y ) , 𝑜 SW-Attn 𝑦 \displaystyle o=\texttt{SW-Attn}\left(y\right), italic_o = SW-Attn ( italic_y ) , (31) where SW-Attn is sliding window attention. The main drawback of this design is that the power of the model is limited by each of the layers and so it cannot take advantage of the complementary data processing of attention and neural memory module. In our experiments, for evaluating memory in this design, we use a similar architecture as H3 \parencite fu2023hungry, where we replace the the sequence model with our neural memory module (LMM). Figure 5 : Memory as a Layer (MAL) Architecture. In this architecture, the memory layer is responsible to compress the past and current context before the attention module. Memory Without Attention. Although in the above, we discussed MAL as the combination of LMMs and attention in a sequential manner, one simple variant of MAL is to treat LMM as a sequence model without any attention. From the memory perspective, as discussed in Section 1 , we expect each part of the memory system to work independently, even if other components are disturbed. Therefore, a long-term memory module should still be a powerful model even without short-term memory (i.e., attention). We refer to this variant as LMM or Titans (LMM) in our experiments. We provide additional discussions on the connection of Titans and other modern recurrent models in Appendix C . 4.4 Architectural Details For the sake of simplicity and presentation, we avoid discussing the implementation details like using residual connection, gating with linear layer, and normalization. In all blocks, we use residual connections. In our implementation, we use SiLU (.) activation \parencite elfwing2018sigmoid as the non-linear activation for computing query, key, and values and normalize queries and keys using ℓ 2 subscript ℓ 2 \ell_{2} roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT -norm. Convolution. Following the recent modern linear recurrent models \parencite yang2024gated, gu2024mamba, we incorporate a 1D depthwise-separable convolution layer after each of the query, key, and value projections. While not significantly affect the performance, these 1D convolutions have shown performance improvement and are also computationally efficient. Gating. We also follow the recent architectures that use normalization and gating with a linear layer before the final output projection \parencite mehta2023long. Theorem 4.1 . Contrary to Transformers, diagonal linear recurrent models, and DeltaNet, all of which are limited to TC 0 0 \>{}^{0} start_FLOATSUPERSCRIPT 0 end_FLOATSUPERSCRIPT \parencite merrill2024the, Titans are capable of solving problems beyond TC 0 0 \>{}^{0} start_FLOATSUPERSCRIPT 0 end_FLOATSUPERSCRIPT , meaning that Titans are theoretically more expressive than Transformers and most modern linear recurrent models in state tracking tasks. 5 Experiments \lettrine [lines=3]Next, we evaluate the performance of Titans and its variants in language modeling, commonsense reasoning, needle in haystack, DNA modeling, and time series forecasting tasks 1 1 1 In the first version of the work, we aim to provide insights/evidences about why the learning paradigms of Titans are effective. We are working on finalizing the results of larger models and will report them in the next version. . In more details, in this section, we answer the following empirical questions: (1) How do Titans perform compared to baselines in downstream tasks? ( see § 5.2 , § 5.6 , and § 5.7 ); (2) What is the actual context length of Titans? ( see § 5.3 and § 5.4 ); (3) How do Titans scale with respect to context length? ( see § 5.8 ); (4) How the depth of memory can affect both performance and efficiency? ( see § 5.5 ); and (5) What is the contribution of each Titans’ component in its performance? ( see § 5.9 ). 5.1 Experimental Setup Models. In our experiments, we focus on the three variants of Titans, which we refer to as: Titans with (1) Memory as a Context (MAC), (2) Memory as a Gate (MAG), and (3) Memory as a Layer (MAL) as well as (4) neural memory module alone. The reason behind using our long-term memory as a separate module is based on our definition of learning. As discussed in Section 1 , we define learning a process for acquiring effective and useful memory. Accordingly, we expect our long-term memory to effectively learn from data, even without attention. For each of these models, we consider four scales with: (i) 170M, (ii) 340M, (iii) 400M, and (iv) 760M parameters. While the first three are trained on 15B tokens sampled from FineWeb-Edu dataset \parencite penedo2024the, the last one is trained on 30B tokens from the same dataset. Table 1 : Performance of Titans and recurrent- and Transformer-based baselines on language modeling and common-sense reasoning tasks. Hybrid models are marked with ∗ . The best results among simple and hybrid models are highlighted. Model Wiki. LMB. LMB. PIQA Hella. Wino. ARC-e ARC-c SIQA BoolQ Avg. ppl ↓ ↓ \downarrow ↓ ppl ↓ ↓ \downarrow ↓ acc ↑ ↑ \uparrow ↑ acc ↑ ↑ \uparrow ↑ acc_n ↑ ↑ \uparrow ↑ acc ↑ ↑ \uparrow ↑ acc ↑ ↑ \uparrow ↑ acc_n ↑ ↑ \uparrow ↑ acc ↑ ↑ \uparrow ↑ acc ↑ ↑ \uparrow ↑ ↑ ↑ \uparrow ↑ 340M params / 15B tokens Transformer++ 31.52 41.08 30.76 62.98 34.76 50.53 45.21 24.05 36.81 58.24 42.92 RetNet 32.50 49.73 28.24 62.61 34.15 50.91 44.27 23.62 36.79 59.72 42.54 GLA 28.51 43.02 28.73 64.05 35.96 50.00 54.19 24.29 37.13 58.39 44.09 Mamba 30.83 40.21 29.94 63.79 35.88 49.82 49.24 24.56 35.41 60.07 43.59 DeltaNet 28.65 47.30 28.43 63.52 35.95 49.63 52.68 25.37 37.96 58.79 44.04 TTT 27.44 34.19 30.06 63.97 35.71 50.08 53.01 26.11 37.32 59.83 44.51 Gated DeltaNet 27.01 30.94 34.11 63.08 38.12 51.60 55.28 26.77 34.89 59.54 45.42 Titans (LMM) 26.18 29.97 34.98 64.73 39.61 51.85 55.60 28.14 34.52 59.99 46.17 Titans (MAC) ∗ 25.43 28.13 36.00 65.32 40.35 51.21 58.17 29.00 38.63 60.18 47.36 Titans (MAG) ∗ 25.07 28.72 36.71 64.88 40.56 52.49 57.72 28.16 39.75 60.01 47.54 Titans (MAL) ∗ 24.69 28.80 35.74 64.97 39.44 51.97 56.58 28.21 38.14 57.32 46.55 400M params / 15B tokens Transformer++ 30.63 37.37 29.64 64.27 37.72 51.53 54.95 27.36 38.07 61.59 45.64 RetNet 29.92 46.83 29.16 65.23 36.97 51.85 56.01 27.55 37.30 59.66 45.47 HGRN2 32.33 47.14 26.12 64.52 35.45 52.24 55.97 25.51 37.35 59.02 44.52 GLA 27.96 36.66 27.86 65.94 37.41 49.56 56.01 26.36 38.94 59.84 45.24 Mamba 29.22 39.88 29.82 65.72 37.93 50.11 58.37 26.70 37.76 61.13 45.94 Mamba2 26.34 33.19 32.03 65.77 39.73 52.48 59.00 27.64 37.92 60.72 46.91 DeltaNet 27.69 44.04 29.96 64.52 37.03 50.82 56.77 27.13 38.22 60.09 45.57 TTT 26.11 31.52 33.25 65.70 39.11 51.68 58.04 28.99 38.26 59.87 46.86 Gated DeltaNet 25.47 29.24 34.40 65.94 40.46 51.46 59.80 28.58 37.43 60.03 47.26 Samba ∗ 25.32 29.47 36.86 66.09 39.24 51.45 60.12 27.20 38.68 58.22 47.23 Gated DeltaNet-H2 ∗ 24.19 28.09 36.77 66.43 40.79 52.17 59.55 29.09 39.04 58.56 47.69 Titans (LMM) 25.03 28.99 35.21 65.85 40.91 52.19 59.97 29.20 38.74 60.85 47.83 Titans (MAC) ∗ 25.61 27.73 36.92 66.39 41.18 52.80 60.24 29.69 40.07 61.93 48.65 Titans (MAG) ∗ 23.59 27.81 37.24 66.80 40.92 53.21 60.01 29.45 39.91 61.28 48.60 Titans (MAL) ∗ 23.93 27.89 36.84 66.29 40.74 52.26 59.85 29.71 38.92 58.40 47.87 760M params / 30B tokens Transformer++ 25.21 27.64 35.78 66.92 42.19 51.95 60.38 32.46 39.51 60.37 48.69 RetNet 26.08 24.45 34.51 67.19 41.63 52.09 63.17 32.78 38.36 57.92 48.46 Mamba 28.12 23.96 32.80 66.04 39.15 52.38 61.49 30.34 37.96 57.62 47.22 Mamba2 22.94 28.37 33.54 67.90 42.71 49.77 63.48 31.09 40.06 58.15 48.34 DeltaNet 24.37 24.60 37.06 66.93 41.98 50.65 64.87 31.39 39.88 59.02 48.97 TTT 24.17 23.51 34.74 67.25 43.92 50.99 64.53 33.81 40.16 59.58 47.32 Gated DeltaNet 21.18 22.09 35.54 68.01 44.95 50.73 66.87 33.09 39.21 59.14 49.69 Samba ∗ 20.63 22.71 39.72 69.19 47.35 52.01 66.92 33.20 38.98 61.24 51.08 Gated DeltaNet-H2 ∗ 19.88 20.83 39.18 68.95 48.22 52.57 67.01 35.49 39.39 61.11 51.49 Titans (LMM) 20.04 21.96 37.40 69.28 48.46 52.27 66.31 35.84 40.13 62.76 51.56 Titans (MAC) 19.93 20.12 39.62 70.46 49.01 53.18 67.86 36.01 41.87 62.05 52.51 Titans (MAG) 18.61 19.86 40.98 70.25 48.94 52.89 68.23 36.19 40.38 62.11 52.50 Titans (MAL) 19.07 20.33 40.05 69.99 48.82 53.02 67.54 35.65 30.98 61.72 50.97 Baselines. We compare our models with the state-of-the-art linear recurrent models, Transformers, and hybrid models (recurrent + attention). More specifically in language tasks, we compare with Transformer++ \parencite touvron2023llama, RetNet \parencite sun2023retentive, Gated Linear Attention (GLA) \parencite yang2024gatedattn, Mamba \parencite gu2024mamba, Mamba2 \parencite dao2024transformers, DeltaNet \parencite yang2024parallelizing, TTT \parencite sun2024learning, and Gated DeltaNet \parencite yang2024gated. In needle in haystack tasks, we also compare with GPT4 \parencite achiam2023gpt, Llama3 with RAG \parencite touvron2023llama, RecurrentGemma2-9B \parencite botev2024recurrentgemma, and Mistral \parencite jiang2023mistral models, all of which are provided in the benchmark \parencite kuratov2024babilong. In time series tasks, we compare with Mamba-based \parencite behrouz2024mambamixer, Transformer-based \parencite nie2022time, liu2023itransformer, zhang2023crossformer, and linear models \parencite das2023longterm, wu2023timesnet, zeng2023transformers, li2023revisiting. Training. In the training, we follow the training procedure of \textcite yang2024gated, and use LLama 2 tokenizer with a vocabulary size of 32K and use training length of 4K tokens. We employ AdamW optimizer with learning rate of 4 ⁢ e 4 𝑒 4e 4 italic_e - 4 4 4 4 with cosine annealing schedule with batch size of 0.5M tokens, and weight decay of 0.1 0.1 0.1 0.1 . 5.2 Language Modeling We first focus on the perplexity in language modeling and also commonsense reasoning tasks. The results for Titans’ variants and also baselines with three different sizes of 340M, 400M, and 760M are reported in Table 1 . Among non-hybrid models, including Transformer++, our neural memory module achieves the best performance in both perplexity and accuracy measures. Comparing our neural memory module and TTT, which is also a gradient-based recurrent model can show us the importance of our weight decay as well as the momentum. As discussed earlier, the weight decay can be interpreted as a gating mechanism to forget the past data, when it is needed. Also, momentum can help us better manage the memory by providing additional memory for the surprise metric. While some baselines also take advantage of gating mechanism, e.g., Mamba, Mamba2, and Gated DeltaNet, the superior performance of our neural memory module shows the importance of both our surprise mechanism and having deep and non-linear memory. We further discuss the later in Section 5.5 . Comparing the hybrid models, we found that all three variants of Titans (MAC, MAG, and MAL) outperform both Samba (Mamba + attention) and Gated DeltaNet-H2 (Gated DeltaNet + atttention). We attribute the superior performance of Titans (MAL) to the power of neural memory module as the architecture design and used attention are all the same. Comparing Titans (MAG) and (MAC), we find that while their performance are close, MAC performs better when dealing with longer dependencies in the data. Interestingly, both MAG and MAC outperform MAL variant, which due to using the same modules, we attribute this to the architecture design of these models. This finding is particularly important as the current hybrid models (except Hymba \parencite dong2024hymba) in the literature are using MAL-style combination of recurrent models and attention. Table 2 : Performance of Titans and baselines on S-NIAH task from RULER benchmark. The best results among simple and hybrid models are highlighted. Model S-NIAH-PK S-NIAH-N S-NIAH-W 2K 4K 8K 16K 2K 4K 8K 16K 2K 4K 8K 16K TTT 98.4 98.8 98.0 88.4 60.2 36.6 10.2 4.4 78.8 28.0 4.4 0.0 Mamba2 98.6 61.4 31.0 5.4 98.4 55.8 14.2 0.0 42.2 4.2 0.0 0.0 DeltaNet 96.8 98.8 98.6 71.4 47.2 15.4 12.8 5.4 46.2 20.0 1.6 0.0 Titans (LMM) 99.8 98.4 98.2 96.2 100.0 99.8 93.4 80.2 90.4 89.4 85.8 80.6 Titans (MAC) 99.2 98.8 99.0 98.4 99.6 98.2 97.6 97.4 98.2 98.2 95.6 95.2 Titans (MAG) 99.4 98.0 97.4 97.4 99.2 98.8 97.2 98.6 98.0 98.0 90.2 88.2 Titans (MAL) 98.8 98.6 98.8 97.8 99.8 98.1 96.8 96.4 98.0 97.4 92.0 90.4 5.3 Needle in a Haystack Scaling a model to longer context window is not always equivalent to being effective for very long sequences \parencite hsieh2024ruler. The needle-in-a-haystack (NIAH) task is designed to measure the actual effective context length of models. In this task, we evaluate the model on retrieving a piece of information (i.e., the “needle”) from long distractor texts (i.e., the “haystack”). In this part, we use Single NIAH (S-NIAH) task from RULER benchmark \parencite hsieh2024ruler and evaluate Titans and baselines on sequences with length 2K, 4K, 8K, and 16K. The results are reported in Table 2 . Neural Memory module achieves the best results compare to baselines in all three tasks. We attribute this superior performance to three key differences of Titans with existing sequence models: (1) Compared to TTT, our Neural Memory can better handle the memory capacity by using momentum and also the forgetting mechanism (i.e., weight decay). Therefore, with increasing the sequence length, the performance of Neural Memory does not drop and show a consistent trend; (2) Compared to Mamba2, which has the gating (forgetting) mechanism, Titans have deep non-linear memory, resulting in better memory management. Also, contrary to our neural memory and DeltaNet, Mamba2 is not capable of removing a memory and so we can see a significant drop in performance when increasing the sequence length; (3) Compared to DeltaNet, although it is capable of removing memory using delta rule, it cannot erase the memory, lacking forgetting mechanism. Finally, As expected we can see on par or better results when using Titans variants, where the best results correspond to MAC. (a) Few-shot Setup (b) Fine-Tuning Setup Figure 6 : Performance of Titans and baselines on BABILong benchmark. Titans (MAC) outperforms all baselines, including extremely large models, e.g., GPT4. 5.4 BABILong Benchmark In the previous section we discussed the results on a simple NIAH tasks where a single needle needs to be retrieved. Although Titans showed better performance compared to baselines, their true advantage over very long sequences is still hidden. To this end, in this section, we use a harder task from BABILong benchmark \parencite kuratov2024babilong, in which the model needs to reason across facts distributed in extremely long documents. We follow the original experimental setup and training process in the benchmark. There are two settings: (1) Few-shot setting, in which we use large pre-trained models, and (2) fine-tuning setting, where we fine-tune the MAC variant of Titans to compare it with other fine-tuned baselines. The results for few-shot setting are reported in 6(a) . In this setup, we can see Titans outperform all baselines–i.e., Mamba2.8B \parencite gu2024mamba, RWKV-6-7B \parencite peng2024eagle, RecurrentGemma-9B \parencite botev2024recurrentgemma, Gemma-9B \parencite team2024gemma, Llama3.1-8B \parencite touvron2023llama, GPT-4, and GPT4o-mini \parencite achiam2023gpt. These results are achieved while Titans (MAC) is having much less number of parameters than baselines. In the fine-tuning setup, we compare the small fine-tuned version of Titans (MAC) with: (i) the fine-tuned version of small models (almost the same number of parameters as Titans) such as Mamba \parencite gu2024mamba, RMT \parencite bulatov2022recurrent, (ii) large models with Retrieval-Augmented Generation (RAG) \parencite lewis2020retrieval such as Llama3.1-8B \parencite touvron2023llama, and (iii) extremely large models such as GPT-4 \parencite achiam2023gpt, GPT4o-mini, Qwen2.5-72B \parencite yang2024qwen2, and Llama3.1-70B \parencite touvron2023llama. Baseline results are reported by \parencite kuratov2024babilong. The results of Titans and baselines are reported in 6(b) . Titans outperform all models even extremely large models like GPT4. Also, compared to Transformer-based with memory models like RMT, Titans show better performance mainly due to their powerful memory. That is, RMT compress the historical data into 16 size vector-valued memory, while Titans with in-context online memory learner are capable of encoding the past into the parameters of the model. Interestingly, even augmenting Llama3.1-8B model with RAG performs worse than Titans with about × \times × 70 less parameters. (a) 170M Parameters (b) 360M Parameters (c) 760M Parameters Figure 7 : The effect of memory depth on the perplexity. Deeper long-term memory results in better scaling in longer sequences. 5.5 The Effect of Deep Memory In this section, we evaluate the effect of deep memory in both wall-clock training time and model performance 2 2 2 Note that, in this experiment, we only focus on the neural memory module to evaluate the effect of memory depth in the memorization process. Combining neural memory with attention as we do in Titans variants, can additionally enhance the performance of the model over long sequences. . To this end, we focus on different variants of our neural memory module, where L ℳ = 1 , 2 , 3 , 4 subscript 𝐿 ℳ 1 2 3 4 L_{\mathcal{M}}=1,2,3,4 italic_L start_POSTSUBSCRIPT caligraphic_M end_POSTSUBSCRIPT = 1 , 2 , 3 , 4 . We also use Mamba as a baseline for the model performance. For a fair comparison, we use the same training process for all models and train them on a subset of the Pile dataset \parencite gao2020pile. We report the perplexity of our models and baselines as the function of the sequence length in Figure 7 . Interestingly, with the increase of memory depth, L ℳ subscript 𝐿 ℳ L_{\mathcal{M}} italic_L start_POSTSUBSCRIPT caligraphic_M end_POSTSUBSCRIPT , the model can achieve better perplexity over all sequence length. Also, deeper memory modules are more robust to the sequence length when the model has less number of parameters. With the increase of the number of parameters, all models show better performance on longer sequences. Figure 8 : The effect of memory depth on training throughput We also evaluate the effect of memory depth ( L ℳ = 1 , 2 , 3 , 4 subscript 𝐿 ℳ 1 2 3 4 L_{\mathcal{M}}=1,2,3,4 italic_L start_POSTSUBSCRIPT caligraphic_M end_POSTSUBSCRIPT = 1 , 2 , 3 , 4 ) on the training throughput. We report the training throughput (the number of tokens per second) as the function of sequence length in Figure 8 . All models scale linearly with respect to the context length (i.e., constant trend in the number of tokens per second with respect to sequence length). Also, by increasing the memory depth, as expected, we can see a linear trend that a deeper memory results in a slower training. Therefore, it is not always efficient to use deeper memory modules, showing a trade-off between effectiveness and efficiency. Table 3 : Performance on long-term forecasting. The best results are highlighted . Neural Memory Simba iTransformer RLinear PatchTST Crossformer TiDE TimesNet DLinear MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTm1 0.358 0.387 0.383 0.396 0.407 0.410 0.414 0.407 0.387 0.400 0.513 0.496 0.419 0.419 0.400 0.406 0.403 0.407 ETTm2 0.261 0.309 0.271 0.327 0.288 0.332 0.286 0.327 0.281 0.326 0.757 0.610 0.358 0.404 0.291 0.333 0.350 0.401 ETTh1 0.420 0.421 0.441 0.432 0.454 0.447 0.446 0.434 0.469 0.454 0.529 0.522 0.541 0.507 0.458 0.450 0.456 0.452 ETTh2 0.336 0.382 0.361 0.391 0.383 0.407 0.374 0.398 0.387 0.407 0.942 0.684 0.611 0.550 0.414 0.427 0.559 0.515 ECL 0.162 0.261 0.169 0.274 0.178 0.270 0.219 0.298 0.205 0.290 0.244 0.334 0.251 0.344 0.192 0.295 0.212 0.300 Traffic 0.415 0.289 0.493 0.291 0.428 0.282 0.626 0.378 0.481 0.304 0.550 0.304 0.760 0.473 0.620 0.336 0.625 0.383 Weather 0.231 0.265 0.255 0.280 0.258 0.278 0.272 0.291 0.259 0.281 0.259 0.315 0.271 0.320 0.259 0.287 0.265 0.317 5.6 Time Series Forecasting To show the effectiveness of our memory module in a broader tasks, we also evaluate its performance in time series forecasting tasks. To this end, we use Simba framework \parencite patro2024simba for time series forecasting, and replace its Mamba module with our neural memory. We report the results on common time series forecasting benchmark datasets–ETT, ECL, Traffic, and Weather \parencite zhou2021informer. The results are reported in Table 3 . Our neural memory module is outperforming all baselines, including Mamba-based, linear-based, and Transformer-based architectures. Table 4 : Downstream evaluation of pre-trained DNA models on GenomicsBenchmarks \parencite grevsova2023genomic. We report top-1 classification accuracy ( % percent \% % ). Model Enhancer Cohn Enhancer Ens Human Reg. Non-TATA Promoters Human OCR Ens. CNN 69.5 68.9 93.3 84.6 68.0 DNABERT 74.0 85.7 88.1 85.6 75.1 GPT 70.5 83.5 91.5 87.7 73.0 HyenaDNA 74.2 89.2 93.8 96.6 80.9 Transformer++ 73.4 89.5 89.9 94.4 79.5 Mamba 73.0 - - 96.6 - Based 74.6 89.5 89.5 96.8 79.0 Neural Memory Module 75.2 89.6 89.3 96.6 79.9 5.7 DNA Modeling In order to understand the capability of Titans beyond natural language, we further evaluate the performance of our neural memory module on DNA modeling tasks. To this end, we evaluate pre-trained models on the downstream tasks in GenomicsBenchmarks \parencite grevsova2023genomic. We follow the same experimental setups from \textcite nguyen2024hyenadna, and re-use the reported results of baselines by \textcite arora2024simple. The performance of Titans (LMM) and baselines are reported in Table 4 . We find that LMM is competitive with state-of-the-art architectures across different downstream genomics tasks. 5.8 Efficiency Figure 9 : Training throughput comparison of Titans and baselines. In this part, we compare the efficiency of our neural memory as well as Titans with state-of-the-art sequence models. The training throughput of models for different sequence length × \times × batch size are reported in Figure 9 . Comparing recurrent models, including our neural memory module, we can see our memory module is slightly slower than Mamba2 and Gated DeltaNet, mainly due to: (1) having deep memory and more expressive
transition process (memory update), and (2) highly optimized kernel in the implementation of Mamba2. Interestingly, Titans (MAL) are faster than baselines as well as the memory module. The main reason for this better throughput is the highly optimized kernel of Flash-Attention \parencite dao2024flashattention, which is used for implementing SWA and full attention module in Titans. 5.9 Ablation Study Finally, we perform ablation studies on the different architectural choices in Titans. We consider our neural memory module as a base model and then changing one component at a time: (1) replacing deep memory with linear memory, removing (2) convolution, (3) momentum in the surprise measure, (4) weight decay (or forgot mechanism), and (5) persistent memory. The results are reported in Table 5 . All components of neural memory design are positively contributing to its performance, where the greatest contribution comes from weight decay, momentum, convolution, and persistent memory, respectively. Table 5 : Ablation Study on Titans. All components of Titans are positively contributing to its performance. Model Language Modeling Reasoning Long Context ppl ↓ ↓ \downarrow ↓ acc ↑ ↑ \uparrow ↑ acc ↑ ↑ \uparrow ↑ LMM 27.01 47.83 92.68 + Attn (MAC) 26.67 48.65 97.95 + Attn (MAG) 25.70 48.60 96.70 + Attn (MAL) 25.91 47.87 96.91 Linear Memory 28.49 46.97 85.34 w/o Convolution 28.73 45.82 90.28 w/o Momentum 28.98 45.49 87.12 w/o Weight Decay 29.04 45.11 85.60 w/o Persistent Memory 27.63 46.35 92.49 The Effect of Architectural Design. To evaluate the effect of architecture design, we compare the performance of three represented variants of Titans in three aspects of (i) language modeling, (ii) commen-sense reasoning, and (iii) long context NIAH (BABILong) tasks. The results are reported in Table 5 . We find that MAC and MAG have close performance in language modeling and common-sense reasoning tasks, while MAC achieve significantly better performance in long-context NIAH. Both of these models achieve better performance than MAL. These results along with Figure 9 , show a trade-off between fast training and more expressive design. 6 Conclusion In this paper, we present a neural long-term memory that, as a meta in-context learner, learns to memorize at test time. The neural memory module is a recurrent model in nature, and is adaptively memorizing tokens that are more surprising or are close to surprising tokens. Comparing to modern recurrent models, it has more expressive memory update and storing mechanism. Using this memory, we present Titans architectures, and its three variants, in which we suggest to incorporate the memory module as (1) a context, (2) gating, and (3) a layer. Our experimental evaluation on diverse tasks tasks validate that Titans are more effective than Transformers and recent modern linear recurrent models, specifically for long context. That is, Titans can scale to larger than 2M context window size with better accuracy than baselines. Titans are implemented in Pytorch and JAX and we intend to make the code we used to train and evaluate our models available soon. \printbibliography Appendix A Related Work There are diverse perspectives that can independently lead to the design of Titans or its components. Accordingly, to further situate our work in a broader context, we review three categories of studies: A.1 Linear Recurrent Models Recently, to address the computational cost of Transformers in both training and inference, linear recurrent models have attracted much attention \parencite tiezzi2024resurgence, mainly due to their fast inference and training. The first generation of models–such as RetNet \parencite sun2023retentive, LRU \parencite orvieto2023resurrecting, RWKV \parencite peng2023rwkv, S5 \parencite smith2023simplified, and S4 \parencite gu2022efficiently–uses data-independent transition matrix/decay mechanism. The second generation of such models started to incorporate gating mechanism, a widely used techniques in traditional RNNs \parencite gers2000learning, greff2016lstm, van2018unreasonable, into such linear architectures–e.g., Griffin \parencite de2024griffin, SSMs \parencite hasani2023liquid, behrouz2024mambamixer, dao2024transformers, gu2024mamba, RWKV6 \parencite peng2024eagle. The third generation of linear recurrent models are based on more complex memory updating rule based on meta-learning, online learning, and/or delta-rule, resulting in more expressive and effective models such as: Longhorn \parencite liu2024longhorn, Gated DeltaNet \parencite yang2024gated, TTT \parencite sun2024learning, and DeltaNet \parencite yang2024parallelizing. Our LMM model can be seen as the next generation of such models, in which we incorporate the token flow into the memory updating mechanism, having more powerful memory updating process. See Appendix C for a detailed discussion of different recurrent models and Titans. A.2 Transformer-based Architectures Transformers. Transformers \parencite transformers as the de facto backbone for many deep learning models are based on attention mechanism \parencite bahdanau2014neural. They, however, suffer from quadratic computational cost, limiting their ability to scale to long context window. To improve the memory consumption and throughput of softmax attention for longer sequences, various studies focused on I/O aware implementations of attention \parencite flashattention-1, dao2024flashattention, designing more efficient attention mechanisms by sparsifying the attention matrix \parencite choromanski2021rethinking, dai2019transformerxl, chen2021scatterbrain, roy2021efficient, chen2021scatterbrain, dong2024flex, approximating the softmax \parencite arora2024simple, or developing kernel-based (linear) attentions \parencite kacham2024polysketchformer, schlag2021linear, yang2024gatedattn, aksenov2024linear. Segment-based Transformers. Another line of research to improve the efficiency of Transformers is segment-based or Chunk Transformers \parencite dai2019transformerxl. The main drawback of chunk Transformers is that segments are fully separated and so the context window is limited to the length of the chunks. To address this issue, various studies discuss the importance of a memory so it can help the model to transfer information across chunks \parencite bulatov2022recurrent, rodkin2024associative, wu2020memformer, zancato2024bmojo, hutchins2022block, feng2022learn, hutchins2022block, bulatov2023scaling, wang2019r, wu2020memformer, zancato2024bmojo. The key differences of Titans with these models are: (1) The memory in such models are simple small size vectors, lacking expressive power to compress complex information; (2) The memory module lacks forget mechanism, leading to a fast memory overflow; (3) only focus on momentary surprise, missing the information flow. More specifically, recalling Recurrent Memory Transformers (RMT) \parencite bulatov2022recurrent, rodkin2024associative, bulatov2023scaling, one can treat Titans (MAC) as the generalization of RMT, where we use a neural memory module instead of a vector-valued small size memory. Memory for Large Language Models. Another interesting research direction has been to incorporate external memory modules to LLMs after training \parencite he2024camelot, Khandelwal2020Generalization, wang2024memoryllm. Such models are different from our approach as we incorporate the memory as a part of initial architecture and so we train it in an end-to-end manner. Also, most of these explicit memory modules suffer from the same limitations as chunk-based Transformers (mentioned above). For a detailed discussion of such models, we refer to the recent study of \textcite wang2024towards. A.3 Test Time Training and Fast Weight Programs Memory Design and Augmentation with Memory. In the literature, a substantial research effort have been toward designing memory modules that are capable of either memorizing the knowledge abstraction (e.g., persistent memory) \parencite sukhbaatar2019augmenting, or memorizing the data-dependent information (also known as contextual memory), through recurrence \parencite zancato2024bmojo, bulatov2022recurrent, rodkin2024associative, Transformers \parencite munkhdalai2024leave, zhang2024memory, cetin2024evolved, berges2024memory, le2020self, feng2022learn, gradient \parencite munkhdalai2019metalearned, irie2022dual, or other learning paradigms \parencite weston2014memory, sukhbaatar2015end. These memory models, however, either (1) are based on momentary surprise, missing the data flow and events, (2) lack forget mechanisms to remove the memory, leading to a fast memory overflow (3) are fixed-size shallow (matrix valued) memory, resulting in poor performance in long context, and (4) are based on fixed parameters at test time, lacking test time adaption. Fast Weight Programs. The idea of seeing linear layers as the key-value (associative) memory system backs to fast weight programs, in which dynamic fast programs are incorporated into recurrent neural networks to serve as writable memory \parencite schlag2021linear, schmidhuber1992learning, schmidhuber1993reducing. The two learning rules of Hebbian \parencite hebb2005organization and delta \parencite prados1989neural are the most popular learning rules for fast weight programs, which have been extensively explored in various studies \parencite munkhdalai2017neural, schmidhuber1992learning, munkhdalai2019metalearned, schlag2021linear, irie2021going, yang2024parallelizing, yang2024gated. All these models, however, are based on momentary surprise, missing the token flow in the sequences (see Section 3.1 ), and most of them lacks a forgetting gate, resulting in a poor memory management. Test Time Training. The key ideas of learning at test time or learning to learn (i.e., \parencite andrychowicz2016learning) backs to very early studies on local learning [ bottou1992local ] , in which each test data sample is trained on its neighbors before making a prediction \parencite zhang2006svm, gandelsman2022test. This approach further has shown promising performance in vision tasks \parencite jain2011online, mullapudi2019online, mostly due to their ability to mitigate out-of-distribution samples. The most similar studies to ours in this direction are MNM \parencite munkhdalai2019metalearned and TTT-layer \parencite sun2024learning, which we discussed the key differences in Appendix C . Appendix B Language Modeling and Common-sense Reasoning Datasets Following recent studies on linear recurrent models \parencite yang2024gated, dao2024transformers, yang2024parallelizing, we use Wikitext \parencite merity2017pointer, LMB \parencite paperno-etal-2016-lambada, PIQA \parencite bisk2020piqa, HellaSwag \parencite zellers-etal-2019-hellaswag, WinoGrande \parencite sakaguchi2021winogrande, ARC-easy (ARC-e) and ARC-challenge (ARC-c) \parencite clark2018think, SIQA \parencite sap-etal-2019-social, and BoolQ \parencite clark-etal-2019-boolq. Also, the baselines results for 400M models are from the reported results by \textcite yang2024gated. Appendix C Long-term Memory Module (LMM) as a Sequence Model In this section, we discuss how LMM as a sequence model is connected to modern linear recurrent models. For the sake of simplicity, we start with a linear memory, where ℳ t = W t ∈ ℝ d in × d in subscript ℳ 𝑡 subscript 𝑊 𝑡 superscript ℝ subscript 𝑑 in subscript 𝑑 in \mathcal{M}_{t}=W_{t}\in\mathbb{R}^{d_{\text{in}}\times d_{\text{in}}} caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_W start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT end_POSTSUPERSCRIPT . In this case, our objective function becomes ℓ ⁢ ( ℳ ; x t ) = 1 2 ⁢ ‖ ℳ t ⁢ 𝐤 t − 𝐯 t ‖ 2 2 ℓ ℳ subscript 𝑥 𝑡 1 2 superscript subscript norm subscript ℳ 𝑡 subscript 𝐤 𝑡 subscript 𝐯 𝑡 2 2 \ell(\mathcal{M};x_{t})=\frac{1}{2}\left\|\mathcal{M}_{t}\mathbf{k}_{t}-%
\mathbf{v}_{t}\right\|_{2}^{2} roman_ℓ ( caligraphic_M ; italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , in which we use gradient descent with momentum and weight decay for the optimization. Accordingly, revisiting the recurrent formula in Equation 13 : ℳ t = diag ⁢ ( 1 − α t ) ⁢ ℳ t + S t subscript ℳ 𝑡 diag 1 subscript 𝛼 𝑡 subscript ℳ 𝑡 subscript 𝑆 𝑡 \displaystyle\mathcal{M}_{t}=\texttt{diag}\left(1-\alpha_{t}\right)\mathcal{M}%
_{t}+S_{t} caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = diag ( 1 - italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) caligraphic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT (32) S t = diag ⁢ ( η t ) ⁢ S t − 1 − diag ⁢ ( θ t ) ⁢ ( ℳ t − 1 ⁢ 𝐤 t ⊤ ⁢ 𝐤 t − 𝐯 t ⊤ ⁢ 𝐤 t ) . subscript 𝑆 𝑡 diag subscript 𝜂 𝑡 subscript 𝑆 𝑡 1 diag subscript 𝜃 𝑡 subscript ℳ 𝑡 1 superscript subscript 𝐤 𝑡 top subscript 𝐤 𝑡 superscript subscript 𝐯 𝑡 top subscript 𝐤 𝑡 \displaystyle S_{t}=\texttt{diag}\left(\eta_{t}\right)S_{t-1}-\texttt{diag}%
\left(\theta_{t}\right)\left(\mathcal{M}_{t-1}\mathbf{k}_{t}^{\top}\mathbf{k}_%
{t}-\mathbf{v}_{t}^{\top}\mathbf{k}_{t}\right). italic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = diag ( italic_η start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) italic_S start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT - diag ( italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ( caligraphic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT bold_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . (33) LMM is Generalized Gated DeltaNet. As discussed by \textcite yang2024gated, DeltaNet \parencite yang2024parallelizing can alternatively be interpreted as an online learning problem that optimizes the ℒ = 1 2 ⁢ ‖ 𝐒 t ⁢ 𝐤 t − 𝐯 t ‖ 2 2 ℒ 1 2 superscript subscript norm subscript 𝐒 𝑡 subscript 𝐤 𝑡 subscript 𝐯 𝑡 2 2 \mathcal{L}=\frac{1}{2}\left\|\mathbf{S}_{t}\mathbf{k}_{t}-\mathbf{v}_{t}%
\right\|_{2}^{2} caligraphic_L = divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ bold_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , resulting in: 𝐒 t + 1 = 𝐒 t − θ t ⁢ ∇ ℒ = 𝐒 t ⁢ ( 𝐈 − θ t ⁢ 𝐤 t ⁢ 𝐤 t ⊤ ) + θ t ⁢ 𝐯 t ⁢ 𝐤 t ⊤ . subscript 𝐒 𝑡 1 subscript 𝐒 𝑡 subscript 𝜃 𝑡 ∇ ℒ subscript 𝐒 𝑡 𝐈 subscript 𝜃 𝑡 subscript 𝐤 𝑡 superscript subscript 𝐤 𝑡 top subscript 𝜃 𝑡 subscript 𝐯 𝑡 subscript superscript 𝐤 top 𝑡 \displaystyle\mathbf{S}_{t+1}=\mathbf{S}_{t}-\theta_{t}\nabla\mathcal{L}=%
\mathbf{S}_{t}\left(\mathbf{I}-\theta_{t}\mathbf{k}_{t}\mathbf{k}_{t}^{\top}%
\right)+\theta_{t}\mathbf{v}_{t}\mathbf{k}^{\top}_{t}. bold_S start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = bold_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∇ caligraphic_L = bold_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_I - italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) + italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_k start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . (34) In this formulation, Gated DeltaNet is the same as above but with an additional weight decay term \parencite yang2024gated. Comparing Equation 32 and Equation 34 , we can see that setting η t = 0 subscript 𝜂 𝑡 0 \eta_{t}=0 italic_η start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0 results in both formulations to be equivalent. Accordingly, we can say LMM is generalizing the very recent study of Gated DeltaNet \parencite yang2024gated from three aspects: • Momentum-based Rule : The Delta Rule is based on momentary surprise, meaning that the flow of tokens cannot affect the memory update rule. LMM, however, is based on a momentum rule, which consider both past and momentary surprise. • Deep Memory : While Gated DeltaNet is limited to a linear (matrix-valued) memory as it requires finding the closed recurrence form, LMM allows using deep memory module by using a gradient-based formulation, resulting in higher expressive power. • Non-Linear Recurrence : While DeltaNet and Gated DeltaNet are based on linear recurrence, our LMM is using inter-chunk non-linear recurrence and intra-chunk linear recurrence. This design allows LMM having a higher expressive power. Here, we discussed Gated DeltaNet as a sample of recent generation of recurrent models. Similar approaches such as RWKV-7 \parencite rwkv-repo are also using the same formulation and loss function, and so LMM is generalizing all such models. LMM is Generalized Longhorn. Similar to DeltaNet, Longhorn \parencite liu2024longhorn uses the same loss function but it derives the closed form using implicit online learning: 𝐒 t + 1 = 𝐒 t ⁢ ( 𝐈 − δ t ⁢ 𝐤 t ⁢ 𝐤 t ⊤ ) + δ t ⁢ 𝐯 t ⁢ 𝐤 t ⊤ , subscript 𝐒 𝑡 1 subscript 𝐒 𝑡 𝐈 subscript 𝛿 𝑡 subscript 𝐤 𝑡 superscript subscript 𝐤 𝑡 top subscript 𝛿 𝑡 subscript 𝐯 𝑡 subscript superscript 𝐤 top 𝑡 \displaystyle\mathbf{S}_{t+1}=\mathbf{S}_{t}\left(\mathbf{I}-\delta_{t}\mathbf%
{k}_{t}\mathbf{k}_{t}^{\top}\right)+\delta_{t}\mathbf{v}_{t}\mathbf{k}^{\top}_%
{t}, bold_S start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = bold_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_I - italic_δ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) + italic_δ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_k start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , (35) where δ t = θ t 1 + θ t ⁢ 𝐤 t ⁢ 𝐤 t ⊤ subscript 𝛿 𝑡 subscript 𝜃 𝑡 1 subscript 𝜃 𝑡 subscript 𝐤 𝑡 superscript subscript 𝐤 𝑡 top \delta_{t}=\frac{\theta_{t}}{1+\theta_{t}\mathbf{k}_{t}\mathbf{k}_{t}^{\top}} italic_δ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG 1 + italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_ARG .
It, however, lacks a forgetting gate, resulting in a faster memory overflow. Therefore, in addition two the abovementioned aspects of (1) Momentum-based Rule , (2) Deep Memory , and (3) Non-Linear Recurrence , LMM has the advantage of using an additional (4) Forget Gate , leading to a better memory management. LMM is Generalized TTT Layer. To the best of our knowledge, TTT \parencite sun2024learning, is the only modern linear recurrent models with a gradient-based updating rule. In addition to different architectural designs and also objective functions, our LMM has three key differences with presented TTT layers \parencite sun2024learning: 1. Forgetting Mechanism : TTT layers are updating memory at each time, without having the chance to forget the past data. Accordingly, when fixing the memory size, the model cannot manage the memory for long sequences. A forget mechanism, such as LMM’s, allows clearing the memory when very past information is not needed anymore. We show that in a general case, this forget mechanism is equivalent to weight decay and provide a fast method to incorporate it into the parallel training. 2. Momentum-based Update Rule : TTT layers are based on momentary surprise, meaning that the flow of tokens cannot affect the memory update rule. LMM, however, is based on a momentum rule, which consider both past and momentary surprise. See Section 3.1 for the motivation of this design. 3. Deep Memory : While TTT-layers allows for deeper memory, the advantages/disadvantages of such deeper memory modules have not been experimentally evaluated. To the best of our knowledge, our neural long-term memory module is the first linear recurrent model with momentum-based update rule. Finally, as a key difference with all the above and other recent linear recurrent studies, note that the hybrid variants of modern linear models–such as Griffin \parencite de2024griffin, DeltaNet \parencite yang2024parallelizing, Gated DeltaNet \parencite yang2024gated, H3 \parencite fu2023hungry, Mamba2 \parencite dao2024transformers, Samba \parencite ren2024samba, etc.–all are based on sequential layer-wise design. We present Titans to show how effectively one can incorporate such memory modules into an architecture.
