Timestamp: 2025-01-09T16:43:59.628988
Title: Efficient and consistent data-driven model selecti.txt
URL: Text file upload
Status: success
Duration: 0:00

Description:
å¥½çš„ï¼Œè¿™æ˜¯å¯¹æ‚¨æä¾›çš„å†…å®¹çš„æ€»ç»“ï¼š

**1. ä¸»è¦æ€æƒ³æ¦‚è¿°**

  * æœ¬æ–‡ç ”ç©¶äº†ä¸€å¤§ç±»å› æœæ—¶é—´åºåˆ—æ¨¡å‹ï¼ˆåŒ…æ‹¬ARMAã€GARCHç­‰ï¼‰ä¸­çš„æ¨¡å‹é€‰æ‹©é—®é¢˜ã€‚
  * é¦–å…ˆç ”ç©¶äº†ç†æƒ³æƒ©ç½šé¡¹çš„æ¸è¿‘è¡Œä¸ºï¼Œè¯¥ç†æƒ³æƒ©ç½šé¡¹æ—¨åœ¨æœ€å°åŒ–åŒ…å«çœŸå®æ¨¡å‹çš„æœ‰é™æ¨¡å‹æ—ä¸­çš„é£é™©ã€‚
  * å…¶æ¬¡ï¼Œä¸ºè·å¾—ä¸€è‡´æ€§å’Œæœ‰æ•ˆæ€§ï¼Œè®ºæ–‡æå‡ºäº†æƒ©ç½šé¡¹éœ€æ»¡è¶³çš„æ¡ä»¶ã€‚
  * è¯æ˜äº†ä¸€è‡´æ¨¡å‹é€‰æ‹©æ ‡å‡†åœ¨æ•ˆç‡æ–¹é¢ä¼˜äºç»å…¸çš„AICæ ‡å‡†ã€‚
  * æœ€åï¼Œä»è´å¶æ–¯æ–¹æ³•æ¨å¯¼å‡ºäº†BICå‡†åˆ™ï¼Œå¹¶é€šè¿‡ä¿ç•™æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼çš„æ‰€æœ‰äºŒé˜¶é¡¹ï¼Œæå‡ºäº†ä¸€ä¸ªæ•°æ®é©±åŠ¨çš„å‡†åˆ™KCâ€™ã€‚
  * Monte-Carloå®éªŒéªŒè¯äº†æ¸è¿‘ç»“æœï¼Œå¹¶è¡¨æ˜KCâ€™å‡†åˆ™åœ¨ä¸€è‡´æ€§å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºAICå’ŒBICå‡†åˆ™ã€‚

**2. è¯¦ç»†æ€»ç»“ï¼ˆå¤§çº²ç»“æ„ï¼‰**
   * **å¼•è¨€**
        * æ¨¡å‹é€‰æ‹©æ˜¯ç»Ÿè®¡å’Œæ•°æ®ç§‘å­¦ä¸­çš„åŸºæœ¬ä»»åŠ¡ã€‚
        * åŸºäºæƒ©ç½šé£é™©æœ€å°åŒ–çš„æ–¹æ³•æ›´é€‚åˆæ—¶é—´åºåˆ—ï¼Œå› ä¸ºå®ƒè€ƒè™‘äº†æ•°æ®ä¹‹é—´çš„ä¾èµ–æ€§ã€‚
        * æƒ©ç½šé¡¹çš„æ ¡å‡†æ˜¯å…³é”®ï¼Œéœ€æ ¹æ®ç›®æ ‡ï¼ˆä¸€è‡´æ€§ã€æ•ˆç‡ç­‰ï¼‰è¿›è¡Œè°ƒæ•´ã€‚
        *  ä¸€è‡´æ€§æ—¨åœ¨ä»¥é«˜æ¦‚ç‡è¯†åˆ«æ•°æ®ç”Ÿæˆè¿‡ç¨‹ï¼›æ•ˆç‡åˆ™æŒ‡æ¨¡å‹é£é™©åœ¨æ¸è¿‘æ„ä¹‰ä¸Šç­‰ä»·äºoracleæ¨¡å‹çš„é£é™©ã€‚
        * æœ¬æ–‡æ—¨åœ¨ä¸ºä»¿å°„å› æœæ—¶é—´åºåˆ—æä¾›æœ‰æ•ˆä¸”ä¸€è‡´çš„æƒ©ç½šæ•°æ®é©±åŠ¨æ ‡å‡†ã€‚
  *  **æ¨¡å‹é€‰æ‹©æ¡†æ¶**
        *   è€ƒè™‘å‚æ•°ä»¿å°„å› æœæ¨¡å‹çš„æœ‰é™æ— â„³ã€‚
        *   ä½¿ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰å’Œå‡†æå¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆQMLEï¼‰ã€‚
        *  å®šä¹‰äº†é£é™©å‡½æ•° R(Î¸) å’ŒæŸå¤±å‡½æ•° â„“(Î¸, Î¸âˆ—)ã€‚
        *  å¼•å…¥äº†æƒ©ç½šç¨‹åºï¼Œé€šè¿‡æ·»åŠ æƒ©ç½šé¡¹æ¥è§£å†³ç»éªŒé£é™©æœ€å°åŒ–ï¼ˆERMï¼‰çš„ä¹è§‚åå·®é—®é¢˜ã€‚
        *   ç†æƒ³çš„æƒ©ç½šé¡¹å®šä¹‰ä¸ºï¼špen_id(m) = R(Î¸Ì‚m) - Î³Ì‚n(Î¸Ì‚m)ï¼Œç›®çš„æ˜¯ä½¿æ¨¡å‹é€‰æ‹©å™¨èƒ½å¤Ÿæ¨¡æ‹Ÿoracleæ¨¡å‹ã€‚
   * **QMLE çš„æ¸è¿‘è¡Œä¸º**
        *  é˜è¿°äº† QMLE çš„è®°å·å’Œä¸»è¦å‡è®¾ï¼ŒåŒ…æ‹¬ï¼š
             *  A0:å…³äºè¿‡ç¨‹ X çš„å‡è®¾ï¼Œä¿è¯å¹³ç¨³å› æœè§£çš„å­˜åœ¨
            *  A1:å‚æ•° Î¸ çš„å¯è¯†åˆ«æ€§
            *  A2:ç¡®ä¿æ¸è¿‘åæ–¹å·®çŸ©é˜µçš„é€†å­˜åœ¨
            *  A3:HÎ¸ çš„ä¸‹ç•Œä¸ºæ­£
             *  A4:å…³äºHessiançŸ©é˜µçš„æ¡ä»¶
             *  A5:ç¡®ä¿ QMLE å¼ºä¸€è‡´æ€§å’Œæ¸è¿‘æ­£æ€æ€§çš„è¡°å‡ç‡
        *   å¾—åˆ°äº† QMLE çš„æ¸è¿‘æ­£æ€æ€§å®šç†ï¼Œå³ä½¿åœ¨æ¨¡å‹è¯¯åˆ¤çš„æƒ…å†µä¸‹ä¹Ÿæˆç«‹ã€‚
  * **æœ‰æ•ˆçš„æ¨¡å‹é€‰æ‹©**
       *  å¯¹ç†æƒ³æƒ©ç½šé¡¹çš„æœŸæœ›è¿›è¡Œåˆ†è§£ï¼Œæ–¹ä¾¿è®¡ç®—ã€‚
       *   è¯æ˜äº†ç†æƒ³æƒ©ç½šé¡¹çš„æœŸæœ›å¯ä»¥è¢«ç»´åº¦æ¨¡å‹ |m| è¿‘ä¼¼ã€‚
       *   æå‡ºäº†ä¸€ä¸ªè¿‘ä¼¼ç†æƒ³æƒ©ç½šé¡¹æœŸæœ›çš„æƒ©ç½šé¡¹ penÌƒ(m)ã€‚
        *   è¯æ˜äº†å¦‚æœæƒ©ç½šé¡¹åœ¨æ¦‚ç‡ä¸Šæ¸è¿‘è¶‹äºé›¶ï¼Œåˆ™è¯¥å‡†åˆ™ä¸ä¼šæ¸è¿‘é€‰æ‹©è¯¯åˆ¤æ¨¡å‹ã€‚
        *   æå‡ºæ¡ä»¶ï¼Œä»¥è·å¾—æ¥è¿‘æ¨¡å‹æ—ä¸­æœ€å°æŸå¤±çš„è¶…é¢æŸå¤±ã€‚
        *   å¯¹æƒ©ç½šé¡¹æå‡ºè¿›ä¸€æ­¥è¦æ±‚ï¼Œä»¥è·å¾—ä¸€è‡´çš„é€‰æ‹©å‡†åˆ™ã€‚
        *   è¯æ˜äº†æ»¡è¶³è¿™äº›æ¡ä»¶çš„å‡†åˆ™ï¼ˆä¾‹å¦‚ï¼ŒBICï¼‰æ»¡è¶³æ›´ä¸¥æ ¼çš„æ•ˆç‡ä¸ç­‰å¼ã€‚
        *  è¯æ˜äº† AIC å‡†åˆ™ç­‰å…·æœ‰å›ºå®šé™¤ä»¥ n çš„æƒ©ç½šé¡¹çš„å‡†åˆ™ï¼Œä¼šäº§ç”Ÿè¿‡åº¦æ‹Ÿåˆä¸”æ•ˆç‡è¾ƒä½ã€‚
  *  **ä»è´å¶æ–¯æ¨¡å‹é€‰æ‹©åˆ°æ•°æ®é©±åŠ¨çš„ä¸€è‡´æ¨¡å‹é€‰æ‹©**
        *   ä»‹ç»äº†è´å¶æ–¯æ¨¡å‹é€‰æ‹©ï¼Œè¿™é€šå¸¸ä¼šå¾—å‡º BIC å‡†åˆ™ã€‚
        *   é€šè¿‡æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼ï¼Œå¾—åˆ°äº†åéªŒæ¦‚ç‡çš„æ¸è¿‘å±•å¼€ï¼Œæ¨å¯¼å‡ºBICå‡†åˆ™ã€‚
        *   æå‡ºäº†è€ƒè™‘æ‰€æœ‰äºŒé˜¶é¡¹çš„KCå‡†åˆ™å’ŒKCâ€™å‡†åˆ™ã€‚
       *   è¯æ˜äº† BICã€KC å’Œ KC' æ˜¯æ¸è¿‘ä¸€è‡´çš„æ¨¡å‹é€‰æ‹©å‡†åˆ™ã€‚
  *   **ç†æƒ³æƒ©ç½šæ¸è¿‘æœŸæœ›çš„è®¡ç®—ç¤ºä¾‹**
         *   ç»™å‡ºäº†åœ¨å‡ ç§å¸¸è§æ—¶é—´åºåˆ—æ¨¡å‹ä¸­ Trace((Fm(Î¸âˆ—m))âˆ’1Gm(Î¸âˆ—m)) çš„è®¡ç®—ç¤ºä¾‹ã€‚
         *  å¯¹äºé«˜æ–¯æ¨¡å‹ï¼Œä¸ç»å…¸çš„ AIC æƒ©ç½šé¡¹ç›¸ä¸€è‡´ã€‚
        *    å¯¹äº ARMA æ¨¡å‹ï¼Œç†æƒ³æƒ©ç½šçš„æœŸæœ›æ¥è¿‘ AIC æƒ©ç½šé¡¹ã€‚
       *    å¯¹äº GARCH å’Œ APARCH æ¨¡å‹ï¼Œç†æƒ³æƒ©ç½šçš„æœŸæœ›ä¸æ¨¡å‹ç»´åº¦æˆæ¯”ä¾‹ã€‚
   *   **æ•°å€¼ç ”ç©¶**
        * ä½¿ç”¨ R è½¯ä»¶ï¼Œé€šè¿‡ Monte-Carlo å®éªŒéªŒè¯äº†ç†è®ºç»“æœã€‚
       *  æ¯”è¾ƒäº†ä¸‰ä¸ªæ•°æ®ç”Ÿæˆè¿‡ç¨‹(AR(2)ã€ARMA(1,1)å’ŒGARCH(1,1))çš„ AICã€BIC å’Œ KCâ€™ æ ‡å‡†ã€‚
        *   å®éªŒç»“æœè¡¨æ˜ï¼ŒBIC å’Œ KCâ€™ åœ¨é€‰æ‹©çœŸå®æ¨¡å‹æ–¹é¢è¶‹äºä¸€è‡´ï¼›KCâ€™ çš„æ€§èƒ½ä¼˜äº BICã€‚
         *   è¿˜è¡¨æ˜ï¼ŒBIC å’Œ KCâ€™ å‡†åˆ™äº§ç”Ÿçš„å‰©ä½™é¡¹ ğ‘€Ì‚ğ¸ ä¼šéšç€ n çš„å¢åŠ è€Œå‡å°‘è‡³ 0ï¼›AIC äº§ç”Ÿçš„ ğ‘€Ì‚ğ¸ ä¸æ”¶æ•›è‡³ 0ï¼Œä»è€ŒéªŒè¯äº†æ•ˆç‡æ–¹é¢çš„ç†è®ºç»“æœã€‚
  *   **è¯æ˜**
        *   ç»™å‡ºäº†æ‰€æœ‰å®šç†å’Œå‘½é¢˜çš„è¯æ˜ã€‚

**3. æ ¸å¿ƒç»“è®ºï¼ˆä¸€å¥è¯ï¼‰**
   *  æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ•°æ®é©±åŠ¨çš„ã€æ¸è¿‘ä¸€è‡´ä¸”æ›´æœ‰æ•ˆçš„æ¨¡å‹é€‰æ‹©å‡†åˆ™ KC' ï¼Œè¯¥å‡†åˆ™åœ¨ç†è®ºå’Œæ•°å€¼ä¸Šå‡ä¼˜äº AIC å’Œ BIC å‡†åˆ™ï¼Œé€‚ç”¨äºå› æœä»¿å°„æ—¶é—´åºåˆ—æ¨¡å‹ã€‚

**4. åŸºæœ¬ç»“è®ºï¼ˆä¸€å¥è¯ï¼‰**
   *  åœ¨æ¨¡å‹é€‰æ‹©ä¸­ï¼Œä¸€è‡´æ€§å‡†åˆ™ï¼ˆå¦‚BICå’ŒKC'ï¼‰ä¸ä»…èƒ½æ¸è¿‘åœ°é€‰æ‹©æ­£ç¡®çš„æ¨¡å‹ï¼Œè¿˜èƒ½å®ç°æ›´é«˜çš„æ•ˆç‡ï¼Œå°¤å…¶æ˜¯åœ¨çœŸå®æ¨¡å‹å­˜åœ¨äºç«äº‰æ¨¡å‹é›†åˆä¸­çš„æƒ…å†µä¸‹ã€‚

**5.  æ€»ä½“æ¡†æ¶**

   è¯¥ç ”ç©¶çš„æ€»ä½“æ¡†æ¶å¯ä»¥æ¦‚æ‹¬ä¸ºï¼šå®šä¹‰å› æœä»¿å°„æ—¶é—´åºåˆ—æ¨¡å‹ -> å¼•å…¥æå¤§ä¼¼ç„¶ä¼°è®¡å’Œå‡†æå¤§ä¼¼ç„¶ä¼°è®¡ -> åŸºäºæœ€å°åŒ–æŸå¤±å‡½æ•°å¼•å…¥æƒ©ç½šæ¨¡å‹é€‰æ‹© -> åˆ†æç†æƒ³æƒ©ç½šçš„æ¸è¿‘è¡Œä¸º -> æ¨å¯¼æ•°æ®é©±åŠ¨çš„ã€ä¸€è‡´ä¸”æ›´æœ‰æ•ˆçš„æ¨¡å‹é€‰æ‹©å‡†åˆ™ -> é€šè¿‡æ•°å€¼æ¨¡æ‹ŸéªŒè¯ç†è®ºç»“æœã€‚

**6. æ¦‚å¿µå›¾**
<Mermaid_Diagram>
graph LR
    A[å› æœä»¿å°„æ—¶é—´åºåˆ—æ¨¡å‹] --> B(æå¤§ä¼¼ç„¶ä¼°è®¡/å‡†æå¤§ä¼¼ç„¶ä¼°è®¡);
    B --> C{å®šä¹‰é£é™©å’ŒæŸå¤±å‡½æ•°};
    C --> D[å¼•å…¥æƒ©ç½šæ¨¡å‹é€‰æ‹©];
    D --> E{åˆ†æç†æƒ³æƒ©ç½šé¡¹çš„æ¸è¿‘è¡Œä¸º};
     E --> F[æ¨å¯¼ BIC/KC/KC' ];
     F --> G(ç†è®ºç»“æœ);
     G --> H[æ•°å€¼å®éªŒéªŒè¯];
     H --> I(ç»“è®º: KC' æ›´æœ‰æ•ˆ);
    style F fill:#f9f,stroke:#333,stroke-width:2px
</Mermaid_Diagram>


Content:
Efficient and consistent data-driven model selection for time series
Jean-Marc Bardet label=e1]bardet@univ-paris1.fr
[
Kamila Kare label=e2]kamilakare@gmail.com
[
William Kengne label=e3]william.kengne@u-cergy.fr
[ University Paris 1 PanthÃ©on-Sorbonne, SAMM, France, , CY Cergy Paris UniversitÃ©, THEMA, France,
Abstract
This paper studies the model selection problem in a large class of causal time series models, which includes both the ARMA or AR(
âˆ
) processes, as well as the GARCH or ARCH(
âˆ
), APARCH, ARMA-GARCH and many others processes. We first study the asymptotic behavior of the ideal penalty that minimizes the risk induced by a quasi-likelihood estimation among a finite family of models containing the true model. Then, we provide general conditions on the penalty term for obtaining the consistency and efficiency properties. We notably prove that consistent model selection criteria outperform classical AIC criterion in terms of efficiency. Finally, we derive from a Bayesian approach the usual BIC criterion, and by keeping all the second order terms of the Laplace approximation, a data-driven criterion denoted KCâ€™. Monte-Carlo experiments exhibit the obtained asymptotic results and show that KCâ€™ criterion does better than the AIC and BIC ones in terms of consistency and efficiency.

62G05,
62G20,
62M05,
keywords: [class=MSC2010]
\startlocaldefs\endlocaldefs
, and

Contents
1Introduction
2Model selection framework
2.1Finite family 
â„³
 of parametric affine causal models
2.2Maximum Likelihood Estimation
2.3Quasi-Maximum Likelihood Estimation
2.4The penalization procedure
3Asymptotic behavior of the QMLE
3.1Notations and main assumptions
3.2New asymptotic results satisfied by 
Î¸
^
m
4Efficient model selection
5From a Bayesian model selection to a data-driven consistent model selection
6Examples of computations of the asymptotic expectation of ideal penalties
7Numerical Studies
8Proofs
8.1Proofs of Section 3
8.2Proofs of Section 4
1Introduction
Model selection is one of the fundamental tasks in Statistics and Data Science. It aims at providing a model (or an algorithm) that is the best, following a criterion, to represent observed data. Two leading model selection procedures have received a lot of attention in the literature. On one hand, the resampling methods such as hold out or more generally 
V
-fold cross-validation are widely used in the machine learning community. On the other hand, the methods based on the minimization of a penalized risk are also now very popular in the community of applied or theoretical statisticians. They are certainly more appropriate to be applied to time series since they take into account the dependence between data. This will be our choice in this paper.
 
The main challenging task when designing a penalized based criterion is the calibration of the penalty. This is mainly dependent on the goal one would like the final criterion achieves. For instance, the objective could be the consistency, the efficiency or the adaptive nature in the minimax sense to name a few.
 
The consistency property aims at identifying the data generating process with high probability. Hence, it requires the assumption whereby there exists a true model in the set of competitive models and the goal is to select this with probability approaches one as the sample size tends to infinity. Although the consistency is a convincing mathematical property, this asymptotic property is not always the most interesting when switching to a practical implementation. Indeed, the true underlying process is generally unknown and trying to identify the true model for any data is quite ambitious. It is often more plausible to assume that the true data generating process is infinite-dimensional, and that one tries to identify a â€goodâ€ finite-dimensional model based on the data ([19]). Therefore, it is common in this framework to let the dimension of the competitive models to depend on the number of observations in order to obtain a better approximation and to reduce the predictionâ€™s risk. Hence, the model selection is said to be efficient when its risk is asymptotically equivalent to the risk of the oracle.
 
In this work, we are interested by providing efficient and consistent penalized data-driven criteria for affine causal time series, which are defined by:
 
Class 
ğ’œ
â€‹
ğ’
â€‹
(
M
,
f
)
:
 A process 
X
=
(
X
t
)
t
âˆˆ
â„¤
 belongs to 
ğ’œ
â€‹
ğ’
â€‹
(
M
,
f
)
 if it satisfies:

X
t
=
M
â€‹
(
(
X
t
âˆ’
i
)
i
âˆˆ
â„•
âˆ—
)
â€‹
Î¾
t
+
f
â€‹
(
(
X
t
âˆ’
i
)
i
âˆˆ
â„•
âˆ—
)
â€‹
for any
â€‹
t
âˆˆ
â„¤
.
(1.1)
where 
(
Î¾
t
)
t
âˆˆ
T
 is a sequence of zero-mean independent and identically distributed random vectors (i.i.d.r.v) satisfying 
ğ”¼
â€‹
(
|
Î¾
0
|
r
)
<
âˆ
 with 
r
â‰¥
1
 and 
M
, 
f
 : 
â„
âˆ
â†’
â„
 are two measurable functions, where 
R
âˆ
 is the set of numeric sequence with finite number of non-zero terms.
 
For instance,

â€¢ if 
M
â€‹
(
(
X
t
âˆ’
i
)
i
âˆˆ
â„•
âˆ—
)
=
Ïƒ
 and 
f
â€‹
(
(
X
t
âˆ’
i
)
i
âˆˆ
â„•
âˆ—
)
=
Ï•
1
â€‹
X
t
âˆ’
1
+
â‹¯
+
Ï•
p
â€‹
X
t
âˆ’
p
, then 
(
X
t
)
t
âˆˆ
â„¤
 is an AR
(
p
)
 process;
â€¢ if 
M
â€‹
(
(
X
t
âˆ’
i
)
i
âˆˆ
â„•
âˆ—
)
=
a
0
+
a
1
â€‹
X
t
âˆ’
1
2
+
â‹¯
+
a
p
â€‹
X
t
âˆ’
p
2
 and 
f
â€‹
(
(
X
t
âˆ’
i
)
i
âˆˆ
â„•
âˆ—
)
=
0
, then 
(
X
t
)
t
âˆˆ
â„¤
 is an ARCH
(
p
)
 process.
Note that numerous classical time series models such as ARMA(
p
,
q
), GARCH(
p
,
q
), ARMA(
p
,
q
)-GARCH(
p
,
q
) (see [13] and [27]) or APARCH
(
Î´
,
p
,
q
)
 processes (see [13]) belong to 
ğ’œ
â€‹
ğ’
â€‹
(
M
,
f
)
.

The study of these causal affine time series more often requires the classical regularity conditions on the functions 
M
 and 
f
 that are not really restrictive and remain valid for many time series.

We will consider the semi-parametric class of models 
ğ’œ
â€‹
ğ’
â€‹
(
M
Î¸
,
f
Î¸
)
 where 
Î¸
âˆˆ
Î˜
 (a compact subset of 
â„
d
, 
d
âˆˆ
â„•
), where 
(
f
Î¸
)
Î¸
âˆˆ
Î˜
 and 
(
M
Î¸
)
Î¸
âˆˆ
Î˜
 are two families of functions such as for 
Î¸
âˆˆ
Î˜
, 
f
Î¸
:
â„
âˆ
â†’
â„
 and 
M
Î¸
:
â„
âˆ
â†’
[
0
,
âˆ
)
 are known and the distribution of 
Î¾
0
 is unknown. A finite family of models 
â„³
 (for instance, the class of AR(p) processes where 
0
â‰¤
p
â‰¤
p
max
) will be considered, where a model 
m
âˆˆ
â„³
 corresponds to a linear subspace of 
â„
d
. A trajectory 
(
X
1
,
â‹¯
,
X
n
)
 generated from the class 
ğ’œ
â€‹
ğ’
â€‹
(
M
Î¸
âˆ—
,
f
Î¸
âˆ—
)
 with the â€trueâ€ model 
m
âˆ—
âˆˆ
â„³
 is supposed to be observed (see Section 2).
 
There already exist several important contributions devoted to the model selection for time series; we refer to the book of [31] and the references therein for an overview on this topic. Also, the time series model selection literature is very extensive and still growing; we refer to the monograph of [33], which provided an excellent summary of existing model selection procedure, including the case of time series models as well as the recent review paper of [12]. The asymptotically efficient selection property has already been tackled in case of linear process type 
A
â€‹
R
â€‹
(
âˆ
)
 by [37], [36], [22], [21], [20], and recently by [18].
 
The study of a consistent model selection in this class of affine causal processes has been also considered by [4] and [25]. As in these papers, we consider here a risk built from the Gaussian conditional log-Likelihood, which is naturally deduced for all causal affine models 
ğ’œ
â€‹
ğ’
â€‹
(
M
Î¸
,
f
Î¸
)
, and consider a model selection criterion as a penalized Gaussian Quasi-Maximum conditional log-Likelihood (see for instance [16] or [5]). This allows us:

1. To study the asymptotic behavior of an ideal penalty that is defined as providing a minimization of the risk;
2. To determine the conditions for obtaining (or not) the asymptotic consistency of a criterion, i.e. that it allows asymptotically to select the true model;
3. To determine the conditions for approaching (or not) in 
1
/
n
 or even in 
o
â€‹
(
1
/
n
)
 the minimal risk, thus to obtain an asymptotic efficiency;
4. To determine from a Bayesian approach the BIC criterion as well as a second data-driven criterion called KCâ€™, which is obtained by keeping all the second order terms in the Laplace approximation and to prove that they verify the properties of asymptotic consistency and efficiency in 
o
â€‹
(
1
/
n
)
.
In the end, we show that in the chosen framework the BIC and KCâ€™ criteria offer all the advantages with respect to the classical AIC criterion, which allows neither the asymptotic consistency nor the same efficiency. Numerical simulations confirm these results and also show that the new data-driven KCâ€™ criterion clearly outperforms the BIC criterion both in terms of consistency and efficiency.
 
The paper is organized as follows. The model selection framework based on Gaussian Likelihood risk is described in Section 2. In Section 3, the precise assumptions are stated and they lead to new asymptotic results satisfied by the Quasi-Maximum Likelihood Estimator. The asymptotic behavior of the ideal penalty is studied in Section 4 as well as some conditions for obtaining the asymptotic efficiency or consistency of a criterion. In Section 5, the usual BIC criterion as well as the data-driven criterion KCâ€™ are studied. Finally, examples are detailed in Section 6, numerical results are presented in Section 7 and Section 8 contains the proofs.

2Model selection framework
2.1Finite family 
â„³
 of parametric affine causal models
Assume a trajectory 
(
X
1
,
â€¦
,
X
n
)
 is observed from a causal stationary solution of (1.1) where 
M
 and 
f
 are two known functions depending on an unknown finite dimensional vector of parameters 
Î¸
âˆ—
.
 
Now consider a finite family 
â„³
 of models belonging to parametric affine causal models. In Proposition 1 of [4], due to the linearity of such models and because 
â„³
 is a finite family, it was established that it is always possible to find a dimension 
d
âˆˆ
â„•
âˆ—
 and a unique couple of known functions 
(
M
Î¸
,
f
Î¸
)
 with 
Î¸
âˆˆ
â„
d
 such as in such a way that any model 
m
âˆˆ
â„³
 belongs to the class 
ğ’œ
â€‹
ğ’
â€‹
(
M
Î¸
,
f
Î¸
)
. More precisely, there is a one-to-one correspondence between each model 
m
âˆˆ
â„³
 and a linear subspace 
Î˜
m
âŠ‚
â„
d
 and 
dim
(
Î˜
m
)
=
|
m
|
 the number of unknown parameters of the model 
m
. As a consequence, if we denote 
m
âˆ—
 the â€trueâ€ model corresponding to 
ğ’œ
â€‹
ğ’
â€‹
(
M
Î¸
âˆ—
,
f
Î¸
âˆ—
)
, we will say:

â€¢ if 
m
âˆˆ
â„³
 is such that 
Î˜
m
âˆ—
âŠ‚
Î˜
m
 and 
Î˜
m
âˆ—
â‰ 
Î˜
m
 (also denoted 
m
âˆ—
âŠ‚
m
 and 
m
âˆ—
â‰ 
m
), this is an overfittingâ€™s case;
â€¢ if 
m
âˆˆ
â„³
 is such that 
Î˜
m
âˆ—
âŠ„
Î˜
m
 (also denoted 
m
âˆ—
âŠ„
m
) , this is a misspecified case.
For example, if 
m
âˆ—
 corresponds to a AR
(
2
)
 process and if 
â„³
 contains AR
(
p
max
)
 processes and ARCH
(
q
max
)
, we have 
d
=
1
+
p
max
+
q
max
 and for 
Î¸
=
(
Î¸
i
)
0
â‰¤
i
â‰¤
d
,

f
Î¸
â€‹
(
(
X
t
âˆ’
k
)
k
â‰¥
1
)
=
âˆ‘
i
=
1
p
max
Î¸
i
â€‹
X
t
âˆ’
i
and
M
Î¸
â€‹
(
(
X
t
âˆ’
k
)
k
â‰¥
1
)
=
(
Î¸
0
+
âˆ‘
i
=
p
max
+
1
p
max
+
q
max
Î¸
i
â€‹
X
t
âˆ’
i
2
)
1
/
2
.
Then 
Î˜
m
âˆ—
=
{
(
Î¸
0
,
Î¸
1
,
Î¸
2
,
0
,
â€¦
,
0
)
,
(
Î¸
0
,
Î¸
1
,
Î¸
2
)
âˆˆ
â„
3
}
, an AR
(
4
)
 process implies an overfitting, while an AR
(
1
)
 or an ARCH
(
2
)
 process implies a misspecified case.
In the sequel, we will always assume that

m
âˆ—
âˆˆ
â„³
.
This true model 
m
âˆ—
 is supposed to be unknown. After observing the trajectory 
(
X
1
,
â€¦
,
X
n
)
, our goal is to find the most probable model (see Section 5) among the finite family 
â„³
 or a â€bestâ€ model that forecasts with a minimum risk. Here we have chosen a risk that is derived from a Quasi-Maximum Likelihood contrast, which is presented below.

2.2Maximum Likelihood Estimation
For each 
Î¸
âˆˆ
Î˜
, we will begin by defining its risk by:

R
â€‹
(
Î¸
)
:=
â„™
â€‹
Î³
â€‹
(
Î¸
)
=
ğ”¼
â€‹
[
Î³
â€‹
(
Î¸
,
X
1
)
]
with
Î³
â€‹
(
Î¸
,
X
t
)
:=
(
X
t
âˆ’
f
Î¸
t
)
2
H
Î¸
t
+
log
â¡
(
H
Î¸
t
)
and
{
f
Î¸
t
:=
f
Î¸
â€‹
(
(
X
t
âˆ’
k
)
k
â‰¥
1
)
M
Î¸
t
:=
M
Î¸
â€‹
(
(
X
t
âˆ’
k
)
k
â‰¥
1
)
H
Î¸
t
:=
(
M
Î¸
t
)
2
.
(2.1)
By referring to [30] or [16], the contrast 
Î³
(
Î¸
,
.
)
 is 
âˆ’
2
 times the Gaussian conditional log-density of 
X
t
. Moreover, the Gaussian Maximum Likelihood Estimator (MLE) is derived from the conditional (with respect to the filtration 
Ïƒ
â€‹
{
(
X
t
)
t
â‰¤
0
}
) log-likelihood of 
(
X
1
,
â€¦
,
X
n
)
 when 
(
Î¾
t
)
 is supposed to be a Gaussian standard white noise. We deduce that this conditional log-likelihood (up to an additional constant) 
L
n
 is defined for a parameter 
Î¸
 by:

L
n
â€‹
(
Î¸
)
:=
âˆ’
1
2
â€‹
âˆ‘
t
=
1
n
Î³
â€‹
(
Î¸
,
X
t
)
.
(2.2)
As it has been proved in [5], under a classical identifiability assumption, the risk function 
R
 achieves its unique minimum at the â€trueâ€â€™ parameter 
Î¸
âˆ—
 over any parameter set 
Î˜
, when 
Î¸
âˆ—
âˆˆ
Î˜
, i.e.

Î¸
âˆ—
=
argmin
Î¸
âˆˆ
Î˜
â€‹
R
â€‹
(
Î¸
)
.
(2.3)
Therefore 
Î¸
âˆ—
 is considered as an ideal predictor for the model selection procedure and serves as a benchmark to compare predictors. Given a model 
m
âˆˆ
â„³
 and 
Î˜
m
 its parameter space that does not necessarily contains 
Î¸
âˆ—
, let us define

Î¸
m
âˆ—
=
argmin
Î¸
âˆˆ
Î˜
m
â€‹
R
â€‹
(
Î¸
)
.
(2.4)
As a consequence, we have:

Î¸
m
âˆ—
âˆ—
=
Î¸
âˆ—
and if 
m
âˆ—
âŠ‚
m
,
Î¸
m
âˆ—
=
Î¸
âˆ—
.
Besides of minimizing the risk 
R
, we also consider the minimization of its associated loss function, which is defined as

â„“
â€‹
(
Î¸
,
Î¸
âˆ—
)
:=
R
â€‹
(
Î¸
)
âˆ’
R
â€‹
(
Î¸
âˆ—
)
â‰¥
0
.
(2.5)
This is a well-known measure of separation between the candidate model generated by 
Î¸
 and the true one indexed by 
Î¸
âˆ—
.
 
Let set by 
Î³
n
 the associated empirical risk defined by

Î³
n
(
Î¸
)
:=
â„™
n
Î³
(
Î¸
,
.
)
=
1
n
âˆ‘
t
=
1
n
Î³
(
Î¸
,
X
t
)
=
âˆ’
2
n
L
n
(
Î¸
)
,
so that maximizing the log-likelihood is equivalent to minimize the empirical criterion 
Î³
n
.

2.3Quasi-Maximum Likelihood Estimation
Since the white noise is not necessarily a Gaussian one and since the log-likelihood (and then the empirical risk) 
L
n
â€‹
(
Î¸
)
 depends on 
(
X
t
)
t
â‰¤
0
 that are unknown, a quasi-log-likelihood 
L
^
n
â€‹
(
Î¸
)
 can be used as an approximation of the log-likelihood. It It consists of replacing 
Î³
â€‹
(
Î¸
,
X
t
)
 by an approximation 
Î³
^
â€‹
(
Î¸
,
X
t
)
 and those statistics are defined for all 
Î¸
âˆˆ
Î˜
 by

L
^
n
â€‹
(
Î¸
)
:=
âˆ’
1
2
â€‹
âˆ‘
t
=
1
n
Î³
^
â€‹
(
Î¸
,
X
t
)
with
â€‹
Î³
^
â€‹
(
Î¸
,
X
t
)
:=
(
X
t
âˆ’
f
^
Î¸
t
)
2
H
^
Î¸
t
+
log
â¡
(
H
^
Î¸
t
)
and
â€‹
{
f
^
Î¸
t
:=
f
Î¸
â€‹
(
X
t
âˆ’
1
,
X
t
âˆ’
2
,
â‹¯
,
X
1
,
u
)
M
^
Î¸
t
:=
M
Î¸
â€‹
(
X
t
âˆ’
1
,
X
t
âˆ’
2
,
â‹¯
,
X
1
,
u
)
H
^
Î¸
t
:=
(
M
^
Î¸
t
)
2
(2.6)
for any deterministic sequence 
u
=
(
u
n
)
n
âˆˆ
â„•
 with finitely many non-zero values (we will use 
u
=
0
 without loss of generality).

In addition the computable empirical risk is then:

Î³
^
n
(
Î¸
)
=
â„™
n
Î³
^
(
Î¸
,
.
)
=
1
n
âˆ‘
t
=
1
n
Î³
^
(
Î¸
,
X
t
)
=
âˆ’
2
n
L
^
n
(
Î¸
)
.
Finally, for each specific model 
m
âˆˆ
â„³
n
, we define a Gaussian Quasi-Maximum Likelihood Estimator (QMLE) 
Î¸
^
m
 as

Î¸
^
m
âˆˆ
argmax
Î¸
âˆˆ
Î˜
m
â€‹
L
^
n
â€‹
(
Î¸
)
=
argmin
Î¸
âˆˆ
Î˜
m
â€‹
Î³
^
n
â€‹
(
Î¸
)
.
(2.7)
The estimator 
Î¸
^
m
 is commonly called the Empirical Risk Minimizer (ERM).

2.4The penalization procedure
For 
m
âˆˆ
â„³
, the ERM provides an estimator in 
Î˜
m
. The goal is to come up with a model that minimizes the excess loss over 
â„³

argmin
m
âˆˆ
â„³
â€‹
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
.
(2.8)
This model is unknown since (2.8) depends on 
Î¸
âˆ—
 and the distribution 
P
(
X
1
,
â€¦
,
X
n
)
 that are unknown.

A classical way to solve (2.8) problem is to design for every 
m
âˆˆ
â„³
 an estimator of 
R
â€‹
(
Î¸
^
m
)
 and we naturally choose 
Î³
^
n
â€‹
(
Î¸
^
m
)
. First, it is well known that the empirical criterion 
Î³
^
n
â€‹
(
Î¸
^
m
)
 is an optimistic version of 
R
â€‹
(
Î¸
^
m
)
 and decreases with the dimension of the model. Therefore, it is common to add a penalty term to counteract this bias.
As a consequence, define a function pen: 
m
âˆˆ
â„³
â†¦
pen
â€‹
(
m
)
âˆˆ
â„
+
, which is called the penalty function and is possibly data-dependent. We will only require that 
pen
â€‹
(
m
1
)
â‰¤
pen
â€‹
(
m
2
)
 when 
m
1
âŠ‚
m
2
. Then define the penalized contrast and the model selected by it:

m
^
pen
=
argmin
m
âˆˆ
â„³
â€‹
{
C
^
pen
â€‹
(
m
)
}
with
C
^
pen
â€‹
(
m
)
:=
Î³
^
n
â€‹
(
Î¸
^
m
)
+
pen
â€‹
(
m
)
.
(2.9)
In order to achieve (2.8), the ideal penalty to consider in (2.9) is

pen
i
â€‹
d
â€‹
(
m
)
=
R
â€‹
(
Î¸
^
m
)
âˆ’
Î³
^
n
â€‹
(
Î¸
^
m
)
.
(2.10)
Using this definition, we obtain an â€idealâ€ model defined by:

m
^
i
â€‹
d
:
âˆˆ
argmin
m
âˆˆ
â„³
{
â„“
(
Î¸
^
m
,
Î¸
âˆ—
)
}
=
argmin
m
âˆˆ
â„³
{
R
(
Î¸
^
m
)
}
=
argmin
m
âˆˆ
â„³
{
C
^
pen
i
â€‹
d
(
m
)
}
.
(2.11)
However, the function 
R
 is unknown except for very few particular and parametric cases and therefore 
pen
i
â€‹
d
 cannot generally be used directly. Therefore the question is how to choose the penalty in (2.9) so that 
m
^
pen
 mimics the oracle 
m
^
i
â€‹
d
. Hence, we would like our final estimator 
Î¸
^
m
^
pen
 to behave asymptotically like the oracle. That is to satisfy:

â„™
â€‹
(
â„“
â€‹
(
Î¸
^
m
^
pen
,
Î¸
âˆ—
)
â‰¤
min
m
âˆˆ
â„³
â€‹
{
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
}
+
C
n
)
â€‹
âŸ¶
n
â†’
âˆ
â€‹
1
(2.12)
and/or for any 
n
â‰¥
n
0

ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
^
pen
,
Î¸
âˆ—
)
]
â‰¤
min
m
âˆˆ
â„³
â€‹
{
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
]
}
+
C
n
.
(2.13)
The aim of this paper is to find a good choice of 
pen
â€‹
(
m
)
 in order to obtain the asymptotic efficiency (2.13) or (2.12).

3Asymptotic behavior of the QMLE
Before considering the problem of model selection, we establish a central limit theorem satisfied by 
Î¸
^
m
 for any model 
m
âˆˆ
â„³
, i.e. as well if 
m
 is an overfitted or a misspecified model. Before this, some notations and assumptions have to be precised.

3.1Notations and main assumptions
In the sequel, we will consider a subset 
Î˜
 of 
â„
d
 which is compact. We will use the following norms:

â€¢ 
âˆ¥
.
âˆ¥
 denotes the usual Euclidean norm on 
â„
Î½
, with 
Î½
â‰¥
1
;
â€¢ for a matrix 
A
, denote 
â€–
A
â€–
 the subordinate matrix norm such that 
â€–
A
â€–
=
sup
v
â‰ 
0
â€‹
â€–
A
â€‹
v
â€–
â€–
v
â€–
;
â€¢ if 
X
 is a 
â„
Î½
-random variable and 
r
â‰¥
1
, we set 
â€–
X
â€–
r
=
(
ğ”¼
â€‹
[
â€–
X
â€–
r
]
)
1
/
r
âˆˆ
[
0
,
âˆ
]
;
â€¢ for 
Î¸
âˆˆ
Î˜
âŠ‚
â„
d
, if 
Î¨
Î¸
:
â„
âˆ
â†’
E
 where 
E
=
â„
Î½
 or 
E
 is a set of square matrix, denote 
â€–
Î¨
Î¸
â€‹
(
â‹…
)
â€–
Î˜
=
sup
Î¸
âˆˆ
Î˜
â€‹
{
â€–
Î¨
Î¸
â€‹
(
â‹…
)
â€–
}
;
â€¢ for 
Î¸
âˆˆ
Î˜
âŠ‚
â„
d
, if 
Î¨
Î¸
:
â„
âˆ
â†’
â„
 is a 
ğ’
2
â€‹
(
Î˜
Ã—
R
âˆ
)
 function, we will denote
âˆ‚
Î¸
Î¨
Î¸
â€‹
(
â‹…
)
=
(
âˆ‚
âˆ‚
Î¸
i
â€‹
Î¨
Î¸
â€‹
(
â‹…
)
)
1
â‰¤
i
â‰¤
d
=
(
âˆ‚
Î¸
i
Î¨
Î¸
â€‹
(
â‹…
)
)
1
â‰¤
i
â‰¤
d
and
âˆ‚
Î¸
2
2
Î¨
Î¸
â€‹
(
â‹…
)
=
(
âˆ‚
2
âˆ‚
Î¸
i
â€‹
âˆ‚
Î¸
j
â€‹
Î¨
Î¸
â€‹
(
â‹…
)
)
1
â‰¤
i
,
j
â‰¤
d
;
â€¢ consider 
Î¨
Î¸
:
â„
âˆ
â†’
â„
 for any 
Î¸
âˆˆ
Î˜
âŠ‚
â„
d
. Then, we define the assumption:
 
Assumption A
(
Î¨
Î¸
,
Î˜
)
: 
â€–
Î¨
Î¸
â€‹
(
0
)
â€–
Î˜
<
âˆ
 and there exists a sequence of non-negative real numbers 
(
Î±
k
â€‹
(
Î¨
Î¸
,
Î˜
)
)
k
â‰¥
1
 such that 
âˆ‘
k
=
1
âˆ
Î±
k
â€‹
(
Î¨
Î¸
,
Î˜
)
<
âˆ
 and satisfying:
â€–
Î¨
Î¸
â€‹
(
x
)
âˆ’
Î¨
Î¸
â€‹
(
y
)
â€–
Î˜
â‰¤
âˆ‘
k
=
1
âˆ
Î±
k
â€‹
(
Î¨
Î¸
,
Î˜
)
â€‹
|
x
k
âˆ’
y
k
|
for all 
x
,
y
âˆˆ
â„
âˆ
.
Several assumptions on the AC class will be considered thereafter:

Assumption A0: The process 
X
âˆˆ
ğ’œ
â€‹
ğ’
â€‹
(
M
Î¸
âˆ—
,
f
Î¸
âˆ—
)
 where 
Î¸
âˆ—
âˆˆ
Î˜
 is defined in (1.1) where:

â€¢ the white noise 
(
Î¾
t
)
t
 is such as 
â€–
Î¾
0
â€–
r
<
âˆ
 with 
8
<
r
;
â€¢ for any 
x
âˆˆ
â„
âˆ
, the functions 
Î¸
â†’
M
Î¸
 and 
Î¸
â†’
f
Î¸
 are 
ğ’
2
â€‹
(
Î˜
)
 functions:
â€¢ 
Î˜
âˆˆ
â„
d
 is a compact set such as
Î˜
âŠ‚
{
Î¸
âˆˆ
â„
d
,
A
(
f
Î¸
,
{
Î¸
}
)
and
A
(
M
Î¸
,
{
Î¸
}
)
hold with
âˆ‘
k
=
1
âˆ
Î±
k
(
f
Î¸
,
{
Î¸
}
)
+
âˆ¥
Î¾
0
âˆ¥
r
âˆ‘
k
=
1
âˆ
Î±
k
(
M
Î¸
,
{
Î¸
}
)
<
1
}
.
(3.1)
Under this assumption, [14] showed that there exists a stationary causal (i.e. 
X
t
 is depending only on 
(
X
t
âˆ’
k
)
k
âˆˆ
â„•
 for any 
t
âˆˆ
â„¤
) and ergodic solution of (1.1) with 
r
-order moment for any 
Î¸
âˆˆ
Î˜
.
 
Now the assumption A0 holds. We will also add several assumptions required for insuring the strong consistency and the asymptotic normality of the QMLE:
 
The first following classical assumption ensures the identifiability of 
Î¸
âˆ—
.

Assumption A1: For all 
Î¸
,
Î¸
â€²
âˆˆ
Î˜
, 
(
f
Î¸
0
=
f
Î¸
â€²
0
a
n
d
M
Î¸
0
=
M
Î¸
â€²
0
)
a
.
s
.
âŸ¹
Î¸
=
Î¸
â€²
.

Remark 1. Even if this assumption is a classic one in an M-estimation framework, it is important to remark that it does not cover all the cases of model selection of usual causal time series. Indeed, in the case of the family of ARMA processes, it is well known that a model is unique when both the polynomials 
P
0
 and 
Q
0
 of AR and MA parts are coprime. Hence, for instance, if the true model is an ARMA
(
p
0
,
q
0
)
 process, any ARMA
(
p
0
+
1
,
q
0
+
1
)
 representation with respective polynomials 
P
â€‹
(
X
)
=
P
0
â€‹
(
X
)
â€‹
(
X
âˆ’
r
)
 and 
Q
â€‹
(
X
)
=
Q
0
â€‹
(
X
)
â€‹
(
X
âˆ’
r
)
 of AR and MA parts, is identically the same as the true model whatever 
r
âˆˆ
â„
. Then, Assumption A1 is never satisfied for ARMA processes in case of overfitting. However, by initializing 
Î¸
 around 
0
 in the optimization algorithm, we have noticed from Monte-Carlo experiments that the algorithm always converges to 
Î¸
âˆ—
 and not other solution.
Next, the following Assumption ensures the invertibility of the asymptotic covariance matrix 
G
 and 
F
 (see below) that is necessary to prove the asymptotic normality of the QMLE (see for instance [5]).

Assumption A2: 
<
Î±
,
âˆ‚
Î¸
f
Î¸
0
>=
0
âŸ¹
Î±
=
0
â€‹
a
.
s
.
 or 
<
Î±
,
âˆ‚
Î¸
M
Î¸
0
>=
0
âŸ¹
Î±
=
0
â€‹
a
.
s
.

The definition of the computable empirical risk and requires that its denominators do not vanish. Hence, we are going to assume throughout this paper that the lower bound of 
H
Î¸
â€‹
(
â‹…
)
=
(
M
Î¸
â€‹
(
â‹…
)
)
2
 is strictly positive:

Assumption A3: 
âˆƒ
h
Â¯
>
0
 such that 
inf
Î¸
âˆˆ
Î˜
â€‹
(
H
Î¸
â€‹
(
x
)
)
â‰¥
h
Â¯
 for all 
x
âˆˆ
â„
âˆ
.

The following assumption is a technical classical condition (see [28]).

Assumption A4: For every 
m
âˆˆ
â„³
, if 
(
Î¸
Â¯
m
,
n
)
 is a sequence of 
Î˜
m
 satisfying 
Î¸
Â¯
m
,
n
â€‹
âŸ¶
a
.
s
.
n
â†’
+
âˆ
â€‹
Î¸
âˆ—
, then

lim sup
n
â†’
âˆ
{
ğ”¼
[
(
âˆ¥
1
n
(
âˆ‚
Î¸
i
â€‹
Î¸
j
2
L
n
(
Î¸
Â¯
m
)
)
i
,
j
âˆˆ
m
)
âˆ’
1
âˆ¥
8
]
}
<
âˆ
.
(3.2)
Remark 2. Note that under assumption A0, if 
Î¸
Â¯
m
,
n
â€‹
âŸ¶
a
.
s
.
n
â†’
+
âˆ
â€‹
Î¸
m
âˆ—
 then
â€–
(
1
n
â€‹
(
âˆ‚
Î¸
i
â€‹
Î¸
j
2
L
n
â€‹
(
Î¸
Â¯
m
)
)
i
,
j
âˆˆ
m
)
âˆ’
1
â€–
8
â€‹
âŸ¶
a
.
s
.
n
â†’
+
âˆ
â€‹
â€–
(
(
âˆ’
1
2
â€‹
âˆ‚
Î¸
i
â€‹
Î¸
j
2
Î³
â€‹
(
Î¸
m
âˆ—
)
)
i
,
j
âˆˆ
m
)
âˆ’
1
â€–
8
Thus, from the Egorovâ€™s Theorem, we can find an event 
Î©
~
 with sufficiently large probability such that the relation (3.2) in the assumption A4 holds if the expectation is taken on the event 
Î©
~
. For the particular case of the linear processes, the assumption A4 holds true under a mild condition on the distribution of 
X
, see for instance [32] and [15].

Finally, the decrease rates of 
(
Î±
j
â€‹
(
f
Î¸
,
Î˜
)
)
j
, 
(
Î±
j
â€‹
(
M
Î¸
,
Î˜
)
)
j
, 
(
Î±
j
â€‹
(
âˆ‚
Î¸
f
Î¸
,
Î˜
)
)
j
 and 
(
Î±
j
â€‹
(
âˆ‚
Î¸
M
Î¸
,
Î˜
)
)
j
 have to be fast enough for insuring the strong consistency and the asymptotic normality of the QMLE:

Assumption A5: Conditions A
(
f
Î¸
,
Î˜
)
, A
(
M
Î¸
,
Î˜
)
, A
(
âˆ‚
Î¸
f
Î¸
,
Î˜
)
, A
(
âˆ‚
Î¸
M
Î¸
,
Î˜
)
, A
(
âˆ‚
Î¸
2
2
f
Î¸
,
Î˜
)
 and A
(
âˆ‚
Î¸
2
2
M
Î¸
,
Î˜
)
 hold with

Î±
j
â€‹
(
f
Î¸
,
Î˜
)
+
Î±
j
â€‹
(
M
Î¸
,
Î˜
)
+
Î±
j
â€‹
(
âˆ‚
Î¸
f
Î¸
,
Î˜
)
+
Î±
j
â€‹
(
âˆ‚
Î¸
M
Î¸
,
Î˜
)
=
O
â€‹
(
j
âˆ’
Î´
)
where
Î´
>
7
/
2
.

Note that Assumption A5 does not allow to consider long-range dependent processes, but usual short memory causal time series satisfy this assumption.

3.2New asymptotic results satisfied by 
Î¸
^
m
The asymptotic normality of 
Î¸
^
m
 has been already established in [5] when 
m
=
m
âˆ—
 and in [4] when 
m
âˆ—
âŠ‚
m
 (overfitting). This property can also be extended in the case of misspecified model, i.e. when 
m
âˆ—
âŠ„
m
.
 
First, the following corollary is a particular case of a more general result, Proposition 4, which is stated in Section 8 devoted to the proofs.
To begin with, and from Assumption A2 and A5, we can define the definite positive matrix

G
â€‹
(
Î¸
)
:=
1
4
â€‹
(
âˆ‘
t
âˆˆ
â„¤
Cov
â€‹
(
âˆ‚
Î¸
i
Î³
â€‹
(
Î¸
,
X
0
)
,
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
,
X
t
)
)
)
1
â‰¤
i
,
j
â‰¤
d
(3.3)
F
â€‹
(
Î¸
)
:=
âˆ’
1
2
â€‹
(
ğ”¼
â€‹
[
âˆ‚
Î¸
i
â€‹
Î¸
j
2
Î³
â€‹
(
Î¸
,
X
0
)
]
)
1
â‰¤
i
,
j
â‰¤
d
.
(3.4)
Now, for any 
m
âˆˆ
â„³
 and 
Î¸
âˆˆ
Î˜
, denote:

G
m
â€‹
(
Î¸
)
:=
1
4
â€‹
(
âˆ‘
t
âˆˆ
â„¤
Cov
â€‹
(
âˆ‚
Î¸
i
Î³
â€‹
(
Î¸
,
X
0
)
,
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
,
X
t
)
)
)
i
,
j
âˆˆ
m
(3.5)
âŸ¹
G
m
â€‹
(
Î¸
âˆ—
)
=
1
4
â€‹
(
Cov
â€‹
(
âˆ‚
Î¸
i
Î³
â€‹
(
Î¸
âˆ—
,
X
0
)
,
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
âˆ—
,
X
0
)
)
)
i
,
j
âˆˆ
m
if 
m
âˆ—
âŠ‚
m
F
m
â€‹
(
Î¸
)
:=
âˆ’
1
2
â€‹
(
ğ”¼
â€‹
[
âˆ‚
Î¸
i
â€‹
Î¸
j
2
Î³
â€‹
(
Î¸
,
X
0
)
]
)
i
,
j
âˆˆ
m
.
(3.6)
Corollary 1. Let 
m
âˆˆ
â„³
 and suppose that Assumptions A0-A5 hold. Then, with 
Î¸
m
âˆ—
 defined in (2.4),
1
n
â€‹
(
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
m
âˆ—
)
)
j
âˆˆ
m
â€‹
âŸ¶
ğ’Ÿ
n
â†’
âˆ
â€‹
ğ’©
â€‹
(
0
,
G
m
â€‹
(
Î¸
m
âˆ—
)
)
.
(3.7)
Using mainly this new result, we also obtain:

Theorem 3.1. Under Assumptions A0-A5, for any 
m
âˆˆ
â„³
,
n
â€‹
(
(
Î¸
^
m
)
i
âˆ’
(
Î¸
m
âˆ—
)
i
)
i
âˆˆ
m
â€‹
âŸ¶
ğ’Ÿ
n
â†’
âˆ
â€‹
ğ’©
â€‹
(
0
,
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
â€‹
G
m
â€‹
(
Î¸
m
âˆ—
)
â€‹
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
)
,
(3.8)
with 
G
m
 and 
F
m
 defined in (3.5) and (3.6).

Hence, even in the misspecified case, 
Î¸
^
m
 satisfies a central limit theorem. We will use this result several times, in particular to prove that the probability of selecting a misspecified model tends quickly enough towards 
0
. Another technical result will also be useful for the sequel:

Proposition 1. Under Assumptions A0-A5, with 
8
/
3
<
r
â€²
â‰¤
(
8
+
r
)
/
6
 and 
r
â€²
<
2
â€‹
(
Î´
âˆ’
1
)
 where 
Î´
>
7
/
2
 is given in Assumption A5 and for any 
m
âˆˆ
â„³
, then we have
sup
n
âˆˆ
â„•
âˆ—
â€–
n
â€‹
(
(
Î¸
^
m
)
i
âˆ’
(
Î¸
m
âˆ—
)
i
)
i
âˆˆ
m
â€–
r
â€²
<
âˆ
.
(3.9)
Note that we also have 
sup
n
âˆˆ
â„•
âˆ—
â€–
n
â€‹
(
(
Î¸
^
m
)
i
âˆ’
(
Î¸
m
âˆ—
)
i
)
i
âˆˆ
m
â€–
2
<
âˆ
. This result will be essential for establishing the asymptotic behavior of the expectation of the ideal penalty.

4Efficient model selection
The expectation of the ideal penalty (2.10) has been computed (or asymptotically approximated) in several frameworks (see [29], [1], [34], [19], [7]; etc) and it is most often proportional to the dimension of the model (denoted 
|
m
|
 in the sequel).

â€¢ the penalty is 
2
â€‹
|
m
|
â€‹
Ïƒ
2
/
n
 in the regression setting, leading to the famous Mallowsâ€™s 
C
p
 criterion [29];
â€¢ the penalty is 
2
â€‹
|
m
|
/
n
 in the density estimation framework and others, leading to the famous AIC criterion [1];
â€¢ the penalty is 
|
m
|
â€‹
log
â¡
(
n
)
/
n
 in the Bayesian density estimation setting and other frameworks, leading to the famous BIC criterion [34] ;
â€¢ the penalty is 
Trace
â€‹
(
B
n
â€‹
A
n
âˆ’
1
)
/
n
 where 
A
n
 is the opposite of the Hessian matrix of the log-likelihood and 
B
n
 the Fisher Information matrix in a general framework issued from a Bayesian setting [28].
In order to approximate (2.10) in this framework, let first provide a decomposition of this term in order to facilitate the computation. For any model 
m
âˆˆ
â„³
, write

pen
i
â€‹
d
â€‹
(
m
)
:=
R
â€‹
(
Î¸
^
m
)
âˆ’
Î³
^
n
â€‹
(
Î¸
^
m
)
=
I
1
â€‹
(
m
)
+
I
2
â€‹
(
m
)
+
I
3
â€‹
(
m
)
,
(4.1)
with
{
I
1
â€‹
(
m
)
:=
R
â€‹
(
Î¸
^
m
)
âˆ’
R
â€‹
(
Î¸
m
âˆ—
)
I
2
â€‹
(
m
)
:=
Î³
^
n
â€‹
(
Î¸
m
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
^
m
)
I
3
â€‹
(
m
)
:=
R
â€‹
(
Î¸
m
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
m
âˆ—
)
.
Next we provide a preliminary result about the asymptotic behavior of the terms 
I
1
â€‹
(
m
)
 and 
I
2
â€‹
(
m
)
. Then we obtain:

Lemma 1. Under Assumptions A0-A5, for any model 
m
âˆˆ
â„³
, there exists a probability distribution 
U
âˆ—
â€‹
(
m
)
 such that
1
.
n
â€‹
I
1
â€‹
(
m
)
=
n
â€‹
(
R
â€‹
(
Î¸
^
m
)
âˆ’
R
â€‹
(
Î¸
m
âˆ—
)
)
â€‹
âŸ¶
ğ’Ÿ
n
â†’
âˆ
â€‹
U
âˆ—
â€‹
(
m
)
(4.7)
and
ğ”¼
â€‹
[
n
â€‹
I
1
â€‹
(
m
)
]
â€‹
âŸ¶
n
â†’
âˆ
â€‹
ğ”¼
â€‹
[
U
âˆ—
â€‹
(
m
)
]
=
âˆ’
Trace
â€‹
(
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
â€‹
G
m
â€‹
(
Î¸
m
âˆ—
)
)
.
2
.
n
â€‹
I
2
â€‹
(
m
)
=
n
â€‹
(
Î³
^
n
â€‹
(
Î¸
m
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
^
m
)
)
â€‹
âŸ¶
ğ’Ÿ
n
â†’
âˆ
â€‹
U
âˆ—
â€‹
(
m
)
(4.13)
and
ğ”¼
â€‹
[
n
â€‹
I
2
â€‹
(
m
)
]
â€‹
âŸ¶
n
â†’
âˆ
âˆ’
Trace
â€‹
(
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
â€‹
G
m
â€‹
(
Î¸
m
âˆ—
)
)
.
The proof of this lemma, as well as all the other proofs, can be found in Section 8. This result leads to our first main result:

Proposition 2. Under Assumptions A0-A5, there exists 
N
0
âˆˆ
â„•
 such as for any 
n
â‰¥
N
0
,
argmin
m
âˆˆ
â„³
â¡
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
]
=
m
âˆ—
.
(4.14)
Another application of Lemma 1 is devoted to an expansion of the expectation of the ideal penalty defined in (2.10):

Proposition 3. Under Assumptions A0-A5 and for any 
m
âˆˆ
â„³
, there exists a bounded sequence 
(
v
n
âˆ—
)
n
âˆˆ
â„•
âˆ—
, not depending on 
m
 when 
m
âˆ—
âŠ‚
m
, and satisfying
ğ”¼
[
pen
i
â€‹
d
(
m
)
]
âˆ¼
n
â†’
âˆ
âˆ’
2
n
Trace
(
(
F
m
(
Î¸
m
âˆ—
)
)
âˆ’
1
G
m
(
Î¸
m
âˆ—
)
)
)
+
v
n
âˆ—
n
.
(4.15)
Note that the Slope Heuristic Procedure, which allows to estimate a so called minimal penalty (see [2]) consists in evaluating the slope of a linear regression of 
Î³
^
n
â€‹
(
Î¸
^
m
)
 onto 
|
m
|
 for 
m
âˆ—
âŠ‚
m
 and this is equivalent to estimating the slope of 
1
n
â€‹
Trace
â€‹
(
G
m
â€‹
(
Î¸
m
âˆ—
)
â€‹
F
m
â€‹
(
Î¸
m
âˆ—
)
âˆ’
1
)
 onto 
|
m
|
 from (4.13). We will see that 
Trace
â€‹
(
G
m
â€‹
(
Î¸
m
âˆ—
)
â€‹
F
m
â€‹
(
Î¸
m
âˆ—
)
âˆ’
1
)
 behaves as a linear function of 
|
m
|
 in many cases, which also gives legitimacy to this approach in the case of time series after having obtained it in the case of linear regression. The minimal penalty is then 
âˆ’
2
Ã—
 the estimated slope and this finally corresponds to an approximation of 
ğ”¼
â€‹
[
pen
i
â€‹
d
â€‹
(
m
)
]
. The trace of the matrix mentioned above is easily computable in some cases using the explicit forms of the matrices 
F
â€‹
(
Î¸
âˆ—
)
, 
G
â€‹
(
Î¸
âˆ—
)
 in [5]. Hence, even if the ideal penalty cannot be explicitely obtained, we can replace it with its expectation, i.e. this trace of matrix. Then, define for 
m
âˆˆ
â„³
,

pen
~
â€‹
(
m
)
:=
âˆ’
2
n
â€‹
Trace
â€‹
(
(
F
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
â€‹
G
â€‹
(
Î¸
m
âˆ—
)
)
.
(4.16)
As shown in Section 6, this trace is proportional to the dimension of the model 
|
m
|
 in some cases, but could be more complex functions of 
|
m
|
. In case of Gaussian process, we will also see that it corresponds to the AIC penalty, 
2
/
n
. However, we will see that this penalty does not provide a consistent model selection and contrary to the ideal penalty, does not provide an optimal efficient model criterion.
Before this, we study the probability of not selecting a misspecified model under a general condition on the penalty that is satisfied for instance by 
pen
~
 or by BIC criterion:

Theorem 4.1. Under Assumptions A0-A5 and if for any 
Îµ
>
0
,
n
â€‹
â„™
â€‹
(
pen
â€‹
(
m
)
â‰¥
Îµ
)
â€‹
âŸ¶
n
â†’
âˆ
â€‹
0
for any 
m
âˆˆ
â„³
.
(4.17)
Then,

n
â€‹
â„™
â€‹
(
m
âˆ—
âŠ„
m
^
pen
)
â€‹
âŸ¶
n
â†’
âˆ
â€‹
0
.
(4.18)
Theorem 4.1 says that if the penalty asymptotically decreases to 
0
 in probability, then the criterion 
C
^
pen
 does not select a misspecified model asymptotically.
Now, we state the main results of this paper, which specify the convergence rate of pen to obtain an excess loss close to the minimal one over 
â„³
:

Theorem 4.2. Under Assumptions A0-A5, and if for any 
Îµ
>
0
 there exists 
K
Îµ
>
0
 such as
lim sup
n
â†’
âˆ
max
m
âˆˆ
â„³
â¡
â„™
â€‹
(
n
â€‹
pen
â€‹
(
m
)
â‰¥
K
Îµ
)
â‰¤
Îµ
.
(4.19)
Then for any 
Îµ
>
0
, there exists 
M
Îµ
>
0
 and 
N
Îµ
âˆˆ
â„•
âˆ—
 such as for any 
n
â‰¥
N
Îµ
,

â„™
â€‹
(
â„“
â€‹
(
Î¸
^
m
^
pen
,
Î¸
âˆ—
)
â‰¤
min
m
âˆˆ
â„³
â€‹
{
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
}
+
M
Îµ
n
)
â‰¥
1
âˆ’
Îµ
.
(4.20)
Remark 3. Let notice that this asymptotic optimality is quite a bit different from the classical one about asymptotic efficiency, where both the cardinal of the collection 
â„³
 and the dimension of competitive models are allowed to depend on 
n
. However, this is done in the framework where the parameter 
Î¸
âˆ—
 is infinite-dimensional (see for example [37], [26], [18]).
Now, we provide a condition on the penalty allowing to obtain a consistent criterion. Moreover, we also prove that such criterion satisfies a sharper efficiency inequality than (4.20):

Theorem 4.3. Under Assumptions A0-A5, if the penalty pen satisfies (4.17), and if for any 
m
âˆˆ
â„³
 such as 
m
âˆ—
âŠ‚
m
 and 
m
â‰ 
m
âˆ—
,
n
â€‹
ğ”¼
â€‹
[
e
n
â€‹
(
m
)
]
â€‹
âŸ¶
n
â†’
âˆ
â€‹
âˆ
and
n
â€‹
ğ”¼
â€‹
[
|
e
n
â€‹
(
m
)
âˆ’
ğ”¼
â€‹
[
e
n
â€‹
(
m
)
]
|
]
â€‹
âŸ¶
n
â†’
âˆ
â€‹
0
(4.21)
with 
e
n
â€‹
(
m
)
=
pen
â€‹
(
m
)
âˆ’
pen
â€‹
(
m
âˆ—
)
>
0
 since 
m
âˆ—
âŠ‚
m
 and 
m
â‰ 
m
âˆ—
. Then we have,

â„™
â€‹
(
m
^
pen
=
m
âˆ—
)
â€‹
âŸ¶
n
â†’
âˆ
â€‹
1
.
(4.22)
Moreover, for any 
Îµ
>
0
 and 
Î·
>
0
, there exists 
N
Îµ
,
Î·
âˆˆ
â„•
âˆ—
 such as for any 
n
â‰¥
N
Îµ
,
Î·
,

â„™
â€‹
(
â„“
â€‹
(
Î¸
^
m
^
pen
,
Î¸
âˆ—
)
â‰¤
â„“
â€‹
(
Î¸
^
m
âˆ—
,
Î¸
âˆ—
)
+
Î·
n
)
â‰¥
1
âˆ’
Îµ
,
(4.23)
and there exists 
N
Î·
âˆˆ
â„•
 such as for any 
n
â‰¥
N
Î·
,

ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
^
pen
,
Î¸
âˆ—
)
]
â‰¤
min
m
âˆˆ
â„³
â¡
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
]
+
Î·
n
.
(4.24)
The best known criterion satisfying the conditions of this theorem and in particular (4.21) is certainly the BIC criterion for which 
pen
â€‹
(
m
)
=
log
â¡
n
n
â€‹
|
m
|
 and therefore 
e
n
â€‹
(
m
)
=
log
â¡
n
n
â€‹
(
|
m
|
âˆ’
|
m
âˆ—
|
)
. This is also such a case for Hannan-Quinn criterion (
pen
â€‹
(
m
)
=
log
â¡
(
log
â¡
n
)
n
â€‹
|
m
|
, see [17]) or if 
pen
â€‹
(
m
)
=
n
n
â€‹
|
m
|
 as we used it in [4]. Note also that both the consistent data-driven criteria mentioned in the next section (see (5.4) defined in [23], and (5.5)) also verify the conditions of Theorem 4.3.
On the contrary, the AIC criterion with 
pen
â€‹
(
m
)
=
2
n
â€‹
|
m
|
, or the criterion with penalty 
pen
~
â€‹
(
m
)
 do not satisfy these conditions. The following theorem even shows that these criteria asymptotically overfit and have a less good asymptotic efficiency than consistent criteria satisfying Theorem 4.3:

Theorem 4.4. Assume that there exists 
g
:
â„³
â†’
[
0
,
âˆ
[
 such as 
pen
â€‹
(
m
)
=
g
â€‹
(
m
)
/
n
 for any 
m
âˆˆ
â„³
. Then, under Assumptions A0-A5, the probability of overfitting is asymptotically positive i.e.
lim inf
n
â†’
âˆ
â„™
â€‹
(
m
^
pen
â€‹
overfits
)
>
0
.
(4.25)
and there exists 
M
>
0
 such as for 
n
 large enough,

ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
^
pen
,
Î¸
âˆ—
)
]
â‰¥
min
m
âˆˆ
â„³
â€‹
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
]
+
M
n
.
(4.26)
Corollary 2. Theorem 4.4 is valid for 
pen
â€‹
(
m
)
=
2
n
â€‹
|
m
|
 (AIC criterion) or 
pen
=
pen
~
.
To conclude, in this context where a true model belonging to a finite family of models exists, the consistent criteria are those which also propose the best asymptotic efficiency. The next section focuses on these. However, a criterion like the AIC criterion or more generally the criterion with penalty 
pen
~
 regain all their optimality properties in asymptotic efficiency when the model family is infinite or when the true model does not belong to the family.

5From a Bayesian model selection to a data-driven consistent model selection
Another classical paradigm for model selection is the Bayesian one, leading typically to the BIC criterion (see [34]). In this approach, the construction of the model selection criterion is first done by assuming that the parameter vector 
Î¸
âˆ—
 is a random vector. Let recall the hierarchical prior sampling scheme in the Bayesian setting: given the finite family of models 
â„³
, a model 
m
 is drawn according to a prior distribution 
(
Ï€
m
)
m
âˆˆ
â„³
 (generally a uniform distribution) and then, conditionally on 
m
, 
Î¸
 is sampled according to some prior distribution 
Î¼
m
â€‹
(
Î¸
)
.
 
The goal of this model selection procedure is to choose the most probable model after observing the trajectory 
X
:=
(
X
1
,
â‹¯
,
X
n
)
, i.e.

m
^
B
=
argmax
m
âˆˆ
â„³
â€‹
{
â„™
â€‹
(
m
|
X
)
}
.
(5.1)
Using Bayes Formula, we can write 
â„™
â€‹
(
m
|
X
)
=
Ï€
m
â€‹
â„™
â€‹
(
X
|
m
)
â„™
â€‹
(
X
)
.
 Moreover, we have:

â„™
â€‹
(
X
|
m
)
=
âˆ«
Î˜
m
â„™
â€‹
(
X
|
Î¸
,
m
)
â€‹
ğ‘‘
Î¼
m
â€‹
(
Î¸
)
.
In addition, since 
â„™
â€‹
(
X
)
 does not depend on 
m
, and 
â„™
â€‹
(
X
|
Î¸
,
m
)
 is the likelihood of 
X
 given 
Î¸
âˆˆ
Î˜
m
 and 
m
âˆˆ
â„³
, maximizing 
â„™
â€‹
(
m
|
X
)
 is equivalent to maximize

S
^
n
â€‹
(
m
,
X
)
:=
log
â¡
(
â„™
â€‹
(
X
|
m
)
)
=
log
â¡
(
âˆ«
Î˜
m
Ï€
m
â€‹
exp
â¡
(
L
n
â€‹
(
Î¸
)
)
â€‹
ğ‘‘
Î¼
m
â€‹
(
Î¸
)
)
.
From now on, we will assume that 
Ï€
m
=
1
/
|
â„³
|
 for any 
m
âˆˆ
â„³
, a priori uniform distribution of the models in the family 
â„³
. We can also assume that there exists a non-negative Borel function 
Î¸
â†’
b
m
â€‹
(
Î¸
)
 such as 
d
â€‹
Î¼
m
â€‹
(
Î¸
)
=
b
m
â€‹
(
Î¸
)
â€‹
d
â€‹
Î¸
. Then we have:

S
^
n
â€‹
(
m
,
X
)
=
âˆ’
log
â¡
(
|
â„³
|
)
+
log
â¡
(
âˆ«
Î˜
m
b
m
â€‹
(
Î¸
)
â€‹
exp
â¡
(
L
n
â€‹
(
Î¸
)
)
â€‹
ğ‘‘
Î¸
)
.
(5.2)
Let us give an asymptotic expansion of the a posteriori probability in order to derive a BIC type criterion that is coherent with our framework where the observed trajectory is that of a causal affine process. This could be obtained from a Laplace approximation, leading to the following theorem:

Theorem 5.1. Under Assumptions A0, A1, A2, A3, A5 and if for any 
x
âˆˆ
R
âˆ
, the functions 
Î¸
â†’
M
Î¸
 and 
Î¸
â†’
f
Î¸
 are 
ğ’
6
â€‹
(
Î˜
)
 functions satisfying A
(
âˆ‚
Î¸
k
k
f
Î¸
,
Î˜
)
 and A
(
âˆ‚
Î¸
k
k
M
Î¸
,
Î˜
)
 for any 
0
â‰¤
k
â‰¤
6
. Then
S
^
n
â€‹
(
m
,
X
)
=
L
^
n
â€‹
(
Î¸
^
m
)
âˆ’
log
â¡
(
n
)
2
â€‹
|
m
|
+
log
â¡
(
b
m
â€‹
(
Î¸
^
m
)
)
+
log
â¡
(
2
â€‹
Ï€
)
2
â€‹
|
m
|
âˆ’
1
2
â€‹
log
â¡
(
det
(
âˆ’
F
^
n
â€‹
(
m
)
)
)
âˆ’
log
â¡
(
|
â„³
|
)
+
O
â€‹
(
n
âˆ’
1
)
a
.
s
.
(5.3)
where 
F
^
n
â€‹
(
m
)
:=
(
âˆ‚
Î¸
i
â€‹
Î¸
j
2
Î³
^
n
â€‹
(
Î¸
^
m
)
)
i
,
j
âˆˆ
m
.

In the above equation, it is clear that 
âˆ’
2
â€‹
S
^
n
â€‹
(
m
,
X
)
â‰ƒ
âˆ’
2
â€‹
L
^
n
â€‹
(
Î¸
^
m
)
+
log
â¡
(
n
)
â€‹
|
m
|
 a.s.. This gives legitimacy to the usual BIC criterion within the framework of causal affine processes since:

m
^
B
â€‹
I
â€‹
C
=
argmin
m
âˆˆ
â„³
â¡
{
âˆ’
2
â€‹
L
^
n
â€‹
(
Î¸
^
m
)
+
log
â¡
(
n
)
â€‹
|
m
|
}
,
and we see that 
m
^
B
â€‹
I
â€‹
C
 maximizes the main terms of 
S
^
n
â€‹
(
m
,
X
)
.
From the relation (5.3), considering certain second order terms of the asymptotic expansion of 
S
^
n
â€‹
(
m
,
X
)
, we also obtain the Kashyap criterion (see Kashyap [23], Sclove [35], Bozdogan [6]), denoted 
K
â€‹
C
 criterion, defined for all 
m
âˆˆ
â„³
 by

K
â€‹
C
^
â€‹
(
m
)
:=
âˆ’
2
â€‹
L
^
n
â€‹
(
Î¸
^
m
)
+
log
â¡
(
n
)
â€‹
|
m
|
+
log
â¡
(
det
(
âˆ’
F
^
n
â€‹
(
m
)
)
)
and
m
^
K
â€‹
C
=
argmin
m
âˆˆ
â„³
â¡
{
K
â€‹
C
^
â€‹
(
m
)
}
.
(5.4)
Therefore the term 
log
â¡
(
det
(
âˆ’
F
^
n
â€‹
(
m
)
)
)
 is added to the usual BIC criterion. Several examples of computations of this term, generally equal to 
c
â€‹
|
m
|
 but not always, are provided in the forthcoming Section 6. It is clear that 
m
^
K
â€‹
C
 can be more interesting that 
m
^
B
â€‹
I
â€‹
C
 in terms of consistency only for non asymptotic framework (typically for 
n
 of the order of a hundred or several hundred). Note also that the data-driven criteria 
K
â€‹
C
 that is â€optimalâ€ in the sense of the a posteriori probability (see Kashyap [23]) is also asymptotically consistent under the Assumption A5.
However, this choice of second order terms of the asymptotic expansion of 
S
^
n
â€‹
(
m
,
X
)
 is somewhere arbitrary. A criterion taking account of all the second order terms could also be defined. For this, we could define a uniform distribution 
b
m
 on a compact set included in 
Î˜
m
. As a consequence, using condition (3.1) of Assumption A0, there always exists 
0
<
C
1
â‰¤
C
2
 such as 
C
1
|
m
|
â‰¤
b
m
â€‹
(
Î˜
m
)
â‰¤
C
2
|
m
|
. As a consequence, we could define a new data-driven consistent criterion, called 
K
â€‹
C
â€²
, such as for any 
m
âˆˆ
â„³

K
â€‹
C
â€²
^
â€‹
(
m
)
:=
âˆ’
2
â€‹
L
^
n
â€‹
(
Î¸
^
m
)
+
(
log
â¡
(
n
)
âˆ’
log
â¡
(
2
â€‹
Ï€
)
)
â€‹
|
m
|
+
log
â¡
(
det
(
âˆ’
F
^
n
â€‹
(
m
)
)
)
+
2
â€‹
log
â¡
(
|
m
|
)
and
m
^
K
â€‹
C
â€²
=
argmin
m
âˆˆ
â„³
â¡
{
K
â€‹
C
^
â€²
â€‹
(
m
)
}
.
(5.5)
Remark 4. We also know that under Assumptions A0-A5, 
F
^
n
â€‹
(
m
)
â€‹
âŸ¶
a
.
s
.
n
â†’
+
âˆ
â€‹
F
â€‹
(
Î¸
m
âˆ—
)
 where 
F
 is defined in (3.4). Therefore the term 
log
â¡
(
det
(
âˆ’
F
^
n
â€‹
(
m
)
)
)
 can also be replaced by 
log
â¡
(
det
(
âˆ’
F
m
â€‹
(
Î¸
m
âˆ—
)
)
)
 in the expression of 
K
â€‹
C
â€²
^
â€‹
(
m
)
.
Corollary 3. The criteria BIC, KC and KCâ€™ are consistent model selection criteria and satisfy Theorem 4.3.
Thus, these three criteria are asymptotically consistent and asymptotically efficient following the inequalities (4.23) and (4.24). Monte-Carlo experiments in Section 7 will also exhibit that 
m
^
K
â€‹
C
â€²
, which is a data-driven criterion, outperforms 
m
^
B
â€‹
I
â€‹
C
 in terms of consistency and efficiency when the 
n
 size of the trajectory is of the order of a hundred or a thousand.

6Examples of computations of the asymptotic expectation of ideal penalties
From [5], with 
Î¼
4
=
ğ”¼
â€‹
[
Î¾
0
4
]
, 
f
Î¸
0
 and 
H
Î¸
0
 defined in (2.1), we have for 
m
âˆ—
âŠ‚
m
 and 
i
,
j
âˆˆ
m
:

(
G
m
â€‹
(
Î¸
m
âˆ—
)
)
i
,
j
=
ğ”¼
â€‹
[
âˆ‚
Î¸
i
f
Î¸
m
âˆ—
0
â€‹
âˆ‚
Î¸
j
f
Î¸
m
âˆ—
0
H
Î¸
m
âˆ—
0
+
(
Î¼
4
âˆ’
1
)
4
â€‹
âˆ‚
Î¸
i
H
Î¸
m
âˆ—
0
â€‹
âˆ‚
Î¸
j
H
Î¸
m
âˆ—
0
(
H
Î¸
m
âˆ—
0
)
2
]
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
i
,
j
=
âˆ’
ğ”¼
â€‹
[
âˆ‚
Î¸
i
f
Î¸
m
âˆ—
0
â€‹
âˆ‚
Î¸
j
f
Î¸
m
âˆ—
0
H
Î¸
m
âˆ—
0
+
1
2
â€‹
âˆ‚
Î¸
i
H
Î¸
m
âˆ—
0
â€‹
âˆ‚
Î¸
j
H
Î¸
m
âˆ—
0
(
H
Î¸
m
âˆ—
0
)
2
]
,
(6.1)
Here there are 3 frameworks where 
Trace
â€‹
(
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
â€‹
G
m
â€‹
(
Î¸
m
âˆ—
)
)
 can be computed for 
m
âˆ—
âŠ‚
m
:
 
1/ A first and well-known case is the Gaussian case. Indeed, when 
(
Î¾
t
)
 is a Gaussian white noise, then 
Î¼
4
=
3
 and then from (6.1), for any 
i
,
j
âˆˆ
m
,

(
G
m
â€‹
(
Î¸
m
âˆ—
)
)
i
,
j
=
âˆ’
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
i
,
j
âŸ¹
âˆ’
2
â€‹
Trace
â€‹
(
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
â€‹
G
m
â€‹
(
Î¸
m
âˆ—
)
)
=
2
â€‹
Trace
â€‹
(
I
|
m
|
)
=
2
â€‹
|
m
|
,
with 
I
â„“
 the identity matrix of size 
â„“
âˆˆ
â„•
âˆ—
. As a consequence, in the Gaussian framework, for 
m
âˆ—
âŠ‚
m
, the expectation of the ideal penalty is exactly the classical Akaike Criterion (AIC).
 
2/ A frequent case is when the parameter 
Î¸
 identifying an affine causal model 
X
t
=
M
Î¸
t
â€‹
Î¾
t
+
f
Î¸
t
 can be decomposed as 
Î¸
=
(
Î¸
1
,
Î¸
2
)
â€²
 with 
f
Î¸
t
=
f
~
Î¸
1
t
 and 
M
Î¸
t
=
M
~
Î¸
2
t
. Let 
p
1
,
p
2
 such that 
p
1
=
|
Î¸
1
|
, 
p
2
=
|
Î¸
2
|
 and 
|
m
|
=
p
1
+
p
2
.
In such a case, from (6.1), it is clear that all the terms 
F
m
â€‹
(
Î¸
m
âˆ—
)
i
,
j
 and 
G
m
â€‹
(
Î¸
m
âˆ—
)
i
,
j
 are equals to zero for 
i
=
1
,
â€¦
,
p
1
 and 
j
=
1
,
â‹¯
,
p
2
 implying

F
m
â€‹
(
Î¸
m
âˆ—
)
=
âˆ’
(
A
1
,
p
1
O
p
1
,
p
2
O
p
2
,
p
1
B
p
1
+
1
,
p
1
+
p
2
)
â€‹
and
G
m
â€‹
(
Î¸
m
âˆ—
)
=
(
A
1
,
p
1
O
p
1
,
p
2
O
p
2
,
p
1
(
Î¼
4
âˆ’
1
)
2
â€‹
B
p
1
+
1
,
p
1
+
p
2
)
where 
O
 is the null matrix and from the expressions of matrix 
G
m
â€‹
(
Î¸
m
âˆ—
)
 and 
F
m
â€‹
(
Î¸
m
âˆ—
)
 in (6.1),
A
1
,
p
1
=
(
ğ”¼
â€‹
[
âˆ‚
Î¸
i
f
Î¸
m
âˆ—
0
â€‹
âˆ‚
Î¸
j
f
Î¸
m
âˆ—
0
H
Î¸
m
âˆ—
0
]
)
1
â‰¤
i
,
j
â‰¤
p
1
 and 
B
p
1
+
1
,
p
1
+
p
2
=
(
1
2
â€‹
ğ”¼
â€‹
[
âˆ‚
Î¸
i
H
Î¸
m
âˆ—
0
â€‹
âˆ‚
Î¸
j
H
Î¸
m
âˆ—
0
(
H
Î¸
m
âˆ—
0
)
2
]
)
p
1
+
1
â‰¤
i
,
j
â‰¤
p
1
+
p
2
. As a consequence,

G
m
â€‹
(
Î¸
m
âˆ—
)
â€‹
F
m
â€‹
(
Î¸
m
âˆ—
)
âˆ’
1
=
âˆ’
Diag
â€‹
(
A
1
,
p
1
,
(
Î¼
4
âˆ’
1
)
2
â€‹
B
p
1
+
1
,
p
1
+
p
2
)
Ã—
Diag
â€‹
(
A
1
,
p
1
âˆ’
1
,
B
p
1
+
1
,
p
1
+
p
2
âˆ’
1
)
=
âˆ’
Diag
â€‹
(
I
p
1
,
(
Î¼
4
âˆ’
1
)
2
â€‹
I
p
2
)
and we obtain

âˆ’
2
â€‹
Trace
â€‹
(
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
â€‹
G
m
â€‹
(
Î¸
m
âˆ—
)
)
=
2
â€‹
p
1
+
(
Î¼
4
âˆ’
1
)
â€‹
p
2
.
(6.2)
This setting includes many classical times series:

â€¢ For ARMA
(
p
,
q
)
 processes, we have 
X
t
=
f
Î¸
t
+
Ïƒ
â€‹
Î¾
t
 since 
X
t
+
a
1
â€‹
X
t
âˆ’
1
+
â‹¯
+
a
p
â€‹
X
t
âˆ’
p
=
Ïƒ
â€‹
(
Î¾
t
+
b
1
â€‹
Î¾
t
âˆ’
1
+
â‹¯
+
b
q
â€‹
Î¾
t
âˆ’
q
)
 for all 
t
âˆˆ
â„¤
. Then 
Î¸
1
=
(
a
1
,
â€¦
,
a
p
,
b
1
,
â€¦
,
b
q
)
 and 
Î¸
2
=
Ïƒ
. The penalty term is slightly different according to 
Ïƒ
 is known or not:
(a) if 
Ïƒ
 is known, then 
Î¸
=
Î¸
1
 and 
G
m
â€‹
(
Î¸
âˆ—
)
=
âˆ’
F
m
â€‹
(
Î¸
âˆ—
)
, so that we recover exactly the AIC penalty term:
âˆ’
2
â€‹
Trace
â€‹
(
G
m
â€‹
(
Î¸
m
âˆ—
)
â€‹
F
m
â€‹
(
Î¸
m
âˆ—
)
âˆ’
1
)
=
2
â€‹
|
m
|
=
2
â€‹
(
p
+
q
)
;
(b) if 
Ïƒ
 is unknown, 
Î¸
=
(
Î¸
1
,
Ïƒ
)
 and simple computations lead to
F
m
â€‹
(
Î¸
âˆ—
)
=
(
(
F
m
â€‹
(
Î¸
âˆ—
)
)
1
â‰¤
i
,
j
â‰¤
|
m
|
âˆ’
1
0
0
âˆ’
1
2
â€‹
Ïƒ
4
)
and
â€‹
G
m
â€‹
(
Î¸
âˆ—
)
=
(
(
G
m
â€‹
(
Î¸
âˆ—
)
)
1
â‰¤
i
,
j
â‰¤
|
m
|
âˆ’
1
0
0
(
Î¼
4
âˆ’
1
)
4
â€‹
Ïƒ
4
)
where 
(
G
m
â€‹
(
Î¸
âˆ—
)
)
1
â‰¤
i
,
j
â‰¤
|
m
|
âˆ’
1
=
âˆ’
(
F
m
â€‹
(
Î¸
âˆ—
)
)
1
â‰¤
i
,
j
â‰¤
|
m
|
âˆ’
1
.
Thus, we obtain 
G
m
â€‹
(
Î¸
âˆ—
)
â€‹
F
m
â€‹
(
Î¸
âˆ—
)
âˆ’
1
=
âˆ’
(
I
1
â‰¤
i
,
j
â‰¤
|
m
|
âˆ’
1
0
0
Î¼
4
âˆ’
1
2
)
 and therefore, with 
|
m
|
=
p
+
q
+
1
 in this case,
âˆ’
2
â€‹
Trace
â€‹
(
G
m
â€‹
(
Î¸
m
âˆ—
)
â€‹
F
m
â€‹
(
Î¸
m
âˆ—
)
âˆ’
1
)
=
2
â€‹
|
m
|
+
(
Î¼
4
âˆ’
3
)
=
2
â€‹
(
p
+
q
)
+
(
Î¼
4
âˆ’
1
)
,
and therefore once again the expectation of the ideal penalty leads to the AIC model selection.
â€¢ For GARCH
(
p
,
q
)
 processes (see [16]), we have 
f
Î¸
=
0
 and 
X
t
=
M
Î¸
t
â€‹
Î¾
t
 since for any 
t
âˆˆ
â„¤
,
{
X
t
=
Ïƒ
t
â€‹
Î¾
t
Ïƒ
t
2
=
Ï‰
0
+
a
1
â€‹
X
t
âˆ’
1
2
+
â‹¯
+
a
p
â€‹
X
t
âˆ’
p
2
+
b
1
â€‹
Ïƒ
t
âˆ’
1
2
+
â‹¯
+
b
q
â€‹
Ïƒ
t
âˆ’
q
2
.
Denote 
Î¸
=
Î¸
2
=
(
Ï‰
0
,
a
1
,
â€¦
,
a
p
,
b
1
,
â€¦
,
b
q
)
.
Then we have 
A
p
1
=
0
 and therefore 
G
m
â€‹
(
Î¸
âˆ—
)
=
âˆ’
(
Î¼
4
âˆ’
1
)
2
â€‹
F
m
â€‹
(
Î¸
âˆ—
)
.
 As a result:
âˆ’
2
â€‹
Trace
â€‹
(
G
m
â€‹
(
Î¸
m
âˆ—
)
â€‹
F
m
â€‹
(
Î¸
m
âˆ—
)
âˆ’
1
)
=
(
Î¼
4
âˆ’
1
)
â€‹
|
m
|
=
(
Î¼
4
âˆ’
1
)
â€‹
(
p
+
q
+
1
)
.
â€¢ For APARCH
(
Î´
,
p
,
q
)
 processes (see [13]), we also have 
f
Î¸
=
0
 and 
X
t
=
M
Î¸
t
â€‹
Î¾
t
 since for any 
t
âˆˆ
â„¤
,
{
X
t
=
Ïƒ
t
â€‹
Î¾
t
Ïƒ
t
Î´
=
Ï‰
0
+
a
1
â€‹
(
X
t
âˆ’
1
âˆ’
Î³
1
â€‹
|
X
t
âˆ’
1
|
)
Î´
+
â‹¯
+
a
p
â€‹
(
X
t
âˆ’
p
âˆ’
Î³
p
â€‹
|
X
t
âˆ’
p
|
)
Î´
+
b
1
â€‹
Ïƒ
t
âˆ’
1
Î´
+
â‹¯
+
b
q
â€‹
Ïƒ
t
âˆ’
q
Î´
.
For such a process, 
Î¸
=
Î¸
2
=
(
Ï‰
0
,
a
1
,
â€¦
,
a
p
,
Î³
1
,
â€¦
,
Î³
p
,
b
1
,
â€¦
,
b
q
)
 when we assume that 
Î´
 is known, and, mutatis mutandis, the result is the same than for GARCH processes:
âˆ’
2
â€‹
Trace
â€‹
(
G
m
â€‹
(
Î¸
m
âˆ—
)
â€‹
F
m
â€‹
(
Î¸
m
âˆ—
)
âˆ’
1
)
=
(
Î¼
4
âˆ’
1
)
â€‹
|
m
|
=
(
Î¼
4
âˆ’
1
)
â€‹
(
2
â€‹
p
+
q
+
1
)
.
3/ Otherwise, the computations are no longer easy. Let us see the example of the family of 
A
â€‹
R
â€‹
(
1
)
âˆ’
A
â€‹
R
â€‹
C
â€‹
H
â€‹
(
p
)
 processes. Then for any 
t
âˆˆ
â„¤
 we have 
X
t
=
Ï•
â€‹
X
t
âˆ’
1
+
Z
t
 where 
Z
t
=
Î¾
t
â€‹
(
Î±
0
+
Î±
1
â€‹
Z
t
âˆ’
1
2
+
â‹¯
+
Î±
p
â€‹
Z
t
âˆ’
p
2
)
1
/
2
. As a consequence, with 
Î¸
=
(
Ï•
,
Î±
0
,
â€¦
,
Î±
p
)
â€²
, we obtain for any 
t
âˆˆ
â„¤
,

X
t
=
f
Î¸
â€‹
(
X
t
âˆ’
1
)
+
M
Î¸
â€‹
(
X
t
âˆ’
1
,
â€¦
,
X
t
âˆ’
p
âˆ’
1
)
â€‹
Î¾
t
with
{
f
Î¸
â€‹
(
X
t
âˆ’
1
)
=
Ï•
â€‹
X
t
âˆ’
1
M
Î¸
â€‹
(
X
t
âˆ’
1
,
â€¦
,
X
t
âˆ’
p
)
=
(
Î±
0
+
âˆ‘
i
=
1
p
Î±
i
â€‹
(
X
t
âˆ’
i
âˆ’
Ï•
â€‹
X
t
âˆ’
i
âˆ’
1
)
2
)
1
/
2
.
Thus the parameter 
Ï•
 is present in 
f
Î¸
 as well as in 
M
Î¸
. From (6.1), and with the notations of 1/, we obtain:

F
m
â€‹
(
Î¸
m
âˆ—
)
=
âˆ’
(
A
1
,
1
O
1
,
p
+
1
O
p
+
1
,
1
O
p
+
1
,
p
+
1
)
âˆ’
B
1
,
p
+
2
and
G
m
â€‹
(
Î¸
m
âˆ—
)
=
(
A
1
,
1
O
1
,
p
+
1
O
p
+
1
,
1
O
p
+
1
,
p
+
1
)
+
(
Î¼
4
âˆ’
1
)
2
â€‹
B
1
,
p
+
2
.
As a consequence,

G
m
â€‹
(
Î¸
m
âˆ—
)
=
âˆ’
(
Î¼
4
âˆ’
1
)
2
â€‹
F
m
â€‹
(
Î¸
m
âˆ—
)
+
(
Î¼
4
âˆ’
3
)
2
â€‹
(
A
1
,
1
O
1
,
p
+
1
O
p
+
1
,
1
O
p
+
1
,
p
+
1
)
.
Thus, with 
|
m
|
=
p
+
2
,

G
m
â€‹
(
Î¸
m
âˆ—
)
â€‹
F
m
âˆ’
1
â€‹
(
Î¸
m
âˆ—
)
=
âˆ’
(
Î¼
4
âˆ’
1
)
2
â€‹
I
|
m
|
+
(
Î¼
4
âˆ’
3
)
2
â€‹
(
A
1
,
1
O
1
,
p
+
1
O
p
+
1
,
1
O
p
+
1
,
p
+
1
)
â€‹
F
m
âˆ’
1
â€‹
(
Î¸
m
âˆ—
)
.
Whatever the matrix 
F
m
âˆ’
1
â€‹
(
Î¸
m
âˆ—
)
, we have 
(
A
1
,
1
O
1
,
p
+
1
O
p
+
1
,
1
O
p
+
1
,
p
+
1
)
â€‹
F
m
âˆ’
1
â€‹
(
Î¸
m
âˆ—
)
=
(
c
â€‹
(
Î¸
m
âˆ—
)
O
1
,
p
+
1
O
p
+
1
,
1
O
p
+
1
,
p
+
1
)
 with 
c
â€‹
(
Î¸
m
âˆ—
)
=
c
â€‹
(
Î¸
âˆ—
)
âˆˆ
â„
 since 
m
âˆ—
âŠ‚
m
. Then for all 
m
âˆ—
âŠ‚
m
,

âˆ’
2
â€‹
Trace
â€‹
(
G
m
â€‹
(
Î¸
m
âˆ—
)
â€‹
F
m
â€‹
(
Î¸
m
âˆ—
)
âˆ’
1
)
=
âˆ’
2
â€‹
c
â€‹
(
Î¸
âˆ—
)
+
(
Î¼
4
âˆ’
1
)
â€‹
|
m
|
,
where 
âˆ’
2
â€‹
c
â€‹
(
Î¸
âˆ—
)
 does not depend on 
m
.

7Numerical Studies
This section aims to investigate the numerical behavior of the model selection criteria studied in Section 4 and Section 5 using R software.

To do that, three Data Generating Processes (DGP) have been considered:

DGP I	AR(2)	
X
t
=
0.4
â€‹
X
t
âˆ’
1
+
0.4
â€‹
X
t
âˆ’
2
+
Î¾
t
,
DGP II	ARMA(1,1)	
X
t
âˆ’
0.5
â€‹
X
t
âˆ’
1
=
Î¾
t
+
0.6
â€‹
Î¾
t
âˆ’
1
,
DGP III	GARCH(1,1)	
X
t
=
Ïƒ
t
â€‹
Î¾
t
 â€ƒwith â€„ 
Ïƒ
t
2
=
1
+
0.35
â€‹
X
t
âˆ’
1
2
+
0.4
â€‹
Ïƒ
t
âˆ’
1
2
,
where 
(
Î¾
t
)
t
 is a Gaussian white noise with variance unity.

Remark 5. As already observed in the Remark 1, Assumption A1 is never satisfied for ARMA processes in case of overfitting. However, in the used optimization under constraint algorithm (program nloptr), we initialized 
Î¸
 at 
0
 (except for the variance estimator). By this way, we have noticed in Monte-Carlo experiments that the algorithm always converges to 
Î¸
âˆ—
 and not other solution due to the overfitting.
In order to illustrate the obtained theoretical asymptotic behaviors, we have realized Monte-Carlo experiments where the performance of the AIC, BIC and KCâ€™ criteria are compared using the following parameters:

â€¢ The considered family of competitive models is the same for the three DGP
â„³
=
{
ARMA
â€‹
(
p
,
q
)
â€‹
and
â€‹
GARCH
â€‹
(
p
,
q
)
â€‹
processes
â€‹
with
â€‹
â€„â€„0
â‰¤
p
,
q
â‰¤
6
}
.
â€¢ Several values of 
n
, the observed trajectory length, are considered: 200, 500, 1000, 2000.
â€¢ For each 
n
 and DGP, we have generated 500 independent replications of the trajectories.
Hence, for each replication, the selected models 
m
^
A
â€‹
I
â€‹
C
, 
m
^
B
â€‹
I
â€‹
C
 and 
m
^
K
â€‹
C
â€²
 are computed. Then,

1. The consistency property is illustrated by the computation of the frequency (percentage) of selecting the true model versus a model other than the true one (called here â€wrongâ€).
2. For the efficiency property (Theorem 4.2, 4.3 and 4.4), we first compute a very sharp estimator 
R
~
 of the risk function 
R
 for each DGP: 
R
~
=
Î³
^
N
 computed from an independent and very large (
N
=
10
6
) trajectory of the DGP. By this way, and we obtain an estimator 
â„“
~
â€‹
(
Î¸
^
m
^
,
Î¸
âˆ—
)
=
R
~
â€‹
(
Î¸
^
m
^
)
âˆ’
R
~
â€‹
(
Î¸
âˆ—
)
 of 
â„“
â€‹
(
Î¸
^
m
^
,
Î¸
âˆ—
)
 for 
m
^
=
m
^
A
â€‹
I
â€‹
C
, 
m
^
B
â€‹
I
â€‹
C
 and 
m
^
K
â€‹
C
â€²
. Then, we compute
M
^
â€‹
E
:=
n
â€‹
(
â„“
~
â€‹
(
Î¸
^
m
^
,
Î¸
âˆ—
)
Â¯
âˆ’
â„“
~
â€‹
(
Î¸
^
m
âˆ—
,
Î¸
âˆ—
)
Â¯
)
where 
â„“
~
Â¯
 is the average of 
â„“
~
 over the 500 replications. Therefore 
M
â€‹
E
^
 is an estimator of 
n
â€‹
(
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
^
,
Î¸
âˆ—
)
]
âˆ’
min
m
âˆˆ
â„³
â€‹
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
]
)
, which appears in (4.24) and (4.26).
The results of Monte-Carlo experiments are reported in Table 1, devoted to the consistency property, and in Table 2, devoted to the efficiency property.

n	
200
500
1000
2000
AIC	BIC	KCâ€™	AIC	BIC	KCâ€™	AIC	BIC	KCâ€™	AIC	BIC	KCâ€™
DGP I	True	17.2	36.2	35.6	30.4	73.2	78.2	36.4	87.4	92.2	32.4	96.2	98.4
Wrong	82.8	63.8	64.4	69.6	26.8	21.8	63.6	13.6	7.8	67.6	03.8	01.6
DGP II	True	27.8	80.8	92.0	30.6	88.4	96.6	31.0	89.1	97.5	33.3	95.2	99.9
Wrong	72.2	19.2	08.0	69.7	11.6	03.4	69.0	10.9	02.5	66.7	04.8	00.1
DGP III	True	00.4	10.8	14.8	01.4	32.2	55.8	01.0	54.8	82.0	02.0	75.8	93.8
Wrong	99.6	89.2	85.2	98.6	67.8	44.2	99.0	45.2	18.0	98.0	24.2	06.2
Table 1:Percentage of â€trueâ€ selected models depending on the criterion and sampleâ€™s length for DGP I-III.
n	
200
500
1000
2000
AIC	BIC	KCâ€™	AIC	BIC	KCâ€™	AIC	BIC	KCâ€™	AIC	BIC	KCâ€™
DGP I	4.91	2.59	5.35	3.46	1.11	1.18	3.08	0.98	0.75	3.05	0.38	0.29
DGP II	3.66	0.87	0.54	3.37	0.42	0.11	2.62	0.15	0.05	2.5	0.10	0.04
DGP III	2.39	4.63	13.16	2.53	4.08	9.54	2.69	2.96	2.52	3.21	2.06	0.76
Table 2:
M
â€‹
E
^
 of selected models depending on the criterion and the sampleâ€™s length for DGP I-III.
Conclusions of numerical experiments:

â€¢ Concerning the consistency properties, the numerical results of Table 1 show that the percentages of choice of the true model tend towards 100 for increasing 
n
 and with the criteria BIC and KCâ€™, and this corresponds well to the obtained asymptotic result (Corollary 3). And as it could also be deduced from the theory (see Corollary 2) the AIC criterion is not a consistent one. Moreover, we observe that the KCâ€™ criterion outperforms BIC when dealing as well as small and large samples for all considered DGP. These results confirm that it is important to also consider the neglected terms in the derivation of the BIC criterion.
â€¢ From the results of Table 2, we notice a decrease of the residual term 
M
â€‹
E
^
 to 
0
 for increasing 
n
 for the consistent criteria BIC and KCâ€™. This corresponds well to the 
o
â€‹
(
1
/
n
)
 term observed in (4.24). We also observe that this convergence to 0 is globally faster with the KCâ€™ criterion than with the BIC one. Thus, in terms of efficiency as well as in terms of consistency, the KCâ€™ criterion performs even better than the BIC one for the selected DGPs. Finally, as shown by Theorems 4.2 and 4.4, the statistic 
M
â€‹
E
^
 seems asymptotically bounded and does not converge to 
0
 when the AIC criterion is applied to select the model. This confirms that BIC and especially KCâ€™ criteria are more accurate in terms of efficiency than AIC criterion.
8Proofs
8.1Proofs of Section 3
The asymptotic normality of 
(
1
n
(
âˆ‚
Î¸
i
L
n
(
Î¸
m
âˆ—
)
)
i
âˆˆ
m
 was established in [5] and [4] when 
m
âˆ—
âŠ‚
m
 using a central limit theorem for stationary martingale difference. Here we extend this result to any 
m
âˆˆ
â„³
:

Proposition 4. Under Assumption A0-A5, for any 
Î¸
âˆˆ
Î˜
, we have
n
â€‹
(
1
n
â€‹
âˆ‚
Î¸
L
n
â€‹
(
Î¸
)
+
1
2
â€‹
ğ”¼
â€‹
[
âˆ‚
Î¸
Î³
â€‹
(
Î¸
,
X
0
)
]
)
â€‹
âŸ¶
ğ’Ÿ
n
â†’
âˆ
â€‹
ğ’©
â€‹
(
0
,
G
â€‹
(
Î¸
)
)
with
G
â€‹
(
Î¸
)
:=
1
4
â€‹
(
âˆ‘
t
âˆˆ
â„¤
Cov
â€‹
(
âˆ‚
Î¸
i
Î³
â€‹
(
Î¸
,
X
0
)
,
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
,
X
t
)
)
)
1
â‰¤
i
,
j
â‰¤
d
.
(8.1)
The main tool we use here for establishing Theorem 4 is the notion of 
Ï„
-dependence for stationary time series and more precisely, the 
Ï„
-dependence coefficients, which are a version of the coupling coefficients introduced in [11] and used for stationary infinite memory chains. The reader is deferred to the lecture notes [10] for complements and details on coupling, based on the Wasserstein distance between probabilities defined as below. Its stationary version is:

Definition 1. Let 
(
Î©
,
ğ’
,
â„™
)
 be a probability space, 
â„³
 a 
Ïƒ
-subalgebra of 
ğ’
 and 
Z
 a random variable with values in 
E
. Assume that 
â€–
Z
â€–
p
<
âˆ
 and define the coefficient 
Ï„
(
p
)
 as
Ï„
(
p
)
â€‹
(
â„³
,
Z
)
=
â€–
sup
f
âˆˆ
Î›
1
â€‹
(
E
)
{
|
âˆ«
f
â€‹
(
x
)
â€‹
â„™
Z
|
â„³
â€‹
(
d
â€‹
x
)
âˆ’
âˆ«
f
â€‹
(
x
)
â€‹
â„™
Z
â€‹
(
d
â€‹
x
)
|
}
â€–
p
.
Using the definition of 
Ï„
, the dependence between the past of the sequence 
(
Z
t
)
t
âˆˆ
â„¤
 and its future 
k
-tuples may be assessed: consider the norm 
â€–
x
âˆ’
y
â€–
=
â€–
x
1
âˆ’
y
1
â€–
+
â‹¯
+
â€–
x
k
âˆ’
y
k
â€–
 on 
E
k
, set 
â„³
p
=
Ïƒ
â€‹
(
Z
t
,
t
â‰¤
p
)
 and define

Ï„
Z
(
p
)
â€‹
(
s
)
=
sup
k
>
0
{
max
1
â‰¤
l
â‰¤
k
â¡
1
l
â€‹
sup
{
Ï„
(
p
)
â€‹
(
â„³
p
,
(
Z
j
1
,
â€¦
,
Z
j
l
)
)
â€‹
 with 
â€‹
p
+
s
â‰¤
j
1
<
â‹¯
<
j
l
}
}
.
Finally, the time series 
(
Z
t
)
t
âˆˆ
â„¤
 is 
Ï„
Z
(
p
)
-weakly dependent when its coefficients 
Ï„
Z
(
p
)
â€‹
(
s
)
 tend to 
0
 as 
s
 tends to infinity.

Lemma 2. Under Assumption A0, then for 
p
â‰¤
r
 and 
b
k
(
p
)
=
Î±
k
â€‹
(
M
Î¸
,
Î˜
)
â€‹
â€–
Î¾
0
â€–
p
+
Î±
k
â€‹
(
f
Î¸
,
Î˜
)
 for any 
j
âˆˆ
â„•
âˆ—
,
Ï„
X
(
p
)
â€‹
(
s
)
â‰¤
C
â€‹
Î»
s
with
Î»
s
=
inf
1
â‰¤
r
â‰¤
s
{
(
âˆ‘
k
=
1
âˆ
b
k
(
p
)
)
s
/
r
+
âˆ‘
t
=
r
+
1
âˆ
b
t
(
p
)
}
for 
s
â‰¥
1
.
(8.2)
Proof of Lemma 2.This Lemma can be directly deduced from Proposition 3.1 of [14] where 
T
â€‹
(
x
,
Î¾
0
)
=
M
Î¸
â€‹
(
x
)
â€‹
Î¾
0
+
f
Î¸
â€‹
(
x
)
 for any 
x
âˆˆ
â„
âˆ
 and therefore
â€–
T
â€‹
(
x
,
Î¾
0
)
âˆ’
T
â€‹
(
y
,
Î¾
0
)
â€–
p
â‰¤
â€–
Î¾
0
â€–
p
â€‹
|
M
Î¸
â€‹
(
x
)
âˆ’
M
Î¸
â€‹
(
y
)
|
+
|
f
Î¸
â€‹
(
x
)
âˆ’
f
Î¸
â€‹
(
y
)
|
inducing 
â€–
T
â€‹
(
x
,
Î¾
0
)
âˆ’
T
â€‹
(
y
,
Î¾
0
)
â€–
p
â‰¤
âˆ‘
k
=
1
âˆ
b
k
(
p
)
. âˆ

Remark 6. Using Assumption A0 and A5, we deduce that 
b
t
(
p
)
=
O
â€‹
(
t
âˆ’
Î´
)
 with 
Î´
>
7
/
2
, and therefore 
Ï„
X
(
p
)
â€‹
(
s
)
â‰¤
Î»
s
=
O
â€‹
(
s
1
âˆ’
Î´
â€‹
log
â¡
s
)
.
Now, under the Assumption A0, since 
X
 is a causal time series, define for any 
j
=
1
,
â€¦
,
d
 and 
Î¸
âˆˆ
Î˜
,

Ï•
Î¸
(
j
)
â€‹
(
(
X
t
âˆ’
k
)
k
â‰¥
0
)
:=
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
,
X
t
)
=
âˆ’
2
â€‹
âˆ‚
Î¸
j
M
Î¸
t
â€‹
(
X
t
âˆ’
f
Î¸
t
)
2
(
M
Î¸
t
)
3
âˆ’
2
â€‹
âˆ‚
Î¸
j
f
Î¸
t
â€‹
X
t
âˆ’
f
Î¸
t
(
M
Î¸
t
)
2
+
2
â€‹
âˆ‚
Î¸
j
M
Î¸
t
M
Î¸
t
.
Then we have:

Lemma 3. Under Assumption A0-A5, for any 
j
=
1
,
â€¦
,
d
, for any 
Î¸
âˆˆ
Î˜
, the sequence 
(
Ï•
Î¸
(
j
)
â€‹
(
(
X
t
âˆ’
k
)
k
â‰¥
0
)
)
t
âˆˆ
â„¤
 is a causal stationary sequence that is 
Ï„
Ï•
Î¸
(
j
)
(
p
)
-weakly dependent where its coefficients 
Ï„
Ï•
Î¸
(
j
)
(
1
)
â€‹
(
s
)
 satisfies:
Ï„
Ï•
Î¸
(
j
)
(
1
)
(
s
)
â‰¤
C
(
âˆ‘
â„“
=
1
s
(
Î±
â„“
(
f
Î¸
,
Î˜
)
+
Î±
â„“
(
M
Î¸
,
Î˜
)
+
Î±
â„“
(
âˆ‚
Î¸
j
M
Î¸
,
Î˜
)
+
Î±
â„“
(
âˆ‚
Î¸
j
f
Î¸
,
Î˜
)
)
Î»
s
+
1
âˆ’
â„“
+
âˆ‘
â„“
=
s
+
1
âˆ
(
Î±
â„“
(
f
Î¸
,
Î˜
)
+
Î±
â„“
(
M
Î¸
,
Î˜
)
+
Î±
â„“
(
âˆ‚
Î¸
j
M
Î¸
,
Î˜
)
+
Î±
â„“
(
âˆ‚
Î¸
j
f
Î¸
,
Î˜
)
)
)
,
(8.3)
for any 
s
â‰¥
0
 where 
(
Î»
s
)
 is defined in (8.2).

Proof of Lemma 3.In the proof of Proposition 4.1 of [3], it has been proven for 
U
=
(
U
i
)
i
â‰¥
1
 and 
V
=
(
V
i
)
i
â‰¥
1
 such as 
sup
i
â‰¥
1
{
â€–
U
i
â€–
4
âˆ¨
â€–
V
i
â€–
4
}
<
âˆ
 that there exists 
C
>
0
 satisfying
ğ”¼
[
sup
Î¸
âˆˆ
Î˜
âˆ¥
Ï•
Î¸
(
j
)
(
U
)
âˆ’
Ï•
Î¸
(
j
)
(
V
)
âˆ¥
]
â‰¤
C
(
âˆ¥
U
1
âˆ’
V
1
âˆ¥
4
+
âˆ‘
i
=
2
âˆ
(
Î±
i
(
f
Î¸
,
Î˜
)
+
Î±
i
(
M
Î¸
,
Î˜
)
+
Î±
i
(
âˆ‚
Î¸
j
f
Î¸
,
Î˜
)
+
Î±
i
(
âˆ‚
Î¸
j
M
Î¸
,
Î˜
)
)
âˆ¥
U
i
âˆ’
V
i
âˆ¥
4
)
.
(8.4)
Using coupling techniques, if 
(
Î¾
~
t
)
t
âˆˆ
â„¤
 is an independent replication of 
(
Î¾
t
)
t
âˆˆ
â„¤
, define also 
(
X
~
t
)
t
âˆˆ
â„¤
 satisfying the assumptions with 
(
Î¾
~
t
)
t
âˆˆ
â„¤
 instead of 
(
Î¾
t
)
t
âˆˆ
â„¤
 and 
(
Ï•
Î¸
(
j
)
â€‹
(
(
X
~
t
âˆ’
k
)
k
â‰¥
0
)
)
t
âˆˆ
â„¤
. Then for 
s
â‰¥
0
, using (8.4),

Ï„
Ï•
Î¸
(
j
)
(
1
)
â€‹
(
s
)
â‰¤
â€–
Ï•
Î¸
(
j
)
â€‹
(
(
X
s
âˆ’
k
)
k
â‰¥
0
)
âˆ’
Ï•
Î¸
(
j
)
â€‹
(
(
X
~
s
âˆ’
k
)
k
â‰¥
0
)
â€–
1
â‰¤
C
(
âˆ¥
X
1
âˆ’
X
~
1
âˆ¥
4
+
âˆ‘
i
=
2
âˆ
(
Î±
i
(
f
Î¸
,
Î˜
)
+
Î±
i
(
M
Î¸
,
Î˜
)
+
Î±
i
(
âˆ‚
Î¸
j
f
Î¸
,
Î˜
)
+
Î±
i
(
âˆ‚
Î¸
j
M
Î¸
,
Î˜
)
)
âˆ¥
X
i
âˆ’
X
~
i
âˆ¥
4
)
â‰¤
C
â€‹
âˆ‘
â„“
=
1
âˆ
(
Î±
â„“
â€‹
(
f
Î¸
,
Î˜
)
+
Î±
â„“
â€‹
(
M
Î¸
,
Î˜
)
+
Î±
â„“
â€‹
(
âˆ‚
Î¸
j
f
Î¸
,
Î˜
)
+
Î±
â„“
â€‹
(
âˆ‚
Î¸
j
M
Î¸
,
Î˜
)
)
â€‹
Î»
s
+
1
âˆ’
â„“
,
that implies (8.3). âˆ

Remark 7. Under Assumption A0 and A5, and therefore with 
Î»
s
=
O
â€‹
(
s
1
âˆ’
Î´
â€‹
log
â¡
s
)
 with 
Î´
>
7
/
2
, we also deduce that 
Ï„
Ï•
Î¸
(
j
)
(
1
)
â€‹
(
s
)
=
O
â€‹
(
s
1
âˆ’
Î´
â€‹
log
â¡
s
)
.
Proof of Proposition 4.If 
Z
 is a 
Ï„
Z
-dependent centered stationary time series satisfying 
ğ”¼
â€‹
[
|
Z
0
|
Îº
]
<
âˆ
 with 
Îº
>
2
, and 
âˆ‘
s
=
1
âˆ
s
1
/
(
Îº
âˆ’
2
)
â€‹
Ï„
Z
â€‹
(
s
)
<
âˆ
, we deduce from Lemma 2, point 2. of [9] that condition D
(
2
,
Î¸
/
2
,
X
)
 is satisfied as 
Î¸
-weakly dependent coefficients are smaller than 
Ï„
-weakly dependent coefficients, see (2.2.13) p.16 of [10], and 
0
<
âˆ‘
t
âˆˆ
â„¤
|
ğ”¼
â€‹
[
Z
0
â€‹
Z
t
]
|
<
âˆ
 from Proposition 2 of [9]. Then,
1
n
â€‹
âˆ‘
t
=
1
n
Z
t
â€‹
âŸ¶
ğ’Ÿ
n
â†’
âˆ
â€‹
ğ’©
â€‹
(
0
,
âˆ‘
t
âˆˆ
â„¤
ğ”¼
â€‹
[
Z
0
â€‹
Z
t
]
)
.
We can apply this central limit theorem to

Z
t
:=
âˆ‘
j
=
1
d
c
j
â€‹
(
Ï•
Î¸
(
j
)
â€‹
(
(
X
t
âˆ’
k
)
k
â‰¥
0
)
âˆ’
ğ”¼
â€‹
[
Ï•
Î¸
(
j
)
â€‹
(
(
X
t
âˆ’
k
)
k
â‰¥
0
)
]
)
with 
(
c
j
)
1
â‰¤
j
â‰¤
d
âˆˆ
â„
d
.
Indeed, using Lemma 3, we easily obtain for 
s
â‰¥
0

Ï„
Z
â€‹
(
s
)
â‰¤
C
â€‹
(
âˆ‘
j
=
1
d
|
c
j
|
)
â€‹
âˆ‘
â„“
=
1
âˆ
(
Î±
â„“
â€‹
(
f
Î¸
,
Î˜
)
+
Î±
â„“
â€‹
(
M
Î¸
,
Î˜
)
+
Î±
â„“
â€‹
(
âˆ‚
Î¸
j
f
Î¸
,
Î˜
)
+
Î±
â„“
â€‹
(
âˆ‚
Î¸
j
M
Î¸
,
Î˜
)
)
â€‹
Î»
s
+
1
âˆ’
â„“
,
and therefore under Assumption A0 and A5, 
Ï„
Z
â€‹
(
s
)
=
O
â€‹
(
s
1
âˆ’
Î´
â€‹
log
â¡
s
)
. Moreover, using Lemma 6, we deduce 
ğ”¼
â€‹
[
|
Z
0
|
8
/
3
]
<
âˆ
. Then with 
Îº
=
8
/
3
, 
âˆ‘
s
=
1
âˆ
s
1
/
(
Îº
âˆ’
2
)
â€‹
Ï„
Z
â€‹
(
s
)
=
âˆ‘
s
=
1
âˆ
s
3
/
2
â€‹
Ï„
Z
â€‹
(
s
)
<
âˆ
 is satisfied since 
Î´
>
7
/
2
.
Therefore, we deduce for any 
Î¸
âˆˆ
Î˜
,

n
â€‹
âˆ‘
j
=
1
d
c
j
â€‹
(
1
n
â€‹
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
)
+
1
2
â€‹
ğ”¼
â€‹
[
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
,
X
0
)
]
)
âŸ¶
ğ’Ÿ
n
â†’
âˆ
â€‹
ğ’©
â€‹
(
0
,
1
4
â€‹
âˆ‘
i
=
1
d
âˆ‘
j
=
1
d
c
i
â€‹
c
j
â€‹
âˆ‘
t
âˆˆ
â„¤
Cov
â€‹
(
âˆ‚
Î¸
i
Î³
â€‹
(
Î¸
,
X
0
)
,
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
,
X
t
)
)
)
,
which implies the multidimensional central limit theorem (8.1). âˆ

Proof of Corollary 1.Firstly, it was already established in [4] that if 
m
âˆ—
âŠ‚
m
 then 
(
âˆ‚
Î¸
i
Î³
â€‹
(
Î¸
m
,
X
t
)
)
t
âˆˆ
â„¤
 is a stationary martingale difference process with respect to 
â„±
t
=
Ïƒ
â€‹
(
(
X
t
âˆ’
k
)
k
âˆˆ
â„•
)
. As a consequence 
Cov
â€‹
(
âˆ‚
Î¸
i
Î³
â€‹
(
Î¸
,
X
0
)
,
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
,
X
t
)
)
=
0
 if 
t
â‰ 
0
.
Secondly, for all 
m
âˆˆ
â„³
, from the definition of 
Î¸
m
âˆ—
 as a local minimum of 
R
 on 
Î˜
m
, and from Assumption A0-A5, then 
âˆ‚
Î¸
j
R
â€‹
(
Î¸
m
âˆ—
)
=
ğ”¼
â€‹
[
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
m
âˆ—
,
X
0
)
]
=
0
 for all 
j
âˆˆ
m
. âˆ
Proof of Theorem 3.1.We use here a standard proof, allowing to show the asymptotic normality of the QMLE and already used in [5].
Firstly, it was established in [4] that 
Î¸
^
m
â€‹
âŸ¶
a
.
s
.
n
â†’
+
âˆ
â€‹
Î¸
m
âˆ—
.
Secondly, a Taylor-Lagrange expansion is applied to 
(
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
^
m
)
)
j
âˆˆ
m
 around 
Î¸
m
âˆ—
:
1
n
â€‹
(
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
^
m
)
)
j
âˆˆ
m
=
1
n
â€‹
(
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
m
âˆ—
)
)
j
âˆˆ
m
+
(
1
n
â€‹
âˆ‚
Î¸
i
â€‹
Î¸
j
2
L
n
â€‹
(
Î¸
Â¯
m
)
)
i
,
j
âˆˆ
m
Ã—
n
â€‹
(
(
Î¸
^
m
)
i
âˆ’
(
Î¸
m
âˆ—
)
i
)
i
âˆˆ
m
(8.5)
with 
Î¸
Â¯
m
=
c
â€‹
Î¸
^
m
+
(
1
âˆ’
c
)
â€‹
Î¸
m
âˆ—
 and 
0
<
c
<
1
.
Using 
Î¸
^
m
â€‹
âŸ¶
a
.
s
.
n
â†’
+
âˆ
â€‹
Î¸
m
âˆ—
 and the ergodic theorem 
1
n
â€‹
(
âˆ‚
Î¸
i
â€‹
Î¸
j
2
L
n
â€‹
(
Î¸
m
)
)
i
,
j
âˆˆ
m
â€‹
âŸ¶
a
.
s
.
n
â†’
+
âˆ
â€‹
F
m
â€‹
(
Î¸
m
)
 for any 
Î¸
m
âˆˆ
Î˜
m
 since 
ğ”¼
â€‹
[
â€–
âˆ‚
Î¸
2
2
Î³
â€‹
(
Î¸
,
X
0
)
â€–
Î˜
]
<
âˆ
 , we obtain:

(
1
n
â€‹
âˆ‚
Î¸
i
â€‹
Î¸
j
2
L
n
â€‹
(
Î¸
Â¯
m
)
)
i
,
j
âˆˆ
m
â€‹
âŸ¶
a
.
s
.
n
â†’
+
âˆ
â€‹
F
m
â€‹
(
Î¸
m
âˆ—
)
.
(8.6)
Finally, by definition of 
Î¸
^
m
, 
âˆ‚
Î¸
j
L
^
n
â€‹
(
Î¸
^
m
)
=
0
 for any 
j
âˆˆ
m
. As a consequence,

1
n
â€‹
(
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
^
m
)
)
j
âˆˆ
m
â€‹
âŸ¶
ğ’«
n
â†’
âˆ
â€‹
0
,
(8.7)
using a Markov Inequality and 
ğ”¼
â€‹
[
1
n
â€‹
â€–
âˆ‚
Î¸
L
^
n
â€‹
(
Î¸
)
âˆ’
âˆ‚
Î¸
L
n
â€‹
(
Î¸
)
â€–
Î˜
]
â€‹
âŸ¶
n
â†’
âˆ
â€‹
0
 established in (5.11) of [5]. Considering (8.5), (8.6) and (8.7), and with the central limit theorem satisfied by 
1
n
â€‹
(
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
m
âˆ—
)
)
j
âˆˆ
m
 provided in Corollary 1, this achieves the proof. âˆ

Now, before establishing Proposition 1, three technical lemmas can be stated:

Lemma 4. Under Assumptions A0-A5, with 
8
/
3
<
r
â€²
â‰¤
r
/
3
 and 
r
â€²
<
2
â€‹
(
Î´
âˆ’
1
)
 where 
Î´
>
7
/
2
 is given in Assumption A5, for any 
m
âˆˆ
â„³
, there exists 
C
>
0
 such as for any 
n
âˆˆ
â„•
âˆ—
â€–
(
1
n
â€‹
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
m
âˆ—
)
)
j
âˆˆ
m
â€–
r
â€²
â‰¤
C
.
(8.8)
Proof.First, for any 
m
âˆˆ
â„³
 and 
n
âˆˆ
â„•
âˆ—
,
â€–
(
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
m
âˆ—
)
)
j
âˆˆ
m
â€–
r
â€²
â‰¤
|
m
|
r
â€²
/
2
âˆ’
1
â€‹
âˆ‘
j
âˆˆ
m
|
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
m
âˆ—
)
|
r
â€²
(8.9)
â‰¤
|
m
|
r
â€²
/
2
âˆ’
1
2
r
â€²
â€‹
âˆ‘
j
âˆˆ
m
|
âˆ‘
t
=
1
n
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
m
âˆ—
,
X
t
)
|
r
â€²
.
Now, for all 
j
âˆˆ
m
, 
(
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
m
âˆ—
,
X
t
)
)
t
âˆˆ
â„¤
 is a centered (from the proof of Corollary 1) stationary 
Ï„
Ï•
Î¸
(
j
)
(
p
)
-weakly dependent where its coefficients 
(
Ï„
Ï•
Î¸
(
j
)
(
1
)
â€‹
(
s
)
)
s
 satisfies (8.3) (see Lemma 3). Moreover, from the proof of Proposition 4, 
Ï„
Ï•
Î¸
(
j
)
(
1
)
â€‹
(
s
)
=
O
â€‹
(
s
1
âˆ’
Î´
â€‹
log
â¡
s
)
.
In Proposition 5.5 of [10], since 
ğ”¼
â€‹
[
|
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
m
âˆ—
,
X
0
)
|
r
â€²
]
<
âˆ
 from Lemma 6, it has been established that:

ğ”¼
â€‹
[
|
âˆ‘
t
=
1
n
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
m
âˆ—
,
X
t
)
|
r
â€²
]
â‰¤
C
r
â€²
â€‹
(
M
r
â€²
,
n
+
M
2
,
n
r
â€²
/
2
)
where
M
m
,
n
:=
2
â€‹
n
â€‹
âˆ‘
i
=
0
n
âˆ’
1
(
i
+
1
)
m
âˆ’
2
â€‹
Ï„
Ï•
Î¸
(
j
)
(
1
)
â€‹
(
i
)
.
Using 
8
/
3
<
r
â€²
<
2
â€‹
(
Î´
âˆ’
1
)
 with 
Î´
>
7
/
2
, we obtain that

M
r
â€²
,
n
â‰¤
C
â€‹
n
â€‹
âˆ‘
i
=
1
n
i
r
â€²
âˆ’
1
âˆ’
Î´
â€‹
log
â¡
(
i
)
â‰¤
C
â€²
â€‹
n
1
+
r
â€²
âˆ’
Î´
â€‹
log
â¡
(
n
)
=
O
â€‹
(
n
r
â€²
/
2
)
and 
M
2
,
n
â‰¤
C
â€‹
n
â€‹
âˆ‘
i
=
1
n
i
1
âˆ’
Î´
â€‹
log
â¡
(
i
)
â‰¤
C
â€²â€²
â€‹
n
. As a consequence, there exists 
C
>
0
 such as for any 
n
âˆˆ
â„•
âˆ—
,

ğ”¼
â€‹
[
|
âˆ‘
t
=
1
n
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
m
âˆ—
,
X
t
)
|
r
â€²
]
â‰¤
C
â€‹
n
r
â€²
/
2
.
(8.10)
Then, using (8.9) and (8.10), the proof is established. âˆ

Lemma 5. Under Assumptions A0-A5, then for any 
m
âˆˆ
â„³
, there exists 
C
>
0
 such as for any 
n
âˆˆ
â„•
âˆ—
,
â€–
(
1
n
â€‹
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
^
m
)
)
j
âˆˆ
m
â€–
r
/
3
â‰¤
C
.
Proof.First, from the definition of 
Î¸
^
m
, we have 
âˆ‚
Î¸
j
L
^
n
â€‹
(
Î¸
^
m
)
=
0
 for any 
j
âˆˆ
m
. Then,
â€–
(
1
n
â€‹
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
^
m
)
)
j
âˆˆ
m
â€–
r
/
3
=
â€–
(
1
n
â€‹
(
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
^
m
)
âˆ’
âˆ‚
Î¸
j
L
^
n
â€‹
(
Î¸
^
m
)
)
)
j
âˆˆ
m
â€–
r
/
3
â‰¤
|
m
|
(
r
âˆ’
6
)
/
2
â€‹
r
n
â€‹
âˆ‘
j
âˆˆ
m
â€–
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
^
m
)
âˆ’
âˆ‚
Î¸
j
L
^
n
â€‹
(
Î¸
^
m
)
â€–
r
/
3
â‰¤
|
m
|
1
/
2
2
â€‹
n
â€‹
âˆ‘
j
âˆˆ
m
âˆ‘
t
=
1
n
â€–
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
^
m
,
X
t
)
âˆ’
âˆ‚
Î¸
j
Î³
^
â€‹
(
Î¸
^
m
,
X
t
)
â€–
r
/
3
.
From the proof of Lemma 2 in [4], there exists 
C
>
0
 such as

ğ”¼
â€‹
[
sup
Î¸
âˆˆ
Î˜
â€–
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
,
X
t
)
âˆ’
âˆ‚
Î¸
j
Î³
^
â€‹
(
Î¸
,
X
t
)
â€–
r
/
3
]
â‰¤
C
â€‹
(
âˆ‘
k
â‰¥
t
Î±
k
â€‹
(
f
Î¸
,
Î˜
)
+
Î±
k
â€‹
(
M
Î¸
,
Î˜
)
+
Î±
k
â€‹
(
âˆ‚
f
Î¸
,
Î˜
)
+
Î±
k
â€‹
(
âˆ‚
M
Î¸
,
Î˜
)
)
r
/
3
.
Therefore,

â€–
(
1
n
â€‹
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
^
m
)
)
j
âˆˆ
m
â€–
r
/
3
â‰¤
C
|
m
|
3
/
2
2
â€‹
n
âˆ‘
t
=
1
n
âˆ‘
k
â‰¥
t
(
Î±
k
(
f
Î¸
,
Î˜
)
+
Î±
k
(
M
Î¸
,
Î˜
)
+
Î±
k
(
âˆ‚
f
Î¸
,
Î˜
)
+
Î±
k
(
âˆ‚
M
Î¸
,
Î˜
)
)
â‰¤
C
â€²
n
â€‹
âˆ‘
t
=
1
n
âˆ‘
j
â‰¥
t
j
âˆ’
Î´
â‰¤
C
â€²â€²
n
â€‹
âˆ‘
t
=
1
n
t
1
âˆ’
Î´
â‰¤
C
â€²â€²â€²
,
with 
C
â€²
>
0
, 
C
â€²â€²
>
0
 and 
C
â€²â€²â€²
>
0
 and where the last inequality holds since 
Î´
>
7
/
2
 under Assumption A5. âˆ

Lemma 6. Under Assumptions A0-A5, for any 
m
âˆˆ
â„³
 and any 
Î¸
âˆˆ
Î˜
m
,
â€–
(
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
,
X
0
)
)
j
âˆˆ
m
â€–
r
/
3
<
âˆ
.
(8.11)
Proof.For 
j
âˆˆ
m
, we have for any 
Î¸
âˆˆ
Î˜
m
,
âˆ‚
Î¸
j
Î³
â€‹
(
Î¸
,
X
0
)
=
âˆ’
2
â€‹
(
M
Î¸
0
)
âˆ’
2
â€‹
(
X
0
âˆ’
f
Î¸
0
)
â€‹
âˆ‚
Î¸
j
f
Î¸
0
âˆ’
2
â€‹
(
M
Î¸
0
)
âˆ’
3
â€‹
(
X
0
âˆ’
f
Î¸
0
)
2
â€‹
âˆ‚
Î¸
j
M
Î¸
0
+
2
â€‹
(
M
Î¸
0
)
âˆ’
1
â€‹
âˆ‚
Î¸
j
M
Î¸
0
.
Therefore, with Assumption A3 and Minkowski Inequality,

âˆ¥
(
âˆ‚
Î¸
j
Î³
(
Î¸
,
X
0
)
)
j
âˆˆ
m
âˆ¥
r
/
3
â‰¤
2
h
Â¯
3
/
2
(
h
Â¯
1
/
2
âˆ¥
(
âˆ‚
Î¸
j
f
Î¸
0
)
,
(
X
0
âˆ’
f
Î¸
0
)
âˆ¥
r
/
3
+
âˆ¥
(
âˆ‚
Î¸
j
M
Î¸
0
)
|
X
0
âˆ’
f
Î¸
0
|
2
âˆ¥
r
/
3
+
h
Â¯
âˆ¥
(
âˆ‚
Î¸
p
M
Î¸
0
)
âˆ¥
r
/
3
)
.
Now, applying the HÃ¶lder Inequality, we obtain that there exists 
C
>
0
 such that for any 
Î¸
âˆˆ
Î˜
m
,

ğ”¼
[
|
âˆ‚
Î¸
p
Î³
(
Î¸
âˆ—
,
X
0
)
|
r
/
3
]
â‰¤
C
(
âˆ¥
âˆ‚
Î¸
j
f
Î¸
0
âˆ¥
2
â€‹
r
/
3
âˆ¥
X
0
âˆ’
f
Î¸
0
âˆ¥
2
â€‹
r
/
3
+
âˆ¥
âˆ‚
Î¸
j
M
Î¸
0
âˆ¥
r
âˆ¥
X
0
âˆ’
f
Î¸
0
âˆ¥
r
2
+
âˆ¥
âˆ‚
Î¸
p
M
Î¸
0
âˆ¥
r
/
3
)
.
(8.12)
Using Assumption A0 and A5 and the proof of Lemma 1 in [5], all the right side terms in (8.12) are finite for any 
Î¸
âˆˆ
Î˜
m
 and this achieves the proof. âˆ

Then Proposition 1 can be established:

Proof of Proposition 1.From (8.5) and (8.6) with 
F
â€‹
(
Î¸
m
âˆ—
)
 the positive definite matrix defined in (3.4), we know that for 
n
 large enough,
â€–
n
â€‹
(
(
Î¸
^
m
)
i
âˆ’
(
Î¸
m
âˆ—
)
i
)
i
âˆˆ
m
â€–
r
â€²
=
â€–
(
(
1
n
â€‹
âˆ‚
Î¸
i
â€‹
Î¸
j
2
L
n
â€‹
(
Î¸
Â¯
m
)
)
i
,
j
âˆˆ
m
)
âˆ’
1
Ã—
1
n
â€‹
(
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
^
m
)
âˆ’
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
m
âˆ—
)
)
j
âˆˆ
m
â€–
r
â€²
.
(8.13)
Therefore, using HÃ¶lder and Minkowski inequalities, we obtain:

â€–
n
â€‹
(
(
Î¸
^
m
)
i
âˆ’
(
Î¸
m
âˆ—
)
i
)
i
âˆˆ
m
â€–
r
â€²
â‰¤
â€–
(
(
1
n
â€‹
âˆ‚
Î¸
i
â€‹
Î¸
j
2
L
n
â€‹
(
Î¸
Â¯
m
)
)
i
,
j
âˆˆ
m
)
âˆ’
1
â€–
r
â€‹
r
â€²
r
âˆ’
3
â€‹
r
â€²
Ã—
â€–
1
n
â€‹
(
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
^
m
)
âˆ’
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
m
âˆ—
)
)
j
âˆˆ
m
â€–
r
3
â‰¤
â€–
(
(
1
n
â€‹
âˆ‚
Î¸
i
â€‹
Î¸
j
2
L
n
â€‹
(
Î¸
Â¯
m
)
)
i
,
j
âˆˆ
m
)
âˆ’
1
â€–
r
â€‹
r
â€²
r
âˆ’
3
â€‹
r
â€²
Ã—
(
â€–
1
n
â€‹
(
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
^
m
)
)
j
âˆˆ
m
â€–
r
3
+
â€–
1
n
â€‹
(
âˆ‚
Î¸
j
L
n
â€‹
(
Î¸
m
âˆ—
)
)
j
âˆˆ
m
â€–
r
3
)
.
Now using Assumption A4, Lemmas 4 and 5, we deduce (3.9). âˆ

8.2Proofs of Section 4
Proof of Lemma 1.1. From the assumptions, the function 
R
:
Î¸
âˆˆ
Î˜
â†¦
R
â€‹
(
Î¸
)
 is a 
ğ’
2
â€‹
(
Î˜
)
 function and the Hessian matrix 
âˆ‚
Î¸
2
R
=
âˆ’
2
â€‹
F
 is a definite positive matrix (see (3.4)). Therefore, from a Taylor-Lagrange expansion:
n
â€‹
(
R
â€‹
(
Î¸
^
m
)
âˆ’
R
â€‹
(
Î¸
m
âˆ—
)
)
=
n
(
R
(
Î¸
m
âˆ—
)
+
(
Î¸
^
m
âˆ’
Î¸
m
âˆ—
)
âŠ¤
âˆ‚
Î¸
R
(
Î¸
m
âˆ—
)
(8.14)
+
1
2
(
Î¸
^
m
âˆ’
Î¸
m
âˆ—
)
âŠ¤
âˆ‚
Î¸
2
R
(
Î¸
Â¯
)
(
Î¸
^
m
âˆ’
Î¸
m
âˆ—
)
âˆ’
R
(
Î¸
m
âˆ—
)
)
=
1
2
â€‹
(
n
â€‹
(
Î¸
^
m
âˆ’
Î¸
m
âˆ—
)
)
âŠ¤
â€‹
âˆ‚
Î¸
2
R
â€‹
(
Î¸
Â¯
)
â€‹
(
n
â€‹
(
Î¸
^
m
âˆ’
Î¸
m
âˆ—
)
)
,
with 
Î¸
Â¯
=
Î¸
m
âˆ—
+
c
â€‹
(
Î¸
^
m
âˆ’
Î¸
m
âˆ—
)
âˆˆ
Î˜
m
 since 
c
âˆˆ
[
0
,
1
]
. Using Lemma 4 of [5] and continuous mapping Theorem, we deduce that:

âˆ‚
Î¸
2
R
â€‹
(
Î¸
Â¯
)
=
âˆ’
2
â€‹
F
â€‹
(
Î¸
Â¯
)
â€‹
âŸ¶
ğ’«
n
â†’
âˆ
âˆ’
2
â€‹
F
â€‹
(
Î¸
m
âˆ—
)
and
G
â€‹
(
Î¸
Â¯
)
â€‹
âŸ¶
ğ’«
n
â†’
âˆ
â€‹
G
â€‹
(
Î¸
m
âˆ—
)
.
(8.15)
Moreover, using the asymptotic normality of 
Î¸
^
m
 established in [5] and [4], we have:

n
â€‹
(
(
Î¸
^
m
)
i
âˆ’
(
Î¸
m
âˆ—
)
i
)
i
âˆˆ
m
â€‹
âŸ¶
ğ’Ÿ
n
â†’
âˆ
â€‹
ğ’©
â€‹
(
0
,
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
â€‹
G
m
â€‹
(
Î¸
m
âˆ—
)
â€‹
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
)
.
(8.16)
As a consequence, with 
Z
n
=
(
G
m
â€‹
(
Î¸
Â¯
)
)
âˆ’
1
/
2
â€‹
F
m
â€‹
(
Î¸
Â¯
)
â€‹
n
â€‹
(
(
Î¸
^
m
)
i
âˆ’
(
Î¸
m
âˆ—
)
i
)
i
âˆˆ
m
â€‹
âŸ¶
ğ’Ÿ
n
â†’
âˆ
â€‹
ğ’©
â€‹
(
0
,
I
|
m
|
)
 and from (8.14), we have

n
â€‹
(
R
â€‹
(
Î¸
^
m
)
âˆ’
R
â€‹
(
Î¸
m
âˆ—
)
)
=
âˆ’
Z
n
âŠ¤
â€‹
(
G
m
â€‹
(
Î¸
Â¯
)
)
1
/
2
â€‹
(
F
m
â€‹
(
Î¸
Â¯
)
)
âˆ’
1
â€‹
F
m
â€‹
(
Î¸
Â¯
)
â€‹
(
F
m
â€‹
(
Î¸
Â¯
)
)
âˆ’
1
â€‹
(
G
m
â€‹
(
Î¸
Â¯
)
)
1
/
2
â€‹
Z
n
=
âˆ’
Z
n
âŠ¤
â€‹
(
G
m
â€‹
(
Î¸
Â¯
)
)
1
/
2
â€‹
(
F
m
â€‹
(
Î¸
Â¯
)
)
âˆ’
1
â€‹
(
G
m
â€‹
(
Î¸
Â¯
)
)
1
/
2
â€‹
Z
n
.
Define 
U
âˆ—
â€‹
(
m
)
:=
âˆ’
Z
âŠ¤
â€‹
(
G
m
â€‹
(
Î¸
m
âˆ—
)
)
1
/
2
â€‹
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
â€‹
(
G
m
â€‹
(
Î¸
m
âˆ—
)
)
1
/
2
â€‹
Z
 where 
Z
â€‹
âˆ¼
ğ’Ÿ
â€‹
ğ’©
â€‹
(
0
,
I
|
m
|
)
. Then using (8.15) we obtain

n
â€‹
(
R
â€‹
(
Î¸
^
m
)
âˆ’
R
â€‹
(
Î¸
m
âˆ—
)
)
â€‹
âŸ¶
ğ’Ÿ
n
â†’
âˆ
â€‹
U
âˆ—
â€‹
(
m
)
.
The computation of the expectation of 
U
m
âˆ—
 follows from

ğ”¼
â€‹
[
U
m
âˆ—
]
=
ğ”¼
â€‹
[
Trace
â€‹
(
U
m
âˆ—
)
]
=
âˆ’
Trace
â€‹
(
(
G
m
â€‹
(
Î¸
m
âˆ—
)
)
1
/
2
â€‹
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
â€‹
(
G
m
â€‹
(
Î¸
m
âˆ—
)
)
1
/
2
)
=
âˆ’
Trace
â€‹
(
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
â€‹
G
m
â€‹
(
Î¸
m
âˆ—
)
)
.
Finally, for establishing 
ğ”¼
â€‹
[
n
â€‹
(
R
â€‹
(
Î¸
^
m
)
âˆ’
R
â€‹
(
Î¸
m
âˆ—
)
)
]
â€‹
âŸ¶
n
â†’
âˆ
â€‹
ğ”¼
â€‹
[
U
m
âˆ—
]
, we have to prove that there exists 
n
0
âˆˆ
â„•
 such as

sup
n
â‰¥
n
0
ğ”¼
â€‹
[
n
â€‹
|
R
â€‹
(
Î¸
^
m
)
âˆ’
R
â€‹
(
Î¸
m
âˆ—
)
|
]
<
âˆ
.
(8.17)
Indeed, from (8.14), we have:

n
â€‹
|
R
â€‹
(
Î¸
^
m
)
âˆ’
R
â€‹
(
Î¸
m
âˆ—
)
|
â‰¤
1
2
â€‹
sup
Î¸
âˆˆ
Î˜
â€–
âˆ‚
Î¸
2
2
R
â€‹
(
Î¸
)
â€–
â€‹
â€–
n
â€‹
(
Î¸
^
m
âˆ’
Î¸
m
âˆ—
)
â€–
2
âŸ¹
ğ”¼
â€‹
[
n
â€‹
|
R
â€‹
(
Î¸
^
m
)
âˆ’
R
â€‹
(
Î¸
m
âˆ—
)
|
]
â‰¤
Î»
max
2
â€‹
ğ”¼
â€‹
[
â€–
n
â€‹
(
Î¸
^
m
âˆ’
Î¸
m
âˆ—
)
â€–
2
]
,
(8.18)
since there exists 
Î»
max
<
âˆ
 such as 
â€–
âˆ‚
Î¸
2
R
â€‹
(
Î¸
)
â€–
â‰¤
Î»
max
 for any 
Î¸
âˆˆ
Î˜
 from Assumption A5 where 
Î˜
 is a compact set.
Using Proposition 1, we know that

sup
n
âˆˆ
â„•
âˆ—
â€–
n
â€‹
(
Î¸
^
m
âˆ’
Î¸
m
âˆ—
)
â€–
2
<
âˆ
.
Finally using (8.18), we deduce (8.17).
 
2. As in the proof of 1., we use a Taylor-Lagrange expansion of 
Î³
^
n
â€‹
(
Î¸
m
âˆ—
)
 around 
Î¸
^
m
 since 
âˆ‚
Î¸
Î³
^
n
â€‹
(
Î¸
^
m
)
=
0
. Then,

n
â€‹
(
Î³
^
n
â€‹
(
Î¸
m
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
^
m
)
)
=
1
2
â€‹
n
â€‹
(
Î¸
^
m
âˆ’
Î¸
m
âˆ—
)
âŠ¤
â€‹
(
âˆ‚
Î¸
2
2
Î³
^
n
â€‹
(
Î¸
Â¯
m
)
)
â€‹
n
â€‹
(
Î¸
^
m
âˆ’
Î¸
m
âˆ—
)
.
But using 
Î¸
^
m
â€‹
âŸ¶
a
.
s
.
n
â†’
+
âˆ
â€‹
Î¸
m
âˆ—
 and 
ğ”¼
â€‹
[
â€–
1
n
â€‹
L
n
â€‹
(
Î¸
)
âˆ’
1
n
â€‹
L
^
n
â€‹
(
Î¸
)
â€–
Î˜
]
â€‹
âŸ¶
n
â†’
âˆ
â€‹
0
, we have

(
âˆ‚
Î¸
2
2
Î³
^
n
â€‹
(
Î¸
Â¯
m
)
)
â€‹
âŸ¶
ğ’«
n
â†’
âˆ
âˆ’
2
â€‹
F
â€‹
(
Î¸
m
âˆ—
)
.
Therefore, using the same reasoning as in 1., we deduce that

n
â€‹
(
Î³
^
n
â€‹
(
Î¸
m
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
^
m
)
)
â€‹
âŸ¶
ğ’Ÿ
n
â†’
âˆ
â€‹
U
âˆ—
â€‹
(
m
)
.
With HÃ¶lder Inequality and using 
8
/
3
<
r
â€²
 defined in Proposition 1, we obtain for 
n
 large enough

ğ”¼
â€‹
[
n
â€‹
(
Î³
^
n
â€‹
(
Î¸
m
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
^
m
)
)
]
â‰¤
â€–
(
âˆ‚
Î¸
2
2
Î³
^
n
â€‹
(
Î¸
Â¯
m
)
)
1
/
2
â€‹
n
â€‹
(
Î¸
^
m
âˆ’
Î¸
m
âˆ—
)
â€–
2
2
(8.19)
â‰¤
â€–
(
âˆ‚
Î¸
2
2
Î³
^
n
â€‹
(
Î¸
Â¯
m
)
)
â€–
r
â€²
r
â€²
âˆ’
2
â€‹
â€–
n
â€‹
(
Î¸
^
m
âˆ’
Î¸
m
âˆ—
)
â€–
r
â€²
2
.
Finally, with Proposition 1 and Lemma 4 of [5], we have 
sup
n
âˆˆ
â„•
âˆ—
ğ”¼
â€‹
[
â€–
âˆ‚
Î¸
2
2
Î³
^
n
â€‹
(
Î¸
Â¯
m
)
â€–
Î˜
4
]
<
âˆ
 and 
r
â€²
r
â€²
âˆ’
2
â‰¤
4
 since 
r
â€²
>
8
/
3
, and therefore

sup
n
âˆˆ
â„•
âˆ—
ğ”¼
â€‹
[
n
â€‹
(
Î³
^
n
â€‹
(
Î¸
m
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
^
m
)
)
]
<
âˆ
,
which concludes the proof. âˆ

Proof of Proposition 2.For 
m
âˆ—
âŠ‚
m
 we have 
Î¸
m
âˆ—
=
Î¸
âˆ—
 and therefore 
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
=
R
â€‹
(
Î¸
^
m
)
âˆ’
R
â€‹
(
Î¸
âˆ—
)
 and using (4.7), we have
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
]
=
1
n
â€‹
ğ”¼
â€‹
[
I
1
â€‹
(
m
)
]
â€‹
âˆ¼
n
â†’
âˆ
âˆ’
1
n
â€‹
Trace
â€‹
(
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
â€‹
G
m
â€‹
(
Î¸
m
âˆ—
)
)
.
But the matrix 
G
â€‹
(
Î¸
m
âˆ—
)
 and 
âˆ’
F
â€‹
(
Î¸
m
âˆ—
)
 are positive definite function from Assumption A2. Thus if 
m
âˆ—
âŠ‚
m
 and 
m
â‰ 
m
âˆ—
, then

âˆ’
Trace
â€‹
(
(
F
m
âˆ—
â€‹
(
Î¸
âˆ—
)
)
âˆ’
1
â€‹
G
m
âˆ—
â€‹
(
Î¸
âˆ—
)
)
<
âˆ’
Trace
â€‹
(
(
F
m
â€‹
(
Î¸
âˆ—
)
)
âˆ’
1
â€‹
G
m
â€‹
(
Î¸
âˆ—
)
)
since all the eigenvalues of 
âˆ’
(
F
â€‹
(
Î¸
âˆ—
)
)
âˆ’
1
â€‹
G
â€‹
(
Î¸
âˆ—
)
 are positive. This implies 
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
âˆ—
,
Î¸
âˆ—
)
]
<
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
]
 for 
n
 large enough.
If 
m
âˆ—
âŠ„
m
, then:

â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
=
(
R
â€‹
(
Î¸
^
m
)
âˆ’
R
â€‹
(
Î¸
m
âˆ—
)
)
+
(
R
â€‹
(
Î¸
m
âˆ—
)
âˆ’
R
â€‹
(
Î¸
âˆ—
)
)
.
Using (4.7), we also have

ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
,
Î¸
m
âˆ—
)
]
=
1
n
â€‹
ğ”¼
â€‹
[
I
1
â€‹
(
m
)
]
â€‹
âˆ¼
n
â†’
âˆ
âˆ’
1
n
â€‹
Trace
â€‹
(
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
â€‹
G
m
â€‹
(
Î¸
m
âˆ—
)
)
.
But as it was established in [4] that 
(
R
â€‹
(
Î¸
m
âˆ—
)
âˆ’
R
â€‹
(
Î¸
âˆ—
)
)
=
2
â€‹
D
â€‹
K
L
â€‹
(
Î¸
âˆ—
âˆ¥
Î¸
m
âˆ—
)
>
0
 since 
m
âŠ„
m
âˆ—
. Therefore, 
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
âˆ—
,
Î¸
âˆ—
)
]
=
o
â€‹
(
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
]
)
 for any 
m
 such as 
m
âˆ—
âŠ„
m
.

âˆ

Proof of Proposition 3.The proof of this proposition can be deduced from
ğ”¼
â€‹
[
n
â€‹
I
3
â€‹
(
m
)
]
=
ğ”¼
â€‹
[
n
â€‹
(
R
â€‹
(
Î¸
m
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
m
âˆ—
)
)
]
=
v
n
âˆ—
(8.20)
for any 
m
âˆˆ
â„³
. For establishing (8.20), we begin by

I
3
â€‹
(
m
)
=
(
R
â€‹
(
Î¸
m
âˆ—
)
âˆ’
Î³
n
â€‹
(
Î¸
m
âˆ—
)
)
+
(
Î³
n
â€‹
(
Î¸
m
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
m
âˆ—
)
)
:=
I
31
â€‹
(
m
)
+
I
32
â€‹
(
m
)
.
(8.21)
Firstly, since 
ğ”¼
â€‹
[
Î³
â€‹
(
Î¸
m
âˆ—
,
X
0
)
]
=
R
â€‹
(
Î¸
m
âˆ—
)
 and 
(
X
t
)
t
âˆˆ
â„¤
 is a stationary times series, then for any 
n
âˆˆ
â„•
âˆ—
,

ğ”¼
â€‹
[
Î³
n
â€‹
(
Î¸
m
âˆ—
)
]
=
1
n
â€‹
âˆ‘
t
=
1
n
ğ”¼
â€‹
[
Î³
â€‹
(
Î¸
m
âˆ—
,
X
t
)
]
=
R
â€‹
(
Î¸
m
âˆ—
)
âŸ¹
ğ”¼
â€‹
[
I
31
â€‹
(
m
)
]
=
0
.
(8.22)
Secondly, from Assumption A0 and [5], there exists 
C
>
0
 such that for any 
t
â‰¥
1

ğ”¼
â€‹
[
â€–
Î³
â€‹
(
Î¸
,
X
t
)
âˆ’
Î³
^
â€‹
(
Î¸
,
X
t
)
â€–
Î˜
]
â‰¤
C
â€‹
âˆ‘
s
â‰¥
t
(
Î±
s
â€‹
(
f
Î¸
,
Î˜
)
+
Î±
s
â€‹
(
M
Î¸
,
Î˜
)
)
.
Therefore, there exist 
C
>
0
 and 
C
â€²
>
0
 such that for any 
m
âˆˆ
â„³
,

ğ”¼
â€‹
[
â€–
Î³
n
â€‹
(
Î¸
m
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
m
âˆ—
)
â€–
Î˜
]
â‰¤
C
n
â€‹
âˆ‘
t
=
1
n
âˆ‘
s
â‰¥
t
(
Î±
s
â€‹
(
f
Î¸
,
Î˜
)
+
Î±
s
â€‹
(
M
Î¸
,
Î˜
)
)
(8.23)
â‰¤
C
n
â€‹
âˆ‘
t
=
1
n
t
1
âˆ’
Î´
â‰¤
C
â€²
n
,
since 
Î´
>
7
/
2
 from Assumption A5. Moreover, for any 
m
âˆˆ
â„³
 such as 
m
âˆ—
âŠ‚
m
 (overfitting setting), we have 
Î³
n
â€‹
(
Î¸
m
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
m
âˆ—
)
=
Î³
n
â€‹
(
Î¸
m
âˆ—
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
m
âˆ—
âˆ—
)
. Using this and (8.23) we deduce that for any 
m
âˆˆ
â„³
, there exists a bounded sequence 
(
v
n
âˆ—
)
n
âˆˆ
â„•
âˆ—
 not depending on 
m
 when 
m
âˆ—
âŠ‚
m
 satisfying

ğ”¼
â€‹
[
I
32
â€‹
(
m
)
]
=
v
n
âˆ—
n
.
(8.24)
Using also Lemma 1, this implies the asymptotic behavior of 
ğ”¼
â€‹
[
pen
i
â€‹
d
â€‹
(
m
)
]
. âˆ

Now we establish a preliminary lemma that is an important step towards the proof of Theorem 4.2.

Lemma 7. Let 
pen
:
m
âˆˆ
â„³
n
â†¦
pen
â€‹
(
m
)
âˆˆ
â„
+
. Then,
â„“
â€‹
(
Î¸
^
m
^
pen
,
Î¸
âˆ—
)
â‰¤
min
m
âˆˆ
â„³
â€‹
{
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
}
+
(
pen
â€‹
(
m
^
i
â€‹
d
)
âˆ’
pen
â€‹
(
m
^
pen
)
)
âˆ’
(
pen
i
â€‹
d
â€‹
(
m
^
i
â€‹
d
)
âˆ’
pen
i
â€‹
d
â€‹
(
m
^
pen
)
)
.
(8.25)
Proof.By definition, for any 
m
âˆˆ
â„³
,
C
^
pen
i
â€‹
d
â€‹
(
m
)
=
R
â€‹
(
Î¸
^
m
)
=
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
+
R
â€‹
(
Î¸
âˆ—
)
.
(8.26)
As a consequence,

min
m
âˆˆ
â„³
â€‹
{
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
}
=
â„“
â€‹
(
Î¸
^
m
^
i
â€‹
d
,
Î¸
âˆ—
)
=
min
m
âˆˆ
â„³
â€‹
{
C
^
pen
i
â€‹
d
â€‹
(
m
)
}
âˆ’
R
â€‹
(
Î¸
âˆ—
)
.
(8.27)
For any 
m
âˆˆ
â„³
, we also have

C
^
pen
â€‹
(
m
)
=
C
^
pen
i
â€‹
d
â€‹
(
m
)
+
pen
â€‹
(
m
)
âˆ’
pen
i
â€‹
d
â€‹
(
m
)
.
By definition of 
m
^
pen
, we have 
C
^
pen
â€‹
(
m
^
pen
)
â‰¤
C
^
pen
â€‹
(
m
^
i
â€‹
d
)
. Therefore,

C
^
pen
â€‹
(
m
^
pen
)
â‰¤
C
^
pen
i
â€‹
d
â€‹
(
m
^
i
â€‹
d
)
+
pen
â€‹
(
m
^
i
â€‹
d
)
âˆ’
pen
i
â€‹
d
â€‹
(
m
^
i
â€‹
d
)
C
^
pen
i
â€‹
d
â€‹
(
m
^
pen
)
+
pen
â€‹
(
m
^
pen
)
âˆ’
pen
i
â€‹
d
â€‹
(
m
^
pen
)
â‰¤
C
^
pen
i
â€‹
d
â€‹
(
m
^
i
â€‹
d
)
+
pen
â€‹
(
m
^
i
â€‹
d
)
âˆ’
pen
i
â€‹
d
â€‹
(
m
^
i
â€‹
d
)
.
By replacing 
C
^
pen
i
â€‹
d
â€‹
(
m
)
 by 
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
+
R
â€‹
(
Î¸
âˆ—
)
 following (8.26) and using (8.27), then (8.25) is established. âˆ

Proof of Theorem 4.1.Let 
â„³
âˆ—
=
{
m
âˆˆ
â„³
,
m
âˆ—
âŠ‚
m
}
 and 
â„³
â€²
=
â„³
âˆ–
â„³
âˆ—
. Let 
m
âˆˆ
â„³
â€²
. We have:
â„™
â€‹
(
m
^
pen
=
m
)
â‰¤
â„™
â€‹
(
C
^
pen
â€‹
(
m
)
â‰¤
C
^
pen
â€‹
(
m
âˆ—
)
)
â‰¤
â„™
â€‹
{
Î³
^
n
â€‹
(
Î¸
^
m
)
âˆ’
Î³
^
n
â€‹
(
Î¸
^
m
âˆ—
)
â‰¤
pen
â€‹
(
m
âˆ—
)
âˆ’
pen
â€‹
(
m
)
}
â‰¤
â„™
{
n
(
Î³
^
n
(
Î¸
^
m
)
âˆ’
Î³
^
n
(
Î¸
m
âˆ—
)
)
+
n
(
Î³
^
n
(
Î¸
m
âˆ—
)
âˆ’
R
(
Î¸
m
âˆ—
)
)
+
n
(
R
(
Î¸
âˆ—
)
âˆ’
Î³
^
n
(
Î¸
âˆ—
)
)
+
n
(
Î³
^
n
(
Î¸
âˆ—
)
âˆ’
Î³
^
n
(
Î¸
^
m
âˆ—
)
)
â‰¤
n
(
R
(
Î¸
âˆ—
)
âˆ’
R
(
Î¸
m
âˆ—
)
)
+
n
(
pen
(
m
âˆ—
)
âˆ’
pen
(
m
)
)
}
â‰¤
â„™
â€‹
{
Z
1
+
Z
2
+
Z
3
+
Z
4
+
Z
â€‹
5
â‰¤
âˆ’
2
â€‹
n
â€‹
D
â€‹
K
L
â€‹
(
Î¸
âˆ—
âˆ¥
Î¸
m
âˆ—
)
}
with 
Z
5
=
n
â€‹
(
pen
â€‹
(
m
)
âˆ’
pen
â€‹
(
m
âˆ—
)
)
 and with 
R
â€‹
(
Î¸
âˆ—
)
âˆ’
R
â€‹
(
Î¸
m
âˆ—
)
=
âˆ’
2
â€‹
D
â€‹
K
L
â€‹
(
Î¸
âˆ—
âˆ¥
Î¸
m
âˆ—
)
<
0
 since 
m
âŠ„
m
âˆ—
 from [4]. Now, using 
â„™
â€‹
(
Z
1
+
â‹¯
+
Z
5
â‰¤
c
)
â‰¤
â„™
â€‹
(
Z
1
â‰¤
c
/
5
)
+
â‹¯
+
â„™
â€‹
(
Z
5
â‰¤
c
/
5
)
 for any random variables 
Z
i
 and real number 
c
, we obtain:

â„™
â€‹
(
m
^
pen
=
m
)
â‰¤
âˆ‘
i
=
1
5
â„™
â€‹
(
Z
i
â‰¤
c
n
)
,
(8.28)
where 
c
n
=
âˆ’
2
5
â€‹
n
â€‹
D
â€‹
K
L
â€‹
(
Î¸
âˆ—
âˆ¥
Î¸
m
âˆ—
)
.
Let 
Z
1
:=
n
â€‹
(
Î³
^
n
â€‹
(
Î¸
^
m
)
âˆ’
Î³
^
n
â€‹
(
Î¸
m
âˆ—
)
)
. Following the same computations than in (8.19), with 
8
/
3
<
r
â€²
â‰¤
r
/
3
 and 
r
â€²
<
2
â€‹
(
Î´
âˆ’
1
)
 defined in Proposition 1, and HÃ¶lder Inequality,

ğ”¼
â€‹
[
|
Z
1
|
3
â€‹
r
â€²
8
]
â‰¤
â€–
(
âˆ‚
Î¸
2
2
Î³
^
n
â€‹
(
Î¸
Â¯
m
)
)
1
/
2
â€‹
n
â€‹
(
Î¸
^
m
âˆ’
Î¸
m
âˆ—
)
â€–
3
â€‹
r
â€²
4
3
â€‹
r
â€²
4
â‰¤
â€–
(
âˆ‚
Î¸
2
2
Î³
^
n
â€‹
(
Î¸
Â¯
m
)
)
â€–
3
â€‹
r
â€²
2
â€‹
â€–
n
â€‹
(
Î¸
^
m
âˆ’
Î¸
m
âˆ—
)
â€–
r
â€²
3
â€‹
r
â€²
4
.
Therefore, using Proposition 1,

â„™
â€‹
(
Z
1
â‰¤
c
n
)
â‰¤
â„™
â€‹
(
|
Z
1
|
3
â€‹
r
â€²
8
â‰¥
(
|
c
n
|
)
3
â€‹
r
â€²
8
)
â‰¤
ğ”¼
â€‹
[
|
Z
1
|
3
â€‹
r
â€²
8
]
â€‹
1
|
c
n
|
3
â€‹
r
â€²
8
âŸ¹
â„™
â€‹
(
Z
1
â‰¤
c
n
)
=
O
â€‹
(
1
n
3
â€‹
r
â€²
8
)
=
o
â€‹
(
1
n
)
,
(8.29)
since 
3
â€‹
r
â€²
/
8
>
1
. The same kind of computations can also be done for 
Z
4
:=
n
â€‹
(
Î³
^
n
â€‹
(
Î¸
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
^
m
âˆ—
)
)
 and we also obtain 
â„™
â€‹
(
Z
4
â‰¤
c
n
)
=
o
â€‹
(
1
n
)
.
 
Consider now 
Z
2
:=
n
â€‹
(
Î³
^
n
â€‹
(
Î¸
m
âˆ—
)
âˆ’
R
â€‹
(
Î¸
m
âˆ—
)
)
. Then,

ğ”¼
â€‹
[
|
Z
2
|
8
/
3
]
â‰¤
2
5
/
3
â€‹
(
ğ”¼
â€‹
[
â€–
L
^
n
â€‹
(
Î¸
)
âˆ’
L
n
â€‹
(
Î¸
)
â€–
Î˜
8
/
3
]
+
n
8
/
3
â€‹
ğ”¼
â€‹
[
|
âˆ‘
k
=
1
n
(
Î³
â€‹
(
Î¸
m
âˆ—
,
X
k
)
âˆ’
R
â€‹
(
Î¸
m
âˆ—
)
)
|
8
/
3
]
)
.
Using [5], we know that 
sup
n
âˆˆ
â„•
âˆ—
E
â€‹
[
â€–
L
^
n
â€‹
(
Î¸
)
âˆ’
L
n
â€‹
(
Î¸
)
â€–
Î˜
8
/
3
]
<
âˆ
 from Assumption A5 and since 
Î´
>
7
/
2
>
2
. Now, consider 
Y
k
:=
Î³
â€‹
(
Î¸
m
âˆ—
,
X
k
)
âˆ’
R
â€‹
(
Î¸
m
âˆ—
)
. Then, 
(
Y
k
)
k
âˆˆ
â„¤
 is a stationary time series, 
Ï„
Y
-weakly dependent because, using the same type of arguments as in the proof of Lemma 3, we have:

Ï„
Y
â€‹
(
s
)
â‰¤
âˆ‘
â„“
=
1
âˆ
(
Î±
â„“
â€‹
(
f
Î¸
,
Î˜
)
+
Î±
â„“
â€‹
(
M
Î¸
,
Î˜
)
)
â€‹
Î»
s
+
1
âˆ’
â„“
,
with 
Î»
 defined in Lemma 2. Therefore, using Assumption A5, we also have 
Ï„
Y
â€‹
(
s
)
=
O
â€‹
(
s
Î´
âˆ’
1
â€‹
log
â¡
(
s
)
)
, with 
Î´
>
7
/
2
. Now, using the same type of arguments as in the proof of Lemma 4,

ğ”¼
â€‹
[
|
âˆ‘
k
=
1
n
Y
k
|
8
/
3
]
â‰¤
C
8
/
3
â€‹
(
M
8
/
3
,
n
+
M
2
,
n
4
/
3
)
,
and 
M
2
,
n
â‰¤
C
â€‹
n
 while 
M
8
/
3
,
n
â‰¤
C
â€‹
n
â€‹
âˆ‘
i
=
1
n
i
8
/
3
âˆ’
1
âˆ’
Î´
â€‹
log
â¡
(
i
)
=
o
â€‹
(
n
4
/
3
)
. Therefore, there exists 
C
>
0
 such that for any 
n
âˆˆ
â„•
âˆ—
,

ğ”¼
â€‹
[
|
âˆ‘
k
=
1
n
Y
k
|
8
/
3
]
â‰¤
C
â€‹
n
4
/
3
.
Finally, we deduce that there exists 
C
>
0
 such that for any 
n
âˆˆ
â„•
âˆ—
,

ğ”¼
â€‹
[
|
Z
2
|
8
/
3
]
â‰¤
C
â€‹
n
4
/
3
.
(8.30)
This result and Markov Inequality imply,

â„™
â€‹
(
Z
2
â‰¤
c
n
)
â‰¤
â„™
â€‹
(
|
Z
2
|
8
/
3
â‰¥
(
|
c
n
|
)
8
/
3
)
â‰¤
ğ”¼
â€‹
[
|
Z
2
|
8
/
3
]
â€‹
1
|
c
n
|
8
/
3
âŸ¹
â„™
â€‹
(
Z
2
â‰¤
c
n
)
=
O
â€‹
(
n
4
/
3
â€‹
1
|
c
n
|
8
/
3
)
=
O
â€‹
(
1
n
4
/
3
)
,
(8.31)
We obtain the same bound for 
Z
3
:=
n
â€‹
(
R
â€‹
(
Î¸
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
âˆ—
)
)
.
Finally using the assumption (4.17), we have:

n
â€‹
â„™
â€‹
(
Z
5
â‰¤
c
n
)
=
n
â€‹
â„™
â€‹
(
(
pen
â€‹
(
m
)
âˆ’
pen
â€‹
(
m
âˆ—
)
)
â‰¤
âˆ’
2
5
â€‹
D
â€‹
K
L
â€‹
(
Î¸
âˆ—
âˆ¥
Î¸
m
âˆ—
)
)
â‰¤
n
â€‹
â„™
â€‹
(
pen
â€‹
(
m
âˆ—
)
â‰¥
2
5
â€‹
D
â€‹
K
L
â€‹
(
Î¸
âˆ—
âˆ¥
Î¸
m
âˆ—
)
)
â€‹
âŸ¶
n
â†’
âˆ
â€‹
0
.
(8.32)
By this way, (4.18) is established. âˆ

Proof of Theorem 4.2.The proof is mainly based on Lemma 7. From the proof of Lemma 1, we deduce that for any 
m
âˆˆ
â„³
, there exists two positive random variables 
Y
â€‹
(
m
)
 and 
Z
â€‹
(
m
)
 such as 
n
â€‹
|
I
1
â€‹
(
m
)
+
I
2
â€‹
(
m
)
|
â‰¤
Y
â€‹
(
m
)
 and 
n
â€‹
|
I
3
â€‹
(
m
)
|
â‰¤
Z
â€‹
(
m
)
 for any 
n
âˆˆ
â„•
âˆ—
. Moreover, 
Y
â€‹
(
m
)
 and 
Z
â€‹
(
m
)
 have bounded expectations. Therefore, using Markov Inequality, since 
â„³
 is supposed to be a finite family of models, for any 
Îµ
>
0
 there exists 
K
Îµ
â€²
>
0
 such as
lim sup
n
â†’
âˆ
max
m
âˆˆ
â„³
â¡
â„™
â€‹
(
n
â€‹
pen
i
â€‹
d
â€‹
(
m
)
â‰¥
K
Îµ
â€²
)
â‰¤
Îµ
.
Therefore, using this inequality and (4.19), we deduce that for any 
Îµ
>
0
 there exist 
M
Îµ
>
0
 and 
N
Îµ
âˆˆ
â„•
âˆ—
 such that for any 
n
â‰¥
N
Îµ
,

â„™
â€‹
(
n
â€‹
|
(
pen
â€‹
(
m
^
i
â€‹
d
)
âˆ’
pen
â€‹
(
m
^
pen
)
)
âˆ’
(
pen
i
â€‹
d
â€‹
(
m
^
i
â€‹
d
)
âˆ’
pen
i
â€‹
d
â€‹
(
m
^
pen
)
)
|
â‰¤
M
Îµ
)
â‰¥
1
âˆ’
Îµ
.
(8.33)
The proof of (4.20) is now completed from (8.25) of Lemma 7 and (8.33). âˆ

Proof of Theorem 4.3.Using the same tricks than in Lemma 7, we obtain:
â„“
â€‹
(
Î¸
^
m
^
pen
,
Î¸
âˆ—
)
â‰¤
â„“
â€‹
(
Î¸
^
m
âˆ—
,
Î¸
âˆ—
)
+
(
pen
â€‹
(
m
âˆ—
)
âˆ’
pen
â€‹
(
m
^
pen
)
)
âˆ’
(
pen
i
â€‹
d
â€‹
(
m
âˆ—
)
âˆ’
pen
i
â€‹
d
â€‹
(
m
^
pen
)
)
.
(8.34)
Let 
â„³
âˆ—
=
{
m
âˆˆ
â„³
,
m
âˆ—
âŠ‚
m
}
 and 
â„³
â€²
=
â„³
âˆ–
â„³
âˆ—
. Now, for 
m
âˆˆ
â„³
âˆ—
 and 
m
â‰ 
m
âˆ—
, as in the beginning of the proof of Theorem 4.1, we have:

â„™
â€‹
(
m
^
pen
=
m
)
â‰¤
â„™
â€‹
(
C
^
pen
â€‹
(
m
)
â‰¤
C
^
pen
â€‹
(
m
âˆ—
)
)
â‰¤
â„™
â€‹
{
n
â€‹
(
Î³
^
n
â€‹
(
Î¸
^
m
)
âˆ’
Î³
^
n
â€‹
(
Î¸
âˆ—
)
)
+
n
â€‹
(
Î³
^
n
â€‹
(
Î¸
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
^
m
âˆ—
)
)
â‰¤
n
â€‹
(
pen
â€‹
(
m
âˆ—
)
âˆ’
pen
â€‹
(
m
)
)
}
â‰¤
â„™
â€‹
{
n
â€‹
(
Î³
^
n
â€‹
(
Î¸
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
^
m
)
)
â‰¥
ğ”¼
â€‹
[
f
n
â€‹
(
m
)
]
}
+
â„™
â€‹
{
n
â€‹
(
Î³
^
n
â€‹
(
Î¸
^
m
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
âˆ—
)
)
â‰¥
ğ”¼
â€‹
[
f
n
â€‹
(
m
)
]
}
+
â„™
â€‹
{
3
â€‹
(
f
n
â€‹
(
m
)
âˆ’
ğ”¼
â€‹
[
f
n
â€‹
(
m
)
]
)
â‰¥
ğ”¼
â€‹
[
f
n
â€‹
(
m
)
]
}
with 
f
n
â€‹
(
m
)
=
n
3
â€‹
e
n
â€‹
(
m
)
 and 
e
n
â€‹
(
m
)
=
pen
â€‹
(
m
)
âˆ’
pen
â€‹
(
m
âˆ—
)
>
0
 since 
m
âˆ—
âŠ‚
m
 and 
m
â‰ 
m
âˆ—
.
Using exactly the same arguments as in the proof of Theorem 4.1, there exists 
C
1
>
0
 such that for 
n
 large enough,

â„™
â€‹
{
n
â€‹
(
Î³
^
n
â€‹
(
Î¸
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
^
m
)
)
â‰¥
ğ”¼
â€‹
[
f
n
â€‹
(
m
)
]
}
+
â„™
â€‹
{
n
â€‹
(
Î³
^
n
â€‹
(
Î¸
^
m
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
âˆ—
)
)
â‰¥
ğ”¼
â€‹
[
f
n
â€‹
(
m
)
]
}
â‰¤
C
1
ğ”¼
â€‹
[
f
n
â€‹
(
m
)
]
3
â€‹
r
â€²
8
(8.35)
where 
r
â€²
>
8
3
. Moreover, from Markov Inequality we have

â„™
â€‹
{
3
â€‹
(
f
n
â€‹
(
m
)
âˆ’
ğ”¼
â€‹
[
f
n
â€‹
(
m
)
]
)
â‰¥
ğ”¼
â€‹
[
f
n
â€‹
(
m
)
]
}
â‰¤
â„™
â€‹
{
|
f
n
â€‹
(
m
)
âˆ’
ğ”¼
â€‹
[
f
n
â€‹
(
m
)
]
|
â‰¥
1
3
â€‹
ğ”¼
â€‹
[
f
n
â€‹
(
m
)
]
}
â‰¤
3
â€‹
ğ”¼
â€‹
[
|
f
n
â€‹
(
m
)
âˆ’
ğ”¼
â€‹
[
f
n
â€‹
(
m
)
]
|
]
ğ”¼
â€‹
[
f
n
â€‹
(
m
)
]
.
(8.36)
As a consequence, from (8.35) and (8.36), with 
Îº
>
1
, for 
m
âˆˆ
â„³
âˆ—
 and 
m
â‰ 
m
âˆ—
 and 
n
 large enough

â„™
â€‹
(
m
^
pen
=
m
)
â‰¤
C
1
â€²
(
n
â€‹
ğ”¼
â€‹
[
e
n
â€‹
(
m
)
]
)
Îº
+
3
â€‹
n
â€‹
ğ”¼
â€‹
[
|
e
n
â€‹
(
m
)
âˆ’
ğ”¼
â€‹
[
e
n
â€‹
(
m
)
]
|
]
n
â€‹
ğ”¼
â€‹
[
e
n
â€‹
(
m
)
]
â€‹
âŸ¶
n
â†’
âˆ
â€‹
0
.
(8.37)
Using this result as well as (8.37), one finally obtain (4.22).
Moreover,

ğ”¼
â€‹
[
|
pen
â€‹
(
m
âˆ—
)
âˆ’
pen
â€‹
(
m
^
pen
)
|
]
=
âˆ‘
m
âˆˆ
â„³
âˆ—
ğ”¼
â€‹
[
|
pen
â€‹
(
m
âˆ—
)
âˆ’
pen
â€‹
(
m
^
pen
)
|
|
m
^
pen
=
m
]
â€‹
â„™
â€‹
{
m
^
pen
=
m
}
+
âˆ‘
m
âˆˆ
â„³
â€²
ğ”¼
â€‹
[
|
pen
â€‹
(
m
âˆ—
)
âˆ’
pen
â€‹
(
m
^
pen
)
|
|
m
^
pen
=
m
]
â€‹
â„™
â€‹
{
m
^
pen
=
m
}
=
âˆ‘
m
âˆˆ
â„³
âˆ—
ğ”¼
â€‹
[
|
pen
â€‹
(
m
âˆ—
)
âˆ’
pen
â€‹
(
m
)
|
]
â€‹
â„™
â€‹
{
m
^
pen
=
m
}
+
âˆ‘
m
âˆˆ
â„³
â€²
ğ”¼
â€‹
[
|
pen
â€‹
(
m
âˆ—
)
âˆ’
pen
â€‹
(
m
)
|
]
â€‹
â„™
â€‹
{
m
^
pen
=
m
}
â‰¤
1
n
â€‹
âˆ‘
m
âˆˆ
â„³
âˆ—
(
C
1
â€²
(
n
â€‹
ğ”¼
â€‹
[
e
n
â€‹
(
m
)
]
)
Îº
âˆ’
1
+
n
â€‹
ğ”¼
â€‹
[
|
e
n
â€‹
(
m
)
âˆ’
ğ”¼
â€‹
[
e
n
â€‹
(
m
)
]
|
]
)
+
C
2
â€²
n
â€‹
âˆ‘
m
âˆˆ
â„³
â€²
ğ”¼
â€‹
[
e
n
â€‹
(
m
)
]
,
from (8.37), where 
Îº
>
1
 and assumption (4.17). As a consequence, using the conditions (4.21) of Theorem 4.3,

n
â€‹
ğ”¼
â€‹
[
|
pen
â€‹
(
m
âˆ—
)
âˆ’
pen
â€‹
(
m
^
pen
)
|
]
â€‹
âŸ¶
n
â†’
âˆ
â€‹
0
.
(8.38)
Moreover, using (8.23), there exists 
C
3
>
0
 such as for any 
m
âˆˆ
â„³
,

n
â€‹
ğ”¼
â€‹
[
|
pen
i
â€‹
d
â€‹
(
m
âˆ—
)
âˆ’
pen
i
â€‹
d
â€‹
(
m
)
|
]
â‰¤
C
3
.
Using once again the decomposition on 
â„³
âˆ—
 and 
â„³
â€²
, and 
â„™
â€‹
{
m
^
pen
=
m
}
â€‹
âŸ¶
n
â†’
âˆ
â€‹
0
 for 
m
â‰ 
m
âˆ—
, we deduce

n
â€‹
ğ”¼
â€‹
[
|
pen
i
â€‹
d
â€‹
(
m
âˆ—
)
âˆ’
pen
i
â€‹
d
â€‹
(
m
^
pen
)
|
]
â€‹
âŸ¶
n
â†’
âˆ
â€‹
0
.
(8.39)
Using the limit (8.39) as well as (8.38), we deduce with Markov inequality that

n
â€‹
[
(
pen
â€‹
(
m
âˆ—
)
âˆ’
pen
â€‹
(
m
^
pen
)
)
âˆ’
(
pen
i
â€‹
d
â€‹
(
m
âˆ—
)
âˆ’
pen
i
â€‹
d
â€‹
(
m
^
pen
)
)
]
â€‹
âŸ¶
ğ’«
n
â†’
âˆ
â€‹
0
inducing the proof of (4.23) from (8.34).
Now, using the expectation of (8.34), we also obtain

ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
^
pen
,
Î¸
âˆ—
)
]
â‰¤
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
âˆ—
,
Î¸
âˆ—
)
]
+
ğ”¼
â€‹
[
|
pen
â€‹
(
m
âˆ—
)
âˆ’
pen
â€‹
(
m
^
pen
)
|
]
+
ğ”¼
â€‹
[
|
pen
i
â€‹
d
â€‹
(
m
âˆ—
)
âˆ’
pen
i
â€‹
d
â€‹
(
m
^
pen
)
|
]
.
Now, by using (8.38) and (8.39) as well as Proposition 2, we obtain the proof of (4.24). âˆ

Proof of Theorem 4.4.First we will prove that the probability of overfitting is asymptotically positive, which is
lim inf
n
â†’
âˆ
â„™
â€‹
(
m
^
pen
âˆˆ
â„³
âˆ—
âˆ–
{
m
âˆ—
}
)
>
0
.
(8.40)
Indeed, let 
m
âˆˆ
â„³
âˆ—
âˆ–
{
m
âˆ—
}
. We have:

â„™
â€‹
(
C
^
pen
â€‹
(
m
)
â‰¤
C
^
pen
â€‹
(
m
âˆ—
)
)
=
â„™
â€‹
(
n
â€‹
(
Î³
^
n
â€‹
(
Î¸
^
m
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
âˆ—
)
)
+
n
â€‹
(
Î³
^
n
â€‹
(
Î¸
âˆ—
)
âˆ’
Î³
^
n
â€‹
(
Î¸
^
m
)
)
â‰¥
g
â€‹
(
m
)
âˆ’
g
â€‹
(
m
âˆ—
)
)
=
â„™
â€‹
(
n
â€‹
I
2
â€‹
(
m
)
âˆ’
n
â€‹
I
2
â€‹
(
m
âˆ—
)
â‰¥
g
â€‹
(
m
)
âˆ’
g
â€‹
(
m
âˆ—
)
)
,
using the notations of Lemma 1 and since 
pen
â€‹
(
m
)
=
g
â€‹
(
m
)
/
n
 for any 
m
âˆˆ
â„³
. But using Lemma 1 and Proposition 2, we know that if 
m
âˆˆ
â„³
âˆ—
âˆ–
{
m
âˆ—
}
 then:

n
â€‹
I
2
â€‹
(
m
)
â€‹
âŸ¶
ğ’Ÿ
n
â†’
âˆ
â€‹
U
m
âˆ—
,
n
â€‹
I
2
â€‹
(
m
âˆ—
)
â€‹
âŸ¶
ğ’Ÿ
n
â†’
âˆ
â€‹
U
m
âˆ—
âˆ—
and
ğ”¼
â€‹
[
U
m
âˆ—
âˆ—
]
<
ğ”¼
â€‹
[
U
m
âˆ—
]
Moreover, 
U
m
âˆ—
=
âˆ’
Z
(
m
)
âŠ¤
â€‹
(
G
m
â€‹
(
Î¸
m
âˆ—
)
)
1
/
2
â€‹
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
â€‹
(
G
m
â€‹
(
Î¸
m
âˆ—
)
)
1
/
2
â€‹
Z
(
m
)
 and 
Z
(
m
)
â€‹
âˆ¼
ğ’Ÿ
â€‹
ğ’©
â€‹
(
0
,
I
|
m
|
)
. With a symmetric matrix diagonalization 
(
G
m
â€‹
(
Î¸
m
âˆ—
)
)
1
/
2
â€‹
(
F
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
â€‹
(
G
m
â€‹
(
Î¸
m
âˆ—
)
)
1
/
2
=
âˆ’
P
(
m
)
â€‹
D
(
m
)
â€‹
P
(
m
)
âŠ¤
 where 
D
(
m
)
 is a diagonal matrix with positive diagonal components, leading to

U
m
âˆ—
=
Z
~
(
m
)
âŠ¤
â€‹
D
(
m
)
â€‹
Z
~
(
m
)
and
Z
~
(
m
)
â€‹
âˆ¼
ğ’Ÿ
â€‹
ğ’©
â€‹
(
0
,
I
|
m
|
)
.
Therefore we can write for 
m
âˆˆ
â„³
âˆ—
âˆ–
{
m
âˆ—
}
, 
U
m
âˆ—
=
V
m
âˆ—
âˆ—
+
W
m
âˆ–
m
âˆ—
âˆ—
 with

W
m
âˆ–
m
âˆ—
âˆ—
=
Z
~
(
m
âˆ–
m
âˆ—
)
âŠ¤
â€‹
D
(
m
âˆ–
m
âˆ—
)
â€‹
Z
~
(
m
âˆ–
m
âˆ—
)
and
Z
~
(
m
âˆ–
m
âˆ—
)
â€‹
âˆ¼
ğ’Ÿ
â€‹
ğ’©
â€‹
(
0
,
I
|
m
|
âˆ’
|
m
âˆ—
|
)
and 
V
m
âˆ—
âˆ—
 and 
W
m
âˆ–
m
âˆ—
âˆ—
 are two independent random variables. Moreover, it is clear that 
W
m
âˆ–
m
âˆ—
âˆ—
 behaves as a weighted 
Ï‡
2
â€‹
(
|
m
|
âˆ’
|
m
âˆ—
|
)
 random variable, and therefore for any 
c
>
0
, 
â„™
â€‹
(
W
m
âˆ–
m
âˆ—
âˆ—
>
c
)
>
0
. Using 
n
â€‹
I
2
â€‹
(
m
)
âˆ’
n
â€‹
I
2
â€‹
(
m
âˆ—
)
â€‹
âŸ¶
ğ’Ÿ
n
â†’
âˆ
â€‹
(
V
m
âˆ—
âˆ—
âˆ’
U
m
âˆ—
âˆ—
)
+
W
m
âˆ–
m
âˆ—
âˆ—
 with 
(
V
m
âˆ—
âˆ—
âˆ’
U
m
âˆ—
âˆ—
)
 and 
W
m
âˆ–
m
âˆ—
âˆ—
 independent random variables, we deduce that

â„™
â€‹
(
n
â€‹
I
2
â€‹
(
m
)
âˆ’
n
â€‹
I
2
â€‹
(
m
âˆ—
)
â‰¥
g
â€‹
(
m
)
âˆ’
g
â€‹
(
m
âˆ—
)
)
â€‹
âŸ¶
n
â†’
âˆ
â€‹
p
m
,
m
âˆ—
>
0
,
(8.41)
and this proves (8.40).
 
Now, using previous notations, we have:

ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
^
pen
,
Î¸
âˆ—
)
]
=
âˆ‘
m
âˆˆ
â„³
âˆ—
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
]
â€‹
â„™
â€‹
(
m
^
pen
=
m
)
+
âˆ‘
m
âˆˆ
â„³
â€²
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
]
â€‹
â„™
â€‹
(
m
^
pen
=
m
)
=
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
âˆ—
,
Î¸
âˆ—
)
]
+
âˆ‘
m
âˆˆ
â„³
âˆ—
âˆ–
m
âˆ—
(
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
]
âˆ’
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
âˆ—
,
Î¸
âˆ—
)
]
)
â€‹
â„™
â€‹
(
m
^
pen
=
m
)
+
âˆ‘
m
âˆˆ
â„³
â€²
(
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
]
âˆ’
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
âˆ—
,
Î¸
âˆ—
)
]
)
â€‹
â„™
â€‹
(
m
^
pen
=
m
)
.
Now using Proposition 2, we know that for 
n
 large enough 
ğ”¼
[
â„“
(
Î¸
^
m
,
Î¸
âˆ—
)
]
âˆ’
ğ”¼
[
â„“
(
Î¸
^
m
âˆ—
,
Î¸
âˆ—
)
]
)
â‰¥
0
 for any 
m
âˆˆ
â„³
. Moreover, for 
m
âˆˆ
â„³
âˆ—
âˆ–
m
âˆ—
 and 
n
 large enough,

ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
,
Î¸
âˆ—
)
]
âˆ’
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
âˆ—
,
Î¸
âˆ—
)
]
â‰¥
1
2
â€‹
n
â€‹
(
Trace
â€‹
(
(
âˆ’
F
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
1
â€‹
G
m
â€‹
(
Î¸
m
âˆ—
)
)
âˆ’
Trace
â€‹
(
(
âˆ’
F
m
âˆ—
â€‹
(
Î¸
m
âˆ—
âˆ—
)
)
âˆ’
1
â€‹
G
m
âˆ—
â€‹
(
Î¸
m
âˆ—
âˆ—
)
)
)
â‰¥
1
n
â€‹
K
â€‹
(
m
,
m
âˆ—
)
,
where 
M
â€‹
(
m
,
m
âˆ—
)
>
0
. As a consequence, for 
n
 large enough, with 
p
â€‹
(
m
,
m
âˆ—
)
 defined in (8.41),

ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
^
pen
,
Î¸
âˆ—
)
]
â‰¥
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
âˆ—
,
Î¸
âˆ—
)
]
+
1
2
â€‹
n
â€‹
âˆ‘
m
âˆˆ
â„³
âˆ—
âˆ–
m
âˆ—
K
â€‹
(
m
,
m
âˆ—
)
â€‹
p
â€‹
(
m
,
m
â€²
)
â‰¥
ğ”¼
â€‹
[
â„“
â€‹
(
Î¸
^
m
âˆ—
,
Î¸
âˆ—
)
]
+
M
n
,
with 
M
=
1
2
â€‹
âˆ‘
m
âˆˆ
â„³
âˆ—
âˆ–
m
âˆ—
K
â€‹
(
m
,
m
âˆ—
)
â€‹
p
â€‹
(
m
,
m
â€²
)
>
0
 and this achieves the proof. âˆ

Proof of Theorem 5.1.We first verify conditions (C1) and (C2) of [8] that are sufficient to imply Conditions (i), (ii) and (iii) of [24]. Condition (C1) requires that 
Ïƒ
^
n
 the largest eigenvalue of 
(
âˆ’
(
âˆ‚
Î¸
i
â€‹
Î¸
j
2
L
^
â€‹
(
Î¸
^
m
)
)
i
,
j
âˆˆ
m
)
âˆ’
1
 satisfies 
Ïƒ
^
n
â€‹
âŸ¶
a
.
s
.
n
â†’
+
âˆ
â€‹
0
, which is satisfied since it was already established that 
1
n
â€‹
(
âˆ‚
Î¸
i
â€‹
Î¸
j
2
L
^
â€‹
(
Î¸
^
m
)
)
i
,
j
âˆˆ
m
â€‹
âŸ¶
a
.
s
.
n
â†’
+
âˆ
â€‹
F
m
â€‹
(
Î¸
m
âˆ—
)
 and 
F
m
â€‹
(
Î¸
m
âˆ—
)
 is a negative definite matrix. Moreover, condition (C2) is also satisfied because 
Î¸
m
âˆˆ
Î˜
m
â†¦
(
âˆ‚
Î¸
i
â€‹
Î¸
j
2
L
^
â€‹
(
Î¸
m
)
)
i
,
j
âˆˆ
m
 and 
Î¸
m
âˆˆ
Î˜
m
â†¦
(
(
âˆ‚
Î¸
i
â€‹
Î¸
j
2
L
^
â€‹
(
Î¸
m
)
)
i
,
j
âˆˆ
m
)
âˆ’
1
 are continuous functions for 
n
 large enough. Therefore, using 
h
n
=
âˆ’
1
n
â€‹
L
^
n
, the assumptions of Theorem 1 of [24] are satisfied and this implies that:
âˆ«
Î˜
m
b
m
â€‹
(
Î¸
)
â€‹
exp
â¡
(
L
^
n
â€‹
(
Î¸
)
)
â€‹
ğ‘‘
Î¸
=
exp
â¡
(
L
^
n
â€‹
(
Î¸
^
m
)
)
â€‹
(
2
â€‹
Ï€
)
|
m
|
/
2
Ã—
det
(
n
(
âˆ’
1
n
âˆ‚
Î¸
i
â€‹
Î¸
j
2
L
^
(
Î¸
^
m
)
)
i
,
j
âˆˆ
m
)
âˆ’
1
/
2
(
b
m
(
Î¸
^
m
)
+
O
(
n
âˆ’
1
)
)
a
.
s
.
As a consequence, we have:

S
^
â€‹
(
m
,
X
)
=
âˆ’
log
â¡
(
|
â„³
|
)
+
log
â¡
[
âˆ«
Î˜
m
b
m
â€‹
(
Î¸
)
â€‹
exp
â¡
(
L
^
n
â€‹
(
Î¸
)
)
â€‹
ğ‘‘
Î¸
]
=
L
^
n
â€‹
(
Î¸
^
m
)
âˆ’
log
â¡
(
n
)
2
â€‹
|
m
|
+
log
â¡
(
b
m
â€‹
(
Î¸
^
m
)
)
+
log
â¡
(
2
â€‹
Ï€
)
2
â€‹
|
m
|
âˆ’
1
2
â€‹
log
â¡
(
det
(
âˆ’
F
^
n
â€‹
(
m
)
)
)
âˆ’
log
â¡
(
|
â„³
|
)
+
O
â€‹
(
n
âˆ’
1
)
a
.
s
