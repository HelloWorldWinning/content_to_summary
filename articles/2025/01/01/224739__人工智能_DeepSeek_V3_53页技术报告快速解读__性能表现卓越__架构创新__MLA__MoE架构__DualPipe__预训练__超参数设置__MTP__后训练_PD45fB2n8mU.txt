Timestamp: 2025-01-01T22:47:39.291086
Title: 【人工智能】DeepSeek V3 53页技术报告快速解读  性能表现卓越  架构创新  MLA  MoE架构  DualPipe  预训练  超参数设置  MTP  后训练 PD45fB2n8mU
URL: https://youtube.com/watch?v=PD45fB2n8mU&si=E7kJ_ybRtkCRV_HQ
Status: success
Duration: 12:43

Description:
* **核心要点:**
    1. **性能卓越:** DipSecV3在多项测试中展现出强大的性能，尤其在数据推理方面超越其他模型。
    2. **创新架构:** 采用多头潜在注意力(MRA)、细粒度专家混合(MOE)和无额外损失的负载均衡策略。
    3. **工程优化:** 通过双向流水线并行、通信优化和内存管理等手段提升训练效率。
    4. **训练数据:**  使用了高质量、多样化的大规模训练数据，并针对性地增强了数学和编程相关数据。
    5. **后训练:** 通过监督微调(SFT)和强化学习(RL)使其更好地对齐人类偏好。

* **核心结论:** DipSecV3 通过架构创新、工程优化、高质量数据和精细化后训练，实现了高性能和低成本。
* **根本结论:** DipSecV3 的成功在于它在模型性能、训练效率和对齐人类偏好上都做了深入的优化。


Content:
大家好 这里是最佳拍道 我是大非因为有很多观众在评论区要求所以今天我们来聊一聊最近引起大家广泛关注的一个模型DipSecV3它以出色的性能和相对较低的成本在国际上展露头角网上一些简单的介绍内容其实已经很多了我也不在过多的复数今天主要还是基于官方发布的DipSecV3技术报告我们从性能架构工程御训练和后训练五个方面来给大家深度的抛弃一下DipSecV3是如何练成的首先 让我们大概来了解一下DipSecV3的性能表现在众多全为的测试极上它都展现出了强大的实力比如说在MMRU PROGPQA DEMO的MAS500 AME2024CodeFossessPresentair和 SWEBANCHWAREFY的等测试中涵盖了知识理解逻辑推理 数据能力代码生成以及软件工程能力等多个关键的围度特别是在数据推理方面像MATS500和 AME2024这样的测试它大幅超越了许多其他的模型这表明它在处理复杂数据的问题上有着独特的优势在于其他开源基础模型比如说DipSecV2BASE前纹2.572BBASE和LAMMA3.1405BBASE进行对比的时候DipSecV3BASE的表现也十分亮眼在BBAHMMRU系列DROPGSMBOKAYMASMGSMCMAS等几乎所有任务上它都取得了最佳的成绩而且经过指令微调之后它的性能进一步提升在于GPCEO,CLOD3.5SONGAIT等顶尖模型的酵略中在MMMRU,MMRU Redux,GPQA DiamondCodefossus,AME2024,MAS500等任务上都展现出了不训色甚至更优的性能而且实现这样优秀的性能按照租用H800每GPU小时两美元来计算的话懂成本只用了大约550万美元这在模型训练领域可以说是一个相当惊人的成绩接下来我们来介绍一下Depthegv3在架构方面做出的三项重要的创新第一个是多头潜在注意力MRA它通过将K和Y6连合应试到DV潜在空间项量上有效地降低了KVCAT的大效在Depthegv3中MMRA的KV压缩伪度设置为了512Query的压缩伪度设置为了1536JOK的头伪度设置为64这种独特的设计在保证模型性能的红时大大减少显存占用和计算的开销使得模型在处理长文本的时候更加高效第二个是Depthegv3它采用了细力度专家共享专家和TOPK路由策略实现了模型容量的高效扩展每个MOE层包含了一个共享专家和256个路由专家每个TOP可以选择了8个路由专家最多可以路由到4个节点这种吸收机火的机制使得模型能够在不显住增加计算成本的情况下拥有庞大的模型容量为处理复达任务提供了强大的支持第三个是无额外损耗的复载军红策略Depthegv3通过引入了一个可以动态调整的偏致像来影响路由的策略从而避免了传通损失对于模型性能的负面影响这个偏致像的更新速度在预训练的前14.3T的童跟中设置为了0.01剩于500B个童跟中设置为0.0而序列及平衡损失因子设置为了0.001通过实际数据对比可以发现训练模型分工更为明确也更好的释放了Moe的潜力此外在工程方面Depthegv3也进行了多项的余化在流水线并行上它采用了Dupive策略以传统的单项流水线不同Dupive采用了双项流水线设计同时从流水线的两端来进行MacroBatch这样能够显著减少流水线气泡提高GPU的利用率此外它还将每个MacroBatch进一步划分为更小的Trunk然后对每个Trunk的计算和通信进行精细的调度通过巧妙的边排计算和通信的顺序实现了两者的高度重点从实际的调度势力中我们可以看到在8个PPRUNK和20个MacroBatch的双项流水线调度下流水线气泡可以得到显著的减少GPU利用率得到极大的提升在流水线气泡数量和机会内存开销等方面双项流水线的设计要好于1F1B和ZeroBubble等方法在通信优化方面由于跨解点的Moe讯练存在着巨大的通信开销所以DipSegV3也采取了一系列的措施通过节点限制路由只允许将每个Token最多路由的4个节点从而有效限制了跨解点通信的范围和规模然后通过定制化AllTowel通信内合充分利用了InfiniteBand和NVLink的贷宽并且最大程度的减少用于通信的SM数量此外通过Wap专业化将不同的通信任务分配给不同的Wap并且根据实际的负载情况来动态的调整每个任务的Wap数量从而实现通信任务的精细化管理和优化最后通过自动调整通信块的大小来减少对于L2缓存的依赖从而降低对于其他计算内合的干润进一步提升通信效率在内存管理上DipSegV3也做到了极致在反向传播过程中它会重新计算RMS-NOM和MROA上投影的输出而不是将这些中间结果存储在显存中虽然这会略微增加一些计算量但是能够显住的降低显存的占用另一方面DipSegV3将模型参数的EMA存储在了CPU内存中进行一步更新从而避免了在GPU上存储EMA参数所带来的额外显存开销而在MTP模化中通过将引白顶层和Otip的Hand与主模型进行共享进一步减少了模型的参数量和内存占用在训练精度上DipSegV3采用了IP8混合精度训练不过对于模型中对于精度比较敏感的组件比如说引白顶、Atanshan等等仍然采用了BF16或者是IP32进行计算来保证模型的性能同时它还采用了细力度的量化策略对机会值采用一程E128的TiRWise量化对全种采用E128成E128的BlocWise量化这种策略能够更好的适应数据的分布减少量化误差为了减少IP8计算过程中的精度损失DipSegV3还将MMA操作中的中间结果累加到IP32计算器中并且将机货值和优化器的状态用IP8或者是BF16的格式进行存储这样在保证模型精度的同时可以大幅降低显存的占用并且提升训练速度我们再来看一看DipSegV3的一训练过程它的一训练与量库达到了14.8万一头肯的规模这些数据都经过了严格的筛选和清洗保证了高者量和多样性与前代的模型相比它的数据构建策略也有了很大的改进在数据内容上大幅提升了数学和编程相关数据在整体数据中的占并这就是模型在相关基础策略中的表现表突出在多语言数据方面进一步扩展了覆盖的范围超越了传统的英语和中文大大提升了模型的多语言处理能力为了保证数据的质量DipSeg还开发了一套完善的数据处理流程在最小化数据荣誉的同时保留了数据的多样性DipSeg还借捐了Docman的Piking方法将多个文档拼接成一个训练样本避免了在传统方法中由于阶段导致上下文信息丢失确保模型能够学习到更加完整的语义信息对于代码数据则借捐了VR中采用的Fewing Middle策略通过舔空的方式来迫使模型学习代码上下文关系从而提升代码生成和补权的准确性在分辞器与辞表方面DipSegV3采用了基于自己极BPE的分辞器并且购建了一个包含128K个头肯的词表为了优化都与原的压缩效率团队对预分辞器和训练数据还进行了专门的调整跟之前的DipSegVR相比新的预分辞器引入了将标点符号和换行符组合成新头肯的机制虽然这种方法能够提高压缩率但是在处理不带换行符的多行数据时可能会带来头肯边界偏差所以团队又在训练过程中以一定的概率随机将这些组合头肯拆分开来从而让模型能够适应更加多样化的数损形式提升模型的卢旺性在模型配置于超三数上DipSegv3也经过了精心的设计它的传辞方法层数设置为了61层隐藏层为度为7168所有可学习的参数都采用了标准差位0.006的随机处置化在MMA2A结构中注意力头的数量设置为了128每个注意力头的为度为128KV压缩伟度为512Pyro压缩伟度为1536结构的K头的伟度为64除了前三层之外其他的FFN层都被替换为了MMA1层每一个MMA1层包含一个共享专家和256个路游专家每个专家中的中间隐藏层伟度为2048每个头肯层会被路游到8个专家并且最多会被路游到4个节线多头肯预测深度为设置为1而且还添加了额外的RMS-NOW层并且复加了额外的缩放音子在训练超线处方面采用了IDAM-WU的话器Bait1-1设置为0.9Bait2-3设置为0.95全中摔减细数摔准为0.1最大训练长度摔准为4K学习率采用组合设的调度策略在前两K部学习率从0线性增加到2.2成一时的肤次次方然后保持这个学习率直到模型处理完10T个头肯接下来在4.3T个头肯过程中学习率按照余玄曲线逐渐摔减到2.2成一时的肤次方在最后的500B个头肯中学习率先保持2.2成一时的肤次方不变然后切换到一个更小的长数学习率7.3成一时的肤次次方Bait3-3采用了动态调整的策略在前469B个头肯的训练过程中先从3702增加到153006在训练中保持153006不变为了实现MOE架锅中的复载均衡DPC-V3还采用了无额外损耗的复载均衡策略将偏着向的更新速度在余玄练的前14.3T个头肯中设置为了0.01在剩余的500B个头肯中设置为0.0区列及平衡损失因子2法设置为0.001多头肯预测MPT损失的权重浪大在前10T个头肯中设置为0.3在剩余的4.8T个头肯中设置为0.1在长上下温破展与多头肯预测方面DPC-V3采用了2阶段的训练策略将模型的上下温窗口从4K逐步困染到了128K第一阶段从4K困染到32K区列长度设置为32KBatchSize设置为1920徐区率设置为7.3成一时的Full6封第二阶段从32K困染到128K区列长度设置为128KBatchSize设置为480徐区率设置为7.3成一时的Full6封在大海楼针测试中可以看到DPC-V3在处理长温本方面的逐位能力此外DPC-V3采用的多头肯预测MPD策略邀请模型在每个位置预测未来的多个头肯而不仅仅是下一个头肯从而增强了模型的预见能力并且提供了更加丰富的训练型号最后是DPC-V3的后训练阶段包括监督微条SFT和强化学习RO两个重要的步骤在监督微条SFT阶段DPC-V3在一个包含了150万条指定想着对的高质量数据级上进行了微条涵盖了多种任务类型和领域并且采用了不同的数据购件策略对于数学代码逻辑推理等等需要复杂推理过程的任务采用了基于DPC-R1模型声承的高质量推理数据对于非推理类的任务比如说创意写作角色扮演简单问答等等则利用DPC-V2.5来声承想一然后由人工进行标准和较业确保数据的准确性和可高兴在强化学习RO随段为了让DPC-V3能够更好的对其人类偏好反对够见了基于规则的奖励模型和基于模型的奖励模型相结合的奖励机制对于可以通过明确的规则来进行判别的任务比如说数学题变成体等等则采用基于规则的奖励模型对于难以通过规则进行判别的任务比如说开放式的问答创意写作等等则采用基于模型的奖励模型在强化学习的过程中反对采用了GPRO算法与传统的PPU算法不同GPR不需要一个单独的Critic模型来估计外流寒暑而是通过比较一组样本的奖励来估计Adventure因此在Reval的半尺测试中DPC-V3在多个方面能够超越或者与GPT-CeO和Kerau的3.5S相当充分展示了它在对其人类偏好方面的能力好了以上就是对DPC-V3技术报告的一些解读了建议对于模型感兴趣的观众还是锦亮去阅训一下原文相信会有更多的收获也欢迎大家在评论区分享自己的模型使用体验感谢大家的观看我们下期再见
