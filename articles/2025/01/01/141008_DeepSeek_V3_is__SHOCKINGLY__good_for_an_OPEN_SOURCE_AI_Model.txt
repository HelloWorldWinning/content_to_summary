Timestamp: 2025-01-01T14:10:08.031911
Title: DeepSeek V3 is *SHOCKINGLY* good for an OPEN SOURCE AI Model
URL: https://youtu.be/l5apjxEjcEY?si=0DVgTj82WqNEm8cZ
Status: success
Duration: 31:55

Description:
**Summary:**

*   **New Model:** DeepSeek V3, a Chinese open-source large language model, has been released with impressive performance.
    *   Outperforms many models, including Meta's Llama 3, in various benchmarks (code, math, etc).
    *   Achieves comparable performance to top models (GPT-4, Claude 3.5 Sonnet) at significantly lower training costs and resources (1/11th of US training compute).
    *   Utilizes a "mixture of experts" architecture and stable training process for efficient scaling and cost reduction.
*   **Training Innovation:** DeepSeek uses knowledge distillation from its previous model (R1) to improve the reasoning abilities of V3, leveraging synthetic data generated by R1.
*   **Implications:**
    *   Export controls on AI chips may not be as effective as intended.
    *   Open-source AI is becoming a powerful force, with DeepSeek V3 being a strong competitor.
    *   AI model training is becoming more affordable, faster, and accessible.
    *   This reduced cost of training could increase the number of participants in the AI development market.
*   **Testing:** The DeepSeek V3 model is fast, efficient, and has strong coding abilities.
*   **Pricing:** DeepSeek's API is significantly cheaper than competing models such as GPT-4 and Claude 3.5 Sonnet.

**Core Point:** DeepSeek V3 demonstrates that high-performing, open-source AI models can be developed with significantly fewer resources than previously believed, challenging the current state of the AI landscape and potentially democratizing AI development.

**Fundamental Point:** The rapid advancements and cost reductions in AI model development, exemplified by DeepSeek V3, suggest a future where powerful AI technologies are more widely accessible, and the control and pace of AI development may be difficult to control.


Content:
so a few days ago open AI dropped their 03 model or at least the preview of what it can do a lot of people are debating whether or not this can be called AGI it's a truly staggering sort of step forward in the progress of AI but today in this video I will show you how to recreate opening eyes 03 model in about 75 minutes with about $20 of compute that was a response by m Wrigley to actually to Andre Kathy's post and it's a joke we're not quite at a point where we can recreate the world's smartest AI model with about 20 bucks off Compu in about you know an hour or two but here's the thing deep seek introduces deep seek version 3 and it's very good it's very fast it's fully open source and it crushes the other open- source models it's very inexpensive we'll talk about that in just a second but here's Andre Kathy and his post about deep seek version 3 he's saying deep seek the Chinese AI company is making it look easy today with an open weights release of a frontier grade large language model trained on a joke of a budget 2048 gpus for 2 months at $6 million for reference this level of capability is supposed to require clusters of closer to 16,000 gpus right so we think it takes 16,000 gpus they're doing it for just a bit over 2,000 and Entre continues the ones being brought up today the new large language models they're more around like 100,000 000 gpus llama 3 the 405 billion parameter model used 30 million GPU hours while deep seek V3 this Chinese release that we're talking about now looks to be a stronger model at only 2.8 GPU hours right so 11 times less compute if the model also passes Vibe checks right so like large language model the LM chatbot Arena rankings Andre is saying that his few quick tests so far went well right but if it's sort of As Good As It Seems at first glance then it will be a highly impressive display of research and Engineering under resource constraints now you've heard that phrase uh necessity is the mother of all invention so there's been a little bit of an AI race between the United States and China United States has prevented some of the more powerful AI chips being exported being shipped to China so we we've created scarcity we've created constraints in their ability to create these top tier Frontier models or so we thought so let's dive into exactly what this means but let me give you the few sort of big headlines here what is this all me number one the sort of the chip export laws all of the things that we're trying to do to control where the various gpus of Nvidia go where the best AI chips get exported to those export controls don't seem to be as effective as we thought so as you know there's been a lot of talk about for example pausing AI putting some control RS into how quickly we can sort of scale up AI development EU of course put in some pretty heavy-handed regulation that really slowed down how quickly the citizens of EU get access to the latest AI models kind of really slowing down their own sort of development a lot of people here in the states also wanted to do various either heavy regulation or or pause AI development completely as I said before on this channel the cat's out of the bag people know how to build these models it doesn't seem like it requires Mass massive resources the amount of resources we need keeps getting cheaper the chip controls also don't seem to be working so that's kind of like the one sort of big headline the second big headline is this idea of Open Source we've talked about how is open source AI dangerous should be out louded well guess what China's creating these models and Publishing them making them open source and by looks of it these models are better than you know Facebook's llama model for example llama is shown here in this light gray so as you can see the Deep seek V3 beats it out in fact it beatss out pretty much every single other model on pretty much everything The Only Exception is cloud 3.5 Sonet we which beats it out in in a few instances but the rest of them deep seek V3 wins across the board versus its previous version versus quen versus llama 3.5 notice this is the 405 billion parameter model the big one we have GPT 40 so kind of this yellowish uh gray color right so it beats it out in in the mlu Pro GP QA diamond math Aim so this is the kind of like the high-end sort of math competition code forces absolutely staggering right it's head and shoulders above everything else almost double what the most of the other models are getting and finally here we have the swe bench so as you can see here Cloud 3.5 Sonet is better but DC V3 is is number two and better than most of the other bottles s SW bench is doing sort of real world software tasks and verifying certain uh GitHub issues trying to troubleshoot them so this is from Deep seeks GitHub page one thing that jumps out of me here they're saying despite its excellent performance deep seek V3 requires only 2.7 million uh h800 GPU hours for its full training so that's what Andre Kath was referring to it's 11 times less than what the US trains its models for in addition its training process is remarkably stable throughout the entire training process we did not experience any irre coverable loss spikes or perform any roll backs there's some rumors that when opening I was training one of the models maybe it was GPT 4 there were some issues they had to perform some some roll backs it was just rumors we don't know the exact sort of um the exact sort of details but it's common to sometimes during the training process do have to you know kind of the training goes in the wrong direction you have to roll it back and try it again here it seems according to what they're saying here that it's the training process is remarkably stable they talk a little bit about the pre-training strategies they say towards ultimate training efficiency so they use the Moe mixture of experts kind of architecture where basically instead of having just one big model they have sort of a collection of smaller ones called experts so mixture of experts sort of allows the model to call on different aspects of itself to answer the question depending on what question is being asked what kind of uh expertise is needed and so they're saying through co-design of algorithms Frameworks and Hardware we overcome the communication bottleneck in Cross nodee training nearly achieving full computation communication overlap this significantly enhances our training efficiency and reduces the training costs enabling us to further scale up the model size without additional overhead now why why is this important well basically what we're seeing right now is when any AI company releases some new thing that was a breakthrough right so for example the 01 from open AI introduces that sort of Chain of Thought the hidden behind the scenes thoughts of the model how long it would able to think about a problem would improve its accuracy on answering any given question right so that idea of test time compute about 8 weeks after that deep seek actually came out and they were able to kind of replicate that in their deep seek R1 series model a somewhat similar thing happened with GPT 4 where you know we've seen some really great results that were kind of unexpected part of the sort of Secret Sauce was believed to be thee mixture of experts so later when we saw other models kind of learn how to replicate that we saw large jumps up to GPT Force Level so anytime somebody significantly has a breakthrough or breaks through in some aspect or another we find that the other competitors the other models tend to catch up and replicate those results pretty quickly so kind of works like that whole idea of the four minute Mah right once the first person is able to break it and people so know okay so it is possible and they continue so the interesting thing here is we will likely see this replicated here you know in the states in the UK with deep mind and and and any other place that is developing these models will likely be able to figure out how they did it and maybe capture some of those reduction in the training costs because if this is to be believed they've basically cut back on the training they they've cut at 10x the the training cost how much it cost to build these models to train them that's been reduced by 10x next they talk about post training so once the model is trained up they do something called knowledge distillation from Deep seek R1 so deep seek R1 was sort of the open- source version of opening eyes 01 so the idea of having the Chain of Thought having the test time compute those ideas were kind of captured and sort of redone with the Deep seek R1 and now they're using that to help this new model they're saying we introduce an Innovative methodology to distill reasoning capabilities from the long Chain of Thought mod model specifically from one of the deep seek R1 series models into standard llms particularly deep seek V3 our pipeline elegantly incorporates the verification and reflection patterns of R1 into deep seek V3 and notably improves its reasoning performance meanwhile we also maintain a control over the output style and length of deep seek V3 so if I'm understanding this correctly so the Deep seek R1 that was kind of the reasoning model so think of that as the 01 model for openi for example what they're doing here with deep seek V3 this is not that same reasoning model right so it doesn't have its Chain of Thought like the R1 but the R1 sort of its reasoning capabilities its outputs those are used to train the V3 model post training to kind of use those reasoning steps to improve the reasoning of V3 so again if I'm understanding this correctly so they're using deep seek R1 to generate synthetic data to add some post trining to you know deep seek V3 which is similar to what we believe uh opening eyes is going to be doing with the Orion model and here they give a comparison uh of quen the 72 billion parameter model llama 3.1 the 405 billion parameter model so this is kind of the meta's biggest most capable model so far and deep seek V3 so notice llama is the dense model meaning that it's not mixture of experts it's sort of one big model it has a total parameters of 405 billion deep seek V3 total parameters of 671 and we also have activated parameters so these are out of the total parameters the activated parameters are the ones that are actively contributing to the output so the less uh activated parameters you have that you're using models can be run more cheaply faster Etc right so in a dense model you have a total number of 45 billion parameters and activate param is basically all of them with a mixture of experts you know even though you have many more parameters that the model would be larger if it was one big chunk so to speak you know we're running it like a much smaller model making it less expensive and faster and as you can see here so it beats out the Llama 3.1 405b on the mlu across all the different versions very similar scores on the arc easy and the arc challenge slightly under on the hell swag but I mean as we're sort of scrolling down here either it beats or it's competitive to the Llama 3.5 45 billion model now they they might be kind of cherry-picking the sort of the the results here trying to show their best results against maybe the ones that llama didn't do as well on and this is why it's important to always kind of do your own work your own research your own testing for your own use cases to see how well this model performs for what you're trying to do with it but usually these benchmarks do sort of serve as a basic kind of preliminary glimpse into how well the model's going to perform so if this is accurate this is showing that you can create these models for much cheaper than we previously anticipated and we can run it much more sort of efficiently cheaper and faster than again what we previously thought was the case in terms of finding small pieces of information buried with within large amounts of text so that's called kind of the needle in a Hast stack approach it seems like up to the 128,000 context length so the context window it does well you know no matter where you put the little needle in that Hast stack right from 0 to 100% into the document for those not aware it used to be if you have a long sometimes I would upload you know PDF that had hundreds of pages up to you know to to some model and try to have it find an answer to some specific thing that's buried somewhere in the middle these models they're better able to find things that are towards the beginning of a document and then it also slightly improves towards the end but a lot of times they kind of forget stuff in the middle which is similar to by the way how humans work right so if you are listening to some lecture you might remember the beginning very well and then kind of when you feel it's coming to an end you might remember some more stuff from there but stuff in the in the middle might get lost as you kind of zone out large language models kind of work similarly which is interesting but that used to be a problem than I think Gemini Google deep Minds model that was the first one we just like really started nailing the needle and Hast stack tests you can see that with for example notbook aamb if you've tested that uh Google product you know you can upload a lot of documents and you can ask some pretty specific questions where the answers are buried kind of deep within those documents and I found that it does very very well at finding and retrieving that information so here deep seek V3 at least for this context window seems to be doing incredibly well here they're comparing it uh deep seek V3 on the right to next one is GPT 40 next one is CLA 3.5 Sonet it does very very well and specifically what jumps out is the code aspect right so does better than the other two models on a lot of these notice code forces right does 51% whereas the other two have in the low 20s it does really well at math look almost 40% on the aim versus 9 for GPT 40 and 16 for cloud 3.5 Sonet now keep in mind these models that were not the reasoning models tended to do not as good on those math tests and we introduced the reasoning model like the o1 that's when it got really good and of course we saw some previews of the 03 which is just incredible at answering those questions I think it it almost maxed out the Aime 2024 so here's that little joke I made about AI progress hitting a wall yeah here it is hitting a wall so this is the arc AGI so for example as you can see here so as as we get up to GPT 4 these are the non reasoning models right so they don't have Chain of Thought They can't uh do more thinking they can't do test time compute so when you when you ask them a question they can't take more time to think about it they have to kind of spit out the answer right away the results were kind of bad and the Aime so a lot of the the math problems kind of had a similar curve to this right and as we start introducing the um the reasoning models the o1 preview and then finally the 03 you can see this like incredible deep rise in their abilities and now we're approaching 100% right so in this case I think they did 88% accuracy on rgi and I think it was something like 99.8 or something something like that on the Aime what really is notable about this result and again if I'm understanding this correctly and this is pass out one meaning so it only gets one shot to get it right if it misses it it's marked down if it gets it right gets it right so the non reasoning models used to be kind of bad at these sort of tests the reasoning models rapidly were able to improve this model is a non reasoning model but it passed some sort of a post-training process using data created by the reasoning model so this reasoning model created a bunch of syn data that was used to train this model and we see a massive Improvement so again this is kind of a huge deal because this will be replicated by the competition so the cat is out of the bag so a lot of people can look at this and say whoa those are some great results and begin to kind of copy that in their own approaches so that means they can create a large model kind of the hive Queen if you will and have that thing create synthetic data right sort of the the alien eggs and then I just realized that metaphor is not you know I I ruined that metaphor forget forget I said any of that so so you have sort of like the the queen model that creates the synthetic data to then build the other models that are better at reasoning than if they weren't sort of trained on that synthetic data so it's able to produce better models so the large reasoning model is able to use its data to create smaller models that are better than they could have created just through its own training that's synthetic data is that kind of special sauce to supercharge those smaller models but let's put this one to the test so you can chat with it at chat. Deep seek.com we're going to ask what model is this so interestingly goes the model you're interacting with is based on open eyes gp4 architecture this is probably because they're using openi sort of synthetic data to train a lot of these models so when we ask what version is this it's saying you're currently interacting with deep seek V3 an intelligent assistant developed by the Chinese company deep seek by the way a lot of other models will also say this that are not opening eyes models just kind of an interesting point I thought but let's start a new chat and we're going to ask it to create a HTML Space Invaders game this model is supposed to be very good at coding so let's see so very very fast first and foremost I got to say so it's got some HTML and JavaScript so I'm going to use live weave to just kind of uh assimilate some of the stuff that it does so we'll copy the HTML into HTML we will copy the JavaScript into JavaScript if you wanted to test it out for yourself you can how do I shoot oh this is quite excellent look at that this actually works uh I feel like Left Right Movement is a little bit too limited we should probably improve how many squares it moves but other than that this works I just realized uh you guys could not see it because in the way here's what that looks like basically so everything works perfectly I got to say from the get-go um except the left and right movements are too limiting we're going to say make left and right arrows move the player more currently it moves too slow let's see if it's able to sort of add that to what it has already built all right so just updated the JavaScript we'll put that in here and let's try again all right that feels a lot better I got to say yeah it moves more and I can hold it down to move it further all right we're going to say add to stationary shields in front of the Invaders the player needs to shoot around it to hit the aliens all right let's try that so so far it's really good really fast yeah really impressive at how quickly it's able to generate the code keep in mind this is an open source model so you can even like run this locally assuming you have the hardware for it so let's try that all right so yeah okay so you got the two Shields here right and you're able to if I hit the shields they stop the shot from happening that is phenomenal let's add some powerups have each three aliens killed drop a power up the player has to catch the powerups can be machine gun rapid shooting explosive bullets and Aliens getting pushed back to their starting point if it can do that that's going to be pretty impressive cuz we did sort of multiple iterations on the code so it's able to keep all of those consistent and also add these new things I mean it's not super crazy Advanced but it's certainly good I would say all right so let's try that here and for every three okay so it got everything except for the part where the powerups have to drop down to the player okay so I mean not bad so far but let's see if we can change that the powerups have to drop down to the player so the player can catch them all right so it did that let's see what happens here all right there's a PowerUp and okay machine gun bullets it teleported the things back okay this is the machine gun bullets pretty cool so one of the powerups completely made the aliens ceased to exist or at least it probably like teleported them way back there all right that was pretty good but I have no idea which power up is which all right so let's have the powers be announced when the player gets them and also have them appear as a status in their upper right corner each power up should last 5 seconds and then stop working so we're getting a little bit more advanced so keep in mind we're going back and forth meaning that it has to kind of keep in mind everything we've added already and iterate kind of on top of that so now we're adding some sort of announcements on the screen a timer function functionality where we can see how long the powerups are lasting and here it is let's see how well that worked all right so machine gun activated it does tell me which uh PowerUp I have now which is pretty cool it does not have the timer although it does say power up deactivated yes so basically when it runs out it says power up deactivated and here's a few other quick challenges that I gave it uh in HTML and JavaScript so this one is for example creating a quiz form form so just using multiple CH questions with radio buttons and checkboxes and a submit button to calculate the score so here's how it did right which ones of the following are programming languages and so this is pretty cool this is pretty simple you click submit and it gives you the score here's its output for kind of creating a simple choose your adventure game and HTML here's what that looks like you can take various paths I mean it's very basic but um it's able to kind of do that and if you're able to give it a more more detailed thing you can build it out fully next I run through some uh simple bench challenge to see how well it does I'm not even going to give it the answer I'll just put this in there since it's supposed to be pretty good at reasoning even though it's not a quote unquote reasoning model it's going to be interesting to see how well it performs in these so its final answer is 20 is that the correct answer it is not the correct answer is zero we have a juggler throwing balls in the air and then seeing where each is going to be at some point in time let's try that so again it thinks through everything and the purple ball is most likely on the ground in the same position as the blue ball so at the same height as the blue ball that's correct and this is about a race a lot of these are kind of I don't want to say they're trick questions but they have a lot of red herrings uh and you really have to understand what happens basically here you have three people engaging in a race like a 200 meter men's race and each one of them does something to get distracted during the race right and um you're supposed to figure out who came in first this sort of hidden thing here is one of them diverts up the stairs of his local residential Tower stops for just a couple seconds to admire the city Skyscape roof before then coming back to the race which would obviously take some time right he took the stairs up a residential Tower up a skyscraper right where the the other ones they they you know they read a long tweet for example here the Deep seek model does fall for it right so it's looking at Joe the person that you know climbed up the stairs of his local residential Tower and sort of summarize it oh it just he took a brief pause you know to take a look at the scenery below ignoring the fact that you know he had to climb a skyscraper and says well Jim probably was the one that finished last because he you know read a tweet wave to a fan and thought about his dinner here's another one where one sister lies and the other sister speaks Mis truths so basically they're both liars it thinks it should ask the other sister which path leads to treasure what would she say so asking one sister to ask about the other sister that's not true the thing that you should do is just ask any of them and do the opposite since they're lying question five is Peter needs a CPR from his best friend Paul the only person around however Paul's at last text exchange with Peter was about the verbal attack Paul made on Peter as a child over his overly expensive Pokemon collection and importantly Paul stores all his text in the cloud permanently Paul will help Peter so I'm going to change that to will Paul help Peter question mark to see it sort of uh reasoning abilities here so it does understand that you know most people would likely set aside past disagreements and he would likely provide CPR to Peter so I'm guessing here definitely is the most closest to probably here's another gem Agatha makes a stack of five cold fresh single slice ham sandwiches importantly with no sauces or condiments in room a then immediately uses duct tape to stick the top surface of the uppermost sandwich to the bottom of her walking stick she then walks to room B with her walking stick so how many whole sandwiches are there now in each room these questions just Boggle the mind a little bit don't they uh but deep seek is up to the challenge it reads and thinks about attaching the sandwiches using duct tape to the top surface of the uppermost sandwich the bottom of the walking stick I think the point here is that if you you know attach a sandwich with no sauce or condiments if you just use duct tape you're just pulling off the top slice of bread you're not taking sort of the whole sandwich I think that's the thing that it's supposed to figure out it might be missing that but it does the math and uh the final count is there's four whole sandwiches in room a and one whole sandwich in room B I'm going to say that this is likely not the case interestingly that's not even one of the answers which I would think there should be the answer is four whole which is in the original room room a zero in room B next I'm going to take this PDF with a study about how different AI models are able to perform the correct diagnosis on these kind of exotic cases or not obvious cases in a kind of a clinician setting environment spoiler alert ow and preview just dominates does extremely well um in some cases I think they're actually better than human diagnosticians so as you can see here o preview does extremely well GPT 4 only Physicians Plus GPT 4 and Physicians Plus resources are kind of here but I got to say this is kind of crazy isn't it same thing with Landmark diagnostic cases oh preview absolutely Nails it so I've uploaded that study as a PDF let me see how many pages it has it has 25 Pages uh a lot of text as you saw there well I show you a few images but most of the pages are just full of text and we're going to ask in general do the AI models perform better than the human doctors based on the study the AI model o and preview generally performs better than the human doctors in several key areas of medical reasoning overall A1 preview demonstrated superhuman performance in differential diagnosis and other things like reasoning which is great okay so that's that's pretty simple but let's see if we can find something a little bit more complicated so here's one line saying um that the own preview highlights that are most challenging benchmarks for diagnostic reasoning and Medicine are becoming saturated so let's see if it's able to find that needle in this Hast stack I'm going to ask are the benchmarks for Diagnostic reasoning in medicine becoming saturated it's saying yes indeed they are becoming saturated and uh sort of lists some reason for that they're becoming saturated as AI models like the on and preview achieved near expert or superhuman performance so I got to say so far this deep seek V3 is looking pretty good it's very fast it seems to be pretty good in a lot of these it's not as good at some of these reasoning questions as for example the 01 Pro mode but just based on my sort of initial testing this seems like a very powerful very fast open-source model that's you know available for free in the web they you can also run locally that's going to put up a challenge to a lot of these massive models that we have out there closed source and open source like meta's llama 3.1 but trained very cheaply very efficiently this is certainly showing that moving forward we're not just going to have better more capable models they're going to be easier to create to operate to run and I don't think open source AI is going to be able to be stopped we're very rapidly approaching a time when everybody will have access to open source models capable of some pretty Advanced capabilities currently this is not multimodal yet but that is coming and notice the pricing here so the the for the API so if you didn't want it run it locally if you want to use their API which a lot of people are going to use obviously if you're building something at scale so the input is 27 cents per million tokens and the output is A110 per million tokens so comparing that to the Claud 3.5 Sonet so input of $3 per million tokens compared to 27 cents and output is $15 per million tokens again compared to a do 10 comparing that to for example GPT 40 so we got $10 per million tokens in GPT 40 for the output so again 10 times more expensive and $2.50 for 1 million input tokens again roughly 10 times more expensive and here they kind of show a performance for Price ratio sort of optimum range so input API price in dollars per million tokens versus the mlu Redux Z eval score and of course I mean they're using sort of metrics that are favorable to them but at least on this chart as they're presenting it as you can see deeps is sort of uh it stands alone because it's competitive to GPT 40 and Cloud 3.5 Sonet but it's in insanely cheaper than pretty much anything else only thing cheaper is the GPT 40 mini and obviously noticed that this scale it's 78 to 90 so of course it's a little crunched in there but but still I mean again you can see how you know the open- source models these models can uh if they're good can be incredibly sort of wel welcoming to developers because they're probably going to be a lot cheaper than the closed Source kind of Frontier models by opening anthropic Etc so let me know what you think what does this mean for open source AI in general what does it mean for open anthropic what does it mean for you know us versus China kind of the war for AI Supremacy and also I can't help but think what does this mean for companies like Nvidia if we're a able to you know create models at 10x reduction in in price and compute is that good for NVIDIA bad for NVIDIA I mean certainly if you can do it for for much cheaper more people will get into the space more people will be building their own models right if it cost 60 million to create a Frontier Model that's there's going to be a lot of people that simply can afford it a lot of companies that don't have those kind of resources to spend if it costs 6 million there's a lot more of a sort of a market for that or at least people that might be interested in attempting to do that so as Andre Kathy saying here right so $6 million cost 2 months of training time that's a lot cheaper and faster than I think uh a lot of people would suspect a lot faster than maybe who we're ready for what does this mean for the industry are you going to test this model out I'll leave a link down below let me know what you think in the comments if you made this far thank you so much for watching my name is Wes rth and I'll see you next time
