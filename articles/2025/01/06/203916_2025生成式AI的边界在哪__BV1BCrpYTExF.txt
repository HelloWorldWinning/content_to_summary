Timestamp: 2025-01-06T20:39:16.607312
Title: 2025生成式AI的边界在哪？ BV1BCrpYTExF
URL: https://b23.tv/4XTBig8
Status: success
Duration: 16:41

Description:
好的，以下是关于您提供的文本的总结，并按照您的要求进行了组织：

**1. Outline and Structured Summary**

*   **Introduction:**
    *   The speaker reflects on the rapid AI development in the past year (2024) and different learning paces among individuals based on their interests and goals.
    *   He wants to share thoughts on past year and future direction (2025), inspired by the death of a Google researcher who wrote about the changes in AI research.
*   **The "Magic" of AI and Its Hidden Boundaries:**
    *   The speaker acknowledges the "magical" aspects of AI (e.g., text/image/video generation).
    *   He points out that many "non-magical" aspects, the boundaries of AI, are not shared by top researchers.
*   **AI Development Pre-2019:**
    *   Prior to 2019, AI research was niche, focusing on rule-based, knowledge-driven approaches.
    *   Example: Language translation relied on linguists' knowledge and rule extraction.
    *   Early models used shallow neural networks, based on the limited computing resource.
    *   They sought to find patterns based on human understanding (inductive bias).
    *   Progress was incremental, not revolutionary.
*   **The Transformer Revolution (2019):**
    *   Transformer architecture introduced parallelism and attention mechanisms (especially multi-head attention).
    *   Key point: attention allowed models to learn relationships without pre-defined knowledge, finding the best combination based on data.
    *   This enabled models like GPT, which used the ability to do language translation without the help of language experts.
    *   GPT models further expanded to multi-language generation by adding human-assisted fine-tuning.
    *   The core is still based on statistical probability through massive data and computing.
*   **The Bitter Lesson:**
    *   2019 paper by Richard Sutton:  *The Bitter Lesson* highlights the shift from human-engineered methods to brute-force computation.
    *   Scale-up approach became dominant over improving model efficiency based on human knowledge.
    *   This makes it challenging for small research groups to publish meaningful new papers.
    *   Leads to "involution" and a focus on scaling resources instead of innovation.
*   **The Current State of AI and Its Limitations:**
    *   Rapid AI development has lowered the entry barrier, which means the competition will become more intense, and the competition is on who has more resources and efficiency.
    *   Core principles are mostly the same, focusing on data, computational resources, and engineering fine-tuning.
    *   The advantage of large companies is in time and resource advantage, not the core technologies.
    *   Examples include early work on stable diffusion and current large models, where the core idea is still the same, the only thing changes are the scale of the training dataset, and the engineering improvement.
    *   Current models do not have human-like "creative" or flexible thinking and problem-solving ability.
    *   They are essentially powerful search engines, not true artificial intelligence.
    *   Current approaches are still about searching patterns in statistics, and fine-tuning, but not truly a human's way of thinking.
*   **2025 Outlook and Opportunities:**
    *   AI will focus more on specific industries with lots of repetitive work that AI can handle, leaving humans to focus on the final 10% of tasks.
    *   Areas like robotics and self-driving cars will advance but are limited by human-defined rules and the data available for training.
    *   The speaker believes that we are reaching the limitation of current methods which is purely based on brute-force statistical probability with human intervention in between.
    *   The next breakthrough will be in how to mimic human's way of thinking, which is dynamic learning and thinking, such as those proposed by Richard Sutton, rather than the current way of statically updating parameters in a deep learning network.
    *   Individuals should find a specific vertical domain to focus on and identify its applications with low-sensitivity requirements for opportunities.
    *   The speaker positions himself as having experience in both commercial applications and core research, and believes there is a need to break through current model approaches.
*   **Disagreement with Current AI Hype:**
    *   The speaker disagrees with the idea that AI will soon pose a threat to human existence, as suggested by some prominent figures.
    *   He aligns with Richard Sutton's vision for the future of AI based on the dynamic updates of neural network, rather than the current approach.

**2. Core Point (One Sentence)**

The current AI landscape is driven by scaling data and computation using existing techniques, rather than revolutionary theoretical breakthroughs, limiting its potential to being a powerful search engine instead of a human-like general intelligence.

**3. Fundamental Point (One Sentence)**

The shift from knowledge-based to brute-force approaches has led to rapid progress in AI but may have inadvertently stifled deeper innovation, revealing a need for a more human-like way of AI thinking, which involves dynamic learning and adaptation.

**4. Overarching Framework**

The overarching framework is a **historical analysis of AI development**, starting with rule-based approaches and transitioning to data-driven models, leading to a reflection on the current state, limitations, and potential future directions.

**5. Conceptual Map**

```
Human Knowledge & Rules ⭠ Early AI Research
       ⭡
Inductive Bias
       ⭡
Shallow Neural Networks
       ⭡
Incremental Progress
       ⭢ 2019 ⭢ Transformer Architecture
                           ⭡
                       Parallelism & Attention
                           ⭡
                Data-driven Learning (No prior Knowledge)
                           ⭡
                       Brute-Force Scaling
                           ⭡
                      Current AI Landscape
                           ⭡
       "Bitter Lesson" ⭢ Focus on Computation ⭢ Large Companies Advantage 
                            ⭡
  Limits of Current AI ⭠ No Human-like Creativity ⭠  Powerful Search Engines
                            ⭡
        Future AI ⭢ Focus on Vertical Applications & Industry Specific Needs
                            ⭡
   More Dynamic Neural Network Approaches(like Richard Sutton)
                            ⭡
                          New Breakthrough
```


Content:
大家好 二两二五年第一个视频咱们大部分人都是从去年一年开始接触的AI的感触很深 总结特别多的东西学习有的同学快要同学慢因为这跟兴趣和自己的定位有关系那么可能我们有的定位就是研究学者那自然要盖上所有的一个新的知识点有的可能同学我们就是从业者就是做一个A证就可以了帮助公司提升某一个垂直流的效率那么有一同学可能纯粹是兴趣就懂一些付钱的概念就可以了没关系 今天我觉得总结过去一年包括未来二二二五年我个人认为的发言方向是什么正好结合一下Google D 饭的有位工作八年的这样一个研究学者去世了吗主要是因为医生 它流了一个医研这个医研其实写了很多东西个人感触很深 所以给大家分享一下正好结合现在AI的神区副秀神区我们就不用说了因为我们能看到GBT 上城文字了对达了包括图片生成 甚至视频生成这感觉很神奇 以前没有过的但愿意人家的身 也觉得其实很多非神秘的东西是行业内的顶尖人并不想让我们知道和不告诉我们的也就是说它的边界在哪里那你研究了之后就知道它边界是什么了其实AI这部可以从2019年做一个分界线2019年之前 其实很多研究AI的领域的人但是比较小错力不为人而且研究的方向是基于什么呢我们换个角度来说基于比如说员翻译在2019年Transformer出来之前所有的语言翻译都要找这种语言学家把这种比较短门本的这种派证这种就是语言学的这些东西给它几千总结出来比如说名字借词 比如说动作后面干什么比如说这个语言之间翻译有什么技巧它会总结很多技巧然后在这些技巧基础时上然后去训练一个短的神经网络不是深层深度神经网络比较短的神经网络让那个模型那么相当于什么呢相当于是我们编程里面的写好很多EF-OS之后然后再去跟这些EF-OS情况下去做样门的泛化那么这是早期的为什么这么做呢因为这个是一个历史传承的原因因为当时的算力没有那么强也无法没有那个Transformer的架构也不能这么泛化所以所有的顶尖学者走的路都是这样的拿一部分的线线支持基础之上去做这个神经网络学习这个在行业里面的专业名词叫做Inv5-Bias叫做龟纳性偏差那个Bias就是偏差 偏致像那个偏差或者说龙统一点时候就是基于人类任职基础之上去寻找这个Petan寻找这种规律然后呢要神经网络给他学习一下那么2001选择之前很多发的论文研究学者了包括RN 啊RSTM这些东西都是一小部分的改变他们整体这个论文出来以后模型出来的结果是在业政级上一小部分提升不是一个化石台的改变的一因所以就是一直没有对我们老百姓普通生活造成特别大的影响这个震撼那直到2019年论文出来之后Transformer它最主要的是这个并行以及那个灯神机制嘛就是灯神一小运内的也就是多头注意力这个里面更关键的是注意力这个东西那么多头就是多个注意力同时在走还是拿语言翻译来举例子原来的语言翻译没有多头的时候大概例比如说一对话有十个词多在找词词之间的这种观点关系名字动词的或者是谁出院在谁之后谁出现在谁的组合之后或者是这样的一个逻辑关系那么我们预先预定了很多这个人类的鲜艳知识但是Transformer这个灯神就改变了一下我不需要鲜艳知识了你就喊亮的把资料堆过来然后我根据我这个灯神机制我多个头同时并行去寻找这种试试寻找这种语言之间的关系我没有任何的预先那种鲜艳的定义我只要样本足够我就能让神经网络你和出来最佳的组合方式这个呢 解就是后来Gpt8的那个前身在Gpt发展之前其实这个灯神已经用于语言翻译就证明了不是得以翻译成英语它就没有任何显示就纯粹划对海浪资料就能翻译出来那么Gpt就把它变成英语翻译语也就是下一个英语单词的生成之后呢Gpt3.5就变成了各种语言的生成加上一个后期的人类的这种问答对这种卫调所以感觉很智能那本质上没有什么Transformer之外多多注意力之外没有本质的改变它还是一种盖就是基于盖绿的那个暴力统计也就是说通过深度的神经网络海浪的数据无限的计算资源去暴力的统计了一下这个盖绿谁可什么词出现在什么词之后那种组合出现在那种组合词之后的这种盖绿情况给他压缩在这个模拟权中里同时模型变得非常大那么也是因为这个时候强化学习之父沙森教授他出了一篇非常有名的文章也是在2019年叫做The Beaters Lyson一个痛苦的教训什么意思呢就是在2010年的之前所有搞人工智能的这些科学家们都在研究如何提升模型的这个效率根据线线支持逮摩村家购等的但是2019年之后完全基于暴力的就已经跨围度大平了这个行业了你只要前组多计算量足够大不需要太深的支持往里面海上的关数去最后模型出来效果就比你们之前做得多好所以正常的一个Beaters Lyson所以呢就讲回来刚才的这个古歌趋势的这个人他在自己的医院里面说了很多东西其中一条就是这个前后变化在AI Research研究这个行业里面前后的变化一部分人呢是可能接受不了这个事实就是原来很努力地去做一些事情研究一些东西现在变成没有价值了相对率没有价值了现在呢只要你有足够的算力你就能出来一个更好的模型我们就拿这个Stay with the Fusion早期来看我记得2023年年下半年的时候Stay with the Fusion出来第一个版本稍微在三秒冬话的那个那个里面这个是Wears the Measier谁就是成成那个冬话他他吃一大历面那个视频呢其实是非常这个不好的效果他的眼睛啊嘴啊和面子有时候分差出来但是一年之后就已经非常准了现在的视频就非常准了那为什么呢其实价格上几乎没有改变还是DF是有一点点的工程上啊这改改那改改啊就是三波的这个效率高一些的等等无所谓不重要重要的是训练预料变得指数被责家了所以呢压缩出来效果就特别好所以这个就BitLyce能让很多以前搞A的人就有一种感觉是小的AI Research这种团队是没有什么发表方向了因为他朋友那么大的算力他一个新的想法在小颜色机上颜色不算什么特别大的效果别人没有新想法只要把数据机搞得大就能出来的效果比大家好所以就摸紧这个好意AI摸紧这个好意进入一个内卷的一个状态二点二四点整个一年就是非常内卷从最早的骚扰出来证明了这个只要海灯的数据就能生成视频到现在我们克林啊回园呢等到大摸紧都出来了效果还越来越好你说一个行业一旦发展的非常快的时候就证明了什么证明了没有门槛也就是说底层的逻迹模型的概念数据逻迹等等大家都是一样的只要你有比别人更多机款资源更多的这种预料的话那么给他压缩起来剩下的就是一点点的工程上的这个这个工作了比如说那预料这么大的时候海量的机算资源如何去兵型续训练效果更好模型参数压缩的更小一些等等这些是这个周边的工程上的修改那么核心原理其实没有什么动大的发展和突出的变化所以这也是造成了这个这个人一于一征的一个部分的原因之一所以呢他写了一个这个这个的Bit's Sweet Lesson就是机统股又甜美的一个一堂课也就是说发展到今天那么只能去接受现在海量数据中暴力的Skating Law这种形式去提升模型的效果比纯粹的稿研究来的效果更好那这是也是一个好事因为它毕竟是时代的一个夸布嘛那也是一件不好的事情突然间就出来这么多的叫做AI就是Resert Scientist研究科学家我可以跟我自己说我也是个Resert Scientist我就做过的资源其实我也可以训练书的一个K-Me我觉得K-Me就没什么这个怎么说呢就是从公司叫做什么来说价值就不是那么大了因为给我足够的钱我也能训练书来K-Me一样的模型但是当然我们的差距什么呢现在这个行里面差距就是时间的差距不是底层轮里上的差距了大家一样的一个海元技术那么你比我做走离一年就比我多了一年或者大半年的这种是错的成本是错的这种继续比如说在提示词里面我们如何浏化它去生成出来一些没有偏差的对不对 敏感词过率等等 包括这个多台机器同时高病发的去训练以及去服务这API那都是这些周边的东西其实这个才是目前我认为这些大型战友工资的领先的地方就是领先一个时间差而已但是从底层上来说大家没什么不一样了那么这就回到了大厂和小厂的竞争关系当中来你说同样的东西没有什么技术逼了一个门槛肯定是长远来说大厂更好更有前途小厂一定会被淘汰掉最终就可能被卖掉因为你没有什么核心的竞争优势包括长少下温也没有竞争优势是一样的其实都是一个工程学生来说就竞争早然后顺利成章的推理出来今天这个效果我们九例子比如说早期的这个悬者被这边板那在最开始的时候只有这个TenshensAoN的这样的一个正于悬被这边板正于悬被这边板当你用了以后聪明的人就慢慢会发现它在长距离上效果不是那么好那如何去改变一下长距离上的位置边板呢你自然然就想到了把这个这个周期波动给它改成像这个罗悬悬转一样对吧而且数学上也没有什么特别复杂的东西都是关于一些人带数那你能想到你就能有办法给它做出来同理在这个长效效纹的时候100万射像纹不太可能就一下给压住在一台这个机皮楼里去做这个KV开始机内存计算所以呢大家想个办法那把它压缩一下对吧因为你100万射像纹你不需要每一个偷跟对面一个偷跟对面都有关注度你会把一些关注度重要性比较高的偷跟抽出来不重要的呢你可以给它比如很多特事啊点儿啊到号啊这些都不重要了你就给它压缩上一个子空间那么压成子空间之后就会自然然的你就把100万偷跟压住的比较小了这是一种技术方式那同理还有很多其他技术方式包括我们后期模型的微调量化所谓的量化就是这个Lorra叫Lorang Adaptation就是把技术巨镇相成把一个大巨镇通过成一个小军的把它降低了一样的道理其实回来回去都是在统计学上来说是特征指的一些变化的寻找和查照没有理论上的不同那你说人脑是不是这样做的呢我认为人脑这个时候论理问题没人说是可能是可能不是但我更认为可能不太可能完全是所以发生到今天为止我们现在这个Lorra的边界是什么呢如果还是包括O1O3这种呢其实就是寻找规律基础之上加了一个桥花学习对吧加了很多人工这种反应吹你的调整本质上没有变化它不是人脑这种跳业性的或者是创造力的思维还是基于一个神经网络反向更新的全动参数然后外围做了很多人工的调整看起来有推理的步骤而且推理步骤非常长看似能解决很复杂的数学问题但是你修改提示词以后一个小问题它就会小步骤它就会出错所以你这样回来就想人类肯定不是这样思考的人类肯定是基于新学习知识之后然后学习海上知识动态的去调整自己在推理过程当中我们可能会动态调整我们以值的新学习知识这是一个动态的深度神经网络而且它不是一个单层的神经网络我们没有视觉对吧视觉脑子后面处理视觉的神经网络我们没有当老屁肠的记忆是神经园我们有处理逻辑神经园脑干里面是中央神经园等等很多不同的这种价格去联合处理我们的所谓的意识这种能犯化的能没有创造力的这种思维现在的模型还是有很长的路要走的所以它的边界更多的在于重迹海量的这种知识的搜索速度比人道快更像是一个聪明的搜索引擎它并不是一个有创造力的一个类人的机器人或者是类人的人工智能大脑所以我们知道的边界之后那么也就可以了解未来2025年的发展方向更多的是因为垂直是行业里面有很多重复的工作需要它俩的搜索准确定的工作可以用AI来替代90%替代另外10%用人工去旧政大去完成最后一步了完善这样的结合才是更好的一个方向包括机器人我这位机器人现在确实发展非常快但是你要机器人去做一些非常颜色的工作好像很难去做一些陪伴性的没有什么特别压危险性要求准确性不高的工作可能很快就会能落地自动价势也是一样去完全基于视觉的特色的FSD现在13.5%这个版本已经很厉害了各种不同的美国路况早点很好当然我看对这还好很多人说这个问题有那个问题不断的修饰完善永远达不到百分之百它是90%就是2%92.5%93%这样一步路来的但是社会的推进就是这样的过程当因为价势也是人类自己定义的规则本来是这上没有路我们修了路我们创造了新后代我们创造了规则所以它是一个有限级它有限级范围之内我会行通过你和去训练也可能达到这个有限级的级线所以它最终能上路但是在中国或者是印度其他国家路况下那么又是一套重新再去强化学习的一个过程所以现在人类呢就是这样的不断的脑前进不过的那种所以换回头来讲如果我们是好一万的人临明了它底层的原理之后就会对吧像我一样我们大概知道它能做到哪这个边界在哪它的级线是什么它不能做到是什么越来像小说里面三体里面所讲的一样你看杨总也是知道这个物理已经被锁死之后它自杀了其实这种级致的科学家对级致整理追求的一种识亡我们现在追求的并没有到级致的整理而是一个共同学上的一个级致性的突破将我们原来的很多的工作通过暴力的计算给它覆盖了所以大多数的人现在在这个行为里面并没有真正去研究地层太多东西都是那研究一些修改提升了小小的改变了很少有像Depth这个这样的公司去做很多深层度的研发当然因为这是很难的一般都是一二十年才会有一个大的突破所以但一个行业大多数人用进来之后感觉爆发它快的时候这个里面一定是大部分予论混杂的很多是利益驱动的那也是为什么大部分的这个鼎阶AI公司一直吹AGB、ASI的原因因为它需要一个进入的动力火车推动我们就考虑往前走但是没钱也不能往前走而成最搞研究的人可能也是在学者的过程当中也部分人继续去深度搞研究那么可能面临这五年十年没有什么发展那么有一部分研究人的创立公司对吧 迅速地在短期间那边线我认为更多的 还有我的粉丝应该是寻找自己的一个像机幕一样 把自己插在什么位置一个非常垂直的粘纸 但是一个领域能够迅速地找到应用落地点而要求不是那么敏感那么高的地方就是我们的一个机会而对于我个人来说我是同时见过商业落地以及一些切人的野板因为自己有自己的想法我相信整个地球上有很多人也在突破自己突破现有的模型方式不太认可我东德拉雪的老师亭振教授他现在在很多会上所讲的宣传短期之内会颠覆人类会威胁人类生命生产安全的我不太相信反而我比较认可这个Satan教授现在关于它是强化学习之父它关于下一步人工之能神经网楼应该怎么去做的比如说动态的更新神经网楼跳舞也是的一个新的框架这个才是真正的果认为学者应该掩抉的方向
