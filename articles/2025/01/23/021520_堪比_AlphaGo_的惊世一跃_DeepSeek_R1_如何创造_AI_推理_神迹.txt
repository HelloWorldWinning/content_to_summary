Timestamp: 2025-01-23T02:15:20.384129
Title: 堪比 AlphaGo 的惊世一跃：DeepSeek R1 如何创造 AI 推理“神迹”？
URL: https://youtube.com/watch?v=WtpZCJwdL9k&si=7g8UmWvHn_Ygvuik
Status: success
Duration: 16:35

Description:
好的，这是对您提供的 DeepSeek R1 内容的总结，包括核心观点、根本观点、框架结构以及一个概念图，全部使用简体中文呈现。

**1. 核心观点 (Core Point):**

DeepSeek R1 通过混合使用监督微调 (SFT) 和强化学习 (RL) 的多阶段交叉训练方法，显著提升了大模型在数学、代码和逻辑推理方面的能力。

**2. 根本观点 (Fundamental Point):**

DeepSeek R1 的成功在于展示了通过精巧的训练策略，可以有效提升大模型的推理能力，而非仅仅依靠模型规模的扩张，并为其他研究者提供了宝贵的实践经验和思路。

**3. 总体框架 (Overarching Framework):**

   *   **问题提出:** 现有大模型在数学推理和逻辑问题上表现不足。
   *   **解决方案演变:**
        *   最初使用链式思维（Chain of Thought）和推理缩放（Inference Scaling）等技术，但仍有不足。
        *  引入监督微调 (SFT) 和强化学习 (RL) 的概念及其特点， SFT 侧重模仿，RL 侧重探索。
        *  DeepSeek R1 利用混合训练方式：先 SFT 后 RL，再进行多阶段交叉训练。
   *   **核心方法:** 交叉利用SFT和RL，SFT 用于约束输出格式，RL 用于提升推理能力。
   *   **实验结果:** 混合训练方法显著提升了模型的推理能力。
   *   **应用:** R1模型合成数据可用于指导较小模型的微调，效果优于直接在小模型上使用强化学习。
   *   **意义:**  提供了一种提升大模型推理能力的标准答案，展示了 AI 工程化的进步和发展方向。

**4. 概念图 (Conceptual Map):**

<Mermaid_Diagram>
    graph LR
    subgraph DeepSeek R1 模型
        A[问题: 推理能力不足] -- 逻辑推理/数学/代码 --> B(现有大模型)
        B -- 链式思维/推理缩放 --> C[初期尝试]
        C -- 效果有限 --> D(SFT & RL 概念引入)
        D -- 监督微调 (SFT) --> E{模仿学习};
        D -- 强化学习 (RL) --> F{探索学习};
        E --  大量数据集/模仿 --> G(SFT 输出约束)
        F -- 结果导向/试错 --> H(RL 解题探索);
        G --> I[R1 第一版:RL训练]
        H --> I
        I --  可读性差/中英文混杂 --> J[R1 第二版: 混合训练]
        J -- SFT约束输出/RL探索解法 --> K{多阶段交叉训练}
        K -- 提高推理能力/可读性 --> L[R1 模型]
        L -- 合成数据 --> M(小模型微调)
        M -- 性能提升 --> N[最终成果]
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ddf,stroke:#333,stroke-width:2px
    style D fill:#aaf,stroke:#333,stroke-width:2px
    style E fill:#afa,stroke:#333,stroke-width:2px
    style F fill:#faa,stroke:#333,stroke-width:2px
    style G fill:#bfb,stroke:#333,stroke-width:2px
    style H fill:#fbb,stroke:#333,stroke-width:2px
    style I fill:#eee,stroke:#333,stroke-width:2px
    style J fill:#eee,stroke:#333,stroke-width:2px
    style K fill:#bbf,stroke:#333,stroke-width:2px
    style L fill:#9cf,stroke:#333,stroke-width:2px
    style M fill:#bbd,stroke:#333,stroke-width:2px
    style N fill:#acf,stroke:#333,stroke-width:2px
    
</Mermaid_Diagram>


Content:
哈喽大家好 就在昨天 DeepSeek 发布了它最新的推理大模型 R1 在业内 DeepSeek 的 R1 模型得到了大家广泛的好评 我们今天就结合它的这篇论文或者说它这篇技术报告 来给大家聊一下 DeepSeek R1 到底做了一件什么样的事情 它到底好在哪里或者说是强在哪里 对整个 AI 的发展方向上来说有着什么样的意义 欢迎大家收看我们今天星期的视频 然后多提一嘴 前段时间我休假去了 所以一直没有更新视频 给大家说一声抱歉 在接下来的新的一年里我会更勤奋一点 然后争取给大家多聊一聊身边所发生的这样一件事情 回归正题 DeepSeek R1 它到底做了一件什么事情 就为什么会有这样的一个需求 首先我们知道 DeepSeek 它前段时间发布了这样的一个 V3 Base 是一个大模型 然后 V3 Base 主要解决的一个问题是 怎么样用一个比较强大的工程化的能力 在比较少的显卡资源下 也能训练出一个非常不错的这样的一个大模型 的确 DeepSeek 展现了它在大模型领域 应该就属于第一梯队的这样的一个能力了 而且 DeepSeek 和 Meta 它们俩有一个比较共同的点 就是它们不是行业里面像 OpenAI 这种先发优势特别明显的企业 但是 OpenAI 由于它做什么都想说我是第一个发 我从 0 到 1 从 5 到 6 所以 OpenAI 很多东西都是避缘的 我就给你一个产品 然后我告诉你我做的很好 把这个胃口吊起来 然后从而吸引很多的人入局 去研究这样的一个东西 Meta 和 DeepSeek 它们俩作为追逐的人 一直在技术上都做得非常的扎实 而且会毫无保留的去把 OpenAI 的技术方向给开源出来 可能 OpenAI 新出了这个东西 两个月之后它们俩就相互就开源了 就通气了 就告诉大家这个东西应该怎么做了 所以不管是 Meta 还是 DeepSeek 的技术报告或者论文 你如果是从业者 尤其是做模型训练 你这两家的文章是一定要仔细的去阅读的 你阅读之后你就知道 原来想要做成那样的一个模型 需要按照这样的一个套路去走 好 那说回来就是 R1 这个模型解决的现实问题是什么 在 V3 的这样的一个模型推出之后呢 人们发现 V3 的模型其实对数学推理或者说是逻辑的问题解决的不是特别好 这也是我们之前看过有一阵儿 就是大家都在讨论怎么样把这个数学模型变得更好 那解决这个问题的方法其实在之前的视频里面我们给大家提过 就是最开始的时候我们用 long chain 去解决 我们通过工程化的方式把一个复杂的问题变成小的这个任务 然后这个小的任务足够简单到基础模型也能够去比较好的完成它 然后形成这样一个链条之后能够把问题解决 然后在有了这样的一个链式思维之后 就是慢慢的我就把这个链的这样的一个概念也放到大模型里面去 是吧 那我就相当于是我这个内容的输出就变长 我让这个大模型内部自己就去做这样的一个思考 叫做 chain of think or chain of thought 这样的一个概念 那其实就是我一个复杂的问题输入进去 我希望它得到的回答是一个更长的答案 但这个更长的答案里面有它对于复杂问题的步骤的思考 或者说是拆解 更多的其实是在数据上面做文章 与此同时还有一个概念就是 inferencing computing 或者说 inferencing scaling law 的这样的一个概念 就是随着我思考时间逻辑链的变长 我能够解决的问题会越复杂 或者说我解决问题的精度会变得越高 这也是我们看到的现在 O1 它回答一个问题会需要 3 到 4 分钟这样的一个原因 是吧 我通过这样的方式 通过在 inferencing 的这样的一个流程之中 增加这样的一个复杂度 或者增加这样的一个精细度 来得到更准确答案 这样的一个思路 那 DeepSeek R1 它也做的就是这样的一件事情 我想要让基础模型 在数学代码逻辑推理的方面 得到一个显著的提高 它不是一个多么新的问题 但是它给到了一个非常标准的大模型的答案 这个就是 DeepSeek 这篇论文最重要的一个地方 我们去看 DeepSeek 的解题思路的时候 我们其实还要去 了解两个概念 一个概念叫做 SFT 另外一个概念叫做 RL 也就是这个地方的 supervised fine-tuning 和这个地方的 reinforced learning 这两个概念其实是两种不同的学习方式 就是模型利用这两种学习方式都能学到信息 也都能够给出你答案 只不过他们俩用的这样的 怎么说呢 路线是不一样的 supervised fine-tuning 通俗的解释 其实就是有一点点像学习防邪 他真正做的学习 其实是学习根据上面的文本内容 我接下来应该说什么 比如说你所提供的所有的学习资料 都是散文的学习资料是吧 他就能够比较好的学会防邪散文 你给 那如果说你的训练期里面只有散文 然后这个时候我给他一个 古诗 我让他接这个古诗的下半句 他大概率是没有办法做到 和哲压韵是吧 你给 7 个字我就对 7 个字 因为他完全没有见过这样的东西 所以他没有办法防邪出来 这其实是对于 SFT 一个比较通俗的解释 所以在 supervise fine-tuning 的训练过程中 我们是需要有大量的数据集 而且这个数据集尽可能的要多种多样 要覆盖到更多的这样的一个地方 它的好处就是你得到的这样的一个结果 是吧 你数据量足够大之后得到结果 肯定就是大恰不差的 肯定就是不会说给你胡说八道 他一定是在考试范围之内的 那 reinforced learning 就有点像去学习解数学题的一种解法 在 reinforced learning 的时候 其实你所提供的学习资料或者说训练集 只需要是这道题目 比如说数学题目和数学题目最终的参考答案 而且这个参考答案是不需要有步骤的 比如说我给你很复杂的一个数学题 我问这个等于几 我们做题的时候 我们肯定是要一步一步的算是吧 但是我提供的训练集 是不需要中间的步骤的 我只需要最后的等于几 等于 0 怎么样去学会这个东西 我就去在拿到数学题之后 我想 10 种解题的方法 然后我根据这 10 种解题的方法 我去算 算谁的结果等于 0 等于 0 我就认为是好结果 不等于 0 我就认为是坏结果 我就去把好的结法我就给它更大的权重 坏的结法我就给它更小的权重 然后通过不断的不断的训练 就去找出像这类的问题应该怎么样解 你可以看到我们这边去描述 reinforced learning 的时候 它不需要去提供中间的解法 如果是同样的问题 我如果用 supervised fine-tuning 去做 我一定要提供中间的解法 它要根据中间的解法 去仿照它仿写这样的一个中间解法去做别的题 但是 reinforced learning 不需要这样的一个中间过程 所以 reinforced learning 相对于 supervised fine-tuning 的优势 就是它所需要的训练级更简单 或者说需要的信息量更少 但是它多出来的一个部分 就是它需要能够比较好去判断 你的结果或者说你所想出来的这样的一个解法 是否是正确的解法 我要做这样的一个判断 从这个角度上来说 像数学题 代码 或者说是逻辑推理 它这种有确定答案的东西 会比较好的去符合 reinforced learning 的一个特性 因为你想一想 我不给中间的这样的一个过程 让他自己去碰 自己去试 试 100 种解法 1000 种解法 他得到的这样的一种解法 可能作为人来说他都没有想过 他可能会创造出一些非常新的东西 新的解法 这些新的解法是他自己训练得到的 这个新的东西 我们就可以把它理解成是一种灵感 或者说是一种创造性的思维 这个是 reinforced learning 相比于 SFT 的优势 我们来了解了这两种特定的学习方式之后 我们就可以比较好的去看待 DeepSeek R1 这样的一个模型 它既然是想要去解决数学 代码 以及逻辑推理这种比较好设计 有确定性结果 比较好设计讲程制度的这样的一类问题 它用 reinforced learning 就比较的顺利成章 因为我们之前去训练大语言模型 其实还是用的是 supervised function 居多 因为大语言模型很多时候你说 这话正着说反着说都可以 它更多的是去模仿人说话 数学推理是另外一种 其实是另外一种场景 DeepSeek 这方最最核心的点就是说 我在我的大圆模型基础上 我纯用 reinforced learning 能不能够得到一个更好的模型 结果是能够得到一个更好的模型 DeepSeek 这方其实在最最开始的时候 他做了一件事情 就是我把大圆模型中间的思考链 就是引入 chain of thought 这样一个概念 然后用 reinforced learning 去做训练 然后得到的结果是 他也能够去解决一些题目 但是中间的思考过程的可读性非常差 有中英文夹杂 有一些他特殊的东西 这个其实是预料之内的 我们之前说过 他是通过去撞 是吧 去试 试各种各样的解法 然后对于这个模型来说 可能中英文夹杂的这种表达方式 是他最熟悉 他最能理解的 就比如说我每个人脑子里面去解题的时候 思路都可能不一样 有的人就习惯几何思维 有的人就喜欢代数思维 有的人就是不喜欢物理 有的人不喜欢化学 对于模型来说也一样 我去训练这个模型 reinforced learning 的时候 它是经过自己千次万次的这样一个实验 试出来了我用中英文夹杂的这样的思考方式 对我自己这个模型来说 我更能够快速的得到 或者说是准确的得到最终的结论 但是我又不仅仅是说是真的想用大圆模型 就单纯的告诉我这个答案 是吧 我想利用的是大圆模型的能力 并且我还想知道大圆模型是怎么样去思考 去解决这样的一个问题的 所以纯用 reinforced learning 去解决这个问题 能够得到好结果 但是得不到好过程 那怎么样 我希望他又得到好结果 又得到好过程 第二版的 deep seek R1 我先做 supervised factoring 然后再做 reinforced learning 再做 supervised 然后再做 reinforced learning 就是我交叉起来做 然后我通过多阶段交叉的训练 然后通过设置不同的这样的一个训练机 让他既能够把中间的过程 用人能够看懂的语言给他描述出来 又能够让模型自己去创造一些解法 自己去学会一些 或者说去获得一些灵感 其实是把两头的优势都占了 为什么能够这样 就像我们之前所说的 我们先用 supervised factoring 去 约束大模型 你必须按照这样的方式去输出中间的内容 然后他学会了这个输出中间内容这样的方式 我再用这个 reinforced learning 让他去思考 让他去发现一些逻辑规律 然后寻找他自己解决问题的一种方式 所以这个是 deepseek R1 他给出来的怎么样去提升大圆模型逻辑思考能力的一种方案 一种混合训练的一种方式 最终这个 deepseek R1 给大家用的版本 就是用的这种混合训练方式这样一种版本 那另外还有一个小的点就是 他用这个得到的 R1 再基于这些更小一点的模型 比如说 70B 的拉曼或者说千问这种模型去做征流 用 R1 的这种合成数据去征流出来的小模型的能力 比直接用 reinforced learning 去训练小模型 得到的这样的一个新的小模型要更好 说的有点复杂 也就是说我用合成数据对小模型做 supervised fine-tuning 得到的模型 其实是比直接去 reinforced learning 小模型的结果要更好 其实这上面所说的东西基本上就是 deepseek AI 它对于 R1 模型的这样的一个总结 我个人觉得最大最大的一个贡献 其实是告诉大家 OpenAI 是怎么样去做这样一件事情的 它是怎么样通过一个基础模型 得到了一个更好的推理模型的 其实 reinforced learning 我们不是第一次去接触了 就是在很多年前 Google 的 deep mind 团队 它去做 alpha go 的时候 alpha go 变得更强 其实是用 reinforced learning 因为你每下一步就会去用一个反馈机制 去判断这一步棋下的好还是坏 但是它并不是去再去模仿棋谱了 而是它去探索 如果我走一千步 下一步里面一千步 哪一步是最好的 它就可能会探索出来一些下法是人类没有下过的 是吧 人类棋谱里面没有的一些下法 这就是一种智能的涌现 所以我们可以看到 这其实是一种工程化能力的体现 就是随着 AI 再往前走是吧 各大公司是怎么样能够一步一步地 把这个大模型往前推得更好 这个其实是我觉得 deepseek 这篇文章 或者说这个公司它做得最好 能够给大家更多的启示 所以推荐大家如果有兴趣 想要深入地了解这个方向 了解怎么样去训练大模型 一定要把这个文章纳入必读教材 不仅仅要知其然 还要知其所以然 今天的主要内容就是这些 如果你觉得我们的内容做得还不错的话 欢迎点赞收藏转发订阅评论我们的频道 这对我们来说非常的重要 感谢你的收看 祝你学习顺利
