Timestamp: 2025-01-26T01:15:44.545326
Title: 堪比 AlphaGo 的惊世一跃：DeepSeek R1 如何创造 AI 推理“神迹”？ BV1DTf8YdEtv
URL: https://www.bilibili.com/video/BV1DTf8YdEtv/?spm_id_from=333.1007.tianma.4-1-11.click&vd_source=0eeb7ad2c1a37164e848fbfa306683ca
Status: success
Duration: 16:34

Description:
好的，这是对您提供的文本的总结和概念图：

**1. 核心要点（Core Point）:**

Depthic R1 模型的核心在于通过混合监督微调（SFT）和强化学习（RL），有效提升大型语言模型在数学、代码和逻辑推理方面的能力。

**2. 根本要点（Fundamental Point）:**

Depthic R1 的根本意义在于展示了一种结合 SFT 的约束性和 RL 的探索性的模型训练方法，这种方法能够使模型不仅获得准确的答案，还能以人类可理解的方式展示其思考过程。

**3. 总体框架（Overarching Framework）:**

这篇内容主要围绕 Depthic R1 模型的创新点，它的开发背景，以及它如何通过结合监督学习和强化学习两种训练方式，来提升大模型在推理任务上的表现。文章同时对比了 Depthic 和 Open AI 的发展策略，并强调了 Depthic 作为技术追赶者，开源技术的价值。

**4. 概念图（Simplified Chinese Conceptual Map）:**

<Mermaid_Diagram>
```mermaid
graph LR
    subgraph 背景 (Background)
        A[Depthic V3] -- 工程化能力 --> B(基础大模型)
        B -- 数学推理短板 --> C[Depthic R1需求]
        D[OpenAI] -- 先发优势 --> E(产品导向)
        F[Depthic & Meta] -- 追赶者 --> G(技术开源)
    end
    
    subgraph R1模型核心 (R1 Core)
        C --> H(混合训练方法)
        H -- SFT约束输出 --> I(可读的中间过程)
        H -- RL探索解法 --> J(更优的推理能力)
        I & J --> K[R1模型]
    end
    
    subgraph 训练方法对比 (Training Methods)
      L[SFT] -- 模仿学习 --> M(大量数据)
      M -- 缺点 --> N(缺乏创造性)
      O[RL] -- 试错学习 --> P(少量数据)
      P -- 优势 --> Q(创造性解法)
    end

    subgraph R1的优势 (R1 Advantages)
        K -- 数学,代码,逻辑推理 --> R(性能提升)
        R --> S(可理解的思考过程)
    end
    
    subgraph 知识蒸馏 (Knowledge Distillation)
      T[R1合成数据] -- 蒸馏 --> U(更强的小模型)
    end
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#fcc,stroke:#333,stroke-width:2px
    style D fill:#afa,stroke:#333,stroke-width:2px
    style F fill:#aaf,stroke:#333,stroke-width:2px
    style H fill:#cfc,stroke:#333,stroke-width:2px
    style K fill:#cff,stroke:#333,stroke-width:2px
    style L fill:#ffc,stroke:#333,stroke-width:2px
    style O fill:#fcc,stroke:#333,stroke-width:2px
    style U fill:#ccf,stroke:#333,stroke-width:2px
```
</Mermaid_Diagram>


Content:
Hello大家好就在昨天Depthic的发布他最新的推理大模型R1在页内的Depthic的R1模型得到了大家广泛的好评那我们今天就结合他的这篇论文或者说他这篇技术报告来给大家聊一下DepthicR1到底做了一件什么样的事情他到底好在哪里或者是抢在哪里对整个AI的发展方向上来说有著什么样的意义欢迎大家收看我们今天星期的视频然后多提一准就成段时间我休假去了所以一直没有更新视频给大家说一声抱歉接下来的新的一年里我会更兴奋一点然后争取给大家多聊一聊身边所发生的这件事回归正题Depthic R1呢他到底做了一件什么事情就为什么会有这样的一个需求首先我们知道Depthic他前段时间发布了这样的一个V3BASE是一个大模型然后V3BASE主要解决的一个问题是怎么样用一个比较强大的工程化的能力在比较少的显卡资源下也能训练出一个非常不错的这样的一个大模型的确Depthic展现了他在大模型领域应该就属于第一T队的能力了而且Depthic和META他们两个有一个比较共同的点就是他们不是那个行业里面像OPENI这种先发优势特别明显的企业但是OPENI有于他做什么的都想说我第一个发我从领到一从我到有所以OPENI很多东西都是避远的是吧我就给你一个产品然后我告诉你我做得很好把这个位口掉起来然后从而吸引很多的人入局去演纠这两个东西META和Depthic他们两个就是作为追逐的这个人一直在技术上都做得非常的扎实而且会毫无保留的去把OPENI的技术方向给开源出来可能OPENI新出了这个东西两个月之后他们两个家就开源了就通气了就告诉大家这个东西应该怎么做了所以不管是META还是Depthic的技术报告或论文你如果是从业的尤其是做模型训练那你这两家的这个文章是一定要仔细地去阅读的你阅读之后你就知道原来想要做成那样的一个模型需要按照这样的一个套路去走那说回来就是阿万这个模型解决的现实问题是什么在V3的这样一个模型推出之后人们发现V3的模型其实对数学推理或者是逻辑的这个问题解决的不是特别好这也是我们之前看过有一阵就是大家都在讨论怎么样把这个数学模型变得更好那解决这个问题的方法其实在之前的视频里面我们给大家提过就就就就开始的时候我们用狼犬去解决我们通过工程化的方式把一个复杂的问题变成小的这个任务然后这个小的任务足够简单到基础模型也能够去比较好的完成它然后形成这样一个练跳之后能够把问题解决然后再有了这样一个练式思维之后呢就是慢慢的我就我就把这个练的这样一个概念也放到大模型里面去那我就相当于是我这个内容的输出就变长了我让这个大模型内部自己就去做一个思考叫做Channel Think或者Channel Thought这样的一个概念那其实就是我一个复杂的问题输进去那我希望它得到了回答是一个更强的答案但这个更强的答案里面有它对于这个复杂问题的步骤的思考或者说是拆解更多的其实是在数据上面做文章那与此同时还有一个概念就是这个Inferencing 这个computing或者是一个Inferencing Scaling Law的这样也概念是吧就是我随著我思考时间裸激练的变长我能够解决的问题回约复杂或者说我解决问题的精度会变得越高这也是我们看到的现在O1它回答问题回答一个问题会需要3到4分钟这样的一个原因我通过这样的方式通过在Inferencing的这样一个流程之中增加这样一个复杂度或者增加这样一个精细度来得到更准确答案这样的一个思路那Defsqr1它也做的就是这样一件事情我想要让基础模型在数学代码逻辑推理的这个方面得到一个显著的提高它不是一个多么新的问题但是它给到了一个非常标准的大模型的答案这个就是Defsqr1这篇论文最重要的一个地方那我们去看Defsqr的解决思路的时候我们其实还要去了解两个概念一个概念叫做SFT另外一个概念叫做RL也就是这个地方的Supervised Feintoring和这个地方的Reinforce Learning这两个概念其实是两种不同的学习方式就是模型利用这两种学习方式都能学到信息也都能够给出你答案不过他们俩用的这样怎么说呢路线是不一样的Supervised Feintoring通俗的解释其实就是有一点点像学习防写它真正做的学习其实是学习根据上面的文文那种我接下来应该说什么比如说你所提供的所有的学习资料都是散文的学习资料它就能够比较好的学会防写散文你给如果说你的训练期里面只有散文然后这个时候我给它一个股师我让它接著股师的下半制它大概就是没有办法做到这个压运你给七个字我就对七个字因为它完全没有见过这样东西所以它没有办法防写出来这其实对于SFT一个比较通俗的一个解释所以在Supervised Feintoring的训练过程中我们是需要有大量的数据级而且这个数据级进可能的要多种多样要覆盖到更多的地方它的好处就是你得到的这样一个结果你数据量足够大之后得到结果可能就是大下不差的可能就是不会说给你胡说八道它一定是在考试范围之内那Reinforce Learning的有点像去学习解数学题的一种解法在Reinforce Learning的时候其实你所提供的学习资料或者说训练级只需要是这道题目比如说这个数学题目和这个数学题目最终的参考答案而且这个参考答案是不需要有步轴的就是比如说我给你得到很复杂的一个数学题我问这个等于几我们作题的时候我们可能是要一步一步的算但是我提供的训练级是不需要中间的步轴的我只需要最后的等于几等于0那怎么样去学会这个东西呢我就去再达到这个数学题之后我想十种解题的方法然后我根据这十种解题的方法我就算算谁的结果等于0等于0我就认为是好结果不等于0我就认为是坏结果我就去把这个好的解法我就给它更大的权重这个坏的解法我就给它更小的权重然后通过不断的训练就是找出像这类的问题应该怎么样解你可以看到我们这方面去描述0force learning的时候就是它不需要去提供中间的解法那如果是同样的问题我如果用这个SuperwiseFindwork去做我一定要提供中间的解法它要根据中间的这个解法去访照它访写这样的一个中间解法去做别的题但是0force learning不需要不需要这样的一个中间过程所以0force learning相对于SuperwiseFinduring的优势就是它所需要的一个训练级更简单或者说需要的星期两更少但它多出来的一个部分就是它需要能够比较好去判断你的这个结果或者说你所想出来的这样的一个解法是否是正确的解法我要做这样的一个判断但从这个角度上来说像数学题代码或者说是逻辑推理它这种有确定答案的东西吧会比较好地去符合Reinforce learning的一个特性因为你想想我不给中间的这样的一个过程然后它自己去碰这一去去试那是一百种解法一千种解法它得到的这样那种解法可能作为人来说它都没有想过它就是它会可能会创造出一些非常新的东西新的解法这些新的解法是它自己训练得到的这个新的东西我们就可以把它的底解成是一种灵感或者是一种创造性的四位这个是Reinforce learning相比于SFT的优势那我们来了解了这两种特定的学习方式之后我们就可以比较好的去看待Dipsickr1这样一个模型它既然是想要去解决数学代码以及这个逻辑推理这种比较好设计有确定性结果比较好设计这个讲成制度的这样一类问题那它Uneure-inforce learning就比较的顺利成章因为我们之前去训练这个大圆模型其实还是用的是这个SFTWiseFound.2有你居多因为大圆模型很多时候你说这个话正确说反正说都可以它更多的是去模仿人说话那数学推理这是另外一种其实是另外一种场景Dipsickr这方最最核心的点就是说我在我的大圆模型技术上我存用Reinforce learning能不能够得到一个更好模型结果是能够得到一个更好的模型Dipsickr这方其实在最最开始的时候它做了一件事情就是我把这个大圆模型中间的思考链就是引入ChannelSort的概念然后用Reinforce learning去做训练然后得到的结果是它也能够去解决一些题目但是中间的思考过程的可读性非常差有中英文夹杂有一些它特殊的东西这个其实是预料之内的我们之前说过它是通过去装去试试各种样的解法对于这个模型来说可能中英文夹杂的这种表达方式是它最熟悉它最能理解的就比如说我每个人脑子里面去解题的时候思路都可能不一样有的人就习惯几何思维有的人就喜欢代数思维有的人就是不喜欢物理有的人不喜欢化学对于模型来说也一样我就是就是这个模型Reinforce learning的时候它是经过自己牵扯万次的这样一个实验试出来了我用中英文夹杂的这样的思考方式对我对于我自己这个模型来说我更能够快速的得到或者说是准确地得到最容易的结论但是我又不仅仅是说真的想用这个大原模型单纯的告诉我这个答案是吧我想利用的是大原模型这个能力并且我还想知道大原模型是怎么样去思考去解决这样一个问题的所以存用Reinforce learning去解这个问题呢能够得到好结果但是得不到好过程的那怎么样我希望它又得到好结果又得好过程那第二版的这个Deficit R1就是我先做SuperwiseFrightering然后再做Reinforce learning再做Superwise然后再做Reinforce learning就是我我交叉起来做然后我我通过多阶段交叉的训练然后通过设置不同的这样的一个训练机让它技能够把这个中间的过程用人能够看懂的语言给它描述出来又能够让模型自己去创造一些解法自己去学会一些或者说去获得一些灵感其实是把两头的优势都站了为什么能够这样呢叫我们今天所说的我们先用这个Superwise Frightering去约束大模型你必须按照这样的方式去输出中间的内容然后它学会了这个输出中间的那种方式呢我再用这个人force learning让它去思考让它去发现一些逻辑规律让学校它自己就觉得问题的一种方式所以这个是DeepSeekr1它给出来的怎么样去提升大圆模型逻辑规律的一种方案用混合训练的一种方式最终这个DeepSeekr1给大家用的一个版本就是用的这种混合训练方式这样一种版本另外还有一个小的点就是它用这个得到的r1在基于这些更小一点模型比如说其实比的拉慢或者说千万那种模型去做征流用r1的这种合成数据去征得出来的小模型的能力比直接用Reinforce learning去训练小模型得到的这样的一个新的小模型要更好说一点负担也就是说我用合成数据对小模型做SupervisedFineTuring得到的这个模型是比直接去Reinforce learning小模型的结果要更好其实这下面所说的东西基本上就是DeepSeekAI它对于这个Avan模型的这样一个总结我个人觉得最大的一个贡献其实是告诉大家OpenS怎么样去做这样一件事情它怎么样能它是怎么样通过一个基本模型得到了一个更好的推理模型的其实Reinforce learning我们不是第一次去接触了就是在很多年前的Google的DeepMine团队它去做那个RFAGO的时候RFAGO变得更强其实是用Reinforce learning因为你每下一步就会去用一个反规机去判断进步旗下的还是坏但是它并不是去再去模仿器坡了而是它去探索如果我走一千步下步里面一千步哪一步是最好的它就可能会探索出来一些下法是人类没有下过的人类请步里面没有我的一些下法这就是一种智能的拥线所以我们可以看到其实是一种工程化能力的体型就是随著AI在往前走各大公司是怎么样能够一步一步的把这个大模型往前推得更好这个其实我觉得DeepSick这篇文章或者说这个公司它做的最好的地方能够给大家更多的歧视所以推荐大家如果有兴趣想要深入的了解这个方向了解怎么样去训练大模型一定要把这个文章那如果这个避读教材不经验要志气然还要志气所以然那今天的主要内容就是这些如果你觉得我们的内容做得还不错的话欢迎点赞收藏 转发订阅评论我们的频道说非常的重要那感谢你的收看祝你学习顺利
