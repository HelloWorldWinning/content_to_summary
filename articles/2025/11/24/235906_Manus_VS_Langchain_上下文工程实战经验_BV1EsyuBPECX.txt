Timestamp: 2025-11-24T23:59:06.726592
Title: Manus VS Langchain：上下文工程实战经验 BV1EsyuBPECX
URL: https://b23.tv/B7LCydD
Status: success
Duration: 3:33

Description:
好的，这是根据您的要求从文本中提炼和总结的核心思想。

### **核心思想摘要**

#### **一、 核心问题：上下文溢出 (Context Overflow)**
*   **背景**: 2024年是“智能体元年”，智能体在运行中频繁调用工具，导致上下文信息越积越多。
*   **挑战**: 大语言模型（LLM）的注意力有限，当上下文窗口过长时，其性能会显著下降，产生“上下文溢出”问题。

#### **二、 核心理念：上下文工程 (Context Engineering)**
*   **定义**: 一种旨在优化AI上下文窗口的技术框架，确保窗口内只装载执行下一步任务真正需要的信息，从而提升智能体性能和效率。
*   **价值**: 在当前阶段，它被认为是替代“提示词工程”的更高级实践，尤其对初创公司而言，是比模型微调风险更低、见效更快的方法。

#### **三. 五大核心技术方向**
1.  **上下文卸载 (Context Offloading)**: 将工具输出等不常用的上下文存储到外部文件系统，仅在上下文中保留路径或线索，需要时再调用。
2.  **上下文缩解 (Context Condensation)**: 对长上下文进行总结。
    *   **压缩 (Compression)**: **可逆**操作。例如，将外部文件信息压缩为文件路径线索，不丢失信息。
    *   **摘要 (Summarization)**: **不可逆**操作。在接近上下文极限时使用，通常采用结构化表单以保证信息稳定，并保留最近几次的调用记录以防逻辑中断。
3.  **上下文检索 (Context Retrieval)**: 通过两种方式查找所需信息：
    *   向量检索。
    *   基于关键词或简单规则的文件系统检索。
4.  **上下文隔离 (Context Isolation)**: 主要针对多智能体系统，管理主智能体与子智能体间的上下文分享。
    *   **分离模式 (Isolated Mode)**: 用于简单任务，主智能体下达明确指令，子智能体执行并返回结果。
    *   **共享模式 (Shared Mode)**: 用于复杂任务（如深度研究），子智能体可以访问全部历史上下文。
5.  **上下文缓存 (Context Caching)**: 通过缓存高频命中的结果，实现规避重复计算，提高效率。

#### **四. 四项实用项目建议**
1.  **优先工程，暂缓微调**: 初创公司应优先使用通用模型配合上下文工程，这能显著降低风险。
2.  **崇尚简化，避免过度**: 系统的进步源于架构简化，而非堆砌复杂性。
3.  **优选文本，慎用标记**: 数据存储优先选择纯文本格式（`.txt`），谨慎使用Markdown，因其标记符号会额外消耗Token。
4.  **少建多组，面向未来**: 减少构建复杂的、一体化的系统，多采用组件化思想。因为下一代更强大的模型可能会让今天的复杂构建变得多余甚至成为阻碍。

---
### **核心结论 (Core Point)**

在当前AI智能体开发中，通过策略化地管理上下文信息（即上下文工程），是比微调模型或构建复杂系统更高效、更具前瞻性的实践路径。

---
### **整体框架 (Overarching Framework)**

本文内容呈现了一个**“问题-方案-实践”**的系统性框架：首先点出AI智能体面临的**“上下文溢出”核心问题**，然后提出**“上下文工程”作为核心解决方案**，并将其分解为**五大技术方向**，最后通过**具体的实战经验和四项项目建议**，为开发者提供了清晰的落地指引。

---
### **概念关系图 (Mermaid Conceptual Map)**

<Mermaid_Diagram>
graph TD
    subgraph "核心问题与解决方案"
        A["上下文溢出 (Context Overflow)<br/>智能体工具调用过多, 性能下降"] --> B["上下文工程 (Context Engineering)<br/>为上下文窗口筛选核心信息"]
    end

    B --> C{"五大核心技术"}
    B --> M{"四大实用项目建议"}

    subgraph "技术详解"
        C --> D["1. 上下文卸载<br/>外部存储, 保留线索"]
        C --> E["2. 上下文缩解"]
        C --> F["3. 上下文检索<br/>向量/关键词查询"]
        C --> G["4. 上下文隔离"]
        C --> H["5. 上下文缓存<br/>规避重复计算"]

        E -- "方法一" --> I["压缩 (可逆)<br/>保留文件路径, 无信息损失"]
        E -- "方法二" --> J["摘要 (不可逆)<br/>接近极限时使用, 结构化输出"]

        G -- "简单任务" --> K["分离模式<br/>指令-执行-返回"]
        G -- "复杂任务" --> L["共享模式<br/>子智能体可访问全部历史"]
    end

    subgraph "项目实践原则"
        M --> N["优先上下文工程, 暂缓微调"]
        M --> O["简化架构, 避免过度工程化"]
        M --> P["优选文本存储 (txt)"]
        M --> Q["少构建, 多组件 (面向未来)"]
    end

    style A fill:#FFCCCC,stroke:#A52A2A,stroke-width:1px
    style B fill:#F9F7D8,stroke:#333,stroke-width:2px
    style C fill:#D6EAF8,stroke:#2E86C1,stroke-width:1.5px
    style M fill:#D5F5E3,stroke:#1E8449,stroke-width:1.5px
    style D fill:#EBF5FB,stroke:#2E86C1
    style E fill:#EBF5FB,stroke:#2E86C1
    style F fill:#EBF5FB,stroke:#2E86C1
    style G fill:#EBF5FB,stroke:#2E86C1
    style H fill:#EBF5FB,stroke:#2E86C1
    style I fill:#FFFFFF,stroke:#2E86C1
    style J fill:#FFFFFF,stroke:#2E86C1
    style K fill:#FFFFFF,stroke:#2E86C1
    style L fill:#FFFFFF,stroke:#2E86C1
    style N fill:#E8F8F5,stroke:#1E8449
    style O fill:#E8F8F5,stroke:#1E8449
    style P fill:#E8F8F5,stroke:#1E8449
    style Q fill:#E8F8F5,stroke:#1E8449
</Mermaid_Diagram>

Content:
前一天 南村的核心开发者南斯马顶和Malice的联合创神Patreon进行了一次一个多小时的对谈全程聚焦一个核心问题上下文工程放弹中有许多他们实战的经验的总结非常值得一看这个视频我给大家做一下想要细的书理此前我有个视频已经一想去介绍我什么事上下文工程以及为什么他在归谷现在特别火为什么他将替代提示此工程还没看过的呢 可以立刻去犯下先说说为啥现在大家这么关注上下文工程今年被称作智能体的原严智能体在运行的时候会经常要调用工具工具反过的信息越积越多上下文就变得特别的长这就出现了所谓的上下文复乱这个问题简单说就是上下文越长的时候B&Boxin由于它的注意力是有限的信念就变得越差大概在今年5月份上下文工程的概念就慢慢活起来了它其实就是想办法给AI的上下文窗口里面只装下一步任务真正需要的信息具体有哪些使用的技术Let's Mati总结了五个核心的方向第一个叫做上下文卸载就是把不常用的上下文像工具的输出这些内容存储到外部文件系统里面只留下一个人找到了路纪作为线索用的时候再掉出来第二个叫做上下文缩解就是把长上下文做摘要比如Claude 3.5Sony的模型它就自带了这个功能第三叫做上下文简索一般来两种办法一种是采用于一简索模股查评另外一种是基于文件系统关键词体配等简单规则的查评第四个这个上下文合理主要是针对多支人体而言主针体如何与它的子之人体之间分享上下文第五个叫上下文缓存提高板子密终率实现规理复有可以减少重复的计算Matsy的Peter也分享他们自己做支人体的一些创新的经验比如说上下文缩解他们分为摘要和压缩两种压缩是可逆的把人从外部找回案的信息去掉只留文件路级作为线索这样其实不丢信息摘要就不可逆了一般到了快上下文锁烂这个预值才使用还得保留最近几次工具调用的完整基督怕AI逻辑练断了而且推荐用结构化的表单做摘要信息更稳在上下文隔离方面Matsy有两种做法第一种简单任务就用分离模式主针体给子针体明确指令子之人体进行该指令输出结果给主针体就行第二种复杂模式比如说深度研究就用购假模式子针体能看到全部历史的上下文我个人猜测实际当中可能存在另外一种叫做混合模式也就是将上下文的一部分作为私有另外一部分作为可怒其他人体法问的购乡上下文我们把整个一个小时的访谈做成的中文版同时做了份可实化的重点突出的报告放在了课程社群里面方面大家快速掌握要点最后 Manus的Patreon和大家分享的四个使用的做项目的建议第一个就是出创公司不要太早搞模型微调专业化先靠同用模型加上上下文工程诈人减少不少风险第二个也不要过多工程化系统进步往往来自于简化价购而不是加复杂的东西第三个数据存储优先选择存温本这种格式用着方便仅甚使用Mark大我猜测应该是Mark大有太多的标记符号占用了Token存影响模型的性能第四个少购件多了一件这句话其实原子与对于模型未来发展的信心复杂的购件会在下一代更强大模型来临的时候显得多与甚至起导反作用
