Timestamp: 2025-11-20T07:09:12.839200
Title: 人工智能教母李飞飞博士：谈AI 就业、机器人以及世界模型为何是未来趋势 BV14XCzBaErf
URL: https://b23.tv/MuZPCgO
Status: success
Duration: 1:19:33

Description:
好的，这是从提供的文本中提炼出的核心思想摘要。

### **核心思想纲要**

**一、 人工智能的历史与转折点：ImageNet 的诞生**
*   **背景（AI寒冬）：** 在21世纪初，AI（当时多称机器学习）领域虽然有算法研究，但普遍缺乏高质量、大规模的数据进行模型训练，导致发展停滞，被称为“AI寒冬”。
*   **突破性洞察（李飞飞）：** 她认为，正如人类儿童通过海量视觉经验学习一样，机器也需要海量数据。她将研究重点从单纯的算法转向了数据本身。
*   **ImageNet 项目：**
    *   **目标：** 创建一个前所未有的、包含数百万张标记图像的大规模数据集，以训练机器识别成千上万种物体。
    *   **影响：** 2012年，使用ImageNet数据集和GPU训练的深度神经网络（AlexNet）在图像识别挑战中取得巨大成功，点燃了现代AI革命。
    *   **黄金配方：** 这一成功确立了“大数据 + 神经网络算法 + GPU算力”的核心范式，至今仍是现代AI发展的基础。

**二、 对人工智能的核心哲学：以人为本**
*   **AI并非“人工”：** 她强调“There's nothing artificial about AI”（AI并无“人工”之处）。它由人创造，灵感源于人，最重要的是，它深刻影响着人。
*   **人类的能动性与责任：** AI的未来走向（无论是好是坏）完全取决于人类的选择和行动。它是一种强大的工具，是“双刃剑”，人类社会和个人必须负责任地引导其发展。
*   **技术乐观主义者的审慎：** 她相信技术总体上对人类是积极的，但并非一个乌托邦主义者。她承认AI会对就业等产生影响，但关键在于我们如何应对和塑造它。

**三、 AI的现状与未来创新：超越语言模型**
*   **当前模型的局限：** 仅靠扩大现有模型（如语言模型）的规模（更多数据、更多算力）是不足以实现更高层次智能的。当前AI在推理、创造力（如牛顿发现定律）和情感认知上仍有巨大差距。
*   **下一个前沿——世界模型（World Models）：**
    *   **定义：** 一种能够理解和生成可交互、符合物理逻辑的三维空间和动态世界的AI模型，核心是“空间智能”（Spatial Intelligence）。
    *   **重要性：** 这是超越语言、迈向更通用智能的关键一步。人类大部分智能建立在对物理世界的感知和交互之上。
    *   **应用领域：** 将极大地推动具身智能（机器人）、科学发现（如DNA结构推演）、创意设计（游戏、电影特效）、模拟训练等领域的发展。

**四、 Marvel的发布与实践**
*   **产品：** 李飞飞的初创公司World Labs推出了全球首个大型世界模型应用Marvel。
*   **功能：** 用户可以通过文本或图像提示，生成一个可无限探索和交互的三维世界。
*   **早期应用：** 已在虚拟制片（提升40倍效率）、游戏开发、机器人模拟训练、心理学研究等领域展现出巨大潜力。

**五、 个人历程与建议**
*   **成功要素：** “智识上的无畏”（intellectually fearless），敢于进入未知领域，并专注于使命和与优秀的人合作。
*   **对年轻人的建议：** 鼓励年轻人专注于真正重要的事情，如工作热情、使命认同和团队信任，而不是在选择工作时过度分析所有细枝末节。

**六、 总结：AI时代中每个人的角色**
*   AI并非只属于技术专家。无论是艺术家、教师、护士还是农民，每个人在AI时代都有自己的角色。
*   人们应该积极拥抱AI作为增强自身能力和创造力的工具，同时作为社会公民，积极参与关于AI应用和治理的讨论，确保技术的发展始终服务于人类的尊严和福祉。

---

### **核心观点（一句话总结）**

人工智能的未来并非预设的命运，而是取决于我们人类以人为本的价值观、责任感和集体选择。

---

### **内容的总览框架**

该内容的核心框架是**“以人为本的人工智能发展观”**。这一框架贯穿了李飞飞对AI历史的解读、对当前技术的评判、对未来方向的展望以及对社会影响的思考。具体体现为：

1.  **历史视角：** 从模仿人类视觉学习的角度出发，通过创建ImageNet解决了AI发展的瓶颈。
2.  **哲学视角：** 强调AI是人类智慧的延伸，其发展必须围绕人的福祉、尊严和能动性。
3.  **技术视角：** 认为当前AI的局限在于缺乏对物理世界的空间理解，因此提出“世界模型”作为通往更高级智能的路径。
4.  **社会视角：** 呼吁跨学科合作（创立斯坦福HAI）和公众参与，确保AI技术被负责任地开发和应用，最终赋能于每一个人。

---

### **Mermaid概念图**

<Mermaid_Diagram>
graph TD
    subgraph "核心哲学 (Core Philosophy)"
        A["李飞飞的人工智能哲学：以人为本"]
    end

    subgraph "历史性突破 (Historical Breakthrough)"
        B["AI 寒冬"] -- "缺乏大规模数据" --> C["瓶颈"]
        D["洞察：模拟人类视觉学习"] -- "启发" --> E{"ImageNet项目"}
        E -- "产出" --> F["大规模标记数据集"]
        G["神经网络"]
        H["GPU算力"]
        F -- "结合" --> I["现代AI革命的开端 (2012)"]
        G -- "结合" --> I
        H -- "结合" --> I
    end

    subgraph "核心观点与社会责任 (Core Views & Social Responsibility)"
        J["AI是双刃剑"]
        K["AI并无'人工'之处"]
        L["人类的责任与能动性"]
        M["技术应增强人类尊严"]
        A --> J;
        A --> K;
        A --> L;
        A --> M;
        K -.-> L["'由人创造，影响于人'"];
    end

    subgraph "未来方向：世界模型 (Future Direction: World Models)"
        N["当前模型（如LLM）的局限"] -- "需要" --> O["新的技术范式"]
        O --> P["世界模型 (World Models)"]
        P -- "核心是" --> Q["空间智能 (Spatial Intelligence)"]
        P -- "商业化实践" --> R["World Labs公司"]
        R -- "推出" --> S["Marvel应用"]
        P -- "赋能" --> T["具身智能/机器人"]
        P -- "赋能" --> U["创意设计(游戏/VFX)"]
        P -- "赋能" --> V["科学发现"]
    end

    A -- "指导" --> D
    I -- "验证了...'大数据'的重要性" --> D
    A -- "指引" --> O

    style A fill:#FFD700,stroke:#333,stroke-width:2px
    style E fill:#87CEEB,stroke:#333,stroke-width:2px
    style I fill:#98FB98,stroke:#333,stroke-width:2px
    style L fill:#F0E68C,stroke:#333,stroke-width:2px
    style P fill:#FFA07A,stroke:#333,stroke-width:2px
    style S fill:#DDA0DD,stroke:#333,stroke-width:2px

    style B fill:#E6E6FA,stroke:#333,stroke-width:1px
    style C fill:#E6E6FA,stroke:#333,stroke-width:1px
    style D fill:#E0FFFF,stroke:#333,stroke-width:1px
    style F fill:#F0FFF0,stroke:#333,stroke-width:1px
    style G fill:#F5FFFA,stroke:#333,stroke-width:1px
    style H fill:#F5FFFA,stroke:#333,stroke-width:1px
    style J fill:#FFFACD,stroke:#333,stroke-width:1px
    style K fill:#FFFACD,stroke:#333,stroke-width:1px
    style M fill:#FFFACD,stroke:#333,stroke-width:1px
    style N fill:#FFE4E1,stroke:#333,stroke-width:1px
    style O fill:#FFE4B5,stroke:#333,stroke-width:1px
    style Q fill:#FFEFD5,stroke:#333,stroke-width:1px
    style R fill:#F8F8FF,stroke:#333,stroke-width:1px
    style T fill:#F0FFFF,stroke:#333,stroke-width:1px
    style U fill:#F0FFFF,stroke:#333,stroke-width:1px
    style V fill:#F0FFFF,stroke:#333,stroke-width:1px
</Mermaid_Diagram>

Content:
A lot of people call you the godmother of AI. The work you did actually was the spark that brought us out of AI winter. In the middle of 2015, middle of 2016, some tech companies avoid using the word AI because they were not sure if AI was a dirty word. 2017-ish was the beginning of companies calling themselves AI companies. There's this line, I think this was when you're presenting to Congress. There's nothing artificial about AI. It's inspired by people, it's created by people, and most importantly, it impacts people. It's not like I think AI will have no impact on jobs or people. In fact, I believe that whatever AI does, currently or in the future, is up to us. It's up to the people. I do believe technology is a net positive for humanity, but I think every technology is a double-edged sword. If we're not doing the right thing, as a society, as individuals, we can screw this up as well. You have this breakthrough insight of just, okay, we can train machines to think like humans, but it's just missing the data that humans have to learn as a child. I chose to look at artificial intelligence through the lens of visual intelligence, because humans are deeply visual animals. We need to train machines with as much information as possible on images of objects, but objects are very, very difficult to learn. A single object can have infinite possibilities that is shown on an image. In order to train computers with tens and thousands of objects concepts, you really need to show it millions of examples. Today, my guest is Dr. Faye Faitley, who's known as the Godmother of AI. Faye Faye has been responsible for and at the center of many of the biggest breakthroughs that sparked the AI revolution that we were currently living through. She spearheaded the creation of ImageNet, which was basically her realizing that AI needed a ton of clean, labeled data to get smarter. And that data set became the breakthrough that led to the current approach to building and scaling AI models. She was chief AI scientist at Google Cloud, which is where some of the biggest early technology breakthroughs emerged from. She was director at SAIL, Stanford's artificial intelligence lab, where many of the biggest AI minds came out of. She's also a co-creator of Stanford's Human Centered AI Institute, which is playing a vital role in a direction that AI is taking. She's also been on the board of Twitter. She was named one of times 100 most influential people in AI. She's also the United Nations advisory board, I could go on. In her conversation, Faye Fait shares a brief history of how we got to today in the world of AI, including this mind-blowing reminder that nine to 10 years ago, calling yourself an AI company was basically a death knell for your brand, because no one believed that AI was actually going to work. Today, it's completely different. Every company is an AI company. We also chat about her take on how she sees AI impacting humanity in the future, how far current technologies will take us, why she's so passionate about building a world model and what exactly world models are. And most exciting of all, the launch of the world's first large world model, Marvel, which just came out as this podcast comes out. Anyone can go play with this at marvel.worldlabs.ai. It's insane. Definitely check it out. Faye Fait is incredible and way too under the radar for the impact that she's had on the world. So I am really excited to have her on and to spread her wisdom with more people. A huge thank you to Ben Horowitz and Condoleezza Rice for suggesting topics for this conversation. If you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube. With that, I bring you Dr. Faye Fait-Lee after a short word from our sponsors. This episode is brought to you by Figma, makers of Figma Make. When I was a PM at Airbnb, I still remember when Figma came out and how much it improved how we operated as a team. Suddenly, I could involve my whole team in the design process, give feedback on design concepts really quickly, and it just made the whole product development process so much more fun. But Figma never felt like it was for me. It was great for giving feedback and designs. But as a builder, I wanted to make stuff. That's why Figma built Figma Make. With just a few prompts, you can make any idea or design into a fully functional prototype or app that anyone can iterate on and validate with customers. Figma Make is a different kind of vite coding tool. Because it's all in Figma, you can use your team's existing design building blocks, making it easy to create outputs that look good and feel real and are connected to how your team builds. Stop spending so much time telling people about your product vision and instead show it to them. Make code-backed prototypes and apps fast with Figma Make. Check it out at figma.com slash-lending. Did you know that I have a whole team that helps me with my podcast and with my newsletter? I want everyone on my team to be super happy and thrive in their roles. JustWorks knows that your employees are more than just your employees. They're your people. My team is spread out across Colorado, Australia, Nepal, West Africa, and San Francisco. My life would be so incredibly complicated to hire people internationally to pay people on time and in their local currencies and to answer their HR questions 24-7. But with JustWorks, it's super easy. Whether you're setting up your own automated payroll, offering premium benefits, or hiring internationally, JustWorks offers simple software and 24-7 human support from small business experts for you and your people. They do your human resources right so that you can do right by your people. JustWorks for your people. Fay Fay, thank you so much for being here and welcome to the podcast. I'm excited to be here, Lenny. I'm even more excited to have you here. It is such a treat to get to chat with you. There's so much that I want to talk about. You've been at the center of this AI explosion that we're seeing right now for so long. We're going to talk about a bunch of the history that I think a lot of people don't even know about how this whole thing started. But let me first read a quote from Wired about you. Just so people get a sense. In the intro, I'll share all of the other epic things you've done, but I think this is a good way to just set context. Fay Fay is one of a tiny group of scientists, a group perhaps small enough to fit around a kitchen table who are responsible for AI's recent remarkable advances. A lot of people call you the godmother of AI. Unlike a lot of AI leaders, you're an AI optimist. You don't think AI is going to replace us. You don't think it's going to take all our jobs. You don't think it's going to kill us. So I thought it'd be fun to start there. Just what's your perspective on how AI is going to impact humanity over time? Yeah. OK, so Lenny, let me be very clear. I'm not a utopian. So it's not like I think AI will have no impact on jobs or people. In fact, I'm a humanist. I believe that whatever AI does currently or in the future is up to us. It's up to the people. So I do believe technology is a net positive for humanity. If you look at the long course of civilization, I think we are fundamentally an innovative species that we, if you look at from written record thousands of years ago to now, humans just kept innovating ourselves and innovating our tools. And with that, we make lives better. We make work better. We build civilization. And I do believe AI is part of that. So that's where the optimism comes from. But I think every technology is a double edged sword. And if we're not doing the right thing as a species, as a society, as communities, as individuals, we can screw this up as well. There's this line. I think this was when you're presenting to Congress. There's nothing artificial about AI. It's inspired by people. It's created by people and most importantly, it impacts people. I don't have a question there, but what a great line. Yeah, I feel pretty deeply. I started working in AI two and a half decades ago. And I've been having students for the past two decades. And almost every student who graduates, I remind them, when they graduate from my lab, that your field is called artificial intelligence. But there's nothing artificial about it. Coming back to the point you just made about how it's kind of up to us, about where this all goes. What is it you think we need to get right? How do we set things on a path? They know this is a very difficult question to answer, but just what's your advice? What do you think we should be in mind? Yeah, like how many hours do we have? How do we align AI? There we go. Let's solve it. I think people should be responsible individuals, no matter what we do. This is what we teach our children. And this is what we need to do as grownups as well, no matter which part of the AI development or AI deployment or AI application you are participating in. And most likely many of us, especially as technologists, were in multiple points, we should act like responsible individuals and care about this. Actually care a lot about this. I think everybody today should care about AI because it is going to impact your individual life. It is going to impact your community. It is going to impact the society and the future generation. And caring about it as a responsible person is the first but also the most important step. OK, so let me actually take a step back and kind of go to the beginning of AI. Most people started hearing and caring about AI is what it's called today. Just like I don't know, a few years ago when Chachi PT came out, maybe like three years ago. Three years ago, almost one more month, three years ago. Wow, OK, that was Chachi PT coming out. Is that the milestone? You have mine? OK, cool. That's exactly how I saw it. But very few people know there was a long, long history of people working on it. It was called machine learning back then. And there's other terms and now it's just everything's AI. And there was kind of like a long period of just a lot of people working on it. And then there's this what people were first used the AI winter where people just gave up almost. Most people did and just, OK, this idea isn't going anywhere. And then the work you did actually was essentially the spark that brought us out of AI winter and is directly responsible for the world where now just AI is all we talk about as you just said, it's going to impact everything we do. So that would be really interesting to hear from you. Just kind of like the brief history of what the world was like before ImageNet, then just the work you did to create ImageNet, why that was so important. And then just what happened after? It is for me hard to keep in mind that AI is so new for everybody. When I lived my entire professional life in AI, it's there's a part of me that is just it's so satisfying to see a personal curiosity that I started barely out of teenage hood and now has become a transformative force of our civilization. It generally is a civilizational level technology. So so that journey is about about 30 years or 20 something, 20 plus years and it's just very satisfying. So where did I all start? Well, I'm not even the first generation AI researcher. The first generation really dates back to the 50s and 60s. And Alan Turing was ahead of his time in the 40s by asking, dare in humanity with the question, can we? Is there thinking machines, right? And of course, he has a specific way of testing this concept of thinking machine, which is a conversational chatbot, which to his standard, we now have a thinking machine. But that was just a more anecdotal inspiration. The field really began in the 50s when computer scientists came together and look at how we can use computer programs and algorithms to to build these programs that can do things that have been only capable by human cognition. So and that was the beginning and the founding fathers, the Dartmouth, the workshop in the 1956, you know, we have Professor John McCarthy, who later came to Stanford who coined the term artificial intelligence. And between the 50s, 60s, 70s and 80s, it was the early days of AI exploration and we had logic systems, we had expert systems. We also had early exploration of neural network. And then it came to around the late 80s, the 90s and the very beginning of the 21st century. That stretch about 20 years is actually the beginning of machine learning. It's the marriage between computer programming and statistical learning. And that marriage brought a very, very critical concept into AI, which is that purely rule based program is not going to account for the vast amount of cognitive capabilities that we imagine computers can do. So we have to use machines to learn the patterns. Once the machines can learn the patterns, it has the hope to do more things. For example, if you give it three cats, the hope is not just for the machines to recognize these three cats. The hope is the machines can recognize the fourth cat, the fifth cat, the sixth cat and all the other cats. And that's a learning ability that is fundamental to humans and the meaning animals. And we, as a field, realize we need machine learning. So that was up till the beginning of the 21st century. I entered the field of AI literally in the year of 2000. That's when my PhD began at Caltech. And so I was one of the first generation machine learning researchers. And we were already studying this concept of machine learning, especially in your network. I remember that I was one of my first courses in a Caltech is called your network. But it was very painful. It was still smack in the middle of the so-called AI winter, meaning the public didn't look at this too much. There wasn't that much funding. But there was also a lot of ideas flowing around. And I think two things happened to myself that brought my own career so close to the birth of modern AI is that I chose to look at artificial intelligence through the lens of visual intelligence. Because humans are deeply visual animals. We can talk a little more later. But so much of our intelligence is built upon visual perceptual spatial understanding, not just language per se. I think they're complementary. So I chose to look at visual intelligence. And my PhD and my early professor years, my students and I are very committed to a North Star problem, which is solving the problem of object recognition. Because it's a building block for the perceptual world. We go around the world interpreting reasoning and interacting with it more or less at the object level. We don't interact with the world at the molecular level. We don't interact with the world as we sometimes do. But we rarely, for example, if you want to lift a teapot, you don't say, okay, the teapot is made of 100 pieces of porcelain. And let me work on these 100 pieces. You look at this as one object and interact with it. So object is really important. So I was among the first researchers to identify this as a North Star problem. But I think what happened is that as a student of AI and then a researcher of AI, I was working on all kinds of mathematical models, including neural network, including Bayesian network, including many models. And there was one singular pain point is that these models don't have data to be trained on. And as a field, we were so focused on these models, but it dawned on me that human learning as well as evolution is actually a big data learning process. Humans will learn with so much experience, you know, constantly and evolution. If you look at time, animals evolved with just experiencing the world. So I think my student and I conjectured that a very critically overlooked ingredient of bringing AI to life is big data. And then we began this image that project in 2006, 2007. We were very ambitious. We want to get the entire internet's image data on objects. Now granted, internet was a lot smaller than today. So I feel like that mission was at least not too crazy. Now it's totally delusional to think a couple of graduate students and the professor can do this. And that's what we did. We curated very carefully 15 million images on the internet, created a taxonomy of 22,000 concepts, borrowing other researchers work like linguists work on WordNet. And it's a particular way of dictionarying words. And we combine that into image that and we open source that to the research community, we held an annual image that challenge to encourage everybody to participate in this. We continue to do our own research. But 2012 was the moment that many people think was the beginning of the deep learning or birth of modern AI because a group of Toronto researchers led by Professor Jeff Hinton participated in image that challenge, used the image that big data and two GPUs from NVIDIA and created successfully the first neural network algorithm that's can... It didn't totally solve but made a huge progress towards solving the problem of object recognition. And that combination of the trio technology, big data, neural network and GPU, was kind of the golden recipe for modern AI. And then fast forward the public moment of AI, which is the chat GPT moment, if you look at the ingredients of what brought chat GPT to the world, technically is still used these three ingredients. Now it's internet scale data, mostly texts is a much more complex neural network architecture than 2012, but it's still neural network and a lot more GPUs, but it's still GPU. So these three ingredients are still at the core of modern AI. Incredible. I've never heard that full story before. I love that it was two GPUs was the first... Yeah. And now it's, I don't know, hundreds of thousands, right? Oh, yeah. Orders and magnitudes are more powerful. And those two GPUs were... They were like gaming GPUs. They just went to the game store, right? They were people used for playing games. As you said, this continues to be in a large way the way models get smarter. Some of the fastest growing companies in the world right now, I've had them all mostly on the podcast, Mercor and Surge and scale. Like they do this, they continue to do this for labs, just give them more and more labeled data of the things they're most excited about. Oh, yeah. I remember Alex Wong from scale very early days. I probably still has his emails when he was starting scale. He was very kind. He keeps sending me emails about how you mentioned that inspired scale. I was very pleased to see that. One of my other favorite takeaways from what you just shared is just such an example of high agency and just doing things. That's kind of a meme on Twitter. You can just do things. You're just like, okay, this is probably necessary to move AI. And it's called machine learning back then, right? Was that the term most people used? I think it was interchangeably. It's true. Like, I do remember the companies, the tech companies. I'm not going to name names, but I was in a conversation in one of the early days, I think it is in the middle of 2015, middle of 2016. Some tech companies avoid using the word AI because they were not sure if AI was a dirty word. And I remember I was actually encouraging everybody to use the word AI because to me, that is one of the most audacious question humanity has ever asked in our quest for science and technology. And I feel very proud of this term. But yes, at the beginning, some people were not sure. What year was that roughly when AI was 20, 16? I think that was the changing. Some people start calling it AI. But I think if you look at the Silicon Valley tech companies, if you trace their marketing term, I think 2017-ish was the beginning of companies calling themselves AI companies. That's incredible. Just how the world has changed. Now you can't not call yourself an AI company. I know. Just nine-ish years later. Yeah. Oh, man. Okay. Is there anything else around the history, that early history that you think people don't know, they think is important before we chat about where things are going and the work that you're doing? I think as all histories, I'm keenly aware that I am recognized for being part of the history, but there are so many heroes and so many researchers. We're talking about generations of researchers. In my own world, there are so many people who have inspired me, which I talked about in my book, but I do feel our culture, especially the Silicon Valley, tends to assign achievements to a signal person while I think it has value. But it's just to be remembered. AI is a field of at this point, 70 years old, and we have gone through many generations. Nobody, no one could have gotten here by themselves. Okay. So let me ask you this question. It feels like we're always on this precipice of AGI, this kind of a term people throw around. AGI is coming. It's going to take over everything. How, what's your take on? How far you think we might be from AGI? Do you think we're going to get there on the current trajectory around? Do you think we need more breakthroughs? Do you think the current approach will get us there? Yeah, this is a very interesting term, Lenny. I don't know if anyone has ever defined AGI. You know, there are many different definitions, including, you know, some kind of superpower for machines, all the way to can, machines can become economically viable agents in the society. In other words, making salaries to live is that the definition of AGI? As a scientist, I take science very seriously, and I enter the field because I was inspired by this audacious question of, can machines think and do things in the way that humans can do? For me, that's always the north star of AI. From that point of view, I don't know what's the difference between AI and AGI. I think we've done very well in achieving parts of the goal, including conversational AI, but I don't think we have completely conquered all the goals of AI. And I think our founding fathers, the Alan Turing, I wonder if Alan Turing is around today and you ask him to contrast the AI versus AGI, he might just shrug and say, well, I asked the same question back in 1940s. So I don't want to get on to a rabbit hole of defining AI versus AGI. I feel AGI is more a marketing term than a scientific term. As a scientist and technologist, AI is my north star, is my fields north star. I'm happy people call it whatever name they want to call it. So let me ask you maybe this way. Like you described, there's kind of these components that from ImageNet and AlexNet kind of took us to where we're today. GPUs, essentially, data, label data, just like the algorithm of the model. There's also just the transformer, feels like an important step in that trajectory. Do you feel like those are the same components that will get us to, I don't know, 10 times smarter models, something that's life changing for the entire world? Or do you think we need more breakthroughs? I know we're going to talk about world models, which I think is a component of this, but is there anything else that you think is like, oh, this is a plateau or okay, this will take us just need more data, more compute, more GPUs? Oh, no, I definitely think we need more innovations. I think scaling laws of more data, more GPUs and bigger current model architecture is there still a lot to be done there, but I absolutely think we need to innovate more. There's not a single deeply scientific discipline in human history that has arrived at a place that says we're done, we're done innovating. And AI is one of the, if not the youngest discipline in human civilization in terms of science and technology, we're still scratching the surface. For example, like I said, we're going to segue into world models. Today you take a model and run it through a video of a couple of office rooms and ask the model to count the number of chairs. And this is something that Toddler could do, or maybe a elementary school kid could do, and AI could not do that, right? So there's just so much AI today could not do. Even let alone thinking about how did someone like Isaac Newton look at the movements of the celestial bodies and derive an equation or a set of equations that governs the movement of all bodies, that level of creativity, extrapolation, abstraction, we have no way of enabling AI to do that today. And then let's look at emotional intelligence. If you look at a student coming to a teacher's office and have a conversation about motivation, passion, what to learn, what's the problem that's really bothering you, that conversation, as powerful as today's conversational bots are, you don't get that level of emotional cognitive intelligence from today's AI. So there's a lot we can do better. And I do not believe we're done innovating. Demis had this really interesting interview recently from DeepMinds.google, where someone asked him just like, what do you think? How far are we from AGI? What does it look like? We're through there. He had a really interesting way of approaching it is if we were to give the most cutting edge model all the information until the end of the 20th century, see if it could come up with all the breakthroughs Einstein had. And so far, we're never near that. So they can just... No, we're not. In fact, it's even worse. Let's give AI all the data, including modern instruments data of celestial bodies, which Newton did not have. And give it to that. And just ask AI to create the 17th century set of equations on the laws of bodily movements. Today's AI cannot do that. All right. We're way the way. So let's talk about world models. To me, this is just another really amazing example of you being ahead of where people end up. So you were way ahead on, okay, we just need a lot of clean data for AI and neural networks to learn. You've been talking about this idea of world models for a long time. You started a company to build. Essentially, there's language models. This is a different thing. This is a world model. We'll talk about what that is. As I was preparing for this, Elon's talking about world models, Jensen's talking about world models. I know Google's working on this stuff. You've been at this for a long time. And you actually just launched something that we're going to talk about right before this podcast airs. Talk about what is a world model, why is it so important? I'm very excited to see that more and more people are talking about world models like Elon, like Jensen. I have been thinking about really how to push AI forward all my life, right? And the large language models that came out of the research world and then open AI and all this for the past few years were extremely inspiring, even for a researcher like me. I remembered when GPT-2 came out. That was in, I think late 2020. I was co-director. I still am, but I was at that time, full-time co-director of Stanford's Human Center AI Institute. And I remember it was, you know, the public was not aware of the power of the large language model yet. But as researchers, we were seeing it. We're seeing the future. And I had pretty long conversations with my natural language processing colleagues, like Percy Leung and Chris Baddin. We were talking about how critical this technology is going to be. And Stanford AI Institute, Human Center AI Institute, HAI was the first one to establish a full research center foundation model. We were Percy Leung and many researchers led the first academic paper foundation model. So it was just very inspiring for me. So of course, I come from the world of visual intelligence. And I was just thinking there's so much we can push forward on a beyond language because humans have used our sense of spatial intelligence, a world understanding to do so many things. And they are beyond language. Think about a very chaotic first responder scene, whether it's fire or some traffic accident or some natural disaster. And if you immerse yourself in the scene, think about how people organize themselves to rescue people, to stop further disasters, to put down fires, to a lot of that is movements is spontaneous understanding of objects, worlds, human situational awareness. Language is part of that. But a lot of those situations, language cannot get you to put down the fire. So that is what is that? I was thinking a lot. And in the meantime, I was doing a lot of robotics research. And it dawned on me that the lynchpin of connecting the additional intelligence in addition to language and connecting embodied AI, which are robotics, connecting visual intelligence is the sense of spatial intelligence about understanding the world. And that's when I think I was 2024, I gave a TED talk about spatial intelligence at world models. And I start formulating this idea back in 2022, based on my robotics and computer vision research. And then one thing that was really clear to me is that I really want to work with the brightest technologist and move as fast as possible to bring this technology to life. And that's when we founded this company called world labs. And you can see that the world world is in the title of our company because we believe so much in world modeling and spatial intelligence. People are so used to just chat box and that's a large language model. The simple way to understand a world model is you basically describe a scene and it generates an infinitely explorable world. We'll link to the thing you'll launch, which we'll talk about. But just as that a simple way to understand it. That's part of it, Lenny. I think a simple way to understand a world model is that this model can allow anyone to create any world in their mind's eye by prompting, whether it's an image or a sentence, and also be able to interact in this world. Whether you're browsing and walking or picking objects up or changing things as well as to reason within this world. For example, if the person consuming, if the agent consuming this output of the world model is a robot, it should be able to plan its path and help to tidy the kitchen, for example. So world model is a foundation that you can use to reason, to interact and to create worlds. Great. Yeah. So robots feels like that's potentially the next big focus for AI researchers and just like the impact on the world. And what you're saying here is this is a key missing piece of making robots actually work in the real world, understanding how the world works. Yeah. Well, first of all, I do think there's more than robots. That's exciting. But I agree with everything you just said. I think world modeling and spatial intelligence is a key missing piece of embody AI. I also think let's not underestimate that humans are embodied agents and humans can be augmented by AI's intelligence. Just like today, humans are language animals, but we're very much augmented by AI helping us to do language tasks, including software engineering. I think that we should underestimate or maybe we tend not to talk about how humans as an embodied agents can actually benefit so much from world models and spatial intelligence models as well as robots can. So the big unlocks here, robots, which a huge deal. If this works out, imagine each of us has robots doing a bunch of stuff for us, goes with disasters, things like games, obviously is a really cool example, just like infinitely playable games that you just invented in your head. And then creativity feels like just like being fun, having fun, being creative, thinking of magic, wild new worlds and environments. And also design. Humans design from machines to buildings to homes and also scientific discovery. There is so much I like to use the example of the discovery of the structure of DNA. If you look at one of the most important piece in DNA's discovery history is the X-ray diffraction photo that was captured by Rosalind Franklin. And it was a flat 2D photo of a structure that looks like it looks like a cross with diffractions. You can Google those photos. But with that 2D flat photo, humans, especially two important humans, James Watson and Francis Crick, in addition to their other information, was able to reason in 3D space and deduce a highly three dimensional double helix structure of the DNA. That structure cannot possibly be 2D. You cannot think in 2D and deduce that structure. You have to think in 3D spatial, the human spatial intelligence. So I think even in scientific discovery, spatial intelligence or AI assistant spatial intelligence is critical. This is such an example of, I think it was Chris Dixon that had this line that the next big thing is going to start off feeling like a toy. When chat GPT just came out, if like, I remember Sam Almond just tweeted, it's like, here's a cool thing we're playing with, check it out. Now it's the fastest growing product in all of history change the world. And it's oftentimes the things that just look like, okay, this is cool, that it's fun to play with and end up changing the world most. This episode is brought to you by Cinch, the Customer Communications Cloud. Here's the thing about digital customer communications. Whether you're sending marketing campaigns, verification codes or account alerts, you need them to reach users reliably. That's where Cinch comes in. Over 150,000 businesses, including eight of the top 10 largest tech companies globally, use Cinch's API to build messaging, email and calling into their products. And there's something big happening in messaging that product teams need to know about, rich communication services or RCS. Think of RCS as SMS 2.0. Instead of getting texts from a random number, your users will see your verified company name and logo without needing to download anything new. It's a more secure and branded experience. Plus you get features like interactive care cells and suggested replies. And here's why this matters. US carriers are starting to adopt RCS. Cinch is already helping major brands send RCS messages around the world. And they're helping Lenny's podcast listeners get registered first before the rush hits the US market. Learn more at getstarted at cinch.com slash Lenny. That's S-I-N-C-H dot com slash Lenny. I reached out to Ben Horowitz, who loves what you're doing. A big fan of yours, their investors, I believe in. Yeah, we've known each other for many years. But yes, right now they are investors of war labs. Amazing. OK, so I asked him what I should ask you about. And he suggested to ask you, why is the bitter? Why is the bitter lesson alone not likely to work for robots? So first of all, just explain what the bitter lesson was in the history of the eye and then just why that won't get us to where we want to be with robots. So well, first of all, there are many bitter lessons. But the bitter lessons everybody refers to is a paper written by Richard Sutton, who won the Turing Award recently. And he does a lot of reinforcement learning. And Richard has said, right, if you look at the history, especially the algorithmic development of AI, it turns out simpler model with a ton of data always win at the end of the day. Instead of the more complex model with less data. I mean, that was actually, this paper came years after ImageNet. That to me was not bitter. It was a sweet lesson. That's why I built ImageNet because I believe that big data plays that role. So why can bitter lesson work in robotics alone? Well, first of all, I think we need to give credit to where we are today. Robotics is very much in the early days of experimentation. The research is not nearly as mature as, say, language models. So many people are still experimenting with different algorithms. And some of those algorithms are driven by big data. So I do think big data will continue to play a role in robotics. And but what is hard for robotics? There are a couple of things. One is that it's harder to get data. It's a lot harder to get data. You can say, well, there's web data. This is where the latest robotics research is using web videos. And I think web videos do play a role. But if you think about what made language model worth a very, as someone who does computer vision and spatial intelligence and robotics, I'm very jealous of my colleagues in language because they had this perfect setup where their training data are in words, eventually tokens. And then they produce a model that outputs words. So you have this perfect alignment between what you hope to get, which we call objective function and what your training data looks like. But robotics is different. Even spatial intelligence is different. You hope to get actions out of robots. But your training data lacks actions in 3D worlds. And that's what robots have to do, right? Actions in 3D worlds. So you have to find different ways to fit a, what do they call a square in a round hole. That what we have is tons of web videos. So then we have to start talking about adding supplementing data such as teleoperation data or synthetic data so that the robots are trained with this hypothesis of bitter lesson, which is large amount of data. I think there's still hope because even what we are doing in world modeling will really unlock a lot of this information for robots. But I think we have to be careful because we're at the early days of this and bitter lesson is still to be tested because we haven't fully figured out the data of form. Another part of the bitter lesson of robotics I think we should be so realistic about is again compared to language models or even spatial models, robots are physical systems. So robots are closer to self-driving cars than a large language model. And that's very important to recognize. That means that in order for robots to work, we not only need brains, we also need the physical body, we also need application scenarios. If you look at the history of self-driving car, my colleague Sebastian Thrun took Stanford's car to win the first DARPA challenge in 2006 or 2005. It's 20 years since that prototype of a self-driving car. Being able to drive 130 miles in the Nevada desert to today's Waymo and on the street of San Francisco. And we're not even done yet. There's still a lot. So that's a 20-year journey. And self-driving cars are much simpler robots. They're just metal boxes running on 2D surfaces. And the goal is not to touch anything. Robot is 3D things running in 3D world. And the goal is to touch things. So the journey is going to be, you know, there's many aspects, elements. And of course, one could say, well, the self-driving car early algorithm were pre-deaflearning era. So deep learning is accelerating the brains. And I think that's true. That's why I'm in robotics. That's why I'm in spatial intelligence. And I'm excited by it. But in the meantime, the car industry is very mature. And productizing also involves the mature use cases, supply chains, the hardware. So I think it's a very interesting time to work in these problems. But it's true. Ben is right. We might still be subject to a number of bitter lessons. Doing this work, do you ever just feel offered the way the brain works and is able to do all of this for us? Just the complexity, just to get a machine to just walk around and not hit things and fall. Does it just give you more respect for what we've already got? Totally. We operate on about 20 watts. It's dimmer than any light bulb in the room I'm in right now. And yet we can do so much. So I think actually the more I work in AI, the more I respect humans. Let's talk about this product you just launched called Marble, a very cute name. Talk about what this is, why this important. I've been playing with it. It's incredible. We're willing to it. And for folks to check it out, what is Marble? Yeah, I'm very excited. First of all, Marble is one of the first products that World Labs has rolled out. World Labs is a foundation frontier model company. We are funded by four co-founders who have deep technical history. My co-founders Justin Johnson, Christoph Lassner and Ben Mildelhall, we all come from the research field of AI, commitments, computer vision. We believe that spatial intelligence and world modeling is as important, if not more, to language models and complementary to language models. So we wanted to seize this opportunity to create a deep tech research lab that can connect the dots between frontier models with products. So Marble is an app that's built upon our frontier models. We've spent a year in plus building the world's first generative model that can output genuinely 3D worlds. That's a very, very hard problem. And it was a very hard process. We have a team of incredible, founding team of incredible technologists from incredible teams. And then around a month or two ago, we saw the first time that we can just prompt with a sentence and an image and multiple images and create worlds that we can just navigate in. If you put it on Google, which we have an option to let you do that, you can even walk around. So even though we've been building this for quite a while, it was still just awe-inspiring. And we wanted to get into the hands of people who need it. And then we know that so many creators, designers, people who are thinking about robotic simulation, who are thinking about different use cases of navigable, interactable immersive worlds, game developers, will find this useful. So we developed Marvel as a first step. It's again still very early, but it's the world's first model doing this and it's the world's first product that allows people to just prompt. We call it Prom2Worlds. Well, I've been playing around with it. It is insane. You could just have a little shire world where you just infinitely walk around Middle Earth basically and there's no one there yet. It's insane. You just go anywhere. There's dystopian world. I'm just looking at all these examples. Yes. And my favorite part, actually, I don't know if there's a feature bug. You can see the dots of the world before it actually renders with all the textures. And I just love to get a glimpse into what is going on with this model. Basically, I like it. That is so cool to hear. Yes. Because this is where as a researcher, I'm learning because the dots that lead you into the world was an intentional feature visualization. It is not part of the model. The model actually just generates the world. We were trying to find a way to guide people into the world. And a number of engineers worked on different versions, but we converged on the dot. And so many people, you know, the only one told us how delightful that experience is. And it was really satisfying for us to hear that this intentional visualization feature that's not just a big hardcore model actually has delighted our users. Wow. And I would add that to make it more like to have humans understand more and more to like, wow, that is hilarious. It makes me think about a lemse in the way they, it's not the same thing, but they talk about what they're thinking and what they're doing. Yes, it is. It is. It also makes me think about just the matrix. Like, it's exactly the matrix experience. I don't know if that was your inspiration. Like I said, a number of engineers worked on that. It could be there in inspiration. It's in their, it's in their subconscious. Yeah. Okay. So just for folks that have made up on it, play around with this, maybe use a what's like, what are some applications today that folks can start using today? What's your goal with this launch? Yeah. So we do believe that world modeling is very horizontal, but we're already seeing some really exciting use cases, virtual production for movies, because what they need are 3D worlds that they can align with the camera. So when the actors are acting on it, they can, you know, they can position the camera and shoot the segments really well. And we're already seeing incredible use. In fact, I don't know if you have seen our launch video showing marble. It was produced by a virtual production company. We collaborated with Sony. And they use marble things to shoot those videos. So we were collaborating with those technical artists and directors and they were saying this has cut our production time by 40 X. In fact, it has to be. 40 X. Yes. In fact, it has to because we only had one month to work on this project and there were so many things they were trying to shoot. So using marble really, really significantly accelerated the production of virtual production for VFX and movies. That's one use cases. We are already seeing our users putting, taking our marble scene and taking the mesh export and putting games, you know, whether it's games on VR or games, just fun games that they have developed. We have had, we were showing an example of robotic simulation because when I was, I mean, I still am a researcher doing robotic training. One of the biggest pain point is to create synthetic data for training robots. At least synthetic data needs to be very diverse. They need to come from different environments with different objects to manipulate. And one path to it is to ask computers to simulate. Otherwise, humans have to, you know, build every single asset for robots. That's just going to take a lot longer. So we already have researchers reaching out and wanting to use marble to create those synthetic environments. We also have unexpected user outreach in terms of how they want to use marble. For example, a psychologist team called us to use marble to do psychology research. It turned out some of the psychiatric patients they study. They need to understand how their brain respond to different immersive scenes of different features, for example, messy scenes or cling scenes or whatever you name it. And it's very hard for researchers to get their hands on these kind of immersive scenes. And it will take them too long and too much budget to create. And marble is a really almost instantaneous way of getting so many of these experimental environments into their hands. So we're seeing multiple use cases at this point. But the VFX, the game developers, the simulation developers, as well as designers are very excited. This is very much the way things work in AI have had other AI leaders on the podcast. And it's always like, put things out there early as soon as you can to discover where the big use cases are. The head of JADJYPT told me how when they first put out JADJYPT, he was just scanning TikTok to see how people were using it and all the things they were talking about. And that's what convinced them where to lean in and help them see how people actually want to use it. I love this last use case of therapy. I'm just imagining heights to people seeing dealing with heights or snakes or spiders. It's amazing. A friend of mine last night literally called me and talked about his height scare. And asked me if Marbleshoot me used. It's amazing you went straight there. That's, you know, because I'm mentioning all the exposure therapy stuff. Like, this could be so good for that. That is so cool. Okay, so let me, I should have asked you this before, but I think there's going to be a question of just how does this differ from things like VO3 and other video generation models. It's pretty clear to me, but I think it might be helpful just to explain how this different from all the video AI tools people have seen. Pronapse thesis is that spatial intelligence is fundamentally very important and spatial intelligence is not just about videos. In fact, the world is not passively watching videos passing by, right? I love Plato has the allegory of the cave analogy to describe vision. He said that imagine a prisoner tied on his chair, not very humane, but in a cave watching a full life theater on the in front of him. But the actual life theater that actors are acting is behind his back. It was just lit so that the projection of the theater, the action is on a wall of the cave. And then the goal, the task of this prisoner is to figure out what's going on. It's a pretty extreme example, but it really shows, it describes what vision is about, is that to make sense of the 3D world or swirty world out of 2D. So spatial intelligence to me is deeper than only creating that flat 2D world. Spatial intelligence to me is the ability to create, reason, interact, make sense of deeply spatial world, whether it's 2D or 3D or 4D, including dynamics and all that. So the world lab is focusing on that. And of course, the ability to create videos per se could be part of this. And in fact, just a couple of weeks ago, we rolled out the world's first real time demo real time video generation and a single H100 GPU. So we, part of our technology includes that. But I think Marbo is very different because we really want creators, designers, developers to having their hands, a model that can give them worlds with 3D structures so they can use it for their work. And that's why Marbo is so different. The way I see it is it's a platform for a ton of opportunity to do stuff. As you describe videos, here's a one-on video that's very fun and cool. And that's it. And you move on. By the way, in Marbo, we couldn't allow people to export in video form. So you could actually, like you said, you go into a world. So let's say it's a Hobbit cave. You can actually, especially as a creator, you have such a specific way of moving the camera in a trajectory in the director's mind. And then you can export that from Marbo into a video. What does it take to create something like this? Just like how big is the team? How many GPUs you work in? Anything you can share there. I don't know how much of this is private information, but just what does it take to create something like this that you've launched here? It takes a lot of brain power. We just talk about 20 watts per brain. So from that point of view, it's a small number, but it's actually incredible. It's a half billion years of evolution to give us those power. We have a team of 30-ish people now, and we are predominantly researchers and research engineers. But we also have designers and products. We actually really believe that we want to create a company that's anchored in the deep attack of spatial intelligence, but we are actually building serious products. So we have this integration of R&D and productization. And of course, we use a ton of GPUs. That's a technical team. That's so happy to hear. Well congrats on the launch. I know there's a huge milestone. I know this took a ton of work. Thank you. I just want to say congrats to you and your team. Let me talk about your founder journey for a moment. So your founder, this company, started a couple years ago, two or three years ago. A year ago. A year ago. A year ago. It's probably 18 months. Yeah. Okay. Well, something you wish you knew before you started this, that you wish you could whisper into the ear of faith A of 18 months ago. Well I continue to wish I know the future of technology. I think actually that's one of our founding advantage is that we see the future earlier in general than most people, but still man, this is so exciting and so amazing that what's unknown and what's coming. But I know the reason you're asking me this question is a lot about the future of technology. You're bringing more. I know, look, I did not start a company of this scale at 20 year old. So I started a dry cleaner when I was 19, but that's a little smaller scale. We got to talk about that. And then I funded Google Cloud AI and then I funded an Institute at Stanford, but those are different beasts. I did feel I was a little more prepared as a founder of the grinding journey that I compared to maybe the 20 year old founders. But I still, I'm surprised and it puts me into paranoia sometimes that how intensely competitive AI landscape is from the model, the technology itself, as well as talents. And when I founded the company, we did not have these incredible stories of how much certain talents would cost. So these are things that continue to surprise me and I have to be very alert about. The competition you're talking about is the competition for talent, the speed at which just they have things are moving. You mentioned this point that I want to come back to that you, if you just look over the course of your career, you were at all of the major collections of humans that led to so many of the breakthroughs that are happening today. Obviously, we talk about ImageNet also just say all at Stanford is where a lot of the work happened at Google Cloud, which a lot of the breakthroughs happened. What brought you to those places? For people looking for how to advance in their career, be at the center of the future, just like is there a through line there of just what pulled you from place to place and pulled you into those groups that might be helpful for people to hear? Yeah, this is actually a great question, Lenny, because I do think about it. And obviously, we talked about its curiosity and passion that brought me to AI. That is more a scientific north start, right? I did not care if AI was a thing or not. So that was one part, but how did I end up choosing in the particular places I work in, in starting world labs, is I think I'm very grateful to myself or maybe to my parents' genes. I'm an intellectually very fearless person. And I have to say when I hire young people, I look for that because I think that's a very important quality if one wants to make a difference, is that when you want to make a difference, you have to accept that you're creating something new or you're diving into something new. People haven't done that. And if you have that self-awareness, you almost have to allow yourself to be fearless and to be courageous. So when I, for example, came to Stanford, in the world of academia, I was very close to this thing called tenure, which is have the job forever at Princeton. But I chose to come to Stanford because I love Princeton. It's my alma mater. It's just at that moment, there are people who are so amazing at Stanford and the Silicon Valley ecosystem was so amazing that I was okay to take a risk of restarting my tenure clock. Going to becoming the first female director of sale, I was actually, relatively speaking, a very young faculty at that time. And I wanted to do that because I care about that community. I didn't spend too much time thinking about all the failure cases. Obviously, I was very lucky that the more senior faculty supported me, but I just wanted to make a difference. And then going to Google was similar. I wanted to work with people like Jeff Dean, Jeff Hinton, and all these incredible demos, the incredible people. So, the same with world labs. I have this passion and I also believe that people with the same mission can do incredible things. That's how it guided my through life. I don't overthink of all possible things that can go wrong because that's too many. I feel like that's an important element. This is not focusing on the downside, focusing more on the people, the mission, what gets you excited, what do you think? I do want to say one thing to all the young talents in AI, the engineers, the researchers out there, because some of you apply to world labs. I feel very privileged you considered world labs. I do find many of the young people today think about every single aspect of an equation when they decide on jobs. At some point, maybe that's the way they want to do it. But sometimes I do want to encourage young people to focus on what's important because I find myself constantly in mentoring mode when I talk to job candidates. Not necessarily recruiting or not recruiting, but just in mentoring mode when I see an incredible young talent who is over focusing on every minute dimension and aspect of considering a job when maybe the most important thing is, where's your passion? Do you align with the mission? Do you believe that have faith in this team? Just focus on the impact and you can make the kind of work and team you can work with. It's tough for people in the AI space now. There's so much at them, so much new, so much happening, so much promo. I could see the stress. I think that advice is really important. What will actually make you feel fulfilled in what you're doing not just where's the fastest-growing company or who's going to win? I don't know. I want to make sure I ask you about the work you're doing today at Stanford at the HCI. HAI Human Centered AI Institute. What are you doing there? I know this is a thing you do on the side still. So, yes, HAI Human Centered AI Institute was co-founded by me and a group of faculty like Professor John H. Mende, Professor James Lande, Professor Chris Manning back in 2018. I was actually finishing my last, last about a go at Google. It was a very, very important decision for me because I could have stayed in industry, but my time at Google taught me one thing is AI is going to be a civilizational technology. It dawned on me how important it is to humanity to the point that I actually wrote a piece in New York Times that year 2018 to talk about the need for a guiding framework to develop and to apply AI. And that framework has to be anchored in human benevolence, is human-centeredness. And I felt that Stanford, one of the world's top university in the heart of Silicon Valley that gave birth to important companies from NVIDIA to Google, should be a thought leader to create this human-centered AI framework and to actually embody that in our research, education, and policy and ecosystem work. So, I founded HAI. You know, after fast forward, after six, seven years, it has become the world's largest AI institute that does human-centered research, education, ecosystem, outreach, and policy impact. It involves hundreds of faculty across all eight schools at Stanford from medicine to education to sustainability to business to engineering to humanities to more. And we support researchers, especially at the interdisciplinary area from digital economy to legal studies to political science to discovery of new drugs to new algorithms to that's beyond transformers. We also actually put a very strong focus on policy because when we started HAI, I realized that Silicon Valley did not talk to Washington DC or Brussels or other parts of the world. And it's given how important this technology is, we need to bring everybody on board. So we created multiple programs from congressional bootcamp to AI index report to policy briefing. And we especially participated in policy making, including advocating for a national AI research cloud bill that was passed in the first Trump administration and participating in state level regulatory AI discussions. So there's a lot we did and I continue to be one of the leaders, even though I'm much less involved operationally, because I care not only we create this technology, but we use it in the right way. Wow. I was not aware of all that other work you were doing. As you were talking, I was reminded Charlie Munger had this quote, take a simple idea and take it very seriously. I feel like you've done that in so many different ways and stayed with it and it's unbelievable the impact that you've had in so many ways over the years. I'm going to skip the lightning round and I'm just looking to ask you one last question. Is there anything else that you wanted to share anything else you want to leave listeners with? I'm very excited by AI lending. I want to answer one question that when I travel around the world, everybody asks me is that if I'm a musician, if I'm a teacher, middle school teacher, if I'm a nurse, if I'm a accountant, if I'm a farmer, do I have a role in AI or is AI just going to take over my life or my work? I think this is the most important question of AI and I find that in Silicon Valley, we tend not to speak hard to heart with people, with people like us and not like us in Silicon Valley but like all of us. We tend to just toss around words like infinite productivity or infinite leisure time or infinite power or whatever. And at the end of the day, AI is about people and when people ask me that question, it's a resounding yes. Everybody has a role in AI. It depends on what you do and what you want, but no technology should take away human dignity and the human dignity and agency should be at the heart of the development, the deployment as well as the governance of every technology. So if you are a young artist and your passion is storytelling, embrace AI as a tool. In fact, embrace Marvel, who I hope it becomes a tool for you because the way you tell your story is unique and the world still needs it. But how do you tell your story? How do you use the most incredible tool to tell your story in the most unique way is important and that that voice needs to be heard? If you're a farmer near retirement, AI still matters because you're a citizen, you can participate in your community. You should have a voice in how AI is used, how AI is applied. You work with people that you can encourage all of you to use AI to make life easier for you. If you're a nurse, I hope you know that at least in my career, I have worked so much in healthcare research because I feel our healthcare workers should be greatly augmented and helped by AI technology, whether it's smart cameras to feed more information or robotic assistants because our nurses are overworked, over-ferticked. And as our society ages, we need more help for people to be taken care of so AI can play that role. So I just want to say that it's so important that even the technologies like me are sincere about that everybody has a role in AI. What a beautiful way to end it. Such a tie back to where we started about how it's up to us and taking individual responsibility for what AI will do in our lives. Final question, where can folks find marble? Where can they go? Try to join world labs if they want to. What's the website? Where do people go? Well, world labs website is www.worldapps.ai. And you can find our research progress there. We have technical blogs. You can find marble the product there. You can sign in there. You can find our job posts link there. You can worry in San Francisco. We love to work with the world's best talents. Amazing. Befei, thank you so much for being here. Thank you, Lenny. Bye, everyone. Thank you so much for listening. If you found this valuable, you can subscribe to the show on Apple Podcasts, Spotify or your favorite podcast app. Also, please consider giving us a rating or a leaving review as that really helps other listeners find the podcast. You can find all past episodes or learn more about the show at lenny'spodcast.com. See you in the next episode.
