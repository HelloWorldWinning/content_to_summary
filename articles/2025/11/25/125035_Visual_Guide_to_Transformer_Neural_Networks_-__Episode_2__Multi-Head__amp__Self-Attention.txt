Timestamp: 2025-11-25T12:50:35.967625
Title: Visual Guide to Transformer Neural Networks - (Episode 2) Multi-Head &amp; Self-Attention
URL: https://youtube.com/watch?v=mMa2PmYJlCo&si=QxrH7t9ORWNqd4y6
Status: success
Duration: 15:24

Description:
好的，这是根据您提供的文本内容提炼和总结的要点。

### **核心要点大纲**

1.  **引言：回顾与引入**
    *   **系列回顾**：简要回顾了系列前两部分的内容——Transformer神经网络简介（Part 0）和位置编码的重要性（Part 1）。
    *   **本文核心**：聚焦于Transformer模型最关键的组件——**多头注意力（Multi-Head Attention）**机制。

2.  **注意力机制的核心思想**
    *   **为何需要注意力**：模型需要像人一样，能够聚焦于输入文本中的关键信息来做出判断（例如，通过 "Dracarys" 一词识别出角色是丹妮莉丝）。
    *   **自注意力 (Self-Attention) vs. 简单注意力 (Simple Attention)**
        *   **简单注意力**：根据一个**外部查询 (external query)** 来关注输入中的重要单词。
        *   **自注意力**：不仅关注重要单词，更重要的是**理解句子内部单词之间的相互关系**，从而解决一词多义等上下文理解问题（例如，通过上下文区分 "bank" 是指银行还是河岸）。

3.  **多头注意力机制的内部结构与流程**
    *   **核心组件：Q, K, V**
        *   机制灵感来源于信息检索系统，包含三个通过线性层生成的关键矩阵：
            *   **查询 (Query, Q)**：代表当前单词，用于去“查询”与其他单词的关系。
            *   **键 (Key, K)**：代表句子中所有单词，用于被查询和匹配。
            *   **值 (Value, V)**：同样代表句子中所有单词，包含了它们的原始信息。
    *   **单头注意力 (Single Head) 计算步骤**
        1.  **生成Q, K, V矩阵**：将带有位置信息的词嵌入复制三份，分别通过三个独立的线性层，生成Q, K, V三个矩阵。
        2.  **计算注意力分数**：通过计算Q和K的点积 (`Q * K^T`) 来衡量每个单词与其他所有单词的相似度或“关联度”。
        3.  **缩放与归一化 (Scale & Softmax)**：将分数除以一个缩放因子（通常是K矩阵维度的平方根）以稳定训练，然后通过Softmax函数将分数转换为0到1之间的概率分布，得到最终的**注意力过滤器 (Attention Filter)**。
        4.  **应用注意力**：将这个“注意力过滤器”与V矩阵相乘，其结果是：与当前单词关系越密切的单词，其信息（Value）在最终输出中的权重就越高。这生成了该注意力头的输出。
    *   **多头 (Multi-Head) 机制的实现**
        1.  **并行处理**：同时运行多个独立的注意力头（例如8个）。每个头都有自己独立的Q, K, V权重，因此可以从不同角度学习文本中的不同语言特征（如同一个场景，有人关注人物，有人关注背景）。
        2.  **结果合并**：将所有注意力头并行计算得到的输出矩阵拼接（Concatenate）在一起。
        3.  **最终输出**：将拼接后的大矩阵再通过一个线性层，将其维度压缩回原始输入的大小，得到多头注意力层的最终输出。

### **核心结论 (一句话)**

多头自注意力机制通过并行计算多个独立的注意力“头”，使模型能够同时从不同角度理解句子内部单词之间的复杂关系，并赋予关键信息更高的权重，从而实现对文本深层次的上下文感知。

### **内容总体框架 (Overarching Framework)**

本文的总体框架是一个由宏观到微观的解析过程：首先通过生动的例子阐述为何需要**“注意力”**这一概念来聚焦关键信息，然后深入剖析**“自注意力”**如何利用查询（Q）、键（K）、值（V）的机制来理解句子内部的上下文关系，最终揭示**“多头注意力”**如何通过并行化多个注意力头来从不同维度捕捉和整合丰富的语言特征，构成了Transformer模型的核心计算引擎。

### **Mermaid 概念图**

<Mermaid_Diagram>
graph TD
    subgraph "输入层 Input"
        A["位置感知的词嵌入 <br> Position-Aware Word Embeddings"]
    end

    subgraph "第一步: 生成Q, K, V矩阵"
        A -- "复制三份" --> Q_Linear["Q 线性层"]
        A -- "复制三份" --> K_Linear["K 线性层"]
        A -- "复制三份" --> V_Linear["V 线性层"]
        
        Q_Linear --> Q_Matrix["查询矩阵 (Q)"]
        K_Linear --> K_Matrix["键矩阵 (K)"]
        V_Linear --> V_Matrix["值矩阵 (V)"]
    end

    subgraph "第二步: 单头注意力计算 (以一个头为例)"
        Q_Matrix --> AttentionCalc
        K_Matrix --> AttentionCalc
        
        subgraph "计算注意力过滤器"
            AttentionCalc{"1. 点积<br>Q · Kᵀ"} --> Scale["2. 缩放<br>Scale"]
            Scale --> Softmax["3. Softmax"]
            Softmax --> AttentionFilter["注意力过滤器<br>Attention Filter"]
        end

        AttentionFilter --> ApplyAttention
        V_Matrix --> ApplyAttention
        
        ApplyAttention{"4. 矩阵乘法<br>Filter · V"} --> HeadOutput["单个注意力头的输出"]
    end
    
    subgraph "第三步: 多头合并与最终输出"
        subgraph "并行计算"
            HeadOutput -- "Head 1" --> Concat
            Head2["..."] -- "Head 2" --> Concat
            HeadN["单个注意力头的输出"] -- "Head N" --> Concat
        end
        
        Concat["拼接 Concatenate"] --> FinalLinear["最终线性层"]
        FinalLinear --> FinalOutput["多头注意力层最终输出"]
    end

    style A fill:#F9F7D8,stroke:#333,stroke-width:2px
    style Q_Linear fill:#BDE0FE,stroke:#333,stroke-width:1px
    style K_Linear fill:#C7E6C7,stroke:#333,stroke-width:1px
    style V_Linear fill:#FFD6A5,stroke:#333,stroke-width:1px
    style Q_Matrix fill:#BDE0FE,stroke:#333,stroke-width:2px
    style K_Matrix fill:#C7E6C7,stroke:#333,stroke-width:2px
    style V_Matrix fill:#FFD6A5,stroke:#333,stroke-width:2px
    style AttentionCalc fill:#FFFACD,stroke:#333,stroke-width:1px
    style Scale fill:#FFFACD,stroke:#333,stroke-width:1px
    style Softmax fill:#FFFACD,stroke:#333,stroke-width:1px
    style AttentionFilter fill:#FFC43D,stroke:#333,stroke-width:2px,color:black
    style ApplyAttention fill:#FFFACD,stroke:#333,stroke-width:1px
    style HeadOutput fill:#90EE90,stroke:#333,stroke-width:1.5px
    style HeadN fill:#90EE90,stroke:#333,stroke-width:1.5px
    style Concat fill:#FFB6C1,stroke:#333,stroke-width:1.5px
    style FinalLinear fill:#ADD8E6,stroke:#333,stroke-width:1.5px
    style FinalOutput fill:#66CDAA,stroke:#333,stroke-width:2px,color:white
</Mermaid_Diagram>

Content:
in the first video that is part zero of, in the first video that is part zero of, the series, the series, the series, we had got introduced to transformer, we had got introduced to transformer, we had got introduced to transformer, neural networks, neural networks, neural networks, in part one of the series we discussed, in part one of the series we discussed, in part one of the series we discussed, the importance of word order information, the importance of word order information, the importance of word order information, and introduced position embeddings so as, and introduced position embeddings so as, and introduced position embeddings so as, to understand that better, to understand that better, to understand that better, we had decided to build a fun dialogue, we had decided to build a fun dialogue, we had decided to build a fun dialogue, completer using transformers, completer using transformers, completer using transformers, and use it to complete cersei, and use it to complete cersei, and use it to complete cersei, lannister's famous dialogue, lannister's famous dialogue, lannister's famous dialogue, when he played the game of thrones, when he played the game of thrones, when he played the game of thrones, to do that we had passed our input text, to do that we had passed our input text, to do that we had passed our input text, to the embedding layer, to the embedding layer, to the embedding layer, we had then added the position, we had then added the position, we had then added the position, information to our word embeddings, information to our word embeddings, information to our word embeddings, and the resultant position aware, and the resultant position aware, and the resultant position aware, embeddings were passed to the next layer, embeddings were passed to the next layer, embeddings were passed to the next layer, in this video we will proceed further, in this video we will proceed further, in this video we will proceed further, and discuss the most important component, and discuss the most important component, and discuss the most important component, of transformer neural networks, of transformer neural networks, of transformer neural networks, the multi-head attention there which let, the multi-head attention there which let, the multi-head attention there which let, me warn you in advance, me warn you in advance, me warn you in advance, is a powerful multi-headed beast, is a powerful multi-headed beast, is a powerful multi-headed beast, let us begin by understanding why do we, let us begin by understanding why do we, let us begin by understanding why do we, need attention, need attention, need attention, imagine i showed you this line she faced, imagine i showed you this line she faced, imagine i showed you this line she faced, her enemies and whispered, her enemies and whispered, her enemies and whispered, drakatus and then i asked you which game, drakatus and then i asked you which game, drakatus and then i asked you which game, of thrones character are we referring to, of thrones character are we referring to, of thrones character are we referring to, here, here, here, to answer that question your mind will, to answer that question your mind will, to answer that question your mind will, most likely not pay equal attention to, most likely not pay equal attention to, most likely not pay equal attention to, all the words in the sentence, all the words in the sentence, all the words in the sentence, in fact if you've watched the show the, in fact if you've watched the show the, in fact if you've watched the show the, word jakartas should be enough to tell, word jakartas should be enough to tell, word jakartas should be enough to tell, you, you, you, that this person is none other than, that this person is none other than, that this person is none other than, daenerys targaryen who says that word, daenerys targaryen who says that word, daenerys targaryen who says that word, each time she wants her dragons to rain, each time she wants her dragons to rain, each time she wants her dragons to rain, fire on her enemies, fire on her enemies, fire on her enemies, and so the attention mechanism helps the, and so the attention mechanism helps the, and so the attention mechanism helps the, model to focus, model to focus, model to focus, on important words in a given input, on important words in a given input, on important words in a given input, sentence but transformers did not steal, sentence but transformers did not steal, sentence but transformers did not steal, the show, the show, the show, using simple attention their weapons of, using simple attention their weapons of, using simple attention their weapons of, choice, choice, choice, is called self-attention consider this, is called self-attention consider this, is called self-attention consider this, sentence, sentence, sentence, he went to the bank and learned of his, he went to the bank and learned of his, he went to the bank and learned of his, empty account, empty account, empty account, after which he went to a riverbank and, after which he went to a riverbank and, after which he went to a riverbank and, cried, cried, cried, note how the same word bank means two, note how the same word bank means two, note how the same word bank means two, different things, different things, different things, here the first occurrence of the word, here the first occurrence of the word, here the first occurrence of the word, bank is referring to, bank is referring to, bank is referring to, a finance institution while the second, a finance institution while the second, a finance institution while the second, bank, bank, bank, refers to the side of a river so how can, refers to the side of a river so how can, refers to the side of a river so how can, a model know, a model know, a model know, which bank refers to what well the same, which bank refers to what well the same, which bank refers to what well the same, way we humans do, way we humans do, way we humans do, we judge the meaning of a word by paying, we judge the meaning of a word by paying, we judge the meaning of a word by paying, attention to the context, attention to the context, attention to the context, in which it appears for instance, in which it appears for instance, in which it appears for instance, an empty account can indicate that the, an empty account can indicate that the, an empty account can indicate that the, first occurrence of the word bank means, first occurrence of the word bank means, first occurrence of the word bank means, a financial institute, a financial institute, a financial institute, the word river indicates that a second, the word river indicates that a second, the word river indicates that a second, occurrence, occurrence, occurrence, means a riverbank likewise the meaning, means a riverbank likewise the meaning, means a riverbank likewise the meaning, of every word can be regarded as the sum, of every word can be regarded as the sum, of every word can be regarded as the sum, of the words it pays the most, of the words it pays the most, of the words it pays the most, attention to now the difference between, attention to now the difference between, attention to now the difference between, simple attention, simple attention, simple attention, and self-attention is that simple, and self-attention is that simple, and self-attention is that simple, attention selectively focuses, attention selectively focuses, attention selectively focuses, on words with respect to some external, on words with respect to some external, on words with respect to some external, query, query, query, the more important a word is in, the more important a word is in, the more important a word is in, determining the answer to that query, determining the answer to that query, determining the answer to that query, the more focus it is given, the more focus it is given, the more focus it is given, self-attention on the other hand, self-attention on the other hand, self-attention on the other hand, also takes the relationship among words, also takes the relationship among words, also takes the relationship among words, within the same sentence, within the same sentence, within the same sentence, into account and this is the layer where, into account and this is the layer where, into account and this is the layer where, attention computations happen, attention computations happen, attention computations happen, so let us zoom in and understand the, so let us zoom in and understand the, so let us zoom in and understand the, components of the multi-head attention, components of the multi-head attention, components of the multi-head attention, layer, layer, layer, the first component in this block are, the first component in this block are, the first component in this block are, three linear layers, three linear layers, three linear layers, a linear layer is simply composed of a, a linear layer is simply composed of a, a linear layer is simply composed of a, bunch of fully connected neurons, bunch of fully connected neurons, bunch of fully connected neurons, without the activation function they may, without the activation function they may, without the activation function they may, serve two main purposes, serve two main purposes, serve two main purposes, number one mapping inputs onto the, number one mapping inputs onto the, number one mapping inputs onto the, outputs, outputs, outputs, number two changing the dimensions of, number two changing the dimensions of, number two changing the dimensions of, the inputs themselves, the inputs themselves, the inputs themselves, let us make this more concrete by, let us make this more concrete by, let us make this more concrete by, considering the example of a single, considering the example of a single, considering the example of a single, position aware, position aware, position aware, word embedding but first we'll need to, word embedding but first we'll need to, word embedding but first we'll need to, transpose it, transpose it, transpose it, before we feed it to the linear layer, before we feed it to the linear layer, before we feed it to the linear layer, there you go each of the five dimensions, there you go each of the five dimensions, there you go each of the five dimensions, of the embedding vector will get to the, of the embedding vector will get to the, of the embedding vector will get to the, linear layer nodes, linear layer nodes, linear layer nodes, as input note how the final output size, as input note how the final output size, as input note how the final output size, shrunk from 5 to 3., shrunk from 5 to 3., shrunk from 5 to 3., this is because in this example our, this is because in this example our, this is because in this example our, linear layer is composed of, linear layer is composed of, linear layer is composed of, only three neurons one of many reasons, only three neurons one of many reasons, only three neurons one of many reasons, why you may want to change especially, why you may want to change especially, why you may want to change especially, shrink the dimensions, shrink the dimensions, shrink the dimensions, of an embedding vector is to save on the, of an embedding vector is to save on the, of an embedding vector is to save on the, computation cost, computation cost, computation cost, the larger the vector the more operation, the larger the vector the more operation, the larger the vector the more operation, it requires, it requires, it requires, upon doing a little more scrutiny of the, upon doing a little more scrutiny of the, upon doing a little more scrutiny of the, linear layer, linear layer, linear layer, you'll see that each node is connected, you'll see that each node is connected, you'll see that each node is connected, to the input, to the input, to the input, using its own set of weights so what are, using its own set of weights so what are, using its own set of weights so what are, these weights, these weights, these weights, well these are just scalar numbers that, well these are just scalar numbers that, well these are just scalar numbers that, the model updates during back, the model updates during back, the model updates during back, propagation, propagation, propagation, as it gets better and better at the, as it gets better and better at the, as it gets better and better at the, downstream task which in our case, downstream task which in our case, downstream task which in our case, is dialog generation it is also, is dialog generation it is also, is dialog generation it is also, important to point, important to point, important to point, out that these weights are fed to the, out that these weights are fed to the, out that these weights are fed to the, model as a matrix, model as a matrix, model as a matrix, great so we are done looking at the, great so we are done looking at the, great so we are done looking at the, functionality of a single linear layer, functionality of a single linear layer, functionality of a single linear layer, but the transformers have three separate, but the transformers have three separate, but the transformers have three separate, linear, linear, linear, layers why is that turns out that each, layers why is that turns out that each, layers why is that turns out that each, one of these layers, one of these layers, one of these layers, has a special function we call them the, has a special function we call them the, has a special function we call them the, query, query, query, the key and the value linear layers, the key and the value linear layers, the key and the value linear layers, this can be partially motivated by the, this can be partially motivated by the, this can be partially motivated by the, way retrieval systems work, way retrieval systems work, way retrieval systems work, you often type search requests on, you often type search requests on, you often type search requests on, youtube don't you, youtube don't you, youtube don't you, let us call your request a query now let, let us call your request a query now let, let us call your request a query now let, us, us, us, assume that youtube search algorithm is, assume that youtube search algorithm is, assume that youtube search algorithm is, quite simplistic, quite simplistic, quite simplistic, what it does is simply go through all, what it does is simply go through all, what it does is simply go through all, video titles in its database, video titles in its database, video titles in its database, these titles can be termed as the keys, these titles can be termed as the keys, these titles can be termed as the keys, now so as to find the best matches it, now so as to find the best matches it, now so as to find the best matches it, will have to compute some sort of, will have to compute some sort of, will have to compute some sort of, similarity between your query, similarity between your query, similarity between your query, and the corresponding keys once the most, and the corresponding keys once the most, and the corresponding keys once the most, similar key has been found, similar key has been found, similar key has been found, it returns the video affiliated with, it returns the video affiliated with, it returns the video affiliated with, that key we will call the contents of a, that key we will call the contents of a, that key we will call the contents of a, video, video, video, its value notice how similarity can be, its value notice how similarity can be, its value notice how similarity can be, thought of, thought of, thought of, as a proxy to attention this is because, as a proxy to attention this is because, as a proxy to attention this is because, the model returns the best video, the model returns the best video, the model returns the best video, only by paying attention to the most, only by paying attention to the most, only by paying attention to the most, similar video title, similar video title, similar video title, when compared to the search query, when compared to the search query, when compared to the search query, great but how do we compute similarity, great but how do we compute similarity, great but how do we compute similarity, between, between, between, a query and a key a great way to compute, a query and a key a great way to compute, a query and a key a great way to compute, similarity between two vectors, similarity between two vectors, similarity between two vectors, is while the cosine similarity and since, is while the cosine similarity and since, is while the cosine similarity and since, we have been dealing with a bunch of, we have been dealing with a bunch of, we have been dealing with a bunch of, vectors anyway, vectors anyway, vectors anyway, why not use it cosine similarity varies, why not use it cosine similarity varies, why not use it cosine similarity varies, from a range of plus one to minus one, from a range of plus one to minus one, from a range of plus one to minus one, where plus one means two vectors point, where plus one means two vectors point, where plus one means two vectors point, in exactly the same direction, in exactly the same direction, in exactly the same direction, while -1 means they point in the, while -1 means they point in the, while -1 means they point in the, opposite direction, opposite direction, opposite direction, here is how it works if two vectors, here is how it works if two vectors, here is how it works if two vectors, point in exactly the same direction, point in exactly the same direction, point in exactly the same direction, the angle between them will be zero and, the angle between them will be zero and, the angle between them will be zero and, if you remember, if you remember, if you remember, your high school trigonometry cosine, your high school trigonometry cosine, your high school trigonometry cosine, zero is equal to one, zero is equal to one, zero is equal to one, that is these two vectors have maximum, that is these two vectors have maximum, that is these two vectors have maximum, similarity, similarity, similarity, now the cosine similarity reduces as the, now the cosine similarity reduces as the, now the cosine similarity reduces as the, vectors, vectors, vectors, start parting ways, start parting ways, start parting ways, and the maximum dissimilarity occurs if, and the maximum dissimilarity occurs if, and the maximum dissimilarity occurs if, they point in complete opposite, they point in complete opposite, they point in complete opposite, directions, directions, directions, cosine similarity between two vectors, cosine similarity between two vectors, cosine similarity between two vectors, can also be obtained, can also be obtained, can also be obtained, by taking the dot product between the, by taking the dot product between the, by taking the dot product between the, elements of the two vectors, elements of the two vectors, elements of the two vectors, and then dividing by their magnitudes, and then dividing by their magnitudes, and then dividing by their magnitudes, for scaling purposes, for scaling purposes, for scaling purposes, more generally we can rewrite our, more generally we can rewrite our, more generally we can rewrite our, equation like so, equation like so, equation like so, now if you are to compute the similarity, now if you are to compute the similarity, now if you are to compute the similarity, between matrix elements instead of, between matrix elements instead of, between matrix elements instead of, vectors, vectors, vectors, we'll have to transpose the second, we'll have to transpose the second, we'll have to transpose the second, matrix to avoid conflicts in dimensions, matrix to avoid conflicts in dimensions, matrix to avoid conflicts in dimensions, during matrix multiplications, during matrix multiplications, during matrix multiplications, and since we are dealing with queries, and since we are dealing with queries, and since we are dealing with queries, and keys let us plug, and keys let us plug, and keys let us plug, those in in our equation awesome, those in in our equation awesome, those in in our equation awesome, but how does that tie back to our, but how does that tie back to our, but how does that tie back to our, attention layer, attention layer, attention layer, and what exactly should we fit to our, and what exactly should we fit to our, and what exactly should we fit to our, query key, query key, query key, and value linear layers well, and value linear layers well, and value linear layers well, to the query layer we feed our position, to the query layer we feed our position, to the query layer we feed our position, aware embeddings, aware embeddings, aware embeddings, we then make two more copies of our, we then make two more copies of our, we then make two more copies of our, embeddings and feed the same to the key, embeddings and feed the same to the key, embeddings and feed the same to the key, and the value layers, and the value layers, and the value layers, i know that makes no sense because, i know that makes no sense because, i know that makes no sense because, in the youtube example didn't the, in the youtube example didn't the, in the youtube example didn't the, queries and the keys and the values mean, queries and the keys and the values mean, queries and the keys and the values mean, different things and had very different, different things and had very different, different things and had very different, contents, contents, contents, so why then here are we using the same, so why then here are we using the same, so why then here are we using the same, content as, content as, content as, input to the query key and the value, input to the query key and the value, input to the query key and the value, layers, layers, layers, well that is where the self-attention, well that is where the self-attention, well that is where the self-attention, part comes into play, part comes into play, part comes into play, let us take our three embedding copies, let us take our three embedding copies, let us take our three embedding copies, and neatly put them on the side, and neatly put them on the side, and neatly put them on the side, we then pass them through each of the, we then pass them through each of the, we then pass them through each of the, linear layers, linear layers, linear layers, and all that means is that we multiply, and all that means is that we multiply, and all that means is that we multiply, our embedding layers with the weights of, our embedding layers with the weights of, our embedding layers with the weights of, the linear layers, the linear layers, the linear layers, note that each linear layer has its own, note that each linear layer has its own, note that each linear layer has its own, set of weights, set of weights, set of weights, since the matrix multiplication will, since the matrix multiplication will, since the matrix multiplication will, require certain dimensions, require certain dimensions, require certain dimensions, we will have to transpose our embedding, we will have to transpose our embedding, we will have to transpose our embedding, matrices accordingly, matrices accordingly, matrices accordingly, after multiplication each linear layer, after multiplication each linear layer, after multiplication each linear layer, outputs a new matrix, outputs a new matrix, outputs a new matrix, and these are called the query the key, and these are called the query the key, and these are called the query the key, and the value matrices for now, and the value matrices for now, and the value matrices for now, let us focus on only the query and the, let us focus on only the query and the, let us focus on only the query and the, key matrices because remember, key matrices because remember, key matrices because remember, how only the query and the keys were, how only the query and the keys were, how only the query and the keys were, used in our video retrieval example to, used in our video retrieval example to, used in our video retrieval example to, compute the similarity, we therefore first do a simple dot, we therefore first do a simple dot, product between, product between, product between, our query and the transpose of our key, our query and the transpose of our key, our query and the transpose of our key, matrix, matrix, matrix, the output of this dot product can be, the output of this dot product can be, the output of this dot product can be, called an attention filter, since this is a very important output, since this is a very important output, let us zoom in, let us zoom in, let us zoom in, and understand its contents at the start, and understand its contents at the start, and understand its contents at the start, of the training process the contents of, of the training process the contents of, of the training process the contents of, the attention filter are more or less, the attention filter are more or less, the attention filter are more or less, random numbers, random numbers, random numbers, but once the training process is done, but once the training process is done, but once the training process is done, they take on more meaningful values, they take on more meaningful values, they take on more meaningful values, if you look closely the scores inside, if you look closely the scores inside, if you look closely the scores inside, this matrix are in fact, this matrix are in fact, this matrix are in fact, attention scores for example let us, attention scores for example let us, attention scores for example let us, consider the row corresponding to the, consider the row corresponding to the, consider the row corresponding to the, word, word, word, game the highest attention a word pace, game the highest attention a word pace, game the highest attention a word pace, is usually, is usually, is usually, to itself for it is the most similar to, to itself for it is the most similar to, to itself for it is the most similar to, itself, itself, itself, the next highest attention score is, the next highest attention score is, the next highest attention score is, given to the word which is the next most, given to the word which is the next most, given to the word which is the next most, similar to it, similar to it, similar to it, in this example that word is play, in this example that word is play, in this example that word is play, finally we scale our attention scores, finally we scale our attention scores, finally we scale our attention scores, the authors of the attention is all, the authors of the attention is all, the authors of the attention is all, unique paper, unique paper, unique paper, divided the score by the dimension of, divided the score by the dimension of, divided the score by the dimension of, the key vector, the key vector, the key vector, in our example that is seven, finally we squash our attention scores, finally we squash our attention scores, between the values of 0, between the values of 0, between the values of 0, and 1 using a soft mass function, and 1 using a soft mass function, and 1 using a soft mass function, and we get our final attention filter, let us take a quick recap of what we've, let us take a quick recap of what we've, done so far we created three copies of, done so far we created three copies of, done so far we created three copies of, our, our, our, embeddings we pass one of those, embeddings we pass one of those, embeddings we pass one of those, duplicates through the valley linear, duplicates through the valley linear, duplicates through the valley linear, layer, layer, layer, and got a value matrix we pass the, and got a value matrix we pass the, and got a value matrix we pass the, remaining embedding copies through the, remaining embedding copies through the, remaining embedding copies through the, query and the, query and the, query and the, key linear layers and scale the results, key linear layers and scale the results, key linear layers and scale the results, to get an attention filter, to get an attention filter, to get an attention filter, so we now have our original value matrix, so we now have our original value matrix, so we now have our original value matrix, which pretty much represents the, which pretty much represents the, which pretty much represents the, original embedding information, original embedding information, original embedding information, because we did not alter them much, because we did not alter them much, because we did not alter them much, except for passing them through a single, except for passing them through a single, except for passing them through a single, linear layer on the other hand we have, linear layer on the other hand we have, linear layer on the other hand we have, our attention filter, our attention filter, our attention filter, with attention scores that were computed, with attention scores that were computed, with attention scores that were computed, using the dot product, using the dot product, using the dot product, between the query and the key matrices, between the query and the key matrices, between the query and the key matrices, all right great, all right great, all right great, but why on earth did we go through all, but why on earth did we go through all, but why on earth did we go through all, of this, of this, of this, what is the purpose of this attention, what is the purpose of this attention, what is the purpose of this attention, filter what is the point of this, filter what is the point of this, filter what is the point of this, value matrix well even though we are, value matrix well even though we are, value matrix well even though we are, dealing with nlp, dealing with nlp, dealing with nlp, it is much easier to understand the, it is much easier to understand the, it is much easier to understand the, intuition, intuition, intuition, behind this using computer vision, behind this using computer vision, behind this using computer vision, suppose you are avatar ang from the most, suppose you are avatar ang from the most, suppose you are avatar ang from the most, awesome anime on the planet, awesome anime on the planet, awesome anime on the planet, avatar the last airbender anyhow so you, avatar the last airbender anyhow so you, avatar the last airbender anyhow so you, are going about, are going about, are going about, doing your avatar business that suddenly, doing your avatar business that suddenly, doing your avatar business that suddenly, you see this now if your brain were to, you see this now if your brain were to, you see this now if your brain were to, process, process, process, all the information in the frame pixel, all the information in the frame pixel, all the information in the frame pixel, by pixel, by pixel, by pixel, by the time you reach just 10 percent of, by the time you reach just 10 percent of, by the time you reach just 10 percent of, the frame, the frame, the frame, you will most likely be blasted off your, you will most likely be blasted off your, you will most likely be blasted off your, skin by lightning bolts, skin by lightning bolts, skin by lightning bolts, that is because this is none other than, that is because this is none other than, that is because this is none other than, princess azila of the fire nation, princess azila of the fire nation, princess azila of the fire nation, the most awesome evil princes there ever, the most awesome evil princes there ever, the most awesome evil princes there ever, lived, lived, lived, yes cersei you better be scared, yes cersei you better be scared, yes cersei you better be scared, and therefore thank god that our brains, and therefore thank god that our brains, and therefore thank god that our brains, work much faster, work much faster, work much faster, rather than taking all the information, rather than taking all the information, rather than taking all the information, in, in, in, they filtered out the unnecessary, they filtered out the unnecessary, they filtered out the unnecessary, background information, background information, background information, and zoom in to focus on what truly, and zoom in to focus on what truly, and zoom in to focus on what truly, matters in that moment, matters in that moment, matters in that moment, and in that context and if you look, and in that context and if you look, and in that context and if you look, closely, closely, closely, the final filtered image is actually a, the final filtered image is actually a, the final filtered image is actually a, combination of two frames, combination of two frames, combination of two frames, an attention filter and the original, an attention filter and the original, an attention filter and the original, image, image, image, that is if you multiply the pixel of the, that is if you multiply the pixel of the, that is if you multiply the pixel of the, attention filter with the original image, attention filter with the original image, attention filter with the original image, you get the filtered image with all the, you get the filtered image with all the, you get the filtered image with all the, unnecessary, unnecessary, unnecessary, details thrown away, details thrown away, details thrown away, in the same way when we multiply our, in the same way when we multiply our, in the same way when we multiply our, attention filter with the valley matrix, attention filter with the valley matrix, attention filter with the valley matrix, we get a filter value matrix which, we get a filter value matrix which, we get a filter value matrix which, assigns high focus, assigns high focus, assigns high focus, to the features that are more important, to the features that are more important, to the features that are more important, and this filtered value matrix is the, and this filtered value matrix is the, and this filtered value matrix is the, final output of our multi-head attention, final output of our multi-head attention, final output of our multi-head attention, layer, layer, layer, well almost there is one more detail, well almost there is one more detail, well almost there is one more detail, remember how we said that the multi-head, remember how we said that the multi-head, remember how we said that the multi-head, attention is a multi-headed beast, attention is a multi-headed beast, attention is a multi-headed beast, we have currently dead with just a, we have currently dead with just a, we have currently dead with just a, single head there are two more heads to, single head there are two more heads to, single head there are two more heads to, go, go, go, so why do we need more than one head, so why do we need more than one head, so why do we need more than one head, let us get back to our princess azula's, let us get back to our princess azula's, let us get back to our princess azula's, example, example, example, the first filter had helped us to focus, the first filter had helped us to focus, the first filter had helped us to focus, on who was in the image, on who was in the image, on who was in the image, but as i told you before she's a super, but as i told you before she's a super, but as i told you before she's a super, villain and so if you, villain and so if you, villain and so if you, are the avatar you may want to weigh, are the avatar you may want to weigh, are the avatar you may want to weigh, your options carefully, your options carefully, your options carefully, maybe you may also want to focus on the, maybe you may also want to focus on the, maybe you may also want to focus on the, clear skies behind her, clear skies behind her, clear skies behind her, to see if you could use some of your, to see if you could use some of your, to see if you could use some of your, awesome airbending skills to blow her, awesome airbending skills to blow her, awesome airbending skills to blow her, away, away, away, or maybe you could also use your earth, or maybe you could also use your earth, or maybe you could also use your earth, bending skills to bury her under those, bending skills to bury her under those, bending skills to bury her under those, rocks, rocks, rocks, similarly transformers don't learn one, similarly transformers don't learn one, similarly transformers don't learn one, attention filter, attention filter, attention filter, they learn multiple each focusing on a, they learn multiple each focusing on a, they learn multiple each focusing on a, different linguistic phenomenon, different linguistic phenomenon, different linguistic phenomenon, each attention head therefore outputs, each attention head therefore outputs, each attention head therefore outputs, its own attention filter, its own attention filter, its own attention filter, which in turns outputs its own filtered, which in turns outputs its own filtered, which in turns outputs its own filtered, value matrix, value matrix, value matrix, each zooming in on a different, each zooming in on a different, each zooming in on a different, combination of linguistic features, combination of linguistic features, combination of linguistic features, in the original transformers paper the, in the original transformers paper the, in the original transformers paper the, authors used a total of eight attention, authors used a total of eight attention, authors used a total of eight attention, heads, heads, heads, we will however stick to three, we will however stick to three, we will however stick to three, awesome so what do we do next we simply, awesome so what do we do next we simply, awesome so what do we do next we simply, go ahead and concatenate them together, since we don't want this vector to grow, since we don't want this vector to grow, longer and longer with each head used, longer and longer with each head used, longer and longer with each head used, we pass it through a linear layer to, we pass it through a linear layer to, we pass it through a linear layer to, shrink its size back to seven, shrink its size back to seven, shrink its size back to seven, by five and this is the final output, by five and this is the final output, by five and this is the final output, of the multi-head attention layer, of the multi-head attention layer, of the multi-head attention layer, that's it it is on the back of these, that's it it is on the back of these, that's it it is on the back of these, multi-headed dragons, multi-headed dragons, multi-headed dragons, that transformers were able to conquer, that transformers were able to conquer, that transformers were able to conquer, the kingdom of nlp in the third and the, the kingdom of nlp in the third and the, the kingdom of nlp in the third and the, final part of this series, final part of this series, final part of this series, we will discuss the remaining components, we will discuss the remaining components, we will discuss the remaining components, as well as the transformers decoders, as well as the transformers decoders, as well as the transformers decoders, must attention model, must attention model, must attention model, until then valar dojades
