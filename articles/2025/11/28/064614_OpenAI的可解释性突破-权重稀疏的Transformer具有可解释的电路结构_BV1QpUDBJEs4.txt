Timestamp: 2025-11-28T06:46:14.889933
Title: OpenAI的可解释性突破-权重稀疏的Transformer具有可解释的电路结构 BV1QpUDBJEs4
URL: https://b23.tv/MposTNw
Status: success
Duration: 47:03

Description:
好的，这是根据您提供的文本提炼和总结的核心思想。

### **论文核心思想与方法总结**

#### **1. 核心论点 (一句话总结)**
通过训练权重高度稀疏的Transformer模型，可以获得与人类自然概念对齐的、紧凑且可解释的内部计算电路，从而在可接受的性能损失范围内，极大地提升了语言模型的可解释性。

#### **2. 整体框架 (Overarching Framework)**
该研究提出了一套完整的框架，旨在解决现有大语言模型（稠密模型）的“黑箱”问题。其核心流程是：
**训练 (Training)** → **分离 (Pruning)** → **验证 (Validation)** → **迁移 (Bridging)**
首先，通过引入“权重稀疏性”约束来训练一种本质上更简单的Transformer模型。然后，利用结构化剪枝算法，从该模型中分离出执行特定任务的“最小电路”。接着，通过一系列实验验证这些电路的有效性和可解释性。最后，提出“桥接”方法，尝试将从稀疏模型中获得的可解释性洞察迁移并应用于解释现有的稠密模型。

---

### **大纲式摘要**

#### **I. 问题与动机**
*   **现有模型的困境**: 传统的稠密Transformer模型因其复杂的权重和激活分布而难以理解，特别是“叠加现象”（Superposition）使得单个神经元可能编码多个混合概念，阻碍了机械可解释性研究。
*   **研究目标**: 逆向工程语言模型内部的算法机制，获得人类可理解的、任务专属的神经电路。

#### **II. 核心方法论**
1.  **全中稀疏训练 (Weight-Sparsity Training)**
    *   **核心思想**: 强制模型在训练时将绝大多数权重参数置零，使得每个神经元仅与少数通道连接。
    *   **效果**: 促使模型学习到更紧凑、更独立的计算结构来表达单一概念，从而自然地形成稀疏的激活模式和更清晰的特征表示。

2.  **结构化剪枝与电路提取 (Structural Pruning & Circuit Extraction)**
    *   **方法**: 提出一种学习型的结构化剪枝算法，针对每个具体任务（如括号匹配、变量类型追踪等），自动找出完成该任务所需的最小核心节点和连接（即“最小电路”）。
    *   **度量标准**: 以电路中的“边数”（非零权重连接数）作为可解释性的核心量化指标。

3.  **能力与可解释性的权衡 (Capability-Interpretability Trade-off)**
    *   **权衡关系**: 模型的稀疏性越强，其可解释性越高（电路规模越小），但通常会伴随模型能力的下降。
    *   **解决方案**: **扩大模型总参数规模**。实验证明，增加模型总参数量可以在保持甚至提升可解释性的同时，改善模型的能力，将“能力-可解释性”帕累托前沿向更优的方向推动。

4.  **“桥接”方法解释稠密模型 (Bridging to Explain Dense Models)**
    *   **目标**: 将稀疏模型的可解释性优势迁移到已有的、性能更强的稠密模型上。
    *   **机制**: 在稀疏模型和稠密模型的每一层之间训练线性“桥接器”（Encoder-Decoder），实现两者激活空间的相互映射。通过这种方式，稀疏模型可以作为稠密模型的一个可解释的“代理”，允许研究者通过干预稀疏电路来编辑和理解稠密模型的行为。

#### **III. 关键实验发现与验证**
*   **电路规模显著减小**: 在相同的预训练损失下，稀疏模型完成任务所需的最小电路规模平均仅为稠密模型的**1/16**。
*   **电路与自然概念对应**: 提取出的电路节点与人类可理解的简单概念高度对应（如引号检测器、括号嵌套深度计数器），连接关系直观。
*   **严格验证**: 通过节点消融实验和逆剪枝实验，证明了所提取电路的**必要性**（删除电路节点导致性能骤降）和**充分性**（仅保留电路节点性能几乎不变）。
*   **模型编辑成功**: 利用“桥接”方法，成功地通过修改稀疏模型中的激活状态，实现了对稠密模型行为的可控编辑（如改变其对引号类型的判断）。

#### **IV. 模型局限性**
*   **计算效率低**: 稀疏模型的训练和推理计算量远高于同等能力的稠密模型。
*   **扩展性挑战**: 方法目前主要在小模型和简单任务上验证，扩展到更复杂的模型和任务时，电路规模和复杂性可能急剧增加。
*   **特征冗余**: 电路中仍存在部分概念分布在多个节点上的情况，未能实现完全的单一化。

<Mermaid_Diagram>
graph TD
    subgraph "问题与动机: Transformer的黑箱特性"
        A["稠密Transformer模型"] -- "存在" --> B{"理解困难"};
        B -- "源于" --> C["复杂的权重与激活"];
        B -- "源于" --> D["叠加现象 (Superposition)"];
        style A fill:#FFB6C1,stroke:#A52A2A,stroke-width:2px
        style B fill:#FF6347,stroke:#A52A2A,stroke-width:2px
        style C fill:#FFE4E1,stroke:#A52A2A
        style D fill:#FFE4E1,stroke:#A52A2A
    end

    subgraph "核心方法论: 走向可解释性"
        E["全中稀疏训练 (Weight-Sparsity Training)"] -- "强制大部分权重为0" --> F["获得结构更简单的稀疏模型"];
        F -- "针对特定任务" --> G["结构化剪枝 (Structural Pruning)"];
        G -- "提取" --> H["最小任务电路 (Minimal Circuit)"];
        F -- "可用于解释" --> A;
        I["桥接方法 (Bridges)"] -- "连接" --> F;
        I -- "连接" --> A;
        style E fill:#90EE90,stroke:#2E8B57,stroke-width:3px
        style F fill:#98FB98,stroke:#2E8B57,stroke-width:2px
        style G fill:#ADD8E6,stroke:#4682B4,stroke-width:2px
        style H fill:#E0FFFF,stroke:#4682B4
        style I fill:#ADD8E6,stroke:#4682B4,stroke-width:2px
    end

    subgraph "关键成果与应用"
        H -- "具备" --> J["高度可解释性"];
        J -- "表现为" --> K["规模小 (约为稠密模型的1/16)"];
        J -- "表现为" --> L["节点与自然概念对应"];
        I -- "实现" --> M["对稠密模型的可解释性编辑"];
        style J fill:#FFD700,stroke:#B8860B,stroke-width:2px
        style K fill:#FFFACD,stroke:#B8860B
        style L fill:#FFFACD,stroke:#B8860B
        style M fill:#FFFACD,stroke:#B8860B
    end

    subgraph "权衡与挑战"
        E -- "带来" --> N{"能力-可解释性权衡"};
        N -- "表现为" --> O["稀疏性增强 vs 性能下降"];
        P["扩大模型总参数"] -- "缓解" --> N;
        F -- "面临" --> Q["计算效率低"];
        H -- "面临" --> R["向复杂模型扩展的挑战"];
        style N fill:#F0E68C,stroke:#BDB76B,stroke-width:2px
        style O fill:#FFFFE0,stroke:#BDB76B
        style P fill:#98FB98,stroke:#2E8B57
        style Q fill:#D3D3D3,stroke:#696969
        style R fill:#D3D3D3,stroke:#696969
    end

    A -- "提出解决方案" --> E;
</Mermaid_Diagram>

Content:
OpenIy最新的可解释性研究重新训练后的全中吸溯网络在通过截止分离后得到的最小电路通常可以和自然概念相对应且连接关系很直观大家好这里给大家介绍一篇名为WATES PARTS TRANSFORMERS have interpretable circuits的论文中文明可以翻译成全中吸溯的Transformer具有可解释的电路结构有OpenI, San Francisco, California, United States,出品本文提出通过将大部分全中智零训练Transformer从而获得人类可理解的电路结构提高可解释性但略有性能损失该方法经过验证并可用于解释筹密模型关于摘要本文提出通过训练全中吸溯的Transformer模型使其内部电路更易与人类理解并通过简直方法分离出执行特定任务的最小电路发现这些电路通常与自然概念对应且连接关系直观全中吸溯性提升了可解释性但降低了模型能力模型规模扩大则改善了能力可解释性全行此外还初步展示了将该方法应用于解释线有筹密模型的可能性通过全中吸溯训练使Transformer模型内部电路更易与人类理解采用简直方法分离出与特定任务相关的最小电路发现其余自然概念高度对应全中吸溯性提升了模型可解释性但降低了能力扩大模型规模可改善能力可解释性全行就模型规模扩大后吸溯模型在保持可解释性的同时能力提升但超过数千万飞灵参数后人面临挑战技机型可初步应用于解释线有筹密模型并通过严格验证提升了电路的可理解性UpStrike部分包含一张图片图一展示了该论文提出的整体方法流程该图氛为两个步骤第一步是训练全中吸溯的Transformer模型通过约束大部分全中为0使每个神经园仅有少数连接从而提升电路的可解释性第二步针对每个精心设计的任务通过修减减汁模型找出完成该任务所需的核心节点和连接并将被修减的节点用期在预训练分布上的平均机活植进行替代该图将论文关注的问题如何获得具有人类可理解性的语言模型内部电路直观的表达出来具体说明了通过全中吸溯和任务专属简直来明确模型执行特定行为的底层计算结构为机械可解释性研究提供了更易于理解的底层电路关于导演本文指在通过全中吸溯训练和简直方法提升Transformer模型的可解释性皆是其内部算法机制当前神经网路如大语言模型能力提升迅速但内部工作机制仍难以理解机械可解释性领域试图逆向工程模型算法Transformer模型的机活和全中难以直接理解部分原因是超位置现象及稠密模型竞进似于更大吸溯网路的计算现有方法多通过机活吸溯化和抽象化获得可解释电路但网网只部分理解复杂计算导致电路机反应抽象也反应真实机制本文提出全中吸溯训练新范视使绝大多数全中为0每个神经园仅有少量连接从而获得更简单可完全理解的电路Introduction部分包含两张图片土耳展示了吸溯Transformer模型相比于传统筹密Transformer在可解释性方面的显著优势具体而言该图比较了在相同预训练损失下吸溯模型和筹密模型完成任务所需的最小回路Serkat规模横周表示简直后的回路规模及可解释性指标纵轴则为任务损失结果显示吸溯模型在任何给定损失水平下其任务特定回路规模源小于筹密模型平均约为筹密模型的16分之一这说明通过全中吸溯化训练模型确实能够学习到更为紧凑和易与解读的任务回路有助于理解模型内部的推理过程土耳在论文中解决的问题是如何通过结构优化提升大语言模型的可解释性使每个任务都被独立切简洁的神经回路所实现从而使模型预测更透明一种涂膛展示了全中吸溯模型在增加总参数数量时对能力和可解释性之间的怕类拖前言的提升作用涂中横周为预训练损失代表模型能力纵轴为经过简直后的最小回路大小代表模型可解释性不同颜色对应总参数数量的不同点的大小表示L0泛速表示吸溯度数值越小代表参数越吸溯涂中驱线显示减小模型的L0泛速使模型更加吸溯会提升可解释性但会以吸身能力为代价而提升总参数量则可以在能力不下降的情况下进一步提升可解释性这能力可解释性怕类拖前言整体向下左方移动机down and to the left is better从而卫训练具备更好可解释性的transformer模型提供了重要的起发与时镇基础这里讨论导演部分的提到的几个重点问题问题1为什么现有的transformer模型难以被人类理解现有的transformer模型难以被人类理解主要原因在于奇迹活和全中分布高度复杂且密集神经缘的激活模式往往与人类可理解的概念不直接对应此外存在所谓的叠加现象superposition集模型内部的表示是多个概念的混合导致很难将具体的神经缘或通道与单一的语忆或功能对应起来布体问题2本文提出了什么核心方法来提升transformer模型的可解释性本文提出通过训练全中系数withpass的transformer模型来提升可解释性具体做法是将大部分全中之零使每个神经缘只与少数通道相连从而强制模型用更少的神经缘和连接来表达单一概念随后针对具体任务通过简直方法隔离出最小的可独立执行任务的电路并验证这些电路的必要性和充分性B-GIRT-WON-T3全中系数transformer模型再可解释性和能力之间存在哪些全痕如何解决全中系数transformer模型虽然极大提升了可解释性但会以一定的能力损失为代价及模型性能下降实验发现模型越吸收可解释性越强但能力越弱扩大模型规模增加总参数量可以在一定程度上缓解这群痕提升能力可解释性前言此外作者还初步探索了将系数方法应用于已有的密集模型通过在美层建立桥接使吸收模型能解释原有密集模型的行为关于核心算法核心算法部分本文提出全中吸收transformer通过大幅度吸收化全中和激火实现可解释的紧凑电路并用结构化简直方法提取最小任务电路提升模型可解释性采用GPT2结构的解码器是transformer所有全中和编制军强制吸收化最吸收模型仅以千分之一全中飞灵激火也有四分之一吸收度通过结构化简直算法学习节点也马联合优化任务损失和电路规模提取实现特定任务的最小吸收电路模型在拍摄代码数聚集上预训练任务为二分类的下一个偷坑预测覆盖辨量类型追踪扩好欠套技术等20个简单任务提出的吸收模型在相同损失下最小电路规模比筹密模型小约16倍且电路节点与人类可理解的概念高度对应通过桥接机制将吸收模型与一有筹密模型的激火对齐使吸收模型可作为筹密模型的可解释替代并用多种损失像联合训练MESSA的部分包含四张图片图四展示了全中吸收变换器Transformer在实现自服穿壁和任务时的具体电路结构途中细致会置了完成该任务所需的全部12个节点和9条边并没有省略任何细节其和新思路式首先临MLP将偷坑的切入转化为Code Detector引号检测器和Code Type ClassifierCellan 引号类型分类器残差通道这些通道分别有时AT中的Key和Value通道读取接下来后续的偷坑会通过注意力机制点制力Value方式关注关键偷坑并复制其Value以预测对应的必和引号途中每个输入偷坑下方数值排列的线数代表其残差流左侧展示了在点行任务一样的一种重要节点的机火值虚线水平线表示Transformer的成边界叫太表示飙亮惩罚直接合并的线表示飙亮加法黑色数字为通道或神经缘的序号红色与蓝色数字分别标示政府全重或偏制整个电路图纸展示了用于该具体任务的相关注意力录进其他为机火或非本任务相关的部分被灰色处理为展线图示接受了全中叙触化处理后Transformer内部推理录进的高度课解释性直接应设为完成自服串笔和任务的清洗纸结构图物展示了用于技术切套深度的电路的简化式意图根据论文上下文图物采用了figure四中的标记和布局方式说明了全中吸收Transformer在处理切套列表结构时的具体内部机制图中的电路围绕一个特定的注意力直通道展开该通道充当左扩号检测器由Token被叫的In�的饮飌延伸而来注意力头会在上下文中对这个简测器的直径型均直操作并将结果写入每个Token的残差流中这个累积直击切套深度后续的注意力头通过查询通道读取切套深度并通过预知操作使其只在切套列表内部机火该电路总计使用了7个节点和4条边明确接受了模型如何通过吸收结构来实现可解释的切套深度跟踪理解该图展示的算法属于对模型进行针对干扰上的对抗性攻击并且在论文中用于解释全中吸收Transformer在Passen代码解析任务中具备的可解释性优势土六展设了一个用于跟踪辨量类型的电路的粗略系统其和新目的是解释吸收全中Transformer在处理可解释性任务实模型内部的运算机制该图详细标注了模型如何利用两个注意力头Attention Hat进行两条式的计算具体来说模型首先将辨量名current复制到SatToken中然后再一次作为key使得模型能够在最后一个Token位置复制SatToken的值并具此读出正确答案这一过程设计四个queryK 通道和三个value通道实现了对辨量类型的追踪为解释Transformer内部结构可解释性提供了直观的电路分析方法对理解Transformer在Passen代码辨量类型时别任务上的决策机制具有重要意义突击展示了密集模型Dance Model和西苏模型LispArts Model之间通过了解Bridge模块进行连接和血通训练的机制具体而言作者提出在正常于训练损失基础上引入多个了解损失效包括规矣化军方物插损失LME该损失向用于训练了解编马器使其能够有效地从密集模型的机火预测西苏模型的机火反之依然了解解马器实现西苏到密集的转换此外,作者还利用了解模块在部分曾之间做混合前向传播通过了解将某个位置的机火从一种模型转换到另一种模型从而实现混合模型在于训练任务下保持良好性能此方法解决了密集模型到西苏模型转换过程中先验对其核心能损失问题保证了模型转换的可解实性和任务表现的一致性这里讨论核心算法部分的提到的几个重点问题为什么全中西苏的Transformer模型在可解释性上由于筹密模型全中西苏的Transformer模型通过将大部分参数之零强制每个神经园只能读取或写入少数残差通道从而限制了模型将概念分散在多个通道或使用过多神经园表示单一概念的能力这种结构促使模型学习出更为紧凑独立的任务特定电路实验表明在相同的预训练损失下西苏模型实现任务所需的最小电路规模比筹密模型小约16倍且这些电路中的神经园几乎往往对应于简单直观的概念如单引号后跟著Token获猎表现套深度全面全中则编码了这些概念之间的直观联系因此西苏模型的行为更容易被拆解和理解问题2 论文提出了哪些方法来量化和验证模型的可解实性论文通过以下方法量化和验证模型的可解实性议题出了一种新影的结构化简直算法针对每个任务简直得到能达到目标损失的最小电路并以电路中的边数级飞灵权种连接数作为主要的可解实性定量指标二通过对手工设计的20个简单拍摄上二分类任务比较筹密与西苏模型在相同损失下的最小电路规模3 通过节点销容实验验证电路的必要性和充分性警保流电路内节点时任务性能几乎不变而删除电路内节点则性能严重下降证明这些紧凑电路真实沉在了模型的决策逻辑问题3 如何将西苏模型的可解释性优势迁移到已有的筹密模型上为了解释已有的筹密模型论文提出了巧结 Bridges 乔方法在每一子层如每个attention和MLP前训练一组巧结器将筹密模型的几乎的应射到西苏模型的几乎空空在应射回去每个巧结器由现性编码器将筹密机火转为西苏机火和现性结码器反向转换组成训练时除了常规的预训练损失还引入了巧结损失如规依化 S1并通过混合前相传播在不同位置切换筹密输出机火来优化使得西苏模型能在功能上负现筹密模型的行为这样西苏模型可作为筹密模型的可解释替代品便与分析和理解原模型的内部机制关于数据实验数据实验部分通过对比西苏和筹密Transformer模型实验展示了西苏模型在可解释性和电路规模上的优势并分析了具体任务下的电路结构和巧结方法被西苏Transformer模型经过简之后其电路规模约为筹密模型的16分之一显著提升了可解释性在具体任务如自符串币和扩号牵涛技术变量类型追踪中西苏模型能提取出结构清晰可人工理解的电路出西苏模型的特征质量与机火西苏性密切相关增加全中西苏性会提升残差留机火的西苏性将通巧结方法可将西苏模型的可解释现优势千一到已有的筹密模型实现对筹密模型行为的可控编辑实验发现模型规模扩大时西苏模型在能力和可解释性上均有提升且西苏电路更易于追踪和理解Experiment部分包含两张图片图巴展示了全中西苏Transformer在扩号技术电路上的对抗样本实验结果接受了模型在处理长列表示的显著上下文西式现象左图横轴为列表元素格数纵轴为模型预测正确币和扩号技术出将的概率曲线显示随著列表长度增加模型正确率持续下降并在某一域之后低于随机水平又图显示同一实验请进下负责技术的特征机活强度 Activation 2ATRizzi Delita 1249随著上下文长度增加逐渐减弱通过这些可解释的电路分析作者发现由于模型利用同一特征的强弱机活分别编码不同扩号欠套深度并对整个上下文家全平均导致增加上下文长度如插入更多Token会显著削弱关键特征的机活幅度造成模型预测能力下降这已发现为Transformer在扩号技术等任务终于被利用的结构脆弱性提供了解释性分析与论文第三2.2结对电路工作的理解相呼应土旧展示了研究者利用吸收模型与桥阶方法对现有蠢密模型的表示进行可解释性修改再次途中包含两组实验结果分别对应于两个任务单双引号任务与Wallwritten处任务左侧图表展试了在修改筹密模型的引号类型分类企机活状态后模型输出单引号的概率随引导强度上升而显著增加这说明通过桥阶并调节模型机活可以让筹密模型向被提示输入单引号一样响应从而成功的编辑了模型对引号类型的内部表真又侧图表则显示了当模型处理以EFWild or Exept开头的代码形式通过对MLPL曾输入通道进行操控使模型行为更像是在处理Wild处而不是Return处通过该操作模型输出貌号的概率提升虽然幅度低于左侧任务但依然反映了对模型型墨判定机制的部分成功修改总体而言土酒证明了吸触模型桥阶方法能够实现对筹密模型表示的可解视性编辑对比不同任务下模型响应验证了该方法的有效性这里讨论数据实验部分的提到的几个重点问题问题1全中吸触性如何提升Transformer模型的可解视性全中吸触性通过减少模型中非灵参数的数量使得模型能够学习更小更局不化的计算回路实验结果显示吸触模型在相同预训练损失下经过减之后其会录规模比密集模型小约16倍这意味著吸触模型中的简单行为回路更加结偶和异于定位此外吸触性还提升了特征的质量和激活吸触性使得模型内部的计算过程更容易被人类理解和追踪不讨论题2如何通过瞧接方法将吸触模型的可解释信迁遗到已有的密集模型论文提出了瞧接bridge方法通过训练一组现心应射将吸触模型和密集模型的激活进行互相转换具体做法是在吸触模型和密集模型之间建立瞧接使得混合前向传播路径在预训练任务上表现良好通过截制和手动选择关键节点可以在吸触模型中进行可解释性干预并通过瞧接将这种干预应射到密集模型实现对密集模型行为的可控编辑实验表明这种方法能够在密集模型中实现与吸触模型类似的可解释性特征编辑CKT问题3吸触模型的可解释性在实际任务中有哪些具体表现和局限在具体任务如自服穿避合扩耗前套技术和编辑类型追踪中吸触模型能够提取出较小且部分可解释的计算回路例如自服穿避合任务只设计少量神经缘和注意力通道易语人工理解扩耗前套技术任务则展示了模型如何通过残差通道的机火军职来表示欠套深度但也暴露了上下文息式等脆弱性及在常序列或有干扰时模型容易出错此外部分回路仍然依赖任务特定数据且在复杂任务中回路规模和编辑交大追踪和解释难度增加关于模型局限性模型局限性部分主要包括计算效率低多一特征特征不可二直化解释性定义不完善简直方法有限级难以扩展到复杂模型等技术模型训练和推理计算量远高于同等能力的筹密模型且当前优化方法导致大量无效神经缘影响效率电路中人存在多一节点部分概念分布在多个节点上完全单一化难以实现模型宽度扩展获可缓解被部分特征无法二直化虚解是其夫之信息增加了解释难度当前对可解释性的定义仅限于紧凑的任务电路为能完全覆盖人类直观理解虚改进解释性度量SD简直方法主要针对节点为能直接简直编且常虚手动补充影响电路简洁性积极方式目前仅在简单任务和小模型上验证扩展到复杂模型实电路规模和复杂度可能极大解释性面临挑战关于论文结论群众吸收Transformer模型显著提升的电路可解释性能分离处于自然概念对应的紧凑电路并为理解现有筹密模型提供了新工具但在效率和扩展性上人虚进一步研究Conclusion部分包含31张图片图使展示了全中吸收性Wade's Parsity与右岛残差流Residual Stream中的激火吸收性具体而言该图使用热力图的方式狠周为菲林参数数量L0纵周为总参数量粮者的不同组合对应一个激火吸收度的度量残差流激火分布的风度Cartusus取对数平均结果显示随著全中的菲林参数数量减少集权中吸收性增加或总参数量增大最终残差流激火的风度会增加由于残差流的激火不场景确等于0论文采用风度来衡量分布的吸收性此图在论文中用于说明即使没有显示对激火加入吸收约束紧通过吸收化的权重模型自然会表现出高吸收性的激火分布这一发现对解是全中吸收Transformer中模型激火与参数配置的关系以及理解吸收性对模型可解释性的贡献具有重要作用图使一展示了Sharkfin学习率Learning Rate的调度方式其中前1%的训练阶段用于Warm Up随后在前50%的训练过程中对L0进行衰减该图呈现了学习率随训练进度的变化曲线可以看到在训练进程的前半段学习率先快速上升至峰值然后缓慢下降值训练墨气这种调度策略在论文中用于优化全中吸收Transformer模型的训练表现帮助模型在训练初期稳定收裂并在后期进一步细化模型参数从而配合全中吸收化机制提高模型结构的可解释性和机活吸收性图使二展示了论文中Transformer结构的注意力Attention模块和MLP模块的试意图该图明确标注了每个模块的主要运算步骤并插入了App Stop KG活韩数与每个节点位置及每步操作之间在注意力快中App Stop KV3次作用于QKV并分别进行处理由于一次为Erem's Norm App Stop K声称KKVV App Stop K进行注意力运算App Stop K在MLP快中由成为Erem's Norm App Stop KMLP全连接 App Stop KMLP投影 App Stop K这张图解决了模型在吸收权重结构下不同节点处的吸收机活韩数插入产明了App Stop C的运用方式以及其在Transformer各自模块中的位置为后续解释模型吸收性于可解释性机制提供了结构基础图示3展示了学习绿月热Learning Weight Wall Map消容实验的结果该图用于比较有无学习绿月热机制下不同模型宽度256、512、1000024在不同学习绿下的交叉商损失表现实现代表激现Baslin学现为没有学习绿月热的实验条件实验的目的是分析学习绿月热对全中吸收Transformer模型性能的影响从图中可以观察到当没有学习绿月热时不同宽度模型的损失军有明显提升说明适当的学习绿月热能有效降低模型的训练损失从而提升模型的表现该图为论文学习绿设计部分提供了重要的实验依据抢掉了预热策略在优化Transformer结构时的重要性图示4展示了不同学习绿月热比例Warm Up Fraction对模型性能的影响这里的预热比例分别为5%和10%从图中可以看出在使用1%的学习绿月热时模型在不同宽度256、512、1024下的焦察商损失随学习绿变化的趋势实验设置为对比Baslin方案以及分别将学习绿月热设为前5%和10%的训练部数该图接受了学习绿月热比例对全中吸收变换器WestPars Spars Transformers训练效果的影响展示了更合理的预热比例能帮助模型收脸至更低的损失解决了学习绿月热在训练深度模型中的最有比例选择问题有助于指导实际训练中的超参数设置图示5展示了不同模型宽度256、512、1024下采用货部采用学习绿摔减Nolar Decay时在不同学习绿条件下的焦察商损失Cross Intro Pellows变化趋势该图只在分析学习绿摔减Learning Red Decay对Transformer模型训练效果的影响通过对比机线Baseline和未采用学习绿摔减的实验可以观察到没有学习绿摔减时不同宽度模型再叫大学习绿下损失急剧上升表明学习绿摔减在提高模型稳定性和收脸性方面起到了至关重要的作用该销容实验帮助论文做著系统性地理解操餐数如学习绿摔减对模型性能的具体影响从而为Transformer模型的训练设置提供时政依据徒师六展示了不同模型宽度256、512、10000、24下是否采用T2采减Grad Clipping对训练过程的焦察商损失Cross Intro Pellows随学习绿变化的影响途中实现代表机线Baseline实验结果虚线表示为使用T2采减No Clips实验结果通过对比可以看出为使用T2采减时在某些学习绿下模型的损失显著增大尤其是在模型宽度较大时说明T2采减对于稳定训练防止损失爆炸非常重要该图为论文的Appleation Study部分接受了T2采减在全中吸收Transformer模型训练中的关键作用以及其对模型收脸性和性能的影响徒师七展示了在不同模型宽度256、512、1000024和不同训练退伙丁志比例NEO Stop Free Calization等于0.2和NEO Stop Free Calization等于0.8下模型在不同学习绿下的交叉商损失Cross Entropy Loss表现其中机线实验使用的是50%训练时间用于退伙纵轴为交叉商损失、横轴为学习绿该图的目的是研究训练过程中采用不同退伙时间比例对模型最终损失的影响从而优化模型的训练策略通过对比不同退伙时间站比的实验曲线可以看出调整退伙7站比对各宽度模型性能的影响有助于理解和改进全中吸收Transformer训练细节徒师八展示了Attention Sync消容实验的结果该图通过对比Baseline机线与应用Attention Sync方法在不同模型宽度256、512、1000024下的Cross Entropy Loss随学习绿变化的曲线分析Attention Sync机制对Transformer模型训练性能的影响Attention Sync是一种针对Transformer模型中注意理机制的微调或修改方法只在提升模型效果或解释性从图中可以看出在添加Attention Sync机制后各种模型宽度下的交叉商损失与机线进行对比有明显的曲线差异说明Attention Sync在模型训练中产生了实际影响该消容实验帮助论文作者评估Attention Sync机制在不同设置下的有效性为Transformer模型设计与优化提供了理论依据图示久展示了在不同模型宽度256、512、1000024和学习绿设置下系数篇制Baseline与虫密篇制在交叉商损失上的对比消容实验该图通过折件趋势展现了Baseline和Dance Bies'两种设置下模型性能的差异横周为学习绿、纵周为交叉商损失结果显示使用西数篇制通常可以获得更低的损失及更优的模型表现该消容实验验证了论文关于西数Transformer篇制结构的有效性说明西数篇制有助于模型解释性和效率同时并不损失性能图二十展示了在不同头部为度DiHeaded设置下全中西数Transformer模型在不同模型宽度256、512、1000024和不同学习绿下的交叉商损失表现实验对比了机线DiHeaded等于16与DiHeaded分别为32和6040模型性能的变化途中可以看到随著DiHeaded的增大模型在部分设置下的损失有所变化因此该图回答了Transformer头部为度对西数模型性能影响的问题帮助研究者理解超参数选择对模型收脸和范画的作用图二十一展示了L0衰减调度对偏制相和签入举证的作用该图通过不同模型宽度256、512、1000024的对比分析了Baseline和引入NBS Fracken No. New Fix的调度策略急征对签入举证和偏制的L0证则衰减对交叉商损失的影响横周为学系率、纵周为交叉商损失、实验发现合理的L0衰减调度有助于降低交叉商损失从而提升模型性能该图解决的问题是在Transformer简直或西数化过程中如何通过控制偏制和签入举证的L0证则画实现更优的西数结构和模型表现图二十二展示了每个神经园保持机火全中数量的销容实验结果基准线为J等于4图中通过对子图线型虚线与点画线和颜色不同模型宽度分别为256、512、10000、24区分不同实验设置如果保持J等于一个J等于两个神经园机火以及完全不限制神经园横周为学系率、纵周为交叉商损失、该图只在分析在全中西数Transformer中不同数量的强制机火全中对模型性能的影响实验结果显示减少每个神经园被强制保留的全中数会对最终模型的损失表现产生影响尤其在较大模型宽度下差异更加显著从而未在西数结构下提供和李超参数选择提供了依据图二十三展示了在不同模型宽度二五六、512、10000、24区和不同学系率下进行L0、NELLIN、销容实验后的交叉商损失变化图中的曲线比较了机线方法和取消L0、NELLIN方法在任务性能上的差异从而分析L0、NELLIN、L0证则化调度对于全中西数Transformer模型泛化性能和学系效果的重要性通过该实验论文只在评估L0、NELLIN机制对模型训练的影响结果表明金融L0、NELLIN会导致模型损失上升尤其在更宽的模型结构中影响更加明显这有助于说明L0、NELLIN机制在促进全中西数性和模型性能中的关键作用支撑文章关于西数Transformer机制有效性的论点和设计选择2.24展示了在全中西数Transformer模型中进行BuyGram表销容实验室的表现该图的横轴为学系率、纵轴为交叉商损失曲线对应不同模型宽度二五六、512、10000、24其中实现表示机线设置、虚线表示为使用BuyGram表的对照情形从图中可以看出去除BuyGram表会导致模型在所有宽度设定下的交叉上损失上升尤其在较高学系率下由微明显这说明BuyGram表对于模型性能提升有重要作用该销容实验主要用于分析BuyGram表在Transformer模型中的作用和贡献验证了其对于提高模型表达能力和训练效果的积极意义图25展示了SharkFin机线方法与普通的WarmupDK学系率调度策略在不同模型宽度二五六、512、10000、24下的对比实验结果、横轴为学系率、纵轴为交叉商损失该图用于解决在全中吸收Transformer训练过程中选择和种学系率调度方法对模型性能的影响这一问题通过对比机线与Unilular Coloredic with Leo-0两种策略下不同宽度模型的交叉商损失可以指观发现SharkFin策略在部分学系率区间和模型宽度设定上能取得更低的损失说明其对训练收脸和性能优化具有显著影响图26展示了位置欠入销容实验的结果图中对比了不同模型宽度二五六、512、10000、24下采用与不采用位置编码时随学习欲编化的交叉商损失表现实现表示机线模型、虚线代表加入了位置编码的实验在论文中这个图用于评估Transformer模型在有五位置编码情况下的性能表现分析位置编码对模型训练效果的影响通过这一实验 作者探讨了位置编码这一结构性设计在全中吸收Transformer模型中的贡献为理解模型架构中各个部分的作用提供了支持图27展示了在Transformer模型中对优化器Adm的超参数艾进行销容实验的结果默认爱衣设置为0.9 该图通过对不同模型宽度256、5121024和不同一直0.8和0.95下的交叉商损失随学习欲编化的取限进行对比展示了二参数对模型性能的影响实验表明调整一的取值在一定范围内会影响Transformer模型在训练过程中的收脸效果特别是对于不同规模宽度的模型 选择适合的同一可以获得更低的损失从而提升模型性能该图的主要目的是帮助研究者理解优化器超参数设置对全中吸收Transformer特性和训练表现的影响为全中吸收Transformer的可解视性分析和优化器参数调优提供时阵依据图28展示了在不同模型宽度256、512、1024下Adm优化器中二参数的销容实验结果极限为0.95、横周为学习欲、纵周为交叉商损失图中用不同现行分别表示机械、二等于0.99、以及二等于0.99的实验情况可以观察到不同的二曲直对最终的模型性能有显著影响且在不同的学习欲和模型宽度下损失表现各有不同这一实验只在分析二超参数对Transformer模型训练性能和稳定性的影响为选择合适的Adm优化器参数提供时阵依据图29展示了Adm优化器的全中衰减参数的销容实验机限制为0.1其目的是探只有不同曲值包括W到0.03和W到0.03对模型表现的影响、横周为学习欲纵周为交叉商损失三条不同颜色的曲线分别代表模型宽度为256、512和1024的数据通过对比机械和不同参数下的损失变化实验分析了在不同模型规模下悬重衰减参数对优化效果和模型性能的作用对指导合理选择Adm群众衰减参数具有重要意义图三时展示了在Transformer模型实验中Adm优化器参数Apple-Long的销容实验结果机限之为0.1该图通过对不同模型宽度256、512、1024和不同系参数设定1、0.01、0.001下的交叉商损失随学习慈滤变化的曲线分析的选择对模型性能的影响曲线对比显示不同一直对模型在训练过程中的稳定性和最终损失有显著影响从而帮助论文探只有在西苏Transformer结构下优化器参数对于学习表现的调节作用极其敏感性图三时解决的问题是评估优化器参数设定对西苏神经网络训练效果的影响为何立选择参数提供依据图三时一展示了两种不同简直算法在Transformer模型上的比较宗州维任务损失业做机梁Loss横周围每个回路中的平均边数MiniGis Per Circuit图中包含两条曲线橙色代表基于归音的简直Attribution-based pruning蓝色代表学习型简直Lend the pruning从图三时一可以明显看出学习型简直在所有损失目标下都能找到更小的回路其他由于归音简直机线该图是在论文DA五节介绍的简直算法背景下验证了通过优化布尔沿马参数直接学习出更经检且高效的可解释电路是实现全中西苏Transformer模型的核心技术手段之一图三时一不仅说明了学习型简直方法在参数量大幅减少的情况下几乎不牺牲模型表现还为论文探索Transformer可解释性和电路结构的进一步分析提供了实验基础图三12展示了逆简直InverseProofing实验结果销融有简直找到的回路会极大地损害整体模型的损失表现途中横著为回路的几乎平均规模解点数送走为销融后的平均损失蓝色折线表示随机解点销融后的损失橙色折线表示销融简直得到回路后的损失虚线为最大可达损失可以看到随著所找到回路规模以及任务表现的提升模型在失去该回路后损失明显上升说明回路对于模型性能至关重要该图展示了全中吸收Transformer中通过简直得到的分析性回路不仅具有可解释性也是整体任务性能的关键组成部分突显了模型结构与功能之间的直接联系图三13展示了吸收模型与虫密模型在单个Token上的Loss之间的强香关性走图为用不同随机种子训练出的两个虫密模型他们在个Token上的Loss分布之间有高度相关性相关系数为0.94又图为吸收模型与虫密模型之间的Loss分布比较相关系数为0.93几乎和虫密模型之间的相关性相当图中的颜色代表各Loss区间对应的Token数量从论文上下文来看此图用于说明尽管吸收模型在参数上更高效但它的性能在Token级别上与虫密模型高度一致因此在实际任务中可以有效替换虫密模型而不会导致明显性能下降这一发现为吸收Transformer模型的进一步应用与优化提供了有力的支持图34展示了在不同参数规模Total Params和不同回路吸收度L0下将回路中所有特征二指化后平均任务损失的热力图纵轴表示总参数数量横轴表示L0吸收度水平颜色申浅代表平均任务损失的高低越潜代表损失越高图中红色插号标注了对于L0大于总参数数目的情况数据确实理由显然该图在论文中主要用于探讨全中吸收Transformer模型再进行特征二指化后的性能表现反映了吸收度和模型容量对回路可解释性和模型损失的影响有助于理解在保持模型性能的同时如何通过吸收化提升模型的解释性Total Params 图35展示了做著提出的方法Basicling与Louisels等于20018提出的方法在不同吸收度下的任务损失表现对比横作标是经验L0泛数吸收度使用对数课度种作标表示任务损失同样为对数课度图中蓝色点Smooth Space代表Louisels等人方法下个模型配置的损失橙色线Basicling Mini-Shot代表作者方法在相同参数量下的最优任务损失可以观察到作者的方法在各个吸收度区间内都能取得更低的损失在极大多数点上由于Louisels等人的方法这说明做著提出的吸收Transformer方法在保持叫强摩行星的同时能够高效的减少参数量实现更优的吸收化效果该图解决了论文关于不同吸收化策略有效性对比的问题为后续进一步讨论起解释性可使用性提供了数据支撑Total Params 图36展示了2AttenResidileted Edix1249与3AttenResidileted Edix 1249在预训练过程中激活的相关性分布该图通过3点图的形式呈现了两个残差通道在不同样本上的激活直之间的关系通过图中的分布可以看出这两个通道的激活模式高度相关无论是在预训练数据分布还是在扩浩技术任务分布中都具有类似的相关性论文中通过此图说明3Atten存在Edix 1249出的输出基本是在复制或放大2Atten的输出使得模型可以通过对3Atten的现性替换用27DTN的输出损失几小实现简化回路结构而不影响整体性能因此图36所展示的数据为作者后续在相关残差通道简化和采检中的理论依据说明了跨层残差通道的荣誉与可简化性图37展示了在保持3个变量L0总参数量激活吸收度中两个不变变化一个的条件下不同参数组合对于预训练损失能力与简之后电路规模可解释性之间平衡关系的对比据起来说图中封为3组DFT在保持总参数量与激活吸收度固定的情况下考察L0表示模型吸收性的变化反映了L0如何延迟延变化对能力和可解释性的影响第二幅T在固定L0和激活吸收度下考察总参数量的变化表明增加参数总数一般推动能力可解释性曲线前移第三幅图在固定L0和总参数量下变化激活吸收度结果显示激活吸收度的提升出息有助于性能提升但当提升到一定程度后其优势被其他方案超越进入帕类拖裂时区域整体上该图用以解决如何在能力与可解释性之间取得最佳权衡量化不同吸收与参数配置对Transformer模型结构化可解释性的作用图39展示了通过电路可是画工具结取的一些文档片段这些片段对应不同百分位数的激活值抽取自整个预训练分布而不仅是单一的双引号任务针对图四中的时ATT Resid Delta83节点在图中可以看到当机活值在高百分位数时内容中出现双引号自服穿时该节点激活为证职当内容为单引号自服穿时机活为复职而在自服穿外部或无引号时机活直接进零这说明即使在大规模预训练分布下该节点的表现仍然具有高度的单一性Monosantic即仅对特定内容如双引号自服穿激活不过论文也特别指出这种情况属于挑选到的特例并非所有节点都具有如此清晰和单一的激活模式从论文整体来看该图用来说明全中吸收Transformer模型中的某些节点能够学习出可解释的功能电路提升模型的可解释性和透明度图四时展示了巧遣吸收模型中用于构建可解释扰动机制的第三层注意力输入通道在引用类型分类任务上的激活可是画效果该图通过结局不同样本的模型激活区分了预训练阶段最强的正激活和附激活显示在顶部并在顶部排列了任务配对激活这样不仅可以直观展现模型的模型对不同输入类型如不同代码片段和纹本标签的想义模式也为分析吸收Transformer在特定分类任务终极活分不及其可解释性电路结构提供了具体证据该内容有效的解决了如何通过可是画方式动查吸收Transformer在处理特定任务如引用类型判别式的内部机制和特征分布的问题为后续人工干预和设计根据可解释性的模型提供支持突次11展示了对桥阶吸收模型中第三层MLP输入通道进行可解释扰动构造时关于当前行以Except开头的激活可是画截图该图主要用于分析模型在面对此类代码结构使得内部机活分布从而即使模型在理解特定与法模式如条件和异常与具方面的表现途中上方展示了预训链阶段的附激活下方则是任务配对阶段的机活分布这种结构有助于研究者理解全重吸收画后Transformer模型对于特定代码逻辑如分支与循环开头的识边能力和决策依据在论文中用于支持模型可解释性和回溯期处理过程从而更好的构建具被透明行为的语言模型Conclusion部分包含两张表格表一展示了作者为研究全重吸收Transformer可解释性电路而手工设计的所有任务列表每个任务都针对Python代码中的具体语境要求模型预测下一个Token或结构例如扩号引号关键自缩进等表中详细列出了每个任务的名称极其描述包括如预测自辅范音以单引号还是双引号避合判断韩数参数是否有默认值区分辨量是Set还是String推断WIS语句的SU用法扩号或大扩号的选择文件读写模式的推断扩号欠套层述的技术符合WIS语的区分Alts和Lift判断FString的识别三人表达事中的贸豪预测辨量交换符语询狂后的缩进Lambda表达是与韩数的区分等号与双等号的选择Inumerate与Ringe的区分F语句与辨量负责的区分以及Wild True或Reach&Tune后的缩进预测的该表格位后续实验和分析提供了任务基础只在系统性的评估模型在不同代码理解与生成长景下的表现和可解释性表二展示了COPS算法的出使搜索中心超参数设置包括KKoEfertiInitNachinistScaleInitNachinistBiasVidiLearnInvBeta2LearnWarmUpFractship和HevsideTankFutant参数极其对应的数值该表格在论文中用于说明在全中吸收Transformer模型的训练和简直过程中COPS方法的超参数出手化选择这些参数的合理设置对于后续吸收性搜索和模型性能具有重要影响帮助研究者负现和理解实验流程
