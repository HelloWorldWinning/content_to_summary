Timestamp: 2025-11-28T07:17:47.812715
Title: Transformers Tutorial (Paper Explained + Implementation in Tensorflow and Pytorch) - Part2 ü§ó‚ö°
URL: https://youtube.com/watch?v=i6RZhEcXv2I&si=HnJL9Vl_ezskcb9w
Status: success
Duration: 26:41

Description:
Â•ΩÁöÑÔºåËøôÊòØÂØπÊâÄÊèê‰æõÊñáÊú¨ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ËøõË°åÁöÑÊèêÁÇºÂíåÊÄªÁªì„ÄÇ

### **ÂÜÖÂÆπÊ¶ÇË¶Å**

#### **I. Transformer Êû∂ÊûÑÂõûÈ°æÔºöÂ§öÂ±ÇÁºñËß£Á†ÅÂô®ËøûÊé•**
*   **ÁºñÁ†ÅÂô®Â†ÜÂè† (Encoder Stack):** Ê®°ÂûãÈááÁî®Â§öÂ±ÇÔºà‰æãÂ¶Ç6Â±ÇÔºâÁºñÁ†ÅÂô®Â†ÜÂè†ÁªìÊûÑÔºåÂÖ∂‰∏≠‰∏ä‰∏ÄÂ±ÇÁºñÁ†ÅÂô®ÁöÑËæìÂá∫‰Ωú‰∏∫‰∏ã‰∏ÄÂ±ÇÁºñÁ†ÅÂô®ÁöÑËæìÂÖ•„ÄÇ
*   **Ëß£Á†ÅÂô®Â†ÜÂè† (Decoder Stack):** ÂêåÊ†∑ÔºåËß£Á†ÅÂô®‰πüËøõË°åÂ†ÜÂè†ÔºåÊØè‰∏ÄÂ±ÇËß£Á†ÅÂô®ÁöÑËæìÂÖ•ÈÉΩÂåÖÂê´ÂÖ∂‰∏ä‰∏ÄÂ±ÇËß£Á†ÅÂô®ÁöÑËæìÂá∫„ÄÇ
*   **ÁºñËß£Á†ÅÂô®ËøûÊé• (Encoder-Decoder Connection):** ÊâÄÊúâËß£Á†ÅÂô®Â±ÇÈÉΩÊé•Êî∂Âπ∂‰ΩøÁî®**ÊúÄÂêé‰∏Ä‰∏™**ÁºñÁ†ÅÂô®ÁöÑÊúÄÁªàËæìÂá∫ÁªìÊûú„ÄÇ

#### **II. Â§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂ (Multi-Head Attention)**
*   **ÊèêÂá∫Âä®Êú∫ (Motivation):**
    *   **ÊÄßËÉΩÁì∂È¢à:** Âçï‰∏ÄÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºàScaled Dot-Product AttentionÔºâÈúÄË¶ÅÂ§ÑÁêÜÂ∑®Â§ßÁöÑÁü©ÈòµÔºåËøô‰ºöÈôç‰ΩéËÆ°ÁÆóÊïàÁéáÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê®°ÂûãÁª¥Â∫¶ÂæàÈ´òÊó∂„ÄÇ
    *   **ÊèêÂçáÊïàÊûú:** Transformer Êû∂ÊûÑÁöÑÁõÆÊ†áÊòØË∂ÖË∂äÂæ™ÁéØÁ•ûÁªèÁΩëÁªú (RNN) ÁöÑÈÄüÂ∫¶ÂíåÊî∂ÊïõÊïàÊûúÔºåËÄåÂ§öÂ§¥Ê≥®ÊÑèÂäõÊòØÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†áÁöÑÂÖ≥ÈîÆ‰ºòÂåñ„ÄÇ

*   **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea):**
    *   **ÂàÜËß£‰∏éÂπ∂Ë°å:** ‰∏éÂÖ∂ÊâßË°å‰∏ÄÊ¨°È´òÁª¥Â∫¶ÁöÑÊ≥®ÊÑèÂäõËÆ°ÁÆóÔºå‰∏çÂ¶ÇÂ∞ÜÊü•ËØ¢ (Query)„ÄÅÈîÆ (Key)„ÄÅÂÄº (Value) ÊäïÂΩ±Âà∞Â§ö‰∏™‰ΩéÁª¥Â≠êÁ©∫Èó¥‰∏≠ÔºåÂπ∂Ë°åÂú∞ÊâßË°åÂ§öÊ¨°Ê≥®ÊÑèÂäõËÆ°ÁÆó„ÄÇ
    *   **Â§öËßÜËßíÂÖ≥Ê≥®:** ËøôÁßçÊú∫Âà∂ÂÖÅËÆ∏Ê®°ÂûãÂú®‰∏çÂêå‰ΩçÁΩÆÂêåÊó∂ÂÖ≥Ê≥®Êù•Ëá™‰∏çÂêåË°®ÂæÅÂ≠êÁ©∫Èó¥ÁöÑ‰ø°ÊÅØÔºå‰ªéËÄåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑË°®ËææËÉΩÂäõ„ÄÇ

*   **ÂÆûÁé∞Ê≠•È™§ (Implementation Steps):**
    1.  **Á∫øÊÄßÊäïÂ∞Ñ‰∏éÂàÜÂâ≤ (Linear Projection & Split):** Â∞ÜËæìÂÖ•ÁöÑ Q, K, V Áü©ÈòµÂàÜÂà´ÈÄöËøáÁã¨Á´ãÁöÑÁ∫øÊÄßÂ±ÇËøõË°åÊäïÂ∞ÑÔºåÁÑ∂ÂêéÂ∞ÜÁªìÊûúÂàÜÂâ≤ÊàêÂ§ö‰ªΩÔºà‰æãÂ¶Ç8‰∏™‚ÄúÂ§¥‚ÄùÔºâ„ÄÇ
    2.  **Âπ∂Ë°åËÆ°ÁÆóÊ≥®ÊÑèÂäõ (Parallel Attention Calculation):** ÂØπÊØè‰∏™ÂàÜÂâ≤ÂêéÁöÑ‚ÄúÂ§¥‚ÄùÁã¨Á´ã‰∏îÂπ∂Ë°åÂú∞ÊâßË°åÁº©ÊîæÁÇπÁßØÊ≥®ÊÑèÂäõËÆ°ÁÆó„ÄÇ
    3.  **ÁªìÊûúÊãºÊé• (Concatenation):** Â∞ÜÊâÄÊúâ‚ÄúÂ§¥‚ÄùÁöÑÊ≥®ÊÑèÂäõËæìÂá∫ÁªìÊûúÊãºÊé•Âú®‰∏ÄËµ∑ÔºåÂΩ¢Êàê‰∏Ä‰∏™Â§ßÁöÑÁâπÂæÅÁü©Èòµ„ÄÇ
    4.  **ÊúÄÁªàÁ∫øÊÄßÊäïÂ∞Ñ (Final Linear Projection):** Â∞ÜÊãºÊé•ÂêéÁöÑÁü©ÈòµÂÜçÈÄöËøá‰∏Ä‰∏™ÊúÄÁªàÁöÑÁ∫øÊÄßÂ±ÇËøõË°åËΩ¨Êç¢ÔºåÂæóÂà∞Â§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊúÄÁªàËæìÂá∫„ÄÇ

#### **III. ‰ª£Á†ÅÂÆûÁé∞ (Code Implementation)**
*   **PyTorch ÂÆûÁé∞:**
    *   ‰ΩøÁî® `nn.Linear` ÂÆûÁé∞Á∫øÊÄßÊäïÂ∞Ñ„ÄÇ
    *   ÈÄöËøáÂº†ÈáèÈáçÂ°ë (`view`) ÂíåËΩ¨ÁΩÆ (`transpose`) Êìç‰ΩúÂÆûÁé∞‚ÄúÂ§¥‚ÄùÁöÑÂàÜÂâ≤‰∏éÂêàÂπ∂„ÄÇ
*   **TensorFlow ÂÆûÁé∞:**
    *   ‰ΩøÁî® `tf.layers.dense`ÔºàÊàñÁé∞‰ª£ÁöÑ `tf.keras.layers.Dense`ÔºâÂÆûÁé∞Á∫øÊÄßÊäïÂ∞Ñ„ÄÇ
    *   ‰ΩøÁî® `tf.split` Âíå `tf.concat` Êù•Â§ÑÁêÜÂ§öÂ§¥ÁöÑÂàÜÂâ≤‰∏éÊãºÊé•„ÄÇ

---

### **Ê†∏ÂøÉËßÇÁÇπ**

Â§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂ÈÄöËøáÂ∞ÜÂçï‰∏ÄÊ≥®ÊÑèÂäõËÆ°ÁÆóÂàÜËß£‰∏∫Â§ö‰∏™Âπ∂Ë°åÁöÑ„ÄÅÂú®‰∏çÂêåË°®Á§∫Â≠êÁ©∫Èó¥‰∏≠ËøõË°åÁöÑÂ∞èÂûãËÆ°ÁÆóÔºå‰ªéËÄåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩÂíåËÆ°ÁÆóÊïàÁéá„ÄÇ

---

### **ÂÜÖÂÆπÊ°ÜÊû∂**

ËØ•ÂÜÖÂÆπÈÅµÂæ™ **‚ÄúÈóÆÈ¢ò -> Ëß£ÂÜ≥ÊñπÊ°à -> ‰ª£Á†ÅÂÆûÁé∞‚Äù** ÁöÑÊ°ÜÊû∂ÔºöÈ¶ñÂÖàÈòêËø∞Âçï‰∏ÄÊ≥®ÊÑèÂäõÊú∫Âà∂Èù¢ÂØπÂ§ßËßÑÊ®°ËÆ°ÁÆóÊó∂ÁöÑÂ±ÄÈôêÊÄßÔºàÈóÆÈ¢òÔºâÔºåÁÑ∂ÂêéËØ¶ÁªÜ‰ªãÁªç‰Ωú‰∏∫Ëß£ÂÜ≥ÊñπÊ°àÁöÑÂ§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÊ†∏ÂøÉÂéüÁêÜ‰∏éÊ≠•È™§ÔºàËß£ÂÜ≥ÊñπÊ°àÔºâÔºåÊúÄÂêéÈÄöËøá PyTorch Âíå TensorFlow ÁöÑ‰ª£Á†ÅÁ§∫‰æãÂ±ïÁ§∫ÂÖ∂ÂÖ∑‰ΩìÂÆûÁé∞Ôºà‰ª£Á†ÅÂÆûÁé∞Ôºâ„ÄÇ

---

### **Ê¶ÇÂøµÂõæ (Mermaid)**

<Mermaid_Diagram>
graph TD
    subgraph "ËæìÂÖ• (Inputs)"
        direction LR
        Q["Êü•ËØ¢ (Query)"]
        K["ÈîÆ (Key)"]
        V["ÂÄº (Value)"]
    end

    subgraph "Â§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂ (Multi-Head Attention)"
        A["1. Á∫øÊÄßÊäïÂ∞Ñ & ÂàÜÂâ≤Êàê H ‰∏™Â§¥ <br> (Linear Projections & Split into H Heads)"]

        subgraph "Âπ∂Ë°åÊ≥®ÊÑèÂäõËÆ°ÁÆó (Parallel Attention Heads)"
            direction LR
            B1["<b>Head 1</b> <br> Áº©ÊîæÁÇπÁßØÊ≥®ÊÑèÂäõ"]
            B2["..."]
            B3["<b>Head H</b> <br> Áº©ÊîæÁÇπÁßØÊ≥®ÊÑèÂäõ"]
        end

        C["2. ÊãºÊé•ÊâÄÊúâÂ§¥ÁöÑÁªìÊûú <br> (Concatenate)"]
        D["3. ÊúÄÁªàÁ∫øÊÄßÂ±Ç <br> (Final Linear Layer)"]
    end

    subgraph "ËæìÂá∫ (Output)"
        E["ÊúÄÁªàËæìÂá∫"]
    end

    Q -- " " --> A
    K -- " " --> A
    V -- " " --> A

    A -- " " --> B1
    A -- " " --> B2
    A -- " " --> B3

    B1 -- " " --> C
    B2 -- " " --> C
    B3 -- " " --> C

    C -- " " --> D
    D -- " " --> E

    style Q fill:#cde4ff,stroke:#333,stroke-width:2px
    style K fill:#cde4ff,stroke:#333,stroke-width:2px
    style V fill:#cde4ff,stroke:#333,stroke-width:2px
    style A fill:#fff2cc,stroke:#333,stroke-width:2px
    style B1 fill:#d5e8d4,stroke:#333,stroke-width:2px
    style B2 fill:#d5e8d4,stroke:#333,stroke-width:2px
    style B3 fill:#d5e8d4,stroke:#333,stroke-width:2px
    style C fill:#fff2cc,stroke:#333,stroke-width:2px
    style D fill:#fff2cc,stroke:#333,stroke-width:2px
    style E fill:#f8cecc,stroke:#333,stroke-width:2px
</Mermaid_Diagram>

Content:
hello everyone my name is welcome back, hello everyone my name is welcome back, to the, to the, to the, second session of Transformer, second session of Transformer, second session of Transformer, architecture series, architecture series, architecture series, um first of all I want to talk about, um first of all I want to talk about, um first of all I want to talk about, something that one of my friends asked, something that one of my friends asked, something that one of my friends asked, me, me, me, um, um, um, some days ago, some days ago, some days ago, um he asked me that uh okay we know that, um he asked me that uh okay we know that, um he asked me that uh okay we know that, in the, in the, in the, simple architecture of Transformer which, simple architecture of Transformer which, simple architecture of Transformer which, we have one encoder and one with decoder, we have one encoder and one with decoder, we have one encoder and one with decoder, how these, how these, how these, the parts are connected together but, the parts are connected together but, the parts are connected together but, when we are saying that we have six, when we are saying that we have six, when we are saying that we have six, encoders and six decoders, encoders and six decoders, encoders and six decoders, um basically how we're connecting these, um basically how we're connecting these, um basically how we're connecting these, things, things, things, the image that is shown to you, the image that is shown to you, the image that is shown to you, um is how decoders and encoders are, um is how decoders and encoders are, um is how decoders and encoders are, connected to each other basically, each encoder encodes the, each encoder encodes the, a result of the previous encoder until, a result of the previous encoder until, a result of the previous encoder until, uh, uh, uh, after the related to this week that we, after the related to this week that we, after the related to this week that we, wanted to have for example when we when, wanted to have for example when we when, wanted to have for example when we when, we want to have six encoders, we want to have six encoders, we want to have six encoders, um as a result, um as a result, um as a result, um we encode the input which is a, um we encode the input which is a, um we encode the input which is a, sentence and I think it is in uh French, sentence and I think it is in uh French, sentence and I think it is in uh French, into up to uh six layers and then the, into up to uh six layers and then the, into up to uh six layers and then the, decoders all of them gets the last, decoders all of them gets the last, decoders all of them gets the last, encoded result and uses the uh output of, encoded result and uses the uh output of, encoded result and uses the uh output of, the previous decoder by itself, the previous decoder by itself, the previous decoder by itself, to decode the sentence and this is how, to decode the sentence and this is how, to decode the sentence and this is how, six encoders and six decoders are, six encoders and six decoders are, six encoders and six decoders are, connected to each other, connected to each other, connected to each other, but let me, but let me, but let me, explain, explain, explain, multi-head attention to you in the, multi-head attention to you in the, multi-head attention to you in the, previous session we talked about, previous session we talked about, previous session we talked about, scale.product attention and how single, scale.product attention and how single, scale.product attention and how single, attention is working but now we are, attention is working but now we are, attention is working but now we are, going to talk about multi-head attention, going to talk about multi-head attention, going to talk about multi-head attention, um, um, um, the main thing that we were talking, the main thing that we were talking, the main thing that we were talking, about why we should use Transformer, about why we should use Transformer, about why we should use Transformer, architecture and we use attention and, architecture and we use attention and, architecture and we use attention and, feed for feed forward neural networks, feed for feed forward neural networks, feed for feed forward neural networks, was that in comparison with, was that in comparison with, was that in comparison with, um, recurring neural networks we want to, recurring neural networks we want to, have more speed and in addition to that, have more speed and in addition to that, have more speed and in addition to that, this speed can help us to converge not, this speed can help us to converge not, this speed can help us to converge not, only faster but better so, only faster but better so, only faster but better so, we know that escape that product if your, we know that escape that product if your, we know that escape that product if your, network can help us but, network can help us but, network can help us but, the problem here is that we have two, the problem here is that we have two, the problem here is that we have two, large matrices when you have a big layer, large matrices when you have a big layer, large matrices when you have a big layer, in our neural network so what we have to, in our neural network so what we have to, in our neural network so what we have to, do, do, do, in this scenario we use multi attention, in this scenario we use multi attention, in this scenario we use multi attention, instead of performing a single attention, instead of performing a single attention, instead of performing a single attention, function, function, function, ah they use something else that's me, turn on my comments yes okay, turn on my comments yes okay, [Music], [Music], [Music], um, it's better instead of performing a, it's better instead of performing a, single attention function, single attention function, single attention function, uh, uh, uh, with demand D Dimensions values and, with demand D Dimensions values and, with demand D Dimensions values and, queries we found this beneficial to, queries we found this beneficial to, queries we found this beneficial to, linearly project the queries, linearly project the queries, linearly project the queries, what are they saying is basically, what are they saying is basically, what are they saying is basically, instead of using a single attention they, instead of using a single attention they, instead of using a single attention they, are going to uh, are going to uh, are going to uh, divide it into different sections and, divide it into different sections and, divide it into different sections and, find the results and then contact them, find the results and then contact them, find the results and then contact them, somehow to find and the main result of, somehow to find and the main result of, somehow to find and the main result of, the attention so, the attention so, the attention so, they do it each times with different, they do it each times with different, they do it each times with different, uh learned linear projects projections, uh learned linear projects projections, uh learned linear projects projections, to DK DK and DV dimensions, to DK DK and DV dimensions, to DK DK and DV dimensions, respectively on each of these projected, respectively on each of these projected, respectively on each of these projected, versions of queries keys and values, versions of queries keys and values, versions of queries keys and values, where we then perform the attention, where we then perform the attention, where we then perform the attention, function in parallel yielding, function in parallel yielding, function in parallel yielding, DV dimensional output values these are, DV dimensional output values these are, DV dimensional output values these are, concatenated, concatenated, concatenated, and one second project is resolving the, and one second project is resolving the, and one second project is resolving the, final values as depicted in figure 2., final values as depicted in figure 2., final values as depicted in figure 2., let me explain it based on this image in, let me explain it based on this image in, let me explain it based on this image in, here as you can see again we have three, here as you can see again we have three, here as you can see again we have three, matrices queries keys and values in here, matrices queries keys and values in here, matrices queries keys and values in here, uh then we have a linear project shape, uh then we have a linear project shape, uh then we have a linear project shape, this is where we do the projection, this is where we do the projection, this is where we do the projection, so, so, so, if I'm going to say here is projection, H time, H time, scaled dot product, scaled dot product, scaled dot product, for each projected met recess, for each projected met recess, for each projected met recess, and then, and then, and then, in here we are concatenate, in here we are concatenate, in here we are concatenate, concatenating the result of, attention heads, attention heads, each time that we are calculating this, each time that we are calculating this, each time that we are calculating this, called attention added me make this, called attention added me make this, called attention added me make this, better for you, better for you, better for you, and finally, and finally, and finally, we are going to, we are going to, we are going to, uh, uh, uh, do the projection again, why we do projection again because, why we do projection again because, um you know finding the, um you know finding the, um you know finding the, attention is not just simply making, attention is not just simply making, attention is not just simply making, divided parts and then concatenating, divided parts and then concatenating, divided parts and then concatenating, them we know that the tension is uh real, them we know that the tension is uh real, them we know that the tension is uh real, attention of each part is related to, attention of each part is related to, attention of each part is related to, another so we need to do another, another so we need to do another, another so we need to do another, projection here so the linear layer in, projection here so the linear layer in, projection here so the linear layer in, this section is doing that thing so, this section is doing that thing so, this section is doing that thing so, it is this is how it works now, it is this is how it works now, it is this is how it works now, now let's look at this formula here, now let's look at this formula here, now let's look at this formula here, which introduces the multi-ad attention, which introduces the multi-ad attention, which introduces the multi-ad attention, as you can see, as you can see, as you can see, for each head we find uh this attention, for each head we find uh this attention, for each head we find uh this attention, as you can see why we are using w i w i, as you can see why we are using w i w i, as you can see why we are using w i w i, is basically representing that, is basically representing that, is basically representing that, projection and uh, instead of passing the old keys and, instead of passing the old keys and, queries and values we are passing the, queries and values we are passing the, queries and values we are passing the, projection The Divided parts of the some, projection The Divided parts of the some, projection The Divided parts of the some, part of our queries keys and values then, part of our queries keys and values then, part of our queries keys and values then, we are concatenating them and then use, we are concatenating them and then use, we are concatenating them and then use, this projection Matrix to get to produce, this projection Matrix to get to produce, this projection Matrix to get to produce, the final result, the final result, the final result, it says that w o is exactly how we are, it says that w o is exactly how we are, it says that w o is exactly how we are, getting the final result we know that, getting the final result we know that, getting the final result we know that, our funnels should be, our funnels should be, our funnels should be, wanted Dimension so this is how it's, wanted Dimension so this is how it's, wanted Dimension so this is how it's, exactly work, exactly work, exactly work, uh, uh, uh, in their paper they are using eight, in their paper they are using eight, in their paper they are using eight, heads but this can be different for, heads but this can be different for, heads but this can be different for, other models that are based on, other models that are based on, other models that are based on, Transformer architecture I can't, Transformer architecture I can't, Transformer architecture I can't, remember exactly how many has for, remember exactly how many has for, remember exactly how many has for, example bird has but I think it's not, example bird has but I think it's not, example bird has but I think it's not, equal to eight, so this is how exactly multi-attention, so this is how exactly multi-attention, works now let's implement it in our, works now let's implement it in our, works now let's implement it in our, model so for pi torch, model so for pi torch, model so for pi torch, I'm going to create a new file multi, head, head, attention, and that's all we need, and that's all we need, towards, towards, towards, neural network, neural network, neural network, we also need to use data skill that's, we also need to use data skill that's, we also need to use data skill that's, probably implemented in the lab, session some models, session some models, let's put it to say, let's put it to say, let's put it to say, um, um, um, yeah from that scale that product import, yeah from that scale that product import, yeah from that scale that product import, what is the name of that thing, kill that product huh Okay so, kill that product huh Okay so, now we are going to create a new class, now we are going to create a new class, now we are going to create a new class, called, called, called, multi-head attention, which, which, inherits from, inherits from, inherits from, the module of neural networks of Pi, the module of neural networks of Pi, the module of neural networks of Pi, torch, torch, torch, then we have, then we have, then we have, its Constructor, its Constructor, its Constructor, as we know we need the number of heads, as we know we need the number of heads, as we know we need the number of heads, and the dimension of model so we are, and the dimension of model so we are, and the dimension of model so we are, passing them, passing them, passing them, and then we are going to say, and then we are going to say, and then we are going to say, first of all uh called The Constructor, first of all uh called The Constructor, first of all uh called The Constructor, of an end module, of an end module, of an end module, which is multi attention, now that we have all of these things we, now that we have all of these things we, need to, need to, need to, save these parameters number of heads is, save these parameters number of heads is, save these parameters number of heads is, saved, saved, saved, uh, uh, uh, attention, attention, attention, is scaled that product, metrics is of queries, metrics is of queries, this linear, cheese and valleys as well and then we, cheese and valleys as well and then we, have, have, have, the weight of content which is w o in, the weight of content which is w o in, the weight of content which is w o in, the paper, the paper, the paper, so, so, so, contact, contact, contact, is another, is another, is another, linear layer, linear layer, linear layer, which needs the model, and the model so, and the model so, this is exactly what is happening in the, this is exactly what is happening in the, this is exactly what is happening in the, paper if you look at this image we have, paper if you look at this image we have, paper if you look at this image we have, three linears, three linears, three linears, and although it's based on the heads but, and although it's based on the heads but, and although it's based on the heads but, we have one linear per each of these, we have one linear per each of these, we have one linear per each of these, values and queries and keys and then we, values and queries and keys and then we, values and queries and keys and then we, have a linear for the output so this is, have a linear for the output so this is, have a linear for the output so this is, what's happening here this is for the, what's happening here this is for the, what's happening here this is for the, queries keys and values and this is for, queries keys and values and this is for, queries keys and values and this is for, concatenation result, next I'm going to implement forward, next I'm going to implement forward, function, function, function, this needs absolutely, this needs absolutely, this needs absolutely, UK and, UK and, UK and, V and as we know we may need to mask, V and as we know we may need to mask, V and as we know we may need to mask, something but the default parameter for, something but the default parameter for, something but the default parameter for, it is not but we're going to talk about, it is not but we're going to talk about, it is not but we're going to talk about, masking as well, query, query, key and value, key and value, key and value, is equal to, is equal to, is equal to, self that, self that, self that, [Music], [Music], [Music], um, this day, this day, itself that, this linear, this linear, layer, layer, layer, and, in addition okay, in addition okay, give me a V for, give me a V for, give me a V for, um, the next, the next, um one you know you know the first one, um one you know you know the first one, um one you know you know the first one, the first one that you are doing this is, the first one that you are doing this is, the first one that you are doing this is, you're using that product with weight, you're using that product with weight, you're using that product with weight, matrices to do the projection and then, matrices to do the projection and then, matrices to do the projection and then, you need to split things so you're going, you need to split things so you're going, you need to split things so you're going, to say self, to say self, to say self, that, split split is defined in, split split is defined in, and modules so self to this plate, and modules so self to this plate, and modules so self to this plate, here, and so for this we talked about the, and so for this we talked about the, split, split, split, a little further but, a little further but, a little further but, for now, for now, for now, just consider this as a smoothie tensor, just consider this as a smoothie tensor, just consider this as a smoothie tensor, by number of heads, by number of heads, by number of heads, as we need to pass them to the different, as we need to pass them to the different, as we need to pass them to the different, heads of detection, and now, and now, we say that output and attention, we say that output and attention, we say that output and attention, is this self that attention, now, now, we're saying that the output is self, we're saying that the output is self, we're saying that the output is self, that Con cat, that Con cat, that Con cat, of, of, of, out, out, out, and out then is equal to self dot w, and out then is equal to self dot w, and out then is equal to self dot w, concat, concat, concat, of, of, of, service not that much harder, service not that much harder, service not that much harder, okay so then we can return the output, okay so then we can return the output, okay so then we can return the output, now we need to, now we need to, now we need to, write a function for concat and, write a function for concat and, write a function for concat and, split so first let's talk about split, split so first let's talk about split, split so first let's talk about split, which have a tensor and it displayed see, which have a tensor and it displayed see, which have a tensor and it displayed see, based on the number of heads okay, based on the number of heads okay, based on the number of heads okay, it's not something hard it's just batch, it's not something hard it's just batch, it's not something hard it's just batch, size, size, size, lines, lines, lines, and D model, and D model, and D model, is then search that size, then, then, let me say that the dimension of tensor, is, is, the model, divided by self that's number of head, divided by self that's number of head, and now we can see the tensor, and now we can see the tensor, and now we can see the tensor, oh better to say we can return, oh better to say we can return, oh better to say we can return, tensor, tensor, tensor, that view which is you know change it, create a different uh tensors in here, create a different uh tensors in here, this is the view by size is still the, this is the view by size is still the, this is the view by size is still the, same it's not different the lines is, same it's not different the lines is, same it's not different the lines is, the same but we are saying that we want, the same but we are saying that we want, the same but we are saying that we want, instead of having just a D model we are, instead of having just a D model we are, instead of having just a D model we are, going to say we want to have, going to say we want to have, going to say we want to have, n heads, n heads, n heads, of the tensor so, this has happening and then we are going, this has happening and then we are going, to make a transpose of that, to make a transpose of that, to make a transpose of that, and this is how a split works and then, and this is how a split works and then, and this is how a split works and then, we have concat function, we have concat function, we have concat function, which gets so which gets tensor and now, which gets so which gets tensor and now, which gets so which gets tensor and now, it's going to, it's going to, it's going to, concaten to each other, concaten to each other, concaten to each other, so again we need to, so again we need to, so again we need to, do something like this, do something like this, do something like this, patch size lengths and all these things, patch size lengths and all these things, patch size lengths and all these things, and here as we're going to concat this, and here as we're going to concat this, and here as we're going to concat this, to each other so we are just going to, to each other so we are just going to, to each other so we are just going to, say D model, say D model, say D model, is, is, is, number of head, number of head, number of head, because as you know here instead we have, because as you know here instead we have, because as you know here instead we have, had the 10 series the opposite of the, had the 10 series the opposite of the, had the 10 series the opposite of the, last function, last function, last function, number of heads and the tensor so, number of heads and the tensor so, number of heads and the tensor so, then 10 we can return, then 10 we can return, then 10 we can return, tensor, tensor, tensor, the transpose, the transpose, the transpose, transpose, by this and, by this and, kind, kind, kind, tea, and then we're going to, and then we're going to, for you, for you, for you, number batch size, number batch size, number batch size, lengths, lengths, lengths, and the model, and the model, and the model, the model okay we do not have any, the model okay we do not have any, the model okay we do not have any, problem with, here I think but about the contagious uh, here I think but about the contagious uh, actually I found it on the internet let, actually I found it on the internet let, actually I found it on the internet let, me see what is contagious contagious, me see what is contagious contagious, me see what is contagious contagious, it's continuous I think I made a mistake, it's continuous I think I made a mistake, it's continuous I think I made a mistake, in typo as well but, in typo as well but, in typo as well but, contagious, um in pie torch but does it actually do, um in pie torch but does it actually do, it contagious tensor is a tensor whose, it contagious tensor is a tensor whose, it contagious tensor is a tensor whose, elements are stored in a contagious, elements are stored in a contagious, elements are stored in a contagious, order without leaving any empty space, order without leaving any empty space, order without leaving any empty space, between them, between them, between them, so it's, so it's, so it's, not doing actually something very, not doing actually something very, not doing actually something very, special here it's just to make sure that, special here it's just to make sure that, special here it's just to make sure that, all the values are uh you know, all the values are uh you know, all the values are uh you know, what one after another, what one after another, what one after another, it's just to make issue of that so this, it's just to make issue of that so this, it's just to make issue of that so this, is multi-head attention it was not, is multi-head attention it was not, is multi-head attention it was not, something hard at all, something hard at all, something hard at all, now let's get back to 10 Servo now we, now let's get back to 10 Servo now we, now let's get back to 10 Servo now we, are going to implement, are going to implement, are going to implement, this thing in tensorflow as well, multi-head attention, multi-head attention, okay, multi had attention in tensorflow is, multi had attention in tensorflow is, exactly the same so only had, exactly the same so only had, exactly the same so only had, attention we have queries keys, attention we have queries keys, attention we have queries keys, values, key masks, key masks, number of heads, number of heads, number of heads, and that's better to say, TRS default is eight we can say we could, TRS default is eight we can say we could, pass it in there but I'm not going to, pass it in there but I'm not going to, pass it in there but I'm not going to, talk about it in here, talk about it in here, talk about it in here, and we have, drop, drop, outrage, outrage, outrage, which is metaphorical so let me do not, which is metaphorical so let me do not, which is metaphorical so let me do not, talk about drama, talk about drama, talk about drama, just simple, just simple, just simple, it was too happy to get there, it was too happy to get there, it was too happy to get there, and, training is true, training is true, causality, is false, is false, for the scope is, for the scope is, for the scope is, moved the head, moved the head, moved the head, let's see, let's see, let's see, hurts, hurts, hurts, tension almost I had attention I'm not, tension almost I had attention I'm not, tension almost I had attention I'm not, sure how should I pronounce that, sure how should I pronounce that, sure how should I pronounce that, but, something like this D model, something like this D model, queries, that gets, that gets, shape, a little bit, a little bit, creating a scope variable scope, placing the scope given, placing the scope given, and reuse, and reuse, and reuse, stf.auto, tf.layers that, tf.layers that, dense which is the linear layer we had, dense which is the linear layer we had, dense which is the linear layer we had, in point torch, in point torch, in point torch, then we need to pass QD model, and use, and use, bias which is past true, bias which is past true, bias which is past true, they have this thing for, they have this thing for, they have this thing for, two other, two other, two other, things as well, things as well, things as well, okay and, okay and, okay and, V, okay and, okay and, V this was just a linear projection now, V this was just a linear projection now, V this was just a linear projection now, we need to do that splitting so, we need to do that splitting so, we need to do that splitting so, we're going to set that Q, we're going to set that Q, we're going to set that Q, let me copy these lines paste them in, let me copy these lines paste them in, let me copy these lines paste them in, here it's exactly like the same but, here it's exactly like the same but, here it's exactly like the same but, instead of these things we need to say, instead of these things we need to say, instead of these things we need to say, it used to split function of tensorflow, it used to split function of tensorflow, it used to split function of tensorflow, so T after this plates, so T after this plates, so T after this plates, Q number of heads and heads, and, and, axis which is equal to, axis which is equal to, axis which is equal to, 2 and X is here is equal to zero because, 2 and X is here is equal to zero because, 2 and X is here is equal to zero because, we are going to let's put in the axis, we are going to let's put in the axis, we are going to let's put in the axis, number two and, number two and, number two and, um, um, um, yeah that's exactly okay then as we are, yeah that's exactly okay then as we are, yeah that's exactly okay then as we are, going to, going to, going to, concat them, concat them, concat them, we are, we are, we are, doing it at X is zero so into the tens, doing it at X is zero so into the tens, doing it at X is zero so into the tens, we have, we have, we have, TF, and we need to copy this form, and we need to copy this form, these two things as well, instead we just need to change this as K, instead we just need to change this as K, and this as a week, that is easy, that is easy, please do not, please do not, please do not, think that this concat is something, think that this concat is something, think that this concat is something, happening, happening, happening, in here there is not discount here it's, in here there is not discount here it's, in here there is not discount here it's, just how the split function for, just how the split function for, just how the split function for, splitting the main, splitting the main, splitting the main, queries keys and values is happening in, queries keys and values is happening in, queries keys and values is happening in, here so this is how we should do it in, here so this is how we should do it in, here so this is how we should do it in, tensor for it's just splitting this is, tensor for it's just splitting this is, tensor for it's just splitting this is, the linear projection this is uh, the linear projection this is uh, the linear projection this is uh, the uh, the uh, the uh, is plating and then we need to find the, is plating and then we need to find the, is plating and then we need to find the, attention of each one so we say outputs, attention of each one so we say outputs, attention of each one so we say outputs, it scaled that product so we need these, it scaled that product so we need these, it scaled that product so we need these, things so passing qkv and then we have, things so passing qkv and then we have, things so passing qkv and then we have, keymage, keymage, keymage, so, so, so, simply need to pass causality, simply need to pass causality, simply need to pass causality, uh as I use drop house I can, uh as I use drop house I can, uh as I use drop house I can, leave dropouts out of here, leave dropouts out of here, leave dropouts out of here, because it I think it's just, make it it's hard, make it it's hard, but I pass causality, yes and training, yes and training, but the scope is, but the scope is, but the scope is, not change others I think yes it's not, not change others I think yes it's not, not change others I think yes it's not, changed now, changed now, changed now, that we find the attention of each one, that we find the attention of each one, that we find the attention of each one, now we need to concat to others okay so, now we need to concat to others okay so, now we need to concat to others okay so, we just say outputs is TF dot contact of, we just say outputs is TF dot contact of, we just say outputs is TF dot contact of, those results so we say t have that, those results so we say t have that, those results so we say t have that, displayed, displayed, displayed, outputs, um, um, Lexus, Lexus, Lexus, zero and this is an axis to this vice, zero and this is an axis to this vice, zero and this is an axis to this vice, versa of what we have done in, versa of what we have done in, versa of what we have done in, this section, this section, this section, now that we have the output we're just, now that we have the output we're just, now that we have the output we're just, gonna we can, gonna we can, gonna we can, add the queries or not it's not that, add the queries or not it's not that, add the queries or not it's not that, much, much, much, important but as we are not going to use, important but as we are not going to use, important but as we are not going to use, a residual Connection in here we're just, a residual Connection in here we're just, a residual Connection in here we're just, going to, going to, going to, um, um, um, returns that so this is how multi, returns that so this is how multi, returns that so this is how multi, attention Works in here although I'm, attention Works in here although I'm, attention Works in here although I'm, going to mention that after this you can, going to mention that after this you can, going to mention that after this you can, do normalization you can do the residual, do normalization you can do the residual, do normalization you can do the residual, collection based on the queries that you, collection based on the queries that you, collection based on the queries that you, had but, had but, had but, um we're not going to do that in here so, um we're not going to do that in here so, um we're not going to do that in here so, it's a simple implementation of, it's a simple implementation of, it's a simple implementation of, multi-head attention, multi-head attention, multi-head attention, and I think that was, and I think that was, and I think that was, so easy, so easy, so easy, okay, okay, okay, uh, yeah that was it for this session and, yeah that was it for this session and, the next session we are going to, the next session we are going to, the next session we are going to, continue our progress, continue our progress, continue our progress, um our plan is to talk about the, um our plan is to talk about the, um our plan is to talk about the, positional encoding and then creating, positional encoding and then creating, positional encoding and then creating, the blocks of encoder and decoder and, the blocks of encoder and decoder and, the blocks of encoder and decoder and, then contact them and finish this uh, then contact them and finish this uh, then contact them and finish this uh, project in this series of videos so to, project in this series of videos so to, project in this series of videos so to, the next, the next, the next, uh video please stay safe goodbye
