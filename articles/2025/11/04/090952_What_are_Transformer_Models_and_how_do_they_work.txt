Timestamp: 2025-11-04T09:09:52.073873
Title: What are Transformer Models and how do they work?
URL: https://youtube.com/watch?v=qaWMOYf4ri8&si=m9IG3mea7HALemMk
Status: success
Duration: 44:25

Description:
好的，这是对所提供文本的深入提炼、总结与可视化。

### **一、 Transformer 模型核心思想与架构拆解**

**1. 核心思想：逐字生成 (One Word at a Time)**
   - Transformer 模型并非一次性生成完整的回答或文章。它的工作方式是**迭代式**的：接收一段输入文本，然后预测出下一个最有可能的词。
   - 随后，它将这个新生成的词加入到输入文本中，再次进行预测，如此循环往复，直到生成完整的句子或段落。这个过程类似于一个极其复杂的“自动联想输入法”。

**2. 架构分步解析 (Step-by-Step Breakdown)**
   - **阶段一：前处理流水线 (Preprocessing Pipeline)**
     - **1. 分词 (Tokenization):** 将输入的原始文本（例如“写一个故事”）分解为模型可以理解的最小单元（Tokens），如 `[写, 一, 个, 故事, 。]`。标点符号也会被视为独立的 Token。
     - **2. 词嵌入 (Embeddings):** 这是连接人类语言与计算机数字世界的关键桥梁。它将每一个 Token 转换成一个高维度的数字向量。在这个向量空间中，意思相近的词（如“国王”和“女王”）其对应的向量在空间上的距离也更近。`word2vec` 就是通过神经网络学习这种表示的经典方法。
     - **3. 位置编码 (Positional Encoding):** 由于基础的 Transformer 架构本身不理解词语的顺序，位置编码向每个词的嵌入向量中添加了关于其在句子中位置的数学信息。这确保了模型能够区分“我打他”和“他打我”这样词序决定语义的句子。

   - **阶段二：核心处理引擎 (The Main Engine - Transformer Blocks)**
     - 模型的核心由多个完全相同的“Transformer 模块”堆叠而成。每个模块都包含两个关键子层：
     - **1. 注意力机制 (Attention Mechanism):** 这是 Transformer 模型最具革命性的创新。它允许模型在处理句子中的某一个词时，能够动态地评估（“关注”）句子中所有其他词对这个词的重要性，并赋予不同的权重。这使得模型能深刻理解复杂的上下文关系，例如，通过上下文判断“Apple”指的是水果还是科技公司。
     - **2. 前馈神经网络 (Feed-Forward Neural Network):** 在经过注意力层处理后，信息会被送入一个标准的前馈神经网络。该网络对信息进行进一步的非线性计算和转换，增强了模型的表达能力。

   - **阶段三：输出与生成 (Output & Generation)**
     - **1. Softmax 函数:** 当信息流经所有 Transformer 模块后，模型会为词典中的每一个词生成一个分数（logits）。Softmax 函数负责将这些原始分数转换成一个总和为 1 的概率分布。分数越高的词，其对应的概率也越高。
     - **2. 概率采样 (Probabilistic Sampling):** 模型并不是每次都选择概率最高的词作为输出，而是根据 Softmax 生成的概率分布进行**随机采样**。这为模型的生成结果带来了多样性和创造性，避免了每次都产生相同、呆板的回答。

**3. 模型的对齐与优化 (Alignment and Optimization)**
   - **微调 (Fine-Tuning):** 一个仅在海量互联网数据上完成“预训练”的模型，可能无法很好地执行特定任务（如回答问题、遵循指令或进行礼貌对话）。因此，必须使用高质量、针对特定任务的数据集（如问答对、对话日志、指令与执行结果）对模型进行第二阶段的训练，即“微调”。这个过程旨在使模型的行为与人类的期望和价值观对齐。

### **二、 核心要点总结 (Core Point)**

Transformer 模型的核心是通过其独特的注意力机制深度理解上下文，并结合庞大的神经网络结构逐词生成文本，最终通过微调使其能够像人一样对话和执行任务。

### **三、 内容的总体框架 (Overarching Framework)**

本内容采用逐层拆解的框架，从 Transformer 最基本的“逐字生成”功能入手，系统地剖析了其从输入（分词、嵌入、位置编码）到核心处理（注意力机制、前馈网络）再到输出（Softmax）的完整工作流，并最终强调了“微调”对于模型实用性的关键作用。

### **四、 Transformer 工作流概念图 (Mermaid Conceptual Map)**

<Mermaid_Diagram>
graph TD
    subgraph "输入 Input"
        A["用户输入文本 (e.g., '写一个故事')"]
    end

    subgraph "前处理流水线 Preprocessing Pipeline"
        B["1. 分词 (Tokenization)"]
        C["2. 词嵌入 (Embeddings) \n 将Token转为向量"]
        D["3. 位置编码 (Positional Encoding) \n 添加词序信息"]
    end

    subgraph "核心引擎: Transformer 模块 (可堆叠N次) Core Engine: Transformer Blocks (Stacked N times)"
        E{"注意力机制 (Attention Mechanism) \n <br>理解上下文，判断词语间关系"}
        F["前馈神经网络 (Feed-Forward Network) \n <br>信息处理与转换"]
    end

    subgraph "输出与生成 Output & Generation"
        G["Softmax 函数 \n 将分数转为概率分布"]
        H["概率采样 \n 根据概率选择下一个词"]
    end

    subgraph "最终输出 Final Output"
        I["生成的单个词 (e.g., '从前')"]
    end

    J["微调 (Fine-Tuning) \n 使用特定数据集对齐模型行为"]

    A -- "原始文本" --> B
    B -- "Tokens" --> C
    C -- "词向量" --> D
    D -- "包含位置信息的向量" --> E
    E -- "加权后的上下文信息" --> F
    F -- "处理后的信息 (重复N次)" --> E
    F -- "最终输出向量" --> G
    G -- "所有词的生成概率" --> H
    H -- "选中的下一个词" --> I
    I -- "作为新输入，循环此过程" --> A
    
    J -.-> E
    J -.-> F

    style A fill:#FFF2CC,stroke:#333,stroke-width:2px
    style I fill:#D5E8D4,stroke:#333,stroke-width:2px
    style B fill:#DAE8FC,stroke:#333,stroke-width:1px
    style C fill:#DAE8FC,stroke:#333,stroke-width:1px
    style D fill:#DAE8FC,stroke:#333,stroke-width:1px
    style E fill:#F8CECC,stroke:#B85450,stroke-width:2px
    style F fill:#FFE6CC,stroke:#D79B00,stroke-width:2px
    style G fill:#E1D5E7,stroke:#9673A6,stroke-width:1px
    style H fill:#E1D5E7,stroke:#9673A6,stroke-width:1px
    style J fill:#BAC8D3,stroke:#6C8EBF,stroke-width:2px,color:#333
</Mermaid_Diagram>

Content:
Hello, my name is Louis Sarrano and this, Hello, my name is Louis Sarrano and this, is Srano Academy. And today I'm going to, is Srano Academy. And today I'm going to, is Srano Academy. And today I'm going to, tell you about transformer models. So, tell you about transformer models. So, tell you about transformer models. So, as you may have seen, transformers are, as you may have seen, transformers are, as you may have seen, transformers are, absolutely amazing. They can do, absolutely amazing. They can do, absolutely amazing. They can do, anything. They can chat with you. They, anything. They can chat with you. They, anything. They can chat with you. They, can answer questions. They can follow, can answer questions. They can follow, can answer questions. They can follow, commands. They can build stories. They, commands. They can build stories. They, commands. They can build stories. They, can build poems. They can even write, can build poems. They can even write, can build poems. They can even write, code, which is pretty amazing. But now, code, which is pretty amazing. But now, code, which is pretty amazing. But now, the question is, how do these, the question is, how do these, the question is, how do these, transformers work? Well, despite the, transformers work? Well, despite the, transformers work? Well, despite the, mystery behind them, you'll be happily, mystery behind them, you'll be happily, mystery behind them, you'll be happily, surprised to know that they're actually, surprised to know that they're actually, surprised to know that they're actually, not that complicated. They do require, not that complicated. They do require, not that complicated. They do require, huge data sets and a lot of computing, huge data sets and a lot of computing, huge data sets and a lot of computing, power to be trained properly. But in, power to be trained properly. But in, power to be trained properly. But in, terms of the architecture, it's actually, terms of the architecture, it's actually, terms of the architecture, it's actually, a pretty simple architecture. This, a pretty simple architecture. This, a pretty simple architecture. This, architecture is formed by several blocks, architecture is formed by several blocks, architecture is formed by several blocks, such as attention, a feed forward neural, such as attention, a feed forward neural, such as attention, a feed forward neural, network, embeddings, etc. In this video, network, embeddings, etc. In this video, network, embeddings, etc. In this video, I'm going to tell you how each of these, I'm going to tell you how each of these, I'm going to tell you how each of these, blocks works separately and then how to, blocks works separately and then how to, blocks works separately and then how to, put them together in order to get the, put them together in order to get the, put them together in order to get the, transformer model architecture., transformer model architecture., transformer model architecture., Are you ready? Let's begin. Transformers, Are you ready? Let's begin. Transformers, Are you ready? Let's begin. Transformers, were introduced in this paper called, were introduced in this paper called, were introduced in this paper called, attention is all you need. Now, this is, attention is all you need. Now, this is, attention is all you need. Now, this is, the third of a series of three videos., the third of a series of three videos., the third of a series of three videos., The first one was about attention, The first one was about attention, The first one was about attention, mechanisms, mostly the idea behind them., mechanisms, mostly the idea behind them., mechanisms, mostly the idea behind them., The second one is also about attention, The second one is also about attention, The second one is also about attention, mechanisms, except this one had more, mechanisms, except this one had more, mechanisms, except this one had more, math involved. And this is the third one, math involved. And this is the third one, math involved. And this is the third one, where I'm going to tell you about the, where I'm going to tell you about the, where I'm going to tell you about the, whole architecture of a transformer, whole architecture of a transformer, whole architecture of a transformer, model. More specifically, in this video, model. More specifically, in this video, model. More specifically, in this video, I will tell you the following. First, I will tell you the following. First, I will tell you the following. First, I'm going to tell you about the, I'm going to tell you about the, I'm going to tell you about the, architecture of a transformer model, architecture of a transformer model, architecture of a transformer model, which includes several steps such as, which includes several steps such as, which includes several steps such as, tokenization embeddings. In particular, tokenization embeddings. In particular, tokenization embeddings. In particular, I will tell you about one type of, I will tell you about one type of, I will tell you about one type of, embedding called word tovec. Then we'll, embedding called word tovec. Then we'll, embedding called word tovec. Then we'll, go a little bit on attention mechanisms, go a little bit on attention mechanisms, go a little bit on attention mechanisms, which is a crucial part of these models., which is a crucial part of these models., which is a crucial part of these models., I've already told you about them in the, I've already told you about them in the, I've already told you about them in the, previous two videos. So, I'm just going, previous two videos. So, I'm just going, previous two videos. So, I'm just going, to tell you the idea here. And finally, to tell you the idea here. And finally, to tell you the idea here. And finally, I will tell you about feed forward, I will tell you about feed forward, I will tell you about feed forward, neural networks which are the main, neural networks which are the main, neural networks which are the main, engine of a transformer model., So let's start with the first question., So let's start with the first question., What is a transformer? Well, this is how, What is a transformer? Well, this is how, What is a transformer? Well, this is how, I imagine a transformer. And as you've, I imagine a transformer. And as you've, I imagine a transformer. And as you've, seen before, it generates text. Now, how, seen before, it generates text. Now, how, seen before, it generates text. Now, how, does it generate text? Well, you'll be, does it generate text? Well, you'll be, does it generate text? Well, you'll be, surprised. It actually does it one word, surprised. It actually does it one word, surprised. It actually does it one word, at a time. So, you may have seen that it, at a time. So, you may have seen that it, at a time. So, you may have seen that it, generates really long and elaborate, generates really long and elaborate, generates really long and elaborate, responses. It actually does them one, responses. It actually does them one, responses. It actually does them one, word at a time. So, for example, if the, word at a time. So, for example, if the, word at a time. So, for example, if the, prompt or the question is, "Hello, how, prompt or the question is, "Hello, how, prompt or the question is, "Hello, how, are you?" then it doesn't generate an, are you?" then it doesn't generate an, are you?" then it doesn't generate an, answer. It only generates the next word, answer. It only generates the next word, answer. It only generates the next word, that would come here. So what would the, that would come here. So what would the, that would come here. So what would the, next word be here? Well, it can be many, next word be here? Well, it can be many, next word be here? Well, it can be many, things, but perhaps the word doing. So, things, but perhaps the word doing. So, things, but perhaps the word doing. So, that now you have the sentence, hello, that now you have the sentence, hello, that now you have the sentence, hello, how are you doing? Now you can come up, how are you doing? Now you can come up, how are you doing? Now you can come up, with more complicated prompts like write, with more complicated prompts like write, with more complicated prompts like write, a story. Now the transformer is not, a story. Now the transformer is not, a story. Now the transformer is not, going to write a story right away. It's, going to write a story right away. It's, going to write a story right away. It's, just going to generate the next word., just going to generate the next word., just going to generate the next word., What's the next word after write a, What's the next word after write a, What's the next word after write a, story? Well, perhaps it's once because, story? Well, perhaps it's once because, story? Well, perhaps it's once because, it's starting the story and then it, it's starting the story and then it, it's starting the story and then it, restarts. So now it prompts write a, restarts. So now it prompts write a, restarts. So now it prompts write a, story once. What's the next word? Well, story once. What's the next word? Well, story once. What's the next word? Well, perhaps upon and then it puts back in, perhaps upon and then it puts back in, perhaps upon and then it puts back in, the prompt and it goes write a story., the prompt and it goes write a story., the prompt and it goes write a story., Once upon what's the next word? Well, Once upon what's the next word? Well, Once upon what's the next word? Well, it's a and it keeps going. Write a story, it's a and it keeps going. Write a story, it's a and it keeps going. Write a story, once upon a generates the word time, once upon a generates the word time, once upon a generates the word time, etc., etc. And if it does this many, etc., etc. And if it does this many, etc., etc. And if it does this many, times, it will actually generate a, times, it will actually generate a, times, it will actually generate a, story. Now, you may have been as, story. Now, you may have been as, story. Now, you may have been as, surprised as I was. I first thought it, surprised as I was. I first thought it, surprised as I was. I first thought it, would generate the entire response right, would generate the entire response right, would generate the entire response right, away. Or maybe you would come up with an, away. Or maybe you would come up with an, away. Or maybe you would come up with an, idea and start elaborating on it. But, idea and start elaborating on it. But, idea and start elaborating on it. But, no, it actually just goes one word at a, no, it actually just goes one word at a, no, it actually just goes one word at a, time. Now, in order to generate one word, time. Now, in order to generate one word, time. Now, in order to generate one word, at a time and actually come up with, at a time and actually come up with, at a time and actually come up with, coherent language, it actually needs a, coherent language, it actually needs a, coherent language, it actually needs a, lot of parts. So this is the, lot of parts. So this is the, lot of parts. So this is the, architecture of a transformer. It looks, architecture of a transformer. It looks, architecture of a transformer. It looks, complicated with a bunch of things, complicated with a bunch of things, complicated with a bunch of things, called input embedding, multi-headed, called input embedding, multi-headed, called input embedding, multi-headed, tension, feed forward, etc. But in this, tension, feed forward, etc. But in this, tension, feed forward, etc. But in this, video, I will break down all of these, video, I will break down all of these, video, I will break down all of these, blocks one by one. Actually, the way I, blocks one by one. Actually, the way I, blocks one by one. Actually, the way I, like to see it is more like this. You, like to see it is more like this. You, like to see it is more like this. You, have an input and an output. And let's, have an input and an output. And let's, have an input and an output. And let's, say the input is write a story. And uh, say the input is write a story. And uh, say the input is write a story. And uh, the output has to be probably the word, the output has to be probably the word, the output has to be probably the word, once. But what happens here? Well, first, once. But what happens here? Well, first, once. But what happens here? Well, first, it goes to a step called tokenization., it goes to a step called tokenization., it goes to a step called tokenization., Then a step called embedding. Then a, Then a step called embedding. Then a, Then a step called embedding. Then a, step called positional encoding. This is, step called positional encoding. This is, step called positional encoding. This is, more of a pre-processing. Then it goes, more of a pre-processing. Then it goes, more of a pre-processing. Then it goes, through the main engine which is a, through the main engine which is a, through the main engine which is a, series of transformer blocks. And these, series of transformer blocks. And these, series of transformer blocks. And these, all have something called attention, all have something called attention, all have something called attention, which you saw in the first two videos, which you saw in the first two videos, which you saw in the first two videos, and the feed forward which is a neural, and the feed forward which is a neural, and the feed forward which is a neural, network. There can be many blocks here., network. There can be many blocks here., network. There can be many blocks here., And then at the end there is a block, And then at the end there is a block, And then at the end there is a block, called a softmax. And out of this, called a softmax. And out of this, called a softmax. And out of this, softmax comes the answer which in this, softmax comes the answer which in this, softmax comes the answer which in this, case would be the word once. So fasten, case would be the word once. So fasten, case would be the word once. So fasten, your seal belts because in this video, your seal belts because in this video, your seal belts because in this video, I'm going to tell you about all these, I'm going to tell you about all these, I'm going to tell you about all these, parts., Now before we get to the architecture of, Now before we get to the architecture of, a transformer, let's take a look at this, a transformer, let's take a look at this, a transformer, let's take a look at this, concept of generating sentences one word, concept of generating sentences one word, concept of generating sentences one word, at a time. Recall that the architecture, at a time. Recall that the architecture, at a time. Recall that the architecture, of transformer is this one where the, of transformer is this one where the, of transformer is this one where the, main engine is in the middle as a series, main engine is in the middle as a series, main engine is in the middle as a series, of transform blocks. In particular, the, of transform blocks. In particular, the, of transform blocks. In particular, the, feed forward part. It's a neural network, feed forward part. It's a neural network, feed forward part. It's a neural network, that tries to find the next word in a, that tries to find the next word in a, that tries to find the next word in a, sentence. And all the other blocks are, sentence. And all the other blocks are, sentence. And all the other blocks are, there to help this neural network work, there to help this neural network work, there to help this neural network work, as best as possible. But this concept of, as best as possible. But this concept of, as best as possible. But this concept of, generating the next word in a sentence, generating the next word in a sentence, generating the next word in a sentence, is something that has been around for a, is something that has been around for a, is something that has been around for a, while. Can you think of a time where you, while. Can you think of a time where you, while. Can you think of a time where you, have used it? Well, if you have texted, have used it? Well, if you have texted, have used it? Well, if you have texted, anybody, you may have noticed that the, anybody, you may have noticed that the, anybody, you may have noticed that the, texting apps have these suggestions. You, texting apps have these suggestions. You, texting apps have these suggestions. You, say the sentence hello, how are it, say the sentence hello, how are it, say the sentence hello, how are it, generates three suggestions, you, your, generates three suggestions, you, your, generates three suggestions, you, your, and things. And let's say you pick the, and things. And let's say you pick the, and things. And let's say you pick the, word your. That means that your sentence, word your. That means that your sentence, word your. That means that your sentence, will be hello, how are your and then, will be hello, how are your and then, will be hello, how are your and then, after you type your or you press the, after you type your or you press the, after you type your or you press the, middle button then three more, middle button then three more, middle button then three more, suggestions will appear based on the, suggestions will appear based on the, suggestions will appear based on the, sentence you've given it. Now let's, sentence you've given it. Now let's, sentence you've given it. Now let's, think about the following. Let's say, think about the following. Let's say, think about the following. Let's say, that you have 10 minutes and you are, that you have 10 minutes and you are, that you have 10 minutes and you are, given the task of creating a model that, given the task of creating a model that, given the task of creating a model that, suggests the next word in a sentence., suggests the next word in a sentence., suggests the next word in a sentence., What would you do if you only have 10, What would you do if you only have 10, What would you do if you only have 10, minutes? Well, here's a simple solution., minutes? Well, here's a simple solution., minutes? Well, here's a simple solution., Let's say that you pick the last word, Let's say that you pick the last word, Let's say that you pick the last word, the word R, and you think, what is the, the word R, and you think, what is the, the word R, and you think, what is the, most common word after the word R? Well, most common word after the word R? Well, most common word after the word R? Well, if you have a large data set of, if you have a large data set of, if you have a large data set of, sentences, you can just look at all the, sentences, you can just look at all the, sentences, you can just look at all the, instances of R and then pick the most, instances of R and then pick the most, instances of R and then pick the most, common word after, in which case here is, common word after, in which case here is, common word after, in which case here is, happy. And so you suggest the word, happy. And so you suggest the word, happy. And so you suggest the word, happy. Obviously, that doesn't work very, happy. Obviously, that doesn't work very, happy. Obviously, that doesn't work very, well, but at least it suggests words, well, but at least it suggests words, well, but at least it suggests words, that could be sensible. This has a name., that could be sensible. This has a name., that could be sensible. This has a name., It's called a one g. Now, can you think, It's called a one g. Now, can you think, It's called a one g. Now, can you think, of how to make it more complicated?, of how to make it more complicated?, of how to make it more complicated?, Well, let's look at a three gram. A, Well, let's look at a three gram. A, Well, let's look at a three gram. A, three gram is when you take the last, three gram is when you take the last, three gram is when you take the last, three words, hello, how are and then, three words, hello, how are and then, three words, hello, how are and then, think of what's the most common word, think of what's the most common word, think of what's the most common word, that comes after. So, you look at all, that comes after. So, you look at all, that comes after. So, you look at all, the instances of hello, how are in your, the instances of hello, how are in your, the instances of hello, how are in your, data set of sentences and let's say the, data set of sentences and let's say the, data set of sentences and let's say the, next most common word is things. So, you, next most common word is things. So, you, next most common word is things. So, you, suggest the word things. Now, how can we, suggest the word things. Now, how can we, suggest the word things. Now, how can we, make this even more powerful? Well, make this even more powerful? Well, make this even more powerful? Well, let's do a 10 gram. We pick the last 10, let's do a 10 gram. We pick the last 10, let's do a 10 gram. We pick the last 10, words. For example, in the sentence, words. For example, in the sentence, words. For example, in the sentence, hello, today I would like to inform you, hello, today I would like to inform you, hello, today I would like to inform you, that and then find that in your data, that and then find that in your data, that and then find that in your data, set. And let's say that it only appears, set. And let's say that it only appears, set. And let's say that it only appears, two times, but both of them are followed, two times, but both of them are followed, two times, but both of them are followed, by the word 'the'. So you suggest the, by the word 'the'. So you suggest the, by the word 'the'. So you suggest the, word 'the'. Now, what's a small problem, word 'the'. Now, what's a small problem, word 'the'. Now, what's a small problem, here? Well, the more words we pick, the, here? Well, the more words we pick, the, here? Well, the more words we pick, the, more educated the guess is, but also the, more educated the guess is, but also the, more educated the guess is, but also the, more words we pick, the less likely that, more words we pick, the less likely that, more words we pick, the less likely that, that sentence could have appeared, that sentence could have appeared, that sentence could have appeared, before. If I take a sentence like the, before. If I take a sentence like the, before. If I take a sentence like the, other day I was walking and five, other day I was walking and five, other day I was walking and five, unicorns said well that sentence may, unicorns said well that sentence may, unicorns said well that sentence may, never appear because it could have been, never appear because it could have been, never appear because it could have been, the first time somebody says this, the first time somebody says this, the first time somebody says this, sentence. So you can't rely on just, sentence. So you can't rely on just, sentence. So you can't rely on just, looking at words that appear before, looking at words that appear before, looking at words that appear before, because it may be that they've never, because it may be that they've never, because it may be that they've never, been said before. However, this method, been said before. However, this method, been said before. However, this method, sometimes is used and it's called the, sometimes is used and it's called the, sometimes is used and it's called the, engram method. N is the number of words, engram method. N is the number of words, engram method. N is the number of words, you're looking at. So this one would be, you're looking at. So this one would be, you're looking at. So this one would be, a 10 g. But it looks like we need, a 10 g. But it looks like we need, a 10 g. But it looks like we need, something more advanced. So what can we, something more advanced. So what can we, something more advanced. So what can we, do? Well, something that's been used for, do? Well, something that's been used for, do? Well, something that's been used for, many years in order to suggest the next, many years in order to suggest the next, many years in order to suggest the next, word is something called a neural, word is something called a neural, word is something called a neural, network. So, in a neural network, you, network. So, in a neural network, you, network. So, in a neural network, you, input the sentence hello, how are it, input the sentence hello, how are it, input the sentence hello, how are it, outputs the word your. Now, I'm not, outputs the word your. Now, I'm not, outputs the word your. Now, I'm not, going to go into detail on what a neural, going to go into detail on what a neural, going to go into detail on what a neural, network is. I will go into some detail, network is. I will go into some detail, network is. I will go into some detail, but if you want to really learn what, but if you want to really learn what, but if you want to really learn what, neural networks are and how they work, neural networks are and how they work, neural networks are and how they work, check out this other video in my channel, check out this other video in my channel, check out this other video in my channel, called neural networks. The link is on, called neural networks. The link is on, called neural networks. The link is on, the comments. However, up until, the comments. However, up until, the comments. However, up until, Transformers, these things didn't work, Transformers, these things didn't work, Transformers, these things didn't work, that well. I don't know if you've ever, that well. I don't know if you've ever, that well. I don't know if you've ever, tried to actually write a text by, tried to actually write a text by, tried to actually write a text by, pressing the suggestion in the middle. I, pressing the suggestion in the middle. I, pressing the suggestion in the middle. I, tried doing it and it gave me the, tried doing it and it gave me the, tried doing it and it gave me the, sentence, "Hello, how are you feeling, sentence, "Hello, how are you feeling, sentence, "Hello, how are you feeling, this week and your kind wishes for a, this week and your kind wishes for a, this week and your kind wishes for a, wonderful weekend with your kindness and, wonderful weekend with your kindness and, wonderful weekend with your kindness and, happiness for your family. Love you and, happiness for your family. Love you and, happiness for your family. Love you and, your kindness." That makes absolutely no, your kindness." That makes absolutely no, your kindness." That makes absolutely no, sense. But if you look at every three or, sense. But if you look at every three or, sense. But if you look at every three or, four consecutive words, it makes sense., four consecutive words, it makes sense., four consecutive words, it makes sense., Hello, how are you feeling? And, Hello, how are you feeling? And, Hello, how are you feeling? And, happiness for you, etc., etc. Because, happiness for you, etc., etc. Because, happiness for you, etc., etc. Because, this neural network is trained to, this neural network is trained to, this neural network is trained to, remember a few words at a time and to, remember a few words at a time and to, remember a few words at a time and to, build short sentences, not to build long, build short sentences, not to build long, build short sentences, not to build long, text. And to build long text, we need a, text. And to build long text, we need a, text. And to build long text, we need a, transformer., transformer., transformer., But first, let's take a little look at, But first, let's take a little look at, But first, let's take a little look at, how these neural networks work because, how these neural networks work because, how these neural networks work because, they are the key to understanding a, they are the key to understanding a, they are the key to understanding a, transformer model., So before we get into how neural, So before we get into how neural, networks generate the next word in a, networks generate the next word in a, networks generate the next word in a, sentence, let's look at a simpler, sentence, let's look at a simpler, sentence, let's look at a simpler, example which is called sentiment, example which is called sentiment, example which is called sentiment, analysis. Sentiment analysis has been, analysis. Sentiment analysis has been, analysis. Sentiment analysis has been, around for a while and it's one of the, around for a while and it's one of the, around for a while and it's one of the, quintessential problems in natural, quintessential problems in natural, quintessential problems in natural, language processing and it basically, language processing and it basically, language processing and it basically, consists on building a model that is, consists on building a model that is, consists on building a model that is, able to tell if a sentence is happy or, able to tell if a sentence is happy or, able to tell if a sentence is happy or, sad. For example, the sentence I am very, sad. For example, the sentence I am very, sad. For example, the sentence I am very, happy is a happy sentence and the, happy is a happy sentence and the, happy is a happy sentence and the, sentence today was awful is a sad, sentence today was awful is a sad, sentence today was awful is a sad, sentence. And the idea is that you can, sentence. And the idea is that you can, sentence. And the idea is that you can, use a data set that has been labeled. So, use a data set that has been labeled. So, use a data set that has been labeled. So, you have a lot of sentences and you have, you have a lot of sentences and you have, you have a lot of sentences and you have, the information on if these sentences, the information on if these sentences, the information on if these sentences, are happy or sad. And based on that, are happy or sad. And based on that, are happy or sad. And based on that, data, you can build a model that is able, data, you can build a model that is able, data, you can build a model that is able, to tell if a new sentence is happy or, to tell if a new sentence is happy or, to tell if a new sentence is happy or, sad. Now the question is, how would you, sad. Now the question is, how would you, sad. Now the question is, how would you, train a sentiment analysis model? So I'd, train a sentiment analysis model? So I'd, train a sentiment analysis model? So I'd, love for you to pause the video and, love for you to pause the video and, love for you to pause the video and, actually give it a thought if you, actually give it a thought if you, actually give it a thought if you, haven't seen it before. Here is a way to, haven't seen it before. Here is a way to, haven't seen it before. Here is a way to, do it. A way to do it is actually to, do it. A way to do it is actually to, do it. A way to do it is actually to, have a set of points, a score for each, have a set of points, a score for each, have a set of points, a score for each, one of the words. So happy words will, one of the words. So happy words will, one of the words. So happy words will, have positive points. Sad words will, have positive points. Sad words will, have positive points. Sad words will, have negative points. And very happy, have negative points. And very happy, have negative points. And very happy, words like magnificent will have more, words like magnificent will have more, words like magnificent will have more, points than okay happy words like happy., points than okay happy words like happy., points than okay happy words like happy., And for the rest of words that are, And for the rest of words that are, And for the rest of words that are, neutral, they will have zero points. So, neutral, they will have zero points. So, neutral, they will have zero points. So, if I were to have such a model, if I, if I were to have such a model, if I, if I were to have such a model, if I, were to have such scores, then I can, were to have such scores, then I can, were to have such scores, then I can, easily tell if a sentence is happy or, easily tell if a sentence is happy or, easily tell if a sentence is happy or, sad. For example, the sentence I am very, sad. For example, the sentence I am very, sad. For example, the sentence I am very, happy. Well, what's the total score of, happy. Well, what's the total score of, happy. Well, what's the total score of, the sentence? The words I am and very, the sentence? The words I am and very, the sentence? The words I am and very, are neutral, so they have zero points., are neutral, so they have zero points., are neutral, so they have zero points., And the word happy has three points. So, And the word happy has three points. So, And the word happy has three points. So, when I add them, I get three points. And, when I add them, I get three points. And, when I add them, I get three points. And, if the score is positive, I conclude, if the score is positive, I conclude, if the score is positive, I conclude, that the sentence is happy. Now, for the, that the sentence is happy. Now, for the, that the sentence is happy. Now, for the, sentence, I had a very bad day, the, sentence, I had a very bad day, the, sentence, I had a very bad day, the, words I had, a very, and day are all, words I had, a very, and day are all, words I had, a very, and day are all, neutral, so they have zero points. And, neutral, so they have zero points. And, neutral, so they have zero points. And, the word bad has minus4 points because, the word bad has minus4 points because, the word bad has minus4 points because, it's a sad word. And therefore, when I, it's a sad word. And therefore, when I, it's a sad word. And therefore, when I, add them, I get minus4, which means that, add them, I get minus4, which means that, add them, I get minus4, which means that, the sentence was sad. Now the idea is, the sentence was sad. Now the idea is, the sentence was sad. Now the idea is, that a good sentiment analysis model, that a good sentiment analysis model, that a good sentiment analysis model, will have points for every single word., will have points for every single word., will have points for every single word., And if you are able to score every word, And if you are able to score every word, And if you are able to score every word, I haven't told you yet how to score, I haven't told you yet how to score, I haven't told you yet how to score, them. But if you are able to score them, them. But if you are able to score them, them. But if you are able to score them, then you have a sentiment analysis, then you have a sentiment analysis, then you have a sentiment analysis, model. And this looks like this. I'm, model. And this looks like this. I'm, model. And this looks like this. I'm, going to put all the words on the left, going to put all the words on the left, going to put all the words on the left, each one with a red node and a node on, each one with a red node and a node on, each one with a red node and a node on, the right, and then connect them all, the right, and then connect them all, the right, and then connect them all, with an edge. And on the edges I'm going, with an edge. And on the edges I'm going, with an edge. And on the edges I'm going, to put the scores that we found. So, to put the scores that we found. So, to put the scores that we found. So, happy is 4.2 for example sad is minus, happy is 4.2 for example sad is minus, happy is 4.2 for example sad is minus, 3.7 and all the other ones. And now when, 3.7 and all the other ones. And now when, 3.7 and all the other ones. And now when, I have a sentence like I am happy then I, I have a sentence like I am happy then I, I have a sentence like I am happy then I, simply locate the words I am and happy I, simply locate the words I am and happy I, simply locate the words I am and happy I, put ones on them and then I multiply, put ones on them and then I multiply, put ones on them and then I multiply, these ones by the scores in the edges. I, these ones by the scores in the edges. I, these ones by the scores in the edges. I, actually multiply every score here by, actually multiply every score here by, actually multiply every score here by, the score in the edges. But most of them, the score in the edges. But most of them, the score in the edges. But most of them, are zero except for the words in the, are zero except for the words in the, are zero except for the words in the, sentence and then I add them. So when I, sentence and then I add them. So when I, sentence and then I add them. So when I, add them I get 3.2 plus 0.1 minus 0.2, add them I get 3.2 plus 0.1 minus 0.2, add them I get 3.2 plus 0.1 minus 0.2, and that's 3.1. So I conclude that the, and that's 3.1. So I conclude that the, and that's 3.1. So I conclude that the, sentence is happy. This is called a, sentence is happy. This is called a, sentence is happy. This is called a, perceptron. And if you've seen, perceptron. And if you've seen, perceptron. And if you've seen, perceptrons before you may have noticed, perceptrons before you may have noticed, perceptrons before you may have noticed, that I didn't talk about the bias unit., that I didn't talk about the bias unit., that I didn't talk about the bias unit., There is also a bias unit. There is a, There is also a bias unit. There is a, There is also a bias unit. There is a, constant that we add to all these, constant that we add to all these, constant that we add to all these, sentences and it takes into account, sentences and it takes into account, sentences and it takes into account, empty sentences. If empty sentences are, empty sentences. If empty sentences are, empty sentences. If empty sentences are, happy or sad, then only the bias can, happy or sad, then only the bias can, happy or sad, then only the bias can, tell us that. But as I said, this video, tell us that. But as I said, this video, tell us that. But as I said, this video, is not about neural networks per se. If, is not about neural networks per se. If, is not about neural networks per se. If, you want to know more about perceptrons, you want to know more about perceptrons, you want to know more about perceptrons, the bias unit and these scores and in, the bias unit and these scores and in, the bias unit and these scores and in, particular how they work for sentiment, particular how they work for sentiment, particular how they work for sentiment, analysis, check out this video on my, analysis, check out this video on my, analysis, check out this video on my, channel called logistic regression. This, channel called logistic regression. This, channel called logistic regression. This, video will also tell you how to train a, video will also tell you how to train a, video will also tell you how to train a, neural network, which is how to find, neural network, which is how to find, neural network, which is how to find, these very good scores for each one of, these very good scores for each one of, these very good scores for each one of, the words. Now, the problem is that this, the words. Now, the problem is that this, the words. Now, the problem is that this, is not going to work all the time, is not going to work all the time, is not going to work all the time, because language is a lot more complex, because language is a lot more complex, because language is a lot more complex, than that. If we could just understand, than that. If we could just understand, than that. If we could just understand, language by adding a number to each word, language by adding a number to each word, language by adding a number to each word, and then we know if sentences are happy, and then we know if sentences are happy, and then we know if sentences are happy, or sad or if they're talking about, or sad or if they're talking about, or sad or if they're talking about, different topics, then language would be, different topics, then language would be, different topics, then language would be, very simple. But it's not. So we need, very simple. But it's not. So we need, very simple. But it's not. So we need, something more complicated than that and, something more complicated than that and, something more complicated than that and, that's when we jump to neural networks., Now let me show you what a neural, Now let me show you what a neural, network is and how it can help us with, network is and how it can help us with, network is and how it can help us with, the sentiment analysis problem. So, the sentiment analysis problem. So, the sentiment analysis problem. So, here's the graphical representation that, here's the graphical representation that, here's the graphical representation that, I showed you before of a very simple, I showed you before of a very simple, I showed you before of a very simple, sentiment analysis model. And notice, sentiment analysis model. And notice, sentiment analysis model. And notice, that it looks a lot like a neuron, like, that it looks a lot like a neuron, like, that it looks a lot like a neuron, like, a brain cell because it has a lot of, a brain cell because it has a lot of, a brain cell because it has a lot of, inputs. In this case, each word is a, inputs. In this case, each word is a, inputs. In this case, each word is a, potential input and one output which is, potential input and one output which is, potential input and one output which is, if the sentence is happy or sad. This is, if the sentence is happy or sad. This is, if the sentence is happy or sad. This is, called a perceptron and it's the basis, called a perceptron and it's the basis, called a perceptron and it's the basis, of neural networks and the output is one, of neural networks and the output is one, of neural networks and the output is one, if it's happy and zero if it's sad. Now, if it's happy and zero if it's sad. Now, if it's happy and zero if it's sad. Now, just like brains, they don't have just, just like brains, they don't have just, just like brains, they don't have just, one neuron. They have lots and lots of, one neuron. They have lots and lots of, one neuron. They have lots and lots of, neurons. Well, a neural network is, neurons. Well, a neural network is, neurons. Well, a neural network is, formed by many, many, many of these, formed by many, many, many of these, formed by many, many, many of these, neurons connected in a very strategic, neurons connected in a very strategic, neurons connected in a very strategic, way. Now, I'm going to tell you the, way. Now, I'm going to tell you the, way. Now, I'm going to tell you the, mental picture I have of neural, mental picture I have of neural, mental picture I have of neural, networks. When I think of a concept like, networks. When I think of a concept like, networks. When I think of a concept like, happiness, it's pretty complex. It can't, happiness, it's pretty complex. It can't, happiness, it's pretty complex. It can't, just be determined by having words or, just be determined by having words or, just be determined by having words or, not having these words in a sentence., not having these words in a sentence., not having these words in a sentence., But maybe there are simpler concepts., But maybe there are simpler concepts., But maybe there are simpler concepts., And maybe one of them could be if you're, And maybe one of them could be if you're, And maybe one of them could be if you're, talking about family. Let's say for the, talking about family. Let's say for the, talking about family. Let's say for the, sake of argument that that is a concept, sake of argument that that is a concept, sake of argument that that is a concept, that is slightly easier to model with a, that is slightly easier to model with a, that is slightly easier to model with a, perceptron like this. And maybe there, perceptron like this. And maybe there, perceptron like this. And maybe there, are some other simple topics that are, are some other simple topics that are, are some other simple topics that are, easy to model like this. Let's say if, easy to model like this. Let's say if, easy to model like this. Let's say if, you are talking about free time or not., you are talking about free time or not., you are talking about free time or not., That's a simple concept that can be, That's a simple concept that can be, That's a simple concept that can be, let's say modeled by a perceptron. And, let's say modeled by a perceptron. And, let's say modeled by a perceptron. And, another one is if you are hungry or not., another one is if you are hungry or not., another one is if you are hungry or not., And so we have three perceptrons. And, And so we have three perceptrons. And, And so we have three perceptrons. And, each one of them now doesn't tell us if, each one of them now doesn't tell us if, each one of them now doesn't tell us if, we're happy or sad, but it tells us if, we're happy or sad, but it tells us if, we're happy or sad, but it tells us if, we're talking about free time, if we're, we're talking about free time, if we're, we're talking about free time, if we're, talking about family, and if we're, talking about family, and if we're, talking about family, and if we're, talking about being hungry. And let's, talking about being hungry. And let's, talking about being hungry. And let's, just say again for the sake of argument, just say again for the sake of argument, just say again for the sake of argument, that these three determine if we're, that these three determine if we're, that these three determine if we're, happy or sad. So if we have free time, happy or sad. So if we have free time, happy or sad. So if we have free time, if we are happy with our family, and if, if we are happy with our family, and if, if we are happy with our family, and if, we're not hungry, then let's say that, we're not hungry, then let's say that, we're not hungry, then let's say that, determines if we're happy or sad. And, determines if we're happy or sad. And, determines if we're happy or sad. And, we're going to put weights here. And the, we're going to put weights here. And the, we're going to put weights here. And the, weights can be positive or negative. So, weights can be positive or negative. So, weights can be positive or negative. So, for example, we have a two for free time, for example, we have a two for free time, for example, we have a two for free time, because the more free time you have, the, because the more free time you have, the, because the more free time you have, the, happier you are. Let's say you have a, happier you are. Let's say you have a, happier you are. Let's say you have a, three for family because if you have, three for family because if you have, three for family because if you have, your family, then you're happy. And, your family, then you're happy. And, your family, then you're happy. And, let's say we have a minus two for hungry, let's say we have a minus two for hungry, let's say we have a minus two for hungry, because if you're hungry, maybe that, because if you're hungry, maybe that, because if you're hungry, maybe that, affects your happiness. So what we have, affects your happiness. So what we have, affects your happiness. So what we have, here is a small neural network. It's a, here is a small neural network. It's a, here is a small neural network. It's a, small set of perceptrons or neurons, small set of perceptrons or neurons, small set of perceptrons or neurons, attached to each other in a way that, attached to each other in a way that, attached to each other in a way that, they work together to determine if the, they work together to determine if the, they work together to determine if the, sentence is happy or sad. Obviously, sentence is happy or sad. Obviously, sentence is happy or sad. Obviously, this is still too simple. Maybe we need, this is still too simple. Maybe we need, this is still too simple. Maybe we need, more nodes like rest, family, friends, more nodes like rest, family, friends, more nodes like rest, family, friends, food, money. And maybe those don't, food, money. And maybe those don't, food, money. And maybe those don't, determine happiness, but they determine, determine happiness, but they determine, determine happiness, but they determine, other slightly more complex things such, other slightly more complex things such, other slightly more complex things such, as emotional stability, health, and, as emotional stability, health, and, as emotional stability, health, and, purpose. And maybe those three actually, purpose. And maybe those three actually, purpose. And maybe those three actually, determine if you're happy or sad. Or, determine if you're happy or sad. Or, determine if you're happy or sad. Or, maybe there's hundreds of these and they, maybe there's hundreds of these and they, maybe there's hundreds of these and they, may not be easily labeled for us, but, may not be easily labeled for us, but, may not be easily labeled for us, but, maybe the computer knows. And in the, maybe the computer knows. And in the, maybe the computer knows. And in the, first layer, you have some simple, first layer, you have some simple, first layer, you have some simple, concepts. Then in the next one, you have, concepts. Then in the next one, you have, concepts. Then in the next one, you have, more complicated concepts and so on and, more complicated concepts and so on and, more complicated concepts and so on and, so on. And you could have tens of these, so on. And you could have tens of these, so on. And you could have tens of these, layers or hundreds. And as you may, layers or hundreds. And as you may, layers or hundreds. And as you may, imagine, if we have a neural network big, imagine, if we have a neural network big, imagine, if we have a neural network big, enough, we may have a much more accurate, enough, we may have a much more accurate, enough, we may have a much more accurate, way to tell if a sentence is happy or, way to tell if a sentence is happy or, way to tell if a sentence is happy or, sad. So this is the mental image that I, sad. So this is the mental image that I, sad. So this is the mental image that I, have of a neural network. Now, these can, have of a neural network. Now, these can, have of a neural network. Now, these can, have many applications. Can you help me, have many applications. Can you help me, have many applications. Can you help me, think of some applications of neural, think of some applications of neural, think of some applications of neural, networks? One can be customer support, networks? One can be customer support, networks? One can be customer support, because you can take a chat with a, because you can take a chat with a, because you can take a chat with a, representative and determine if it was, representative and determine if it was, representative and determine if it was, happy or sad. You can also use it for, happy or sad. You can also use it for, happy or sad. You can also use it for, emails. You can use it for spam, emails. You can use it for spam, emails. You can use it for spam, detection. Let's say that you train a, detection. Let's say that you train a, detection. Let's say that you train a, neural network to tell you if the email, neural network to tell you if the email, neural network to tell you if the email, is spam or not spam. However, notice, is spam or not spam. However, notice, is spam or not spam. However, notice, that these two examples have two, that these two examples have two, that these two examples have two, outputs, happy or sad or spam or not, outputs, happy or sad or spam or not, outputs, happy or sad or spam or not, spam. You could actually have a neural, spam. You could actually have a neural, spam. You could actually have a neural, network that works for more than two, network that works for more than two, network that works for more than two, output. For example, email, output. For example, email, output. For example, email, classification. You can classify your, classification. You can classify your, classification. You can classify your, email into three. Personal email, work, email into three. Personal email, work, email into three. Personal email, work, email, and invitations or even more. So, email, and invitations or even more. So, email, and invitations or even more. So, this opens a new door. Now, let's think, this opens a new door. Now, let's think, this opens a new door. Now, let's think, of an extreme case where there's not two, of an extreme case where there's not two, of an extreme case where there's not two, or three outputs, but there's lots of, or three outputs, but there's lots of, or three outputs, but there's lots of, them. How many of them? As many as, them. How many of them? As many as, them. How many of them? As many as, words. So I can think of a neural, words. So I can think of a neural, words. So I can think of a neural, network where the input is a sentence, network where the input is a sentence, network where the input is a sentence, given by a bunch of words and the output, given by a bunch of words and the output, given by a bunch of words and the output, is one word out of all the words that, is one word out of all the words that, is one word out of all the words that, exist and that is a neural network that, exist and that is a neural network that, exist and that is a neural network that, can predict the next word at a sentence., can predict the next word at a sentence., can predict the next word at a sentence., Now before we get to that let's look at, Now before we get to that let's look at, Now before we get to that let's look at, how the architecture would look. So, how the architecture would look. So, how the architecture would look. So, let's look at email classification., let's look at email classification., let's look at email classification., Here's a neural network that could tell, Here's a neural network that could tell, Here's a neural network that could tell, you if your email is personal, you if your email is personal, you if your email is personal, workreated or an invitation. Notice that, workreated or an invitation. Notice that, workreated or an invitation. Notice that, it doesn't have one output. It has three, it doesn't have one output. It has three, it doesn't have one output. It has three, outputs. And I can think of many more, outputs. And I can think of many more, outputs. And I can think of many more, outputs. I can think of all these, outputs. I can think of all these, outputs. I can think of all these, outputs up to here which are as many as, outputs up to here which are as many as, outputs up to here which are as many as, the words that are inputed. So in this, the words that are inputed. So in this, the words that are inputed. So in this, neural network, the first layer and the, neural network, the first layer and the, neural network, the first layer and the, last layer have the same size. And when, last layer have the same size. And when, last layer have the same size. And when, you input a sentence like how are it, you input a sentence like how are it, you input a sentence like how are it, will output a word which is for example, will output a word which is for example, will output a word which is for example, you. So this is a neural network that, you. So this is a neural network that, you. So this is a neural network that, can predict the next word at a sentence, can predict the next word at a sentence, can predict the next word at a sentence, and is very similar to the neural, and is very similar to the neural, and is very similar to the neural, network that appears in the transformer., network that appears in the transformer., network that appears in the transformer., Of course the neural network in the, Of course the neural network in the, Of course the neural network in the, transformer is bigger has a more, transformer is bigger has a more, transformer is bigger has a more, structure is more complicated but at the, structure is more complicated but at the, structure is more complicated but at the, end of the day it's a big big huge, end of the day it's a big big huge, end of the day it's a big big huge, neural network that tells the next word, neural network that tells the next word, neural network that tells the next word, of a sentence. However, as I said before, of a sentence. However, as I said before, of a sentence. However, as I said before, just a neural network doesn't do the, just a neural network doesn't do the, just a neural network doesn't do the, job. A transformer needs all these other, job. A transformer needs all these other, job. A transformer needs all these other, blocks to help it work so well. So now, blocks to help it work so well. So now, blocks to help it work so well. So now, we're ready to go into all these blocks., we're ready to go into all these blocks., we're ready to go into all these blocks., [Music], [Music], [Music], So the first one of the blocks in a, So the first one of the blocks in a, So the first one of the blocks in a, transformer is tokenization and it goes, transformer is tokenization and it goes, transformer is tokenization and it goes, right after you input the text. And, right after you input the text. And, right after you input the text. And, tokenization pretty much breaks, tokenization pretty much breaks, tokenization pretty much breaks, everything into units. For the most, everything into units. For the most, everything into units. For the most, part, each unit is a word. So for, part, each unit is a word. So for, part, each unit is a word. So for, example, the sentence write a story gets, example, the sentence write a story gets, example, the sentence write a story gets, broken into a bunch of tokens. One for, broken into a bunch of tokens. One for, broken into a bunch of tokens. One for, the verb write, one for the word a, for, the verb write, one for the word a, for, the verb write, one for the word a, for, the word story, and for the period, the word story, and for the period, the word story, and for the period, because punctuation signs also have, because punctuation signs also have, because punctuation signs also have, tokens. Sometimes you will also have, tokens. Sometimes you will also have, tokens. Sometimes you will also have, tokens for the beginning and the end of, tokens for the beginning and the end of, tokens for the beginning and the end of, a sentence. However, some words like, a sentence. However, some words like, a sentence. However, some words like, doesn't get broken into two tokens. The, doesn't get broken into two tokens. The, doesn't get broken into two tokens. The, token for does and the token for int., token for does and the token for int., token for does and the token for int., There's a whole science into building, There's a whole science into building, There's a whole science into building, good tokens and I'm not going to, good tokens and I'm not going to, good tokens and I'm not going to, elaborate here. But if you want to learn, elaborate here. But if you want to learn, elaborate here. But if you want to learn, all about tokens, I want to recommend, all about tokens, I want to recommend, all about tokens, I want to recommend, you this video from Jay Alamar called, you this video from Jay Alamar called, you this video from Jay Alamar called, Did the LLM Even See What You Typed?, Did the LLM Even See What You Typed?, Did the LLM Even See What You Typed?, Which is also a chapter on his great, Which is also a chapter on his great, Which is also a chapter on his great, book, Hands on Large Language Models., The next step is embeddings and it's, The next step is embeddings and it's, right here. Now, I've talked about, right here. Now, I've talked about, right here. Now, I've talked about, embeddings on the previous two videos on, embeddings on the previous two videos on, embeddings on the previous two videos on, attention, but I'll tell you a brief, attention, but I'll tell you a brief, attention, but I'll tell you a brief, summary here. So embedded are the most, summary here. So embedded are the most, summary here. So embedded are the most, important part of a large language model, important part of a large language model, important part of a large language model, because it's really where the, because it's really where the, because it's really where the, translation happens from humans who, translation happens from humans who, translation happens from humans who, speak in words to computers who speak in, speak in words to computers who speak in, speak in words to computers who speak in, numbers. The stronger this bridge is, numbers. The stronger this bridge is, numbers. The stronger this bridge is, the better large language models are., the better large language models are., the better large language models are., And the bridge is exactly an embedding., And the bridge is exactly an embedding., And the bridge is exactly an embedding., If you recall from the previous video, If you recall from the previous video, If you recall from the previous video, we had a quiz where the question is, we had a quiz where the question is, we had a quiz where the question is, where would you put the word apple? So, where would you put the word apple? So, where would you put the word apple? So, here we have all the coordinates of the, here we have all the coordinates of the, here we have all the coordinates of the, words where the first number is the, words where the first number is the, words where the first number is the, horizontal coordinate and the second one, horizontal coordinate and the second one, horizontal coordinate and the second one, the vertical coordinate. And the, the vertical coordinate. And the, the vertical coordinate. And the, question is where would you put apple?, question is where would you put apple?, question is where would you put apple?, in position A, B or C? And the answer, in position A, B or C? And the answer, in position A, B or C? And the answer, was C because it should go around the, was C because it should go around the, was C because it should go around the, fruits. So a good place for apple would, fruits. So a good place for apple would, fruits. So a good place for apple would, be the coordinates 55. Now that's as, be the coordinates 55. Now that's as, be the coordinates 55. Now that's as, much as we talked about embeddings and, much as we talked about embeddings and, much as we talked about embeddings and, we also mentioned that you don't just, we also mentioned that you don't just, we also mentioned that you don't just, associate them to two numbers, you can, associate them to two numbers, you can, associate them to two numbers, you can, associate them to thousands of numbers., associate them to thousands of numbers., associate them to thousands of numbers., But the point is that similar words get, But the point is that similar words get, But the point is that similar words get, sent to similar numbers. And in some way, sent to similar numbers. And in some way, sent to similar numbers. And in some way, the embedding is a bit of a description, the embedding is a bit of a description, the embedding is a bit of a description, of the word. So every coordinate, of the word. So every coordinate, of the word. So every coordinate, actually means something. But here we, actually means something. But here we, actually means something. But here we, can get a little further. Actually, how, can get a little further. Actually, how, can get a little further. Actually, how, do you think we build embeddings? And I, do you think we build embeddings? And I, do you think we build embeddings? And I, encourage you to pause this video and, encourage you to pause this video and, encourage you to pause this video and, think about it. There are many ways and, think about it. There are many ways and, think about it. There are many ways and, I'm going to show you one that actually, I'm going to show you one that actually, I'm going to show you one that actually, has to do with neural networks. So bear, has to do with neural networks. So bear, has to do with neural networks. So bear, with me. I'd like to think of the, with me. I'd like to think of the, with me. I'd like to think of the, following thought experiment. Imagine, following thought experiment. Imagine, following thought experiment. Imagine, that you actually want to associate, that you actually want to associate, that you actually want to associate, numbers to every single word. How do you, numbers to every single word. How do you, numbers to every single word. How do you, do it? Well, let's say that we can take, do it? Well, let's say that we can take, do it? Well, let's say that we can take, ourselves or any of our friends and put, ourselves or any of our friends and put, ourselves or any of our friends and put, them into this experiment. So, here's a, them into this experiment. So, here's a, them into this experiment. So, here's a, human and a human has a brain. So, what, human and a human has a brain. So, what, human and a human has a brain. So, what, we're going to do is we are going to put, we're going to do is we are going to put, we're going to do is we are going to put, two sensors somewhere in the head. Now, two sensors somewhere in the head. Now, two sensors somewhere in the head. Now, I don't know any neuroscience, so I'm, I don't know any neuroscience, so I'm, I don't know any neuroscience, so I'm, going to make up a lot of stuff. So, as, going to make up a lot of stuff. So, as, going to make up a lot of stuff. So, as, I said, please bear with me. But this is, I said, please bear with me. But this is, I said, please bear with me. But this is, the thought experiment that I like to, the thought experiment that I like to, the thought experiment that I like to, use to imagine embeddings. So, we have, use to imagine embeddings. So, we have, use to imagine embeddings. So, we have, two sensors that we locate somewhere in, two sensors that we locate somewhere in, two sensors that we locate somewhere in, the head of our subject. and we plug, the head of our subject. and we plug, the head of our subject. and we plug, them into a machine and each one of the, them into a machine and each one of the, them into a machine and each one of the, sensors is going to measure some, sensors is going to measure some, sensors is going to measure some, activity on that part of the brain. In, activity on that part of the brain. In, activity on that part of the brain. In, particular, maybe if we can get it to, particular, maybe if we can get it to, particular, maybe if we can get it to, measure the activity on one brain cell, measure the activity on one brain cell, measure the activity on one brain cell, that would be wonderful. But let's just, that would be wonderful. But let's just, that would be wonderful. But let's just, say it measures something. Now, in front, say it measures something. Now, in front, say it measures something. Now, in front, of the subject, we put a strawberry and, of the subject, we put a strawberry and, of the subject, we put a strawberry and, then we measure what each of the sensors, then we measure what each of the sensors, then we measure what each of the sensors, records. Let's say the first one records, records. Let's say the first one records, records. Let's say the first one records, a five and then the second one records a, a five and then the second one records a, a five and then the second one records a, four. That's our embedding. So for the, four. That's our embedding. So for the, four. That's our embedding. So for the, word strawberry, we associate it with, word strawberry, we associate it with, word strawberry, we associate it with, the numbers five and four. Now what do, the numbers five and four. Now what do, the numbers five and four. Now what do, you think happens if we change the, you think happens if we change the, you think happens if we change the, strawberry to an apple and measure the, strawberry to an apple and measure the, strawberry to an apple and measure the, same thing? Some numbers will come out, same thing? Some numbers will come out, same thing? Some numbers will come out, and maybe it's 55. So we record that the, and maybe it's 55. So we record that the, and maybe it's 55. So we record that the, embedding for apple is 55. Now if we put, embedding for apple is 55. Now if we put, embedding for apple is 55. Now if we put, a castle instead then some numbers will, a castle instead then some numbers will, a castle instead then some numbers will, come out and let's say they are two and, come out and let's say they are two and, come out and let's say they are two and, one. So we record two and one as the, one. So we record two and one as the, one. So we record two and one as the, embedding for castle. Now, as I said, I, embedding for castle. Now, as I said, I, embedding for castle. Now, as I said, I, don't know any neuroscience, but I, don't know any neuroscience, but I, don't know any neuroscience, but I, imagine that a strawberry and an apple, imagine that a strawberry and an apple, imagine that a strawberry and an apple, being similar must fire similar places, being similar must fire similar places, being similar must fire similar places, in the brain, whereas a castle would, in the brain, whereas a castle would, in the brain, whereas a castle would, fire different places. Therefore, the, fire different places. Therefore, the, fire different places. Therefore, the, numbers associated to strawberry and, numbers associated to strawberry and, numbers associated to strawberry and, apple are likely to be similar and the, apple are likely to be similar and the, apple are likely to be similar and the, numbers associated to castle are likely, numbers associated to castle are likely, numbers associated to castle are likely, to be different than these two. So, we, to be different than these two. So, we, to be different than these two. So, we, have pretty much an embedding. I like to, have pretty much an embedding. I like to, have pretty much an embedding. I like to, imagine that somewhere in the brain the, imagine that somewhere in the brain the, imagine that somewhere in the brain the, words are located and somewhere, words are located and somewhere, words are located and somewhere, strawberry and apple are close by and, strawberry and apple are close by and, strawberry and apple are close by and, castle is far away. And this is how I, castle is far away. And this is how I, castle is far away. And this is how I, imagine an embedding. Now of course we, imagine an embedding. Now of course we, imagine an embedding. Now of course we, don't have subjects with brains to, don't have subjects with brains to, don't have subjects with brains to, experiment and to build embeddings like, experiment and to build embeddings like, experiment and to build embeddings like, that. But what's the closest thing we, that. But what's the closest thing we, that. But what's the closest thing we, have to a brain in machine learning?, have to a brain in machine learning?, have to a brain in machine learning?, Well, we have a neural network which, Well, we have a neural network which, Well, we have a neural network which, loosely emulates the brain. So let's try, loosely emulates the brain. So let's try, loosely emulates the brain. So let's try, this experiment on a neural network., this experiment on a neural network., this experiment on a neural network., Let's show it as strawberry. What does, Let's show it as strawberry. What does, Let's show it as strawberry. What does, it mean to show it a strawberry? Well, it mean to show it a strawberry? Well, it mean to show it a strawberry? Well, this was a neural network that takes a, this was a neural network that takes a, this was a neural network that takes a, sentence and builds the next word. So, sentence and builds the next word. So, sentence and builds the next word. So, let's just feed the word strawberry to, let's just feed the word strawberry to, let's just feed the word strawberry to, it and see what comes out. Maybe the, it and see what comes out. Maybe the, it and see what comes out. Maybe the, next word comes out as is or ate or, next word comes out as is or ate or, next word comes out as is or ate or, delicious. It doesn't matter. It doesn't, delicious. It doesn't matter. It doesn't, delicious. It doesn't matter. It doesn't, really matter what comes out. The pink, really matter what comes out. The pink, really matter what comes out. The pink, layer doesn't really matter. Let's take, layer doesn't really matter. Let's take, layer doesn't really matter. Let's take, a look at the green layer and put our, a look at the green layer and put our, a look at the green layer and put our, sensors there. So, let's measure what, sensors there. So, let's measure what, sensors there. So, let's measure what, numbers came out of here when making, numbers came out of here when making, numbers came out of here when making, that prediction. And let's say that the, that prediction. And let's say that the, that prediction. And let's say that the, numbers, for example, 1, 3, and two came, numbers, for example, 1, 3, and two came, numbers, for example, 1, 3, and two came, out. Well, that's going to be the, out. Well, that's going to be the, out. Well, that's going to be the, embedding for strawberry. It's going to, embedding for strawberry. It's going to, embedding for strawberry. It's going to, be 1 3 and two. Now, let's do something, be 1 3 and two. Now, let's do something, be 1 3 and two. Now, let's do something, similar and put an apple. And let's say, similar and put an apple. And let's say, similar and put an apple. And let's say, the numbers 1.1, 2.9, and 2.2 came out, the numbers 1.1, 2.9, and 2.2 came out, the numbers 1.1, 2.9, and 2.2 came out, which are similar to the words for, which are similar to the words for, which are similar to the words for, strawberry. Why? Because the reaction, strawberry. Why? Because the reaction, strawberry. Why? Because the reaction, that a neural network has to strawberry, that a neural network has to strawberry, that a neural network has to strawberry, is similar to the one it has for apple, is similar to the one it has for apple, is similar to the one it has for apple, because the same words or very similar, because the same words or very similar, because the same words or very similar, words would come out. So one would, words would come out. So one would, words would come out. So one would, imagine that in the previous layer, imagine that in the previous layer, imagine that in the previous layer, whatever numbers came in must be pretty, whatever numbers came in must be pretty, whatever numbers came in must be pretty, similar and remember that the neural, similar and remember that the neural, similar and remember that the neural, network has layers and each layer, network has layers and each layer, network has layers and each layer, understands deeper and deeper properties, understands deeper and deeper properties, understands deeper and deeper properties, of the word. So the penultimate layer, of the word. So the penultimate layer, of the word. So the penultimate layer, has to understand the word pretty well., has to understand the word pretty well., has to understand the word pretty well., Therefore the three numbers coming out, Therefore the three numbers coming out, Therefore the three numbers coming out, of the penultimate layer in this neural, of the penultimate layer in this neural, of the penultimate layer in this neural, network must be similar for strawberry, network must be similar for strawberry, network must be similar for strawberry, and apple and must be different for, and apple and must be different for, and apple and must be different for, castle. Maybe for castle they are 7 -, castle. Maybe for castle they are 7 -, castle. Maybe for castle they are 7 -, 5.4 and 0.4. In other words, when we, 5.4 and 0.4. In other words, when we, 5.4 and 0.4. In other words, when we, train a neural network to guess the next, train a neural network to guess the next, train a neural network to guess the next, word, it kind of has to understand words, word, it kind of has to understand words, word, it kind of has to understand words, pretty well. And understanding words, pretty well. And understanding words, pretty well. And understanding words, pretty well means that these layers must, pretty well means that these layers must, pretty well means that these layers must, capture properties of the word. And the, capture properties of the word. And the, capture properties of the word. And the, penultimate is going to capture some, penultimate is going to capture some, penultimate is going to capture some, pretty deep properties. And therefore, pretty deep properties. And therefore, pretty deep properties. And therefore, if we train a neural network, well, for, if we train a neural network, well, for, if we train a neural network, well, for, free, we get a pretty good embedding by, free, we get a pretty good embedding by, free, we get a pretty good embedding by, simply looking at the penultimate layer., simply looking at the penultimate layer., simply looking at the penultimate layer., Now, in this case, the layer has three, Now, in this case, the layer has three, Now, in this case, the layer has three, nodes, but it can have a hundred or, nodes, but it can have a hundred or, nodes, but it can have a hundred or, thousands. And so therefore we can, thousands. And so therefore we can, thousands. And so therefore we can, create embeddings of thousands of, create embeddings of thousands of, create embeddings of thousands of, numbers like this. This is a pretty, numbers like this. This is a pretty, numbers like this. This is a pretty, common method to find embeddings and, common method to find embeddings and, common method to find embeddings and, it's called word tovec. Why word to vec?, it's called word tovec. Why word to vec?, it's called word tovec. Why word to vec?, Because it sends the words to vectors., Because it sends the words to vectors., Because it sends the words to vectors., As I said there are many ways to create, As I said there are many ways to create, As I said there are many ways to create, embeddings but those embeddings created, embeddings but those embeddings created, embeddings but those embeddings created, with neural networks are some of the, with neural networks are some of the, with neural networks are some of the, most powerful ones., Now the next step is positional, Now the next step is positional, encoding. Positional encoding is very, encoding. Positional encoding is very, encoding. Positional encoding is very, important because it takes care of, important because it takes care of, important because it takes care of, order. Now, to be honest, positional, order. Now, to be honest, positional, order. Now, to be honest, positional, encoding looks a little strange to me, encoding looks a little strange to me, encoding looks a little strange to me, because it doesn't really take care of, because it doesn't really take care of, because it doesn't really take care of, the order in a way that captures the, the order in a way that captures the, the order in a way that captures the, semantics. It just kind of, semantics. It just kind of, semantics. It just kind of, differentiates sentences that have, differentiates sentences that have, differentiates sentences that have, different order. But at the end of the, different order. But at the end of the, different order. But at the end of the, day, it works well, so we use it. So, day, it works well, so we use it. So, day, it works well, so we use it. So, let's say you have the sentence, write a, let's say you have the sentence, write a, let's say you have the sentence, write a, story period that has four tokens. And, story period that has four tokens. And, story period that has four tokens. And, that's not the same as saying story, that's not the same as saying story, that's not the same as saying story, period I write. that makes no sense. But, period I write. that makes no sense. But, period I write. that makes no sense. But, now imagine that you make it into, now imagine that you make it into, now imagine that you make it into, something that makes sense by, something that makes sense by, something that makes sense by, reorganizing the words and the answer, reorganizing the words and the answer, reorganizing the words and the answer, would be different. So at the end of the, would be different. So at the end of the, would be different. So at the end of the, day, the order of the words actually, day, the order of the words actually, day, the order of the words actually, matters. So we would not want to feed, matters. So we would not want to feed, matters. So we would not want to feed, the same thing into a neural network, the same thing into a neural network, the same thing into a neural network, with the sentence on the left and with, with the sentence on the left and with, with the sentence on the left and with, the sentence on the right. So what do we, the sentence on the right. So what do we, the sentence on the right. So what do we, do? Well, we just learned embeddings. So, do? Well, we just learned embeddings. So, do? Well, we just learned embeddings. So, let's look at what the embedding looks, let's look at what the embedding looks, let's look at what the embedding looks, for these two sentences. Let's say that, for these two sentences. Let's say that, for these two sentences. Let's say that, this is where the words are located in, this is where the words are located in, this is where the words are located in, the embedding. And what we're going to, the embedding. And what we're going to, the embedding. And what we're going to, do is perturb each of the words slightly, do is perturb each of the words slightly, do is perturb each of the words slightly, in such a way that the first word gets, in such a way that the first word gets, in such a way that the first word gets, perturbed in the same way, then the, perturbed in the same way, then the, perturbed in the same way, then the, second word gets perturbed in a, second word gets perturbed in a, second word gets perturbed in a, different way, etc., etc. So, I'm going, different way, etc., etc. So, I'm going, different way, etc., etc. So, I'm going, to symbolize that by arrows. The first, to symbolize that by arrows. The first, to symbolize that by arrows. The first, word gets moved to the right, the second, word gets moved to the right, the second, word gets moved to the right, the second, one up, the third one diagonally to the, one up, the third one diagonally to the, one up, the third one diagonally to the, left, and the fourth one a little bit, left, and the fourth one a little bit, left, and the fourth one a little bit, more towards the left. It doesn't really, more towards the left. It doesn't really, more towards the left. It doesn't really, matter. The fact is that we are moving, matter. The fact is that we are moving, matter. The fact is that we are moving, the words in this direction. And for the, the words in this direction. And for the, the words in this direction. And for the, second sentence, we're doing the same, second sentence, we're doing the same, second sentence, we're doing the same, thing. Now, when the words move now, we, thing. Now, when the words move now, we, thing. Now, when the words move now, we, get different embeddings for these two, get different embeddings for these two, get different embeddings for these two, sentences. And as I said, there's no, sentences. And as I said, there's no, sentences. And as I said, there's no, semantics here. It's really adding, semantics here. It's really adding, semantics here. It's really adding, different numbers to the first word, the, different numbers to the first word, the, different numbers to the first word, the, second word, the third word, the fourth, second word, the third word, the fourth, second word, the third word, the fourth, word, etc. in such a way that now the, word, etc. in such a way that now the, word, etc. in such a way that now the, embedding captures order in some way., embedding captures order in some way., embedding captures order in some way., and then the neural network would take, and then the neural network would take, and then the neural network would take, care of learning this order more. I, care of learning this order more. I, care of learning this order more. I, understand if it's not as satisfying as, understand if it's not as satisfying as, understand if it's not as satisfying as, you wanted it to be, but it works really, you wanted it to be, but it works really, you wanted it to be, but it works really, well. And now these arrows, what is the, well. And now these arrows, what is the, well. And now these arrows, what is the, rule for these arrows? Well, the reason, rule for these arrows? Well, the reason, rule for these arrows? Well, the reason, I'm not being more specific is because, I'm not being more specific is because, I'm not being more specific is because, they change a lot. I've seen them using, they change a lot. I've seen them using, they change a lot. I've seen them using, signs and cosiness. I've seen them using, signs and cosiness. I've seen them using, signs and cosiness. I've seen them using, exponential function, but better and, exponential function, but better and, exponential function, but better and, better positional encoding functions, better positional encoding functions, better positional encoding functions, keep coming out. So maybe by the time, keep coming out. So maybe by the time, keep coming out. So maybe by the time, you see this video, there's a completely, you see this video, there's a completely, you see this video, there's a completely, different one. All you need to know is, different one. All you need to know is, different one. All you need to know is, that these numbers normally follow a, that these numbers normally follow a, that these numbers normally follow a, sequence. So for example, this sequence, sequence. So for example, this sequence, sequence. So for example, this sequence, of arrows that rotate slower and slower, of arrows that rotate slower and slower, of arrows that rotate slower and slower, every time are one that works as, every time are one that works as, every time are one that works as, positional encoding. But if you want to, positional encoding. But if you want to, positional encoding. But if you want to, remember one thing about position, remember one thing about position, remember one thing about position, coding, you just find some kind of, coding, you just find some kind of, coding, you just find some kind of, sequence that you add to each one of the, sequence that you add to each one of the, sequence that you add to each one of the, words in the sentence in order to move, words in the sentence in order to move, words in the sentence in order to move, them around and perturb the order., Now the next step is attention which is, Now the next step is attention which is, crucial in the transformer models. As a, crucial in the transformer models. As a, crucial in the transformer models. As a, matter of fact, it's really the step, matter of fact, it's really the step, matter of fact, it's really the step, that made them work really well because, that made them work really well because, that made them work really well because, it's the step that really captures the, it's the step that really captures the, it's the step that really captures the, context. Attention can appear many times, context. Attention can appear many times, context. Attention can appear many times, in the architecture because it appears, in the architecture because it appears, in the architecture because it appears, in each one of the transformer blocks, in each one of the transformer blocks, in each one of the transformer blocks, followed by some feed forward layers., followed by some feed forward layers., followed by some feed forward layers., And I know that I already talked about, And I know that I already talked about, And I know that I already talked about, attention for two long videos which I, attention for two long videos which I, attention for two long videos which I, really recommend you to watch if you, really recommend you to watch if you, really recommend you to watch if you, haven't. But I can't help it. I love, haven't. But I can't help it. I love, haven't. But I can't help it. I love, talking about attention and so I'm going, talking about attention and so I'm going, talking about attention and so I'm going, to tell you a little bit more about it, to tell you a little bit more about it, to tell you a little bit more about it, here. So to summarize, if you have an, here. So to summarize, if you have an, here. So to summarize, if you have an, embedding, let's say you have all the, embedding, let's say you have all the, embedding, let's say you have all the, fruits around here. You have a, fruits around here. You have a, fruits around here. You have a, strawberry, an orange, banana, and a, strawberry, an orange, banana, and a, strawberry, an orange, banana, and a, cherry. And here you have a bunch of, cherry. And here you have a bunch of, cherry. And here you have a bunch of, computer brands and computer words. You, computer brands and computer words. You, computer brands and computer words. You, have laptops, smartphone, Android, and, have laptops, smartphone, Android, and, have laptops, smartphone, Android, and, Microsoft. The question is where would, Microsoft. The question is where would, Microsoft. The question is where would, you put the word Apple? Well, it kind of, you put the word Apple? Well, it kind of, you put the word Apple? Well, it kind of, belongs to both. So it's hard to tell., belongs to both. So it's hard to tell., belongs to both. So it's hard to tell., So the best you can do is to put it, So the best you can do is to put it, So the best you can do is to put it, somewhere in the middle. And that says, somewhere in the middle. And that says, somewhere in the middle. And that says, that no matter how good embedding is, that no matter how good embedding is, that no matter how good embedding is, there are some things it cannot capture, there are some things it cannot capture, there are some things it cannot capture, because it's not going to know where to, because it's not going to know where to, because it's not going to know where to, put apple. So we are forced to use the, put apple. So we are forced to use the, put apple. So we are forced to use the, context. And in the context, we have to, context. And in the context, we have to, context. And in the context, we have to, look at the sentence. If you have the, look at the sentence. If you have the, look at the sentence. If you have the, sentence, please buy an apple and an, sentence, please buy an apple and an, sentence, please buy an apple and an, orange, then apple means a fruit. And if, orange, then apple means a fruit. And if, orange, then apple means a fruit. And if, you have the sentence, Apple, the new, you have the sentence, Apple, the new, you have the sentence, Apple, the new, phone, then you're talking about the, phone, then you're talking about the, phone, then you're talking about the, technology brand. So that means the word, technology brand. So that means the word, technology brand. So that means the word, apple gets influenced by the word orange, apple gets influenced by the word orange, apple gets influenced by the word orange, in the first sentence which means that, in the first sentence which means that, in the first sentence which means that, the orange has a gravitational pull over, the orange has a gravitational pull over, the orange has a gravitational pull over, the word apple and pulls it closer to, the word apple and pulls it closer to, the word apple and pulls it closer to, it. And for the second sentence the word, it. And for the second sentence the word, it. And for the second sentence the word, phone affects the word apple. So the, phone affects the word apple. So the, phone affects the word apple. So the, phone pulls the word apple here. And, phone pulls the word apple here. And, phone pulls the word apple here. And, therefore now we're not going to use the, therefore now we're not going to use the, therefore now we're not going to use the, coordinates of the apple in the middle., coordinates of the apple in the middle., coordinates of the apple in the middle., We're going to use for the first, We're going to use for the first, We're going to use for the first, sentence the coordinates of the apple on, sentence the coordinates of the apple on, sentence the coordinates of the apple on, the top right that is pulled by the, the top right that is pulled by the, the top right that is pulled by the, orange. And in the second sentence, orange. And in the second sentence, orange. And in the second sentence, we're going to use the coordinates for, we're going to use the coordinates for, we're going to use the coordinates for, Apple when it's pulled by the word, Apple when it's pulled by the word, Apple when it's pulled by the word, phone. So, I like to imagine attention, phone. So, I like to imagine attention, phone. So, I like to imagine attention, as gravitational pull between words that, as gravitational pull between words that, as gravitational pull between words that, appear in the same sentence or in the, appear in the same sentence or in the, appear in the same sentence or in the, same context. Now, speaking about, same context. Now, speaking about, same context. Now, speaking about, context, there is this example that I, context, there is this example that I, context, there is this example that I, really like. Imagine the sentence, the, really like. Imagine the sentence, the, really like. Imagine the sentence, the, bear ate the honey because it was blank., bear ate the honey because it was blank., bear ate the honey because it was blank., We haven't finished the sentence. The, We haven't finished the sentence. The, We haven't finished the sentence. The, question for you is, what does the word, question for you is, what does the word, question for you is, what does the word, it describe? Is it talking about the, it describe? Is it talking about the, it describe? Is it talking about the, bear or is it talking about honey? Well, bear or is it talking about honey? Well, bear or is it talking about honey? Well, we don't know because it depends on the, we don't know because it depends on the, we don't know because it depends on the, next word. It could really be either, next word. It could really be either, next word. It could really be either, one. In particular, if the blank word is, one. In particular, if the blank word is, one. In particular, if the blank word is, hungry, the bear ate the honey because, hungry, the bear ate the honey because, hungry, the bear ate the honey because, it was hungry, then we're definitely, it was hungry, then we're definitely, it was hungry, then we're definitely, talking about the bear. But if the, talking about the bear. But if the, talking about the bear. But if the, sentence says, "The bear ate the honey, sentence says, "The bear ate the honey, sentence says, "The bear ate the honey, because it was delicious," then we're, because it was delicious," then we're, because it was delicious," then we're, definitely talking about the honey. So, definitely talking about the honey. So, definitely talking about the honey. So, that last word is the one that makes a, that last word is the one that makes a, that last word is the one that makes a, difference. Now, how do we explain this, difference. Now, how do we explain this, difference. Now, how do we explain this, using attention and gravitational pull, using attention and gravitational pull, using attention and gravitational pull, between words? Well, let's look at all, between words? Well, let's look at all, between words? Well, let's look at all, the words and let's put them in an, the words and let's put them in an, the words and let's put them in an, embedding like we know how to do and, embedding like we know how to do and, embedding like we know how to do and, let's forget about the last word. So, let's forget about the last word. So, let's forget about the last word. So, let's see what happens before we've, let's see what happens before we've, let's see what happens before we've, reached the end of the sentence. In this, reached the end of the sentence. In this, reached the end of the sentence. In this, embedding, words are similar if they, embedding, words are similar if they, embedding, words are similar if they, appear in the same context many times., appear in the same context many times., appear in the same context many times., So, two words that are likely to be, So, two words that are likely to be, So, two words that are likely to be, replaceable are similar. So, for, replaceable are similar. So, for, replaceable are similar. So, for, example, it and bear are similar because, example, it and bear are similar because, example, it and bear are similar because, many times a sentence could have the, many times a sentence could have the, many times a sentence could have the, word it and could also have the word, word it and could also have the word, word it and could also have the word, bear. And the same thing happens with it, bear. And the same thing happens with it, bear. And the same thing happens with it, and honey. They are similar because I, and honey. They are similar because I, and honey. They are similar because I, could have sentences with the word it, could have sentences with the word it, could have sentences with the word it, and the same sentence with the word, and the same sentence with the word, and the same sentence with the word, honey. Therefore, these two words exert, honey. Therefore, these two words exert, honey. Therefore, these two words exert, strong gravitational pull to the word, strong gravitational pull to the word, strong gravitational pull to the word, it. All the other words also exert, it. All the other words also exert, it. All the other words also exert, gravitational pull but not as strong, gravitational pull but not as strong, gravitational pull but not as strong, because they're not as similar. So, because they're not as similar. So, because they're not as similar. So, let's forget about them and let's only, let's forget about them and let's only, let's forget about them and let's only, think of the word it being pulled, think of the word it being pulled, think of the word it being pulled, towards bear and towards honey but at, towards bear and towards honey but at, towards bear and towards honey but at, equal amounts because it's kind of, equal amounts because it's kind of, equal amounts because it's kind of, similar to both. So it kind of stays in, similar to both. So it kind of stays in, similar to both. So it kind of stays in, between gets here in a game of tugofwar, between gets here in a game of tugofwar, between gets here in a game of tugofwar, but it's between the two and if we don't, but it's between the two and if we don't, but it's between the two and if we don't, know the end of the sentence then it, know the end of the sentence then it, know the end of the sentence then it, can't really gravitate towards bear or, can't really gravitate towards bear or, can't really gravitate towards bear or, towards honey. However once we know the, towards honey. However once we know the, towards honey. However once we know the, next word something happens. If the next, next word something happens. If the next, next word something happens. If the next, word is hungry well hungry is similar to, word is hungry well hungry is similar to, word is hungry well hungry is similar to, bear more than hungry similar to honey, bear more than hungry similar to honey, bear more than hungry similar to honey, because hungry will be often describing, because hungry will be often describing, because hungry will be often describing, a bear. Therefore, now we have two words, a bear. Therefore, now we have two words, a bear. Therefore, now we have two words, pulling in the bear direction and one, pulling in the bear direction and one, pulling in the bear direction and one, pulling in the honey direction and, pulling in the honey direction and, pulling in the honey direction and, therefore it will gravitate towards bear, therefore it will gravitate towards bear, therefore it will gravitate towards bear, and hungry in particular be closer to, and hungry in particular be closer to, and hungry in particular be closer to, bear. So we know that it describes the, bear. So we know that it describes the, bear. So we know that it describes the, word bear. On the other hand, if the, word bear. On the other hand, if the, word bear. On the other hand, if the, word at the end of the sentence is, word at the end of the sentence is, word at the end of the sentence is, delicious, well, delicious is much, delicious, well, delicious is much, delicious, well, delicious is much, closer to honey than to bear because, closer to honey than to bear because, closer to honey than to bear because, delicious describes the honey and, delicious describes the honey and, delicious describes the honey and, delicious is going to pull the word it., delicious is going to pull the word it., delicious is going to pull the word it., So now we have two words pulling towards, So now we have two words pulling towards, So now we have two words pulling towards, honey and one towards bear. So it goes, honey and one towards bear. So it goes, honey and one towards bear. So it goes, in this direction and after many, in this direction and after many, in this direction and after many, iterations it's going to go very close, iterations it's going to go very close, iterations it's going to go very close, to the word honey and therefore we know, to the word honey and therefore we know, to the word honey and therefore we know, that it describes the word honey and, that it describes the word honey and, that it describes the word honey and, this is how attention works., this is how attention works., this is how attention works., [Music], [Music], [Music], Now I'm going to skip ahead a little bit, Now I'm going to skip ahead a little bit, Now I'm going to skip ahead a little bit, and tell about the soft max. Now I, and tell about the soft max. Now I, and tell about the soft max. Now I, haven't told the whole story about the, haven't told the whole story about the, haven't told the whole story about the, series of transformer blocks but assume, series of transformer blocks but assume, series of transformer blocks but assume, that it's a big huge neural network that, that it's a big huge neural network that, that it's a big huge neural network that, predicts the next word. Now we saw that, predicts the next word. Now we saw that, predicts the next word. Now we saw that, before, right? Because we have this big, before, right? Because we have this big, before, right? Because we have this big, neural network that is used to predict, neural network that is used to predict, neural network that is used to predict, the next word or a sentence. If I input, the next word or a sentence. If I input, the next word or a sentence. If I input, how are, it will output you. Now, we, how are, it will output you. Now, we, how are, it will output you. Now, we, don't exactly want that. We don't want a, don't exactly want that. We don't want a, don't exactly want that. We don't want a, deterministic answer. We don't want that, deterministic answer. We don't want that, deterministic answer. We don't want that, every time I say how are, it responds, every time I say how are, it responds, every time I say how are, it responds, you because in reality, you wouldn't, you because in reality, you wouldn't, you because in reality, you wouldn't, want the exact same answer every time, want the exact same answer every time, want the exact same answer every time, you ask a question. You want something, you ask a question. You want something, you ask a question. You want something, probabilistic, something stochastic. So, probabilistic, something stochastic. So, probabilistic, something stochastic. So, we're just going to add a mini step that, we're just going to add a mini step that, we're just going to add a mini step that, makes this into a probability. So, makes this into a probability. So, makes this into a probability. So, instead of outputting a bunch of zeros, instead of outputting a bunch of zeros, instead of outputting a bunch of zeros, and one one for the next word, we're, and one one for the next word, we're, and one one for the next word, we're, just going to make it output some, just going to make it output some, just going to make it output some, scores. And these scores can be, scores. And these scores can be, scores. And these scores can be, anything. They are high numbers for, anything. They are high numbers for, anything. They are high numbers for, words that are likely to appear and low, words that are likely to appear and low, words that are likely to appear and low, numbers, even negative numbers for words, numbers, even negative numbers for words, numbers, even negative numbers for words, that are very unlikely to appear. Now, that are very unlikely to appear. Now, that are very unlikely to appear. Now, we would like to turn this into, we would like to turn this into, we would like to turn this into, probabilities because at the end of the, probabilities because at the end of the, probabilities because at the end of the, day, we would like to draw a word out of, day, we would like to draw a word out of, day, we would like to draw a word out of, a bag given probabilities where the, a bag given probabilities where the, a bag given probabilities where the, words of high scores have higher, words of high scores have higher, words of high scores have higher, probabilities and the words with low, probabilities and the words with low, probabilities and the words with low, scores have lower probabilities because, scores have lower probabilities because, scores have lower probabilities because, we still want it to be accurate. We just, we still want it to be accurate. We just, we still want it to be accurate. We just, want it to be a little different every, want it to be a little different every, want it to be a little different every, time. But what property the, time. But what property the, time. But what property the, probabilities have? Well, all these, probabilities have? Well, all these, probabilities have? Well, all these, numbers must add to one. So, how do we, numbers must add to one. So, how do we, numbers must add to one. So, how do we, make all these numbers add to one? Well, make all these numbers add to one? Well, make all these numbers add to one? Well, one way is to divide them all by the, one way is to divide them all by the, one way is to divide them all by the, sum. So, the sum of these numbers is 10., sum. So, the sum of these numbers is 10., sum. So, the sum of these numbers is 10., And therefore if we divide everything by, And therefore if we divide everything by, And therefore if we divide everything by, 10 we get 1 over 10 0 over 10 etc etc, 10 we get 1 over 10 0 over 10 etc etc, 10 we get 1 over 10 0 over 10 etc etc, etc and those are the probabilities for, etc and those are the probabilities for, etc and those are the probabilities for, each word and then we just draw a word, each word and then we just draw a word, each word and then we just draw a word, based on these probabilities. So the, based on these probabilities. So the, based on these probabilities. So the, fourth word for example has probability, fourth word for example has probability, fourth word for example has probability, 4 out of 10 and therefore it's more, 4 out of 10 and therefore it's more, 4 out of 10 and therefore it's more, likely to appear than for example the, likely to appear than for example the, likely to appear than for example the, third word which has probability 0 out, third word which has probability 0 out, third word which has probability 0 out, of 10. The problem is that we have some, of 10. The problem is that we have some, of 10. The problem is that we have some, negatives and we can't have negative, negatives and we can't have negative, negatives and we can't have negative, probabilities minus 1 / 10 is not a, probabilities minus 1 / 10 is not a, probabilities minus 1 / 10 is not a, probability. So we actually need these, probability. So we actually need these, probability. So we actually need these, numbers to be positive. What do we do, numbers to be positive. What do we do, numbers to be positive. What do we do, for these numbers to be positive? Well, for these numbers to be positive? Well, for these numbers to be positive? Well, one very popular way is to raise e to, one very popular way is to raise e to, one very popular way is to raise e to, the score because e to the anything is a, the score because e to the anything is a, the score because e to the anything is a, positive number. We can actually raise, positive number. We can actually raise, positive number. We can actually raise, two or three or any number we want, but, two or three or any number we want, but, two or three or any number we want, but, e works pretty well. And this is, e works pretty well. And this is, e works pretty well. And this is, actually called the softmax function if, actually called the softmax function if, actually called the softmax function if, you've heard about it. So we take one, you've heard about it. So we take one, you've heard about it. So we take one, becomes e to the 1. Zero becomes e to, becomes e to the 1. Zero becomes e to, becomes e to the 1. Zero becomes e to, the 0. 4 becomes e to the 4 minus 1, the 0. 4 becomes e to the 4 minus 1, the 0. 4 becomes e to the 4 minus 1, becomes e to the minus1 which is okay, becomes e to the minus1 which is okay, becomes e to the minus1 which is okay, because e to the minus1 is a small, because e to the minus1 is a small, because e to the minus1 is a small, number but it's positive. And now we, number but it's positive. And now we, number but it's positive. And now we, have a bunch of positive numbers and we, have a bunch of positive numbers and we, have a bunch of positive numbers and we, divide by the sum. And when we divide by, divide by the sum. And when we divide by, divide by the sum. And when we divide by, the sum we get the following, the sum we get the following, the sum we get the following, probabilities. And notice that the, probabilities. And notice that the, probabilities. And notice that the, highest one is the one corresponding to, highest one is the one corresponding to, highest one is the one corresponding to, four which is 0.54. The lowest one is, four which is 0.54. The lowest one is, four which is 0.54. The lowest one is, corresponding to minus one which is not, corresponding to minus one which is not, corresponding to minus one which is not, zero but it's so close to zero that I, zero but it's so close to zero that I, zero but it's so close to zero that I, rounded it to zero. And the higher the, rounded it to zero. And the higher the, rounded it to zero. And the higher the, score, the higher the probability. And, score, the higher the probability. And, score, the higher the probability. And, the lower the score, the lower the, the lower the score, the lower the, the lower the score, the lower the, probability. So we are being faithful to, probability. So we are being faithful to, probability. So we are being faithful to, our scores. And if we draw a word out of, our scores. And if we draw a word out of, our scores. And if we draw a word out of, here with this probabilities, well, most, here with this probabilities, well, most, here with this probabilities, well, most, likely we're going to get the word you., likely we're going to get the word you., likely we're going to get the word you., But we can also get the word they or the, But we can also get the word they or the, But we can also get the word they or the, word things sometimes. So this is a way, word things sometimes. So this is a way, word things sometimes. So this is a way, to select words out of a probability, to select words out of a probability, to select words out of a probability, distribution formed by the neural, distribution formed by the neural, distribution formed by the neural, network in a way that you're always, network in a way that you're always, network in a way that you're always, going to get good words coming out, going to get good words coming out, going to get good words coming out, according to the input sentence but, according to the input sentence but, according to the input sentence but, you're not always going to get the same, you're not always going to get the same, you're not always going to get the same, ones. Okay. So now that I told you, ones. Okay. So now that I told you, ones. Okay. So now that I told you, softmax let's go back a little bit and, softmax let's go back a little bit and, softmax let's go back a little bit and, take a look at the whole architecture., take a look at the whole architecture., take a look at the whole architecture., [Music], Okay. So now we have all the parts to, Okay. So now we have all the parts to, build the transformer. And the, build the transformer. And the, build the transformer. And the, architecture is this. There is one part, architecture is this. There is one part, architecture is this. There is one part, that is still not super clear which is, that is still not super clear which is, that is still not super clear which is, the series of transformer blocks formed, the series of transformer blocks formed, the series of transformer blocks formed, by attention and feed forward. So this, by attention and feed forward. So this, by attention and feed forward. So this, can be multiple ones. There could be 12, can be multiple ones. There could be 12, can be multiple ones. There could be 12, it could be more. And at the end of the, it could be more. And at the end of the, it could be more. And at the end of the, day, each block has the similar, day, each block has the similar, day, each block has the similar, architecture. So to see this clearly, architecture. So to see this clearly, architecture. So to see this clearly, let's go through the whole architecture., let's go through the whole architecture., let's go through the whole architecture., First, we start with the input text, First, we start with the input text, First, we start with the input text, which is let's say write a story. It, which is let's say write a story. It, which is let's say write a story. It, goes through tokenization, which means, goes through tokenization, which means, goes through tokenization, which means, everything becomes a bunch of tokens., everything becomes a bunch of tokens., everything becomes a bunch of tokens., Now it goes to an embedding which means, Now it goes to an embedding which means, Now it goes to an embedding which means, these get put into some big big space, these get put into some big big space, these get put into some big big space, with lots of coordinates. Then we go, with lots of coordinates. Then we go, with lots of coordinates. Then we go, through positional encoding which means, through positional encoding which means, through positional encoding which means, each one of these words gets disturbed, each one of these words gets disturbed, each one of these words gets disturbed, in some different way in a way that now, in some different way in a way that now, in some different way in a way that now, position gets recorded and two sentences, position gets recorded and two sentences, position gets recorded and two sentences, with the same words but in different, with the same words but in different, with the same words but in different, order get sent to different embeddings., order get sent to different embeddings., order get sent to different embeddings., Next these numbers these coordinates are, Next these numbers these coordinates are, Next these numbers these coordinates are, fed to a neural network and that neural, fed to a neural network and that neural, fed to a neural network and that neural, network is trained to find the next word, network is trained to find the next word, network is trained to find the next word, in that sentence. However, that neural, in that sentence. However, that neural, in that sentence. However, that neural, network is not very good because it, network is not very good because it, network is not very good because it, still doesn't capture context. This, still doesn't capture context. This, still doesn't capture context. This, neural network is made by a bunch of, neural network is made by a bunch of, neural network is made by a bunch of, feed forward layers. Now, what we're, feed forward layers. Now, what we're, feed forward layers. Now, what we're, going to do is we're going to throw in, going to do is we're going to throw in, going to do is we're going to throw in, an attention layer in between each one, an attention layer in between each one, an attention layer in between each one, of these. And now, the feed forward, of these. And now, the feed forward, of these. And now, the feed forward, doesn't need to be one layer. It can be, doesn't need to be one layer. It can be, doesn't need to be one layer. It can be, multiple layers at a time. But at the, multiple layers at a time. But at the, multiple layers at a time. But at the, end of the day, what we have is that in, end of the day, what we have is that in, end of the day, what we have is that in, between each of the feed forward layers, between each of the feed forward layers, between each of the feed forward layers, we do one attention step, one gravity, we do one attention step, one gravity, we do one attention step, one gravity, step. And if you see the second video on, step. And if you see the second video on, step. And if you see the second video on, attention, there's actually something, attention, there's actually something, attention, there's actually something, called multi-head attention. That's what, called multi-head attention. That's what, called multi-head attention. That's what, happens between two feet forward blocks., happens between two feet forward blocks., happens between two feet forward blocks., This part may be sounding a little, This part may be sounding a little, This part may be sounding a little, abstract, but just imagine a big neural, abstract, but just imagine a big neural, abstract, but just imagine a big neural, network with a bunch of attention layers, network with a bunch of attention layers, network with a bunch of attention layers, thrown in between. So as you train the, thrown in between. So as you train the, thrown in between. So as you train the, neural network to guess the next word, neural network to guess the next word, neural network to guess the next word, you always remember to add context, you always remember to add context, you always remember to add context, often. And this is going to not give us, often. And this is going to not give us, often. And this is going to not give us, the next word, but it's going to give us, the next word, but it's going to give us, the next word, but it's going to give us, for every single word in the language., for every single word in the language., for every single word in the language., It's going to give us actually for every, It's going to give us actually for every, It's going to give us actually for every, single token, it's going to give us a, single token, it's going to give us a, single token, it's going to give us a, score. And the softmax is going to give, score. And the softmax is going to give, score. And the softmax is going to give, us a probability for each word or for, us a probability for each word or for, us a probability for each word or for, each token. For example, a token could, each token. For example, a token could, each token. For example, a token could, be end of sentence. That could be the, be end of sentence. That could be the, be end of sentence. That could be the, one that dictates when the transformer, one that dictates when the transformer, one that dictates when the transformer, basically needs to stop talking. So from, basically needs to stop talking. So from, basically needs to stop talking. So from, here, the highest probability word is, here, the highest probability word is, here, the highest probability word is, the most likely one to appear. And let's, the most likely one to appear. And let's, the most likely one to appear. And let's, just say that when we draw out of here, just say that when we draw out of here, just say that when we draw out of here, then the word once comes out and so, then the word once comes out and so, then the word once comes out and so, that's the output of the transformer. So, that's the output of the transformer. So, that's the output of the transformer. So, all this process to guess the next word., all this process to guess the next word., all this process to guess the next word., But because we've trained it that way, But because we've trained it that way, But because we've trained it that way, then this word is pretty good because, then this word is pretty good because, then this word is pretty good because, first of all position is taken into, first of all position is taken into, first of all position is taken into, account. Second of all context is taken, account. Second of all context is taken, account. Second of all context is taken, into account and the neural network that, into account and the neural network that, into account and the neural network that, guesses the next word is pretty strong, guesses the next word is pretty strong, guesses the next word is pretty strong, because we're talking about a huge, because we're talking about a huge, because we're talking about a huge, architecture with lots of nodes, lots of, architecture with lots of nodes, lots of, architecture with lots of nodes, lots of, data and that's really what makes, data and that's really what makes, data and that's really what makes, transformers so good. So, as you can, transformers so good. So, as you can, transformers so good. So, as you can, see, the architecture is not really that, see, the architecture is not really that, see, the architecture is not really that, complicated. It's really just a big, complicated. It's really just a big, complicated. It's really just a big, neural network with a bunch of padding, neural network with a bunch of padding, neural network with a bunch of padding, and a bunch of boosters around that make, and a bunch of boosters around that make, and a bunch of boosters around that make, it work really well. And that's it., it work really well. And that's it., it work really well. And that's it., That's the transformer architecture. And, That's the transformer architecture. And, That's the transformer architecture. And, so, are we done yet? Well, let me show, so, are we done yet? Well, let me show, so, are we done yet? Well, let me show, you some problems that transformer have, you some problems that transformer have, you some problems that transformer have, and also some solutions that are, and also some solutions that are, and also some solutions that are, provided., So, as I mentioned before, transfer, So, as I mentioned before, transfer, models don't always work as intended if, models don't always work as intended if, models don't always work as intended if, you simply train them to talk using the, you simply train them to talk using the, you simply train them to talk using the, entire internet as a data set. And to, entire internet as a data set. And to, entire internet as a data set. And to, fix that, we have a process called, fix that, we have a process called, fix that, we have a process called, fine-tuning. So, why wouldn't a model, fine-tuning. So, why wouldn't a model, fine-tuning. So, why wouldn't a model, work if it's trained on the internet?, work if it's trained on the internet?, work if it's trained on the internet?, It's a huge data set. Well, maybe, It's a huge data set. Well, maybe, It's a huge data set. Well, maybe, because the internet is not exactly the, because the internet is not exactly the, because the internet is not exactly the, best place for many things. As a matter, best place for many things. As a matter, best place for many things. As a matter, of fact, the internet is not a question, of fact, the internet is not a question, of fact, the internet is not a question, answer repository. For example, if I, answer repository. For example, if I, answer repository. For example, if I, train the model to guess the next word, train the model to guess the next word, train the model to guess the next word, and I ask it a question, what is the, and I ask it a question, what is the, and I ask it a question, what is the, capital of Nigeria? Well, the answer is, capital of Nigeria? Well, the answer is, capital of Nigeria? Well, the answer is, Abuja. But that may not be the most, Abuja. But that may not be the most, Abuja. But that may not be the most, common word right after that sentence. I, common word right after that sentence. I, common word right after that sentence. I, could have the internet looking like, could have the internet looking like, could have the internet looking like, this. And there could be some quizzes, this. And there could be some quizzes, this. And there could be some quizzes, where after what is the capital of, where after what is the capital of, where after what is the capital of, Nigeria? The next thing is a question., Nigeria? The next thing is a question., Nigeria? The next thing is a question., What is the capital of Chad or what is, What is the capital of Chad or what is, What is the capital of Chad or what is, the capital of Lebanon? And the next, the capital of Lebanon? And the next, the capital of Lebanon? And the next, word is what. I could also have stories, word is what. I could also have stories, word is what. I could also have stories, where we have the sentence, what is the, where we have the sentence, what is the, where we have the sentence, what is the, capital of Nigeria? She asked. In this, capital of Nigeria? She asked. In this, capital of Nigeria? She asked. In this, case, the next word is she. Or it could, case, the next word is she. Or it could, case, the next word is she. Or it could, be a chat where you have the sentence, be a chat where you have the sentence, be a chat where you have the sentence, what is the capital of Nigeria? And, what is the capital of Nigeria? And, what is the capital of Nigeria? And, somebody answering, that's a good, somebody answering, that's a good, somebody answering, that's a good, question, in which case the word is, question, in which case the word is, question, in which case the word is, that. Or I could have a more elaborate, that. Or I could have a more elaborate, that. Or I could have a more elaborate, answer that doesn't start with the, answer that doesn't start with the, answer that doesn't start with the, answer. It simply says since 1991 blah, answer. It simply says since 1991 blah, answer. It simply says since 1991 blah, blah blah. And in that case, the word is, blah blah. And in that case, the word is, blah blah. And in that case, the word is, since. So for the model, it's not very, since. So for the model, it's not very, since. So for the model, it's not very, clear what the next word should be. And, clear what the next word should be. And, clear what the next word should be. And, remember that the transform model is not, remember that the transform model is not, remember that the transform model is not, trying to answer questions. It's trying, trying to answer questions. It's trying, trying to answer questions. It's trying, to find the next word. So how do we get, to find the next word. So how do we get, to find the next word. So how do we get, the model to know that it's not just the, the model to know that it's not just the, the model to know that it's not just the, next word that we want, it's the answer., next word that we want, it's the answer., next word that we want, it's the answer., Well, what we have to do is train it on, Well, what we have to do is train it on, Well, what we have to do is train it on, question answer data sets. So we can use, question answer data sets. So we can use, question answer data sets. So we can use, experts to curate a bunch of data sets, experts to curate a bunch of data sets, experts to curate a bunch of data sets, that have particular questions and, that have particular questions and, that have particular questions and, answers so that the model starts, answers so that the model starts, answers so that the model starts, learning that when it gets a question it, learning that when it gets a question it, learning that when it gets a question it, has to answer with the answer. That's, has to answer with the answer. That's, has to answer with the answer. That's, called fine-tuning a model and it's a, called fine-tuning a model and it's a, called fine-tuning a model and it's a, very very expensive part of training, very very expensive part of training, very very expensive part of training, because you actually require people to, because you actually require people to, because you actually require people to, create custom data sets and you need to, create custom data sets and you need to, create custom data sets and you need to, be very careful and curate these data, be very careful and curate these data, be very careful and curate these data, sets very well. Now this is not only for, sets very well. Now this is not only for, sets very well. Now this is not only for, question and answer. If you want the, question and answer. If you want the, question and answer. If you want the, model to actually chat, if you say, model to actually chat, if you say, model to actually chat, if you say, "Hello, how are you?" and it says, "Good, "Hello, how are you?" and it says, "Good, "Hello, how are you?" and it says, "Good, and you." And then you continue talking, and you." And then you continue talking, and you." And then you continue talking, and the model continues talking to you, and the model continues talking to you, and the model continues talking to you, then you need to fine-tune the model by, then you need to fine-tune the model by, then you need to fine-tune the model by, post-training it with tons and tons and, post-training it with tons and tons and, post-training it with tons and tons and, tons of chats. In that way, the model, tons of chats. In that way, the model, tons of chats. In that way, the model, starts learning that if you're talking, starts learning that if you're talking, starts learning that if you're talking, to it, it has to reply as a person who's, to it, it has to reply as a person who's, to it, it has to reply as a person who's, chatting back at you. And if you, chatting back at you. And if you, chatting back at you. And if you, continue talking to it, it should, continue talking to it, it should, continue talking to it, it should, remember the previous steps of the, remember the previous steps of the, remember the previous steps of the, conversation. Another thing you should, conversation. Another thing you should, conversation. Another thing you should, fine-train the models for is for, fine-train the models for is for, fine-train the models for is for, commands. So for example, if you say do, commands. So for example, if you say do, commands. So for example, if you say do, the following thing, then the model must, the following thing, then the model must, the following thing, then the model must, do it. For example, you can tell it to, do it. For example, you can tell it to, do it. For example, you can tell it to, write a poem about elephants or to, write a poem about elephants or to, write a poem about elephants or to, correct your code or to write code that, correct your code or to write code that, correct your code or to write code that, does a particular thing, to write a, does a particular thing, to write a, does a particular thing, to write a, particular essay about something or to, particular essay about something or to, particular essay about something or to, simply give you lists of things, etc., simply give you lists of things, etc., simply give you lists of things, etc., etc. So you need to build big data sets, etc. So you need to build big data sets, etc. So you need to build big data sets, with commands and then the command being, with commands and then the command being, with commands and then the command being, executed in order for the model to learn, executed in order for the model to learn, executed in order for the model to learn, that when it's given a command, it has, that when it's given a command, it has, that when it's given a command, it has, to follow it. And as you can imagine, to follow it. And as you can imagine, to follow it. And as you can imagine, there are a lot of other use cases that, there are a lot of other use cases that, there are a lot of other use cases that, you would need to fine-tune your model., you would need to fine-tune your model., you would need to fine-tune your model., So I'd love for you to actually think of, So I'd love for you to actually think of, So I'd love for you to actually think of, other cases where you would have to, other cases where you would have to, other cases where you would have to, fine-tune a model for it to work well., fine-tune a model for it to work well., fine-tune a model for it to work well., And feel free to put them in the, And feel free to put them in the, And feel free to put them in the, comments. It would be lovely to have a, comments. It would be lovely to have a, comments. It would be lovely to have a, discussion about it., All right, that's all about, All right, that's all about, Transformers. Thank you very much for, Transformers. Thank you very much for, Transformers. Thank you very much for, your attention with this video, which, your attention with this video, which, your attention with this video, which, was a little long, but we went through, was a little long, but we went through, was a little long, but we went through, the whole thing. Just like the previous, the whole thing. Just like the previous, the whole thing. Just like the previous, two videos, this would have not happened, two videos, this would have not happened, two videos, this would have not happened, if not for my friends Joel, Jay, and, if not for my friends Joel, Jay, and, if not for my friends Joel, Jay, and, Omar, who actually explained, Omar, who actually explained, Omar, who actually explained, Transformers to me in much detail. In, Transformers to me in much detail. In, Transformers to me in much detail. In, particular, there's a podcast I have, particular, there's a podcast I have, particular, there's a podcast I have, with Omar that I've linked down in the, with Omar that I've linked down in the, with Omar that I've linked down in the, comments. It's in Spanish, but if you, comments. It's in Spanish, but if you, comments. It's in Spanish, but if you, speak Spanish, I think you will like it., speak Spanish, I think you will like it., speak Spanish, I think you will like it., And if you like this content, we have a, And if you like this content, we have a, And if you like this content, we have a, lot more at LLM University by Coher that, lot more at LLM University by Coher that, lot more at LLM University by Coher that, I built with my two colleagues, Mior and, I built with my two colleagues, Mior and, I built with my two colleagues, Mior and, again Jay. This is a comprehensive, again Jay. This is a comprehensive, again Jay. This is a comprehensive, course where we have a lot about large, course where we have a lot about large, course where we have a lot about large, language models including introduction, language models including introduction, language models including introduction, and a very conceptual explanation, and a very conceptual explanation, and a very conceptual explanation, followed by a bunch of labs on many many, followed by a bunch of labs on many many, followed by a bunch of labs on many many, topics including prompt engineering, topics including prompt engineering, topics including prompt engineering, search deployment and much more. And in, search deployment and much more. And in, search deployment and much more. And in, particular if you want to go for more, particular if you want to go for more, particular if you want to go for more, details on transformers I recommend the, details on transformers I recommend the, details on transformers I recommend the, illustrated transformer again by Jay, illustrated transformer again by Jay, illustrated transformer again by Jay, which is a really really good video and, which is a really really good video and, which is a really really good video and, blog post. This is where I learned what, blog post. This is where I learned what, blog post. This is where I learned what, the transformers are. So, thank you very, the transformers are. So, thank you very, the transformers are. So, thank you very, much. Here is some information. If you, much. Here is some information. If you, much. Here is some information. If you, like this video, please subscribe to my, like this video, please subscribe to my, like this video, please subscribe to my, channel. It's srano.academy, channel. It's srano.academy, channel. It's srano.academy, and uh hit like and uh add a comment and, and uh hit like and uh add a comment and, and uh hit like and uh add a comment and, share with your friends. I love reading, share with your friends. I love reading, share with your friends. I love reading, the comments that you leave in, the comments that you leave in, the comments that you leave in, particular when you leave good topics, particular when you leave good topics, particular when you leave good topics, about future videos. Many times I've, about future videos. Many times I've, about future videos. Many times I've, made a video about it because it gives, made a video about it because it gives, made a video about it because it gives, me good ideas. You can also tweet at me, me good ideas. You can also tweet at me, me good ideas. You can also tweet at me, srano.academy or check out my page, srano.academy or check out my page, srano.academy or check out my page, srano.academy where I have all these, srano.academy where I have all these, srano.academy where I have all these, videos and also a bunch of blog posts, videos and also a bunch of blog posts, videos and also a bunch of blog posts, and code and different things. And I, and code and different things. And I, and code and different things. And I, also have the book rocking machine, also have the book rocking machine, also have the book rocking machine, learning which I really enjoyed writing, learning which I really enjoyed writing, learning which I really enjoyed writing, and it was about basically supervised, and it was about basically supervised, and it was about basically supervised, learning and in detail the way I like to, learning and in detail the way I like to, learning and in detail the way I like to, explain things with a lot of code, explain things with a lot of code, explain things with a lot of code, examples etc. If you'd like to get it, examples etc. If you'd like to get it, examples etc. If you'd like to get it, the discount code is sarrano yt and the, the discount code is sarrano yt and the, the discount code is sarrano yt and the, information on how to get it is down on, information on how to get it is down on, information on how to get it is down on, the comments. This discount code is for, the comments. This discount code is for, the comments. This discount code is for, 40%. So thank you very much and see you, 40%. So thank you very much and see you, 40%. So thank you very much and see you, in the next video., in the next video., in the next video., [Music]
