Timestamp: 2025-11-21T10:42:18.032924
Title: AI 提示词工程 上下文工程 15分钟弄懂！
URL: https://www.youtube.com/watch?v=-8Ygq9AVWZ8
Status: success
Duration: 16:24

Description:
好的，这是根据您提供的文本内容提炼的核心思想摘要。

### **核心思想摘要**

#### **一、 提示词工程 (Prompt Engineering)**

提示词工程是通过精心设计用户向AI提问的方式，来引导和约束AI，以获得更精准、风格特定且错误更少的回答。

*   **基本构成:**
    *   **系统提示词 (System Prompt):** 为AI设定角色、背景和行为准则（如“你来扮演一个猫娘”），通常由应用内置，但用户可通过特定功能间接影响。
    *   **用户提示词 (User Prompt):** 用户在对话框中输入的具体问题或指令。

*   **核心技巧:**
    *   **零样本 (Zero-Shot):** 只提出要求，不提供具体范例。例如，要求AI在回答数学题前“仔细检查”。
    *   **少样本 (Few-Shot):** 在提示词中提供几个具体范例，让AI模仿其格式或风格。这在要求AI稳定返回特定格式的输出时尤其有效。
    *   **思维链 (Chain of Thought, CoT):** 引导AI“一步一步地拆解问题”并展示中间过程，通过引导其推理过程来提升复杂问题（尤其是数学和逻辑问题）的准确率。

#### **二、 上下文工程 (Context Engineering)**

上下文工程是在AI执行复杂的、多步骤的自主任务（如AI Agent调用工具）时，通过一系列程序化规则来管理和修改对话历史（即“上下文”），以确保AI在没有用户实时干预的情况下，始终记得并对齐最初的任务目标。

*   **问题根源:**
    *   **AI无记忆:** AI本身是无状态的，其“记忆”是通过每次都将完整的对话历史（上下文）发送给模型来实现的。
    *   **上下文过长:** 在AI Agent执行任务时，会产生大量的工具调用（Tool Calls）和工具响应（Tool Responses），导致上下文变得极长，充满了中间信息，AI容易“迷失”方向，忘记初衷。

*   **核心技巧:**
    *   **让AI学会记笔记:**
        1.  提供一个“记笔记”的工具。
        2.  通过系统提示词指导AI使用该工具，如“先分解任务，生成任务清单，并记录在笔记中”。
        3.  将记录了核心目标的笔记放在上下文的开头或结尾（Transformer架构最敏感的位置），以不断提醒AI其首要任务。
    *   **上下文长度管理与压缩:**
        1.  **直接丢弃:** 舍弃掉过时的消息。
        2.  **总结压缩:** 让AI将较早的消息总结成精华，用总结替换原文。
        3.  **高级压缩 (RAG-like):** 将超长内容（如网页）存入临时数据库，在上下文中只留下一句摘要和查询该数据库的工具指令。
        4.  **优化工具返回值:** 从源头上减少信息冗余，如在返回网页内容前先去除不必要的HTML标签。

---

### **核心结论 (Core Point)**

无论是提示词工程还是上下文工程，其核心都是通过精心设计与AI的交互方式，来引导AI在不同复杂度的任务中，更精准、可控地实现我们的目标。

---

### **内容框架 (Overarching Framework)**

本文采用了一种**“问题递进与方案演化”**的框架。它从最基础的人机交互单元“提示词”出发，指出了其局限性（回答泛泛），然后引入了**“提示词工程”**作为解决方案。接着，它进一步探讨了在连续对话和复杂任务中产生的新问题——AI的“记忆”机制（上下文）及其在自主任务中带来的“迷失”风险，并最终提出了更高级的解决方案——**“上下文工程”**。整个框架从简单到复杂，从用户直接干预到系统自动管理，清晰地展示了控制和引导AI行为的两种核心思路及其技术演进。

---

### **概念关系图 (Mermaid Conceptual Map)**

<Mermaid_Diagram>
graph TD
    subgraph "与AI的交互与引导 (Interaction & Guidance of AI)"
        direction LR
        subgraph "A. 提示词工程 (Prompt Engineering)"
            direction TD
            A1["用户直接引导\n(Direct User Guidance)"]
            A2("提示词 (Prompt)")
            A3{{"目标:\n更精准、稳定的回答\n(Goal: More Precise & Stable Replies)"}}
            
            subgraph "提示词构成"
                A2_1["系统提示词 (System Prompt)\n- 设定角色/规则"]
                A2_2["用户提示词 (User Prompt)\n- 具体问题"]
            end

            subgraph "核心技巧 (Techniques)"
                A4["零样本 (Zero-Shot)\n- 仅指令"]
                A5["少样本 (Few-Shot)\n- 提供范例"]
                A6["思维链 (CoT)\n- 分步思考"]
            end
            
            A1 --> A2
            A2 --> A3
            A2 -- "包含" --> A2_1
            A2 -- "包含" --> A2_2
            A3 -- "通过技巧实现" --> A4
            A3 -- "通过技巧实现" --> A5
            A3 -- "通过技巧实现" --> A6
        end

        subgraph "B. 上下文工程 (Context Engineering)"
            direction TD
            B1["系统程序化管理\n(Programmatic System Management)"]
            B2("上下文 (Context)\n- 完整的对话历史\n- 含工具调用/响应")
            B3{{"目标:\n确保AI在复杂任务中不偏离\n(Goal: Keep AI Aligned in Complex Tasks)"}}

            subgraph "核心挑战 (Challenge)"
                B2_1["AI无状态 (Stateless)"]
                B2_2["上下文过长导致“迷失”\n(Long Context causes Deviation)"]
            end

            subgraph "核心技巧 (Techniques)"
                B4["让AI记笔记 (AI Note-Taking)\n- 任务清单置于首尾"]
                B5["上下文压缩 (Context Compression)\n- 总结/RAG/优化工具返回"]
            end

            B1 --> B2
            B2 --> B3
            B2 -- "面临" --> B2_1 & B2_2
            B3 -- "通过技巧实现" --> B4
            B3 -- "通过技巧实现" --> B5
        end
    end

    style A fill:#E3F2FD,stroke:#1565C0,stroke-width:2px
    style B fill:#FFF3E0,stroke:#EF6C00,stroke-width:2px
    style A1 fill:#BBDEFB,stroke:#1565C0,stroke-width:1px
    style A2 fill:#90CAF9,stroke:#1565C0,stroke-width:1px
    style A3 fill:#C8E6C9,stroke:#2E7D32,stroke-width:2px,color:#000
    style B1 fill:#FFE0B2,stroke:#EF6C00,stroke-width:1px
    style B2 fill:#FFCC80,stroke:#EF6C00,stroke-width:1px
    style B3 fill:#C8E6C9,stroke:#2E7D32,stroke-width:2px,color:#000
    style A4 fill:#E1F5FE,stroke:#0277BD,stroke-width:1px
    style A5 fill:#E1F5FE,stroke:#0277BD,stroke-width:1px
    style A6 fill:#E1F5FE,stroke:#0277BD,stroke-width:1px
    style B4 fill:#FFF9C4,stroke:#F9A825,stroke-width:1px
    style B5 fill:#FFF9C4,stroke:#F9A825,stroke-width:1px
    style B2_2 fill:#FFCDD2,stroke:#C62828,stroke-width:1px
</Mermaid_Diagram>

Content:
不知从什么时候开始,很多简单的事情都得换一个高级的说法卖东西叫分销、赚钱、变县、写文章、内容书出、拉人进群叫思欲隐留就连AI写代码也得有个新词叫外部Coding人们总是倾向用高级的词会来演改内容的规法今天我们来聊几个AI时代的流行词提示词、提示词工程、商下文以及那个宣称可以把提示词工程采在脚下摩擦的上下文工程大园模型刚出现的时候主要就是一个聊天机器人用户说你好,大园模型可能会回复,我很好你呢?经过几年的发展,AI的能力已经远远的超过了普通的聊天机器人但聊天依然是AI最核心,也是最实用的功能之一在聊天这个场景下,用户说的你好,就叫做Proud中文翻译过来提示词,所以提示词最基本的定义就是用户发给AI的话对于我很好你呢?这种四平八稳的回答虽然能满足大多数人在多数情况下的期望但这也注定这种回答将会是感巴巴了毫不特色的所以即使只是聊天也可以用上一些聊天机巧比如说我们可以用提示词给AI设定一个角色举个例子,你跟他说,你来扮演一个猫娘,接下来我说你好这个时候AI就可能回复,我很好你呢?名,把这种玩法推向极致的一个软件叫做Clythawan为了保证这期视频不被和解,这里我就不展开多说了有兴趣的朋友可以自己去搜一下但是问题来了,你扮演猫娘这种设定,并不是用户对话内容本身把这种设定和用户真正想说的话混在一起不仅容易出戏,逻辑上也有点乱所以大模型的厂商们就把提示词分成了两部分向你扮演一个猫娘这种设定叫做Sitant prompt也就是系统提示词,而用户说的你好则叫做User prompt也就是用户提示词我们平时在聊天矿里输入的就是用户提示词而系统提示词呢?通常是聊天机器人内置的用户一般不能直接修改不过很多AI应用来提供了一些功能可以间接的影响系统提示词比如在CatGPT里面有一个叫做Customize ChatGPT的功能可以设定一些用户的个人偏好这些信息最终就会成为系统提示词的一部分从而影响CatGPT的输出内容这种通过系统提示词和用户提示词的组合来引导AI返回特定风格回复的做法就叫做提示词工程Prob to Engineering当然了提示词工程可不止是用来玩角色扮演的它最核心的目的是通过一系列技巧来约束AI的行为让它的回复更加稳定减少错误和意外下面我们就来介绍几种常见的技巧比如说AI不太擅长处理数学问题再让它解数学体的时候就比较容易做错这个时候我们就会要求它在回复前仔细的检查确保它的答案是正确的在比如说有时候AI会拒绝回答我们一些问题这个时候就可以在提示词中告诉AI这件事情的严重性于是AI就会偏向于输出更加完整的答案了这些都属于行为的约束像这种只提要求但是不给AI模型具体例子的技巧还有一个专门的叫法叫做ZeroShot与此对应的就是FuelShot这种方法是指用户可以在提示词里面先给AI几个具体的例子当作参考比如说我希望AI帮我把正常的句子转化成猫娘体我就可以在提示词里面先举例比如我们去吃饭吧转化成我们去吃饭好不好压秒我写了一个程序转化成我把程序录好了秒给出范例之后我们再提出真正的需求现在来转化这个句子亲爱的我买了包包送给你这样AI就会根据我们提供的例子给出更贴近预期的答案了比如说回复亲爱的我抓了老数送给你秒FuelShot这个技巧在要求AI返回特径格式的场景下会特别的有用比如说当我们写一个AI应用或者AIAZN的时候通常需要AI稳定的返回带有某种格式的自负串这种场景下用FuelShot就非常的合适如果有朋友想了解AIZN的是什么可以参考这两期视频或者我的知识星球我会分别从原理和编程实现讲明白AIAZN的到底是怎么回事连接我会制定在评论区除了ZeroShot和FuelShot还有一种更加旋学的技巧叫做思维链Chin of Salt 简称COT举个简单的例子我们问AI一到数学体1加2成3等一多少因为大圆模型本质上是一个概率模型它其实并不擅长做精确的数学运算对于这种问题尤其是在早期的模型上精度是比较低的这个时候我们就可以在问题后面多加一句话比如说不要先给出答案请一步一步的拆解问题并且给出每一步的中间结果收到这个指令之后AI的回复就可能变成了根据运算优先级第一步需要先计算二唱一三结果16然后再计算1加6最终结果是7虽然模型本身是没有变的但我们通过提示词引导它一步步思考然后再输出最终算对的概率就会大大的提升这就是所谓的思维链当然了我们觉得这个例子有点过于的简单了即使不用思维链AI基本也不会算错而且如今的网页版聊天机器人大多数都内置了比我们这个例子强大的多的思维链功能不再需要拥护手动输入额外的提示词了但他们背后的原理都是一样的都是通过提示词让AI自己分解问题输出推理过程从而解决更加复杂的问题总结一下所谓的提示词工程本质上就是通过精心设计我们向AI提问的方式来获得更精准更符合我们预期的回答在聊上下文上下文工程之前我们首先要澄清一个事实AI模型本身是没有记忆的这意味著每次我们给AI发送一个消息对它来说都是一次全新的独立的请求但很显然我们在进行连续对话的时候是需要AI寄入我们聊过什么的否则对话就无法成立了那么这个记忆到底是怎么实现的其实在我们和AI聊天的时候并不是直接把消息发送给大语言模型的拥护和AI之间还隔了一个AI agent或者聊天机器人的服务器正确的流程是这样的拥护把消息先发给AI agent或者聊天机器人服务器聊天机器人会把消息发送给AI模型的同时还会保留下完整的历史纪录然后当它收到一条新的用户消息的时候它会把这条消息附加到历史纪录的末尾最后再把这个包含了所有过往信息的完整历史纪录一起发给AI模型这样一来AI模型本身是失异的但它们一次收到了信息都是完整的对话所以看上去就像有了记忆一样这个被一次性发给AI的完整的历史纪录就叫做上下温Context而如何管理和修改这段历史纪录的技巧就被叫做上下温工程Context and Engineering那么这个上下闻到底有什么好管理的呢其实如果AI只是一个简单的一问一答形式的聊天机器人那我们之前聊的提示词工程基本就够用了因为在那种一来一回的对话中用户总有机会通过新的提示词来修正和引导AI的行为确保它的回答不会跑偏但是AI A-ZENTA的出现让情况变得复杂了起来AI A-ZENTA除了传递消息维护历史之外它还拥有一个工具箱里面有一些它自己定义的工具可以供AI模型来调用如果你觉得AI A-ZENTA的概念有点复杂我们还是以往夜版聊天机器人来举例许多往夜版聊天机器人提供了网页流览功能这种聊天机器人本质上就是一个简单的AI A-ZENTA而流览网页就是它提供了一个工具比如说当用户问猫娘的口头产是什么这个时候聊天机器人就会把当前的上下文连同它能使用了所有的工具说明一起都打包发送给AI模型AI模型收到消息之后就会发现工具箱里面有一个用于流览网页的工具可以用于是它可能会决定不直接回答用户的问题而是先用Google搜索一下此时AI模型就会返回一个特殊的指令叫做Torco这个指令大意就是帮我访问这个Google网址搜索猫娘的口头产是什么聊天机器人收到指令之后就会去调用访问网页的工具访问网页的工具就会去访问Google访回网页的内容接下来网页的内容会被打包成一条Torco Response消息和对应的Torco一起放到上下文之中而这个变得更常的上下文会被重新的发给AI模型这个时候AI可能还觉得信息还是不够又想去萌娘摆颗里面再查一下于是就又重复了一次刚才的那个过程上下文里面因此又多出了一对Torco和Torco Response对于比较复杂的问题在AI生成最终答案之前这样的一来一回可能会重复即时甚至上百次上下文就会变得特别特别的长这理请注意一下在这个漫长的探索过程之中用户能实家的影响只有在最最开头的那一句提示词而后续几十次Torco和Torco Response不仅数量多而且像是流兰网页这种Torco Response内容通常也非常的长不然想像当AI模型面对一个越来越长充斥了各种中间信息的上下文而用户又没办法即时的纠正它的行为的时候它的行动方向就很容易跑偏忘记自己到底要干什么所以如何通过一套程序化的规则来自动的管理和修改上下文确保AI在漫长的自主行动中使中符合用户的最初要求这就是上下文工程要解决的核心问题很可惜的是关于上下文到底要怎么管理目前还没有一个公认的完美方案不过这里我可以介绍几种在夜界比较常见比较有效的做法一招上AI学会记比计它的原理类似于我们之前提到的网页流览工具不过这次我们再给AI模型提供一个专门用来记比计的工具当AI在处理任务的时候如果想要记下一些关键信息就可以调用这个工具当然了只提供工具然后让AI自由的发挥记比计效果一般都不会太好所以我们还需要在系统提示词里面明确的给出笔记的使用策略比如说我们可以这样知道模型一在行动前先将任务分解二在笔记中写下你的任务清单三严格根据任务清单来执行任务四美完成一项就在笔记中更新该项的状态这样以来当AI再拿到猫娘的口头产是什么这个问题的时候大模型自己就会先头脑风暴出一个任务清单比如说一用谷歌搜索猫娘二用蒙娘百科搜索猫娘三综合两次的搜索结果总结出口头产然后AI就会通过之前我们介绍的tool code方法调用记比计的工具让Agent把这个清单记录下来现在我们来看一下整个上下文是什么样子里面包含了最开始的系统提示词和用户提示词还有AI调用记比计的tool code和tool response其中的tool response一般不会包含什么有用的信息只是告诉AI笔记更新成功了而这里最关键的一步来了这个被保存的笔记会被Agent插入到整个上下文的开头或者结尾这个插入的位置其实也是有讲究的因为现在的大语言模型普遍采用的是Transformer架构而这种架构天生就对输入信息的开头和结尾部分特别的敏感所以即使上下文变得非常长无论AI中间执行了多少次tool code和tool response开头结尾的信息也基本不会被模型忽略写著核心目标的任务清单和最初的提示词使中都处在最显眼的位置当AI完成一项任务比如用Google搜索猫娘它又会根据系统提示词的指示再次调用记比计的工具把第一项任务标记为一完成然后开始执行第二项通过这种方式就能很大的程度上保证AI在执行复杂的任务的时候不会跑偏当然了比计里面也不仅限于记录任务的清单比如说我们还可以通过系统提示词让AI记录搜索出的关键信息便于后续进行总结等等但无论记录什么这项技术的本质都是人类拥护通过控制最初的提示词来指导AI如何写比计再通过把比计放到上下文中最显眼的位置来间接引导整个AI的处理流程那么既然上下文太长是问题的根源另一个优化的方向就是让它变短最直接的做法就是直接丢掉太老的消息只保留新的消息当然了最开始的系统和用户提示词部分是必须要保留的不然AI就不知道自己要干什么了如果你觉得直接舍弃信息这种方式太暴力了可能会丢失关键的内容那么还有一种更加优雅的做法就是压缩许多Agent会把较老的消息提取出来然后让AI模型去总结其中的关键信息再用这个精链的总结去替换掉原来的上下文从而达到压缩的目的除了这些还有更高级的压缩方法某些Touris Bounce可能会非常的长比如说包含了一篇上万字的文章这个时候Agent就会先把Touris Bounce的内容处理后存到一个脸时的相量数据库里面这个过程就类似我们之前讲过的Rug技术Rug的知识可以回顾我做的这一期视频连接我会指点到评论区文章存入之后Agent就会修改这次的Touris Bounce不再包含原文而是用一句话来代替比如说文章已经存入知识库我为你提供了一个新的工具叫做Corey Document你可以用它来查学文章的片段然后AAM模型就只要查找自己感兴趣的片段就可以了这样以来一个几万字的Touris Bounce就会压缩成了一句几十个字的指令和一些AI主动查出来的片段上下文的长度就得到了很大的控制还有一种方法是直接优化工具的反回值比如说对网页流暖工具Agent可以先去掉网页里面不必要的HTML标签只把最核心的内容反回给AI模型从源头就减少信息的荣誉当然了这里我提到的只是机种比较常见的上下文观力方法这是一个非常热门的AI应用研究方向新的研究和技术也在不断的永限之中总而言之无论是我们前面聊的记忆技还是刚刚所说的减少上下文长度的各种方法所有所有的一切目的都只有一个在人类用户无法实实干预AI行动的时候确保AI模型时中都记得自己最初的任务是什么小学的时候老师问我们长大以后想做什么当时我在作业本上写下的答案是当老师因为我觉得他们是超人什么都懂什么都能讲的明明白白的也不知道老师当时花了多少心思到底用了什么神器的方法过去了几十年我现在依然还记得这里是成全老王我们下期再见
