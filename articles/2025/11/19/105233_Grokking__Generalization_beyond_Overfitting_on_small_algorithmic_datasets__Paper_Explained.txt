Timestamp: 2025-11-19T10:52:33.798993
Title: Grokking: Generalization beyond Overfitting on small algorithmic datasets (Paper Explained)
URL: https://youtube.com/watch?v=dND-7llwrpw&si=HN-9okgw2Apddghr
Status: success
Duration: 29:47

Description:
好的，这是根据您的要求，对所提供文本内容进行提炼、总结和可视化的结果。

### **内容概要与核心思想**

#### **1. 核心现象：Grokking (顿悟)**

*   **定义**：指神经网络在训练中首先达到对训练数据的**完全过拟合**（训练准确率100%，但验证准确率为0），然后在继续训练很长一段时间后，其泛化能力突然“顿悟”式地跃升，在验证集上也达到**近乎完美的准确率**。
*   **过程**：这是一个从“记忆”到“理解”的转变。网络先是死记硬背训练数据，之后在持续优化下，突然找到了数据背后隐藏的算法规律，从而实现了真正的泛化。
*   **背景**：该现象主要在小规模、结构化、低噪声的**算法数据集**（如模运算、群论排列组合等）上被观察到。

#### **2. 关键驱动因素与原理解释**

*   **两种解的竞争**：
    *   **记忆解**：在训练初期，最容易达到的解是记住所有训练样本。这种解在训练集上表现完美，但模型权重通常更复杂、范数更大，无法泛化。
    *   **泛化解**：代表模型学到了数据生成的一般规律。这种解不仅适用于训练集，也能完美预测未见过的数据。它的权重结构通常更简单、范数更小。
*   **权重衰减 (Weight Decay) 的核心作用**：
    *   权重衰减是一种正则化技术，它会惩罚较大的模型权重，从而引导优化器寻找**更简单、权重范数更小**的解。
    *   在Grokking现象中，权重衰减是促使模型从复杂的“记忆解”最终跃迁到简单的“泛化解”的关键驱动力。没有它，模型可能永远停留在过拟合状态。
*   **其他影响因素**：
    *   **训练数据量**：数据越多，Grokking发生得越快。
    *   **数据噪声**：数据中的噪声或异常值会显著阻碍或延后Grokking的发生。

#### **3. 与“双下降 (Double Descent)”现象的关联**

*   **双下降**描述的是，当模型**参数量**持续增加，超过足以拟合训练数据的临界点后，模型的泛化能力反而会再次提升的现象。
*   **Grokking**描述的是，对于一个**固定参数量**的模型，在**训练时间**持续增加后，泛化能力突然提升的现象。
*   两者共同揭示了深度学习中一个深刻的特性：在“过参数化”或“长时间训练”这些超越传统统计学认知的领域，模型能够找到具备优秀泛化能力的简单解。

#### **4. 意义与启发**

*   **模型的可理解性**：Grokking现象表明，神经网络在没有被显式指导的情况下，也能够自发地学习和内化抽象的数学或逻辑规则。
*   **科学发现的潜力**：通过分析实现了Grokking的模型的内部权重结构（如文中提到的t-SNE可视化），我们或许能够反向发现和理解复杂数据背后未知的规律。

---

### **核心结论 (一句话)**

神经网络在长时间的正则化训练压力下，能够从单纯记忆数据的局部最优解“跃迁”到理解底层规则的全局更优解，从而实现从完全过拟合到突然完美泛化的“顿悟”现象。

---

### **内容 overarching 框架**

该内容的核心框架是，通过对比**“记忆解”**和**“泛化解”**这两种优化目标，阐述了在正则化（尤其是权重衰减）的引导下，神经网络优化过程如何从一个局部最优（记忆）跃迁到全局更优（泛化）的动态机制。它揭示了训练时间、模型容量和正则化三者之间复杂的相互作用，最终导向了超越过拟合的深度泛化。

---

### **Mermaid 概念图**

<Mermaid_Diagram>
graph TD
    subgraph "Grokking (顿悟) 现象核心"
        A["Grokking (顿悟) 现象"] -- "表现为" --> B{"训练动态曲线"};
        B --> B1["训练准确率 (Training Acc)"];
        B1 -- "训练初期" --> B1_1["迅速达到 100%"];
        B --> B2["验证准确率 (Validation Acc)"];
        B2 -- "训练初期" --> B2_1["长期接近随机 (过拟合)"];
        B2_1 -- "经过极长时间训练" --> B2_2["突然跃迁至 100% (泛化)"];
    end

    subgraph "背后机制 (Underlying Mechanism)"
        C{"优化过程中的两种解"}
        C --> D["记忆解 (Memorization)"];
        C --> E["泛化解 (Generalization)"];
        D ==> F["权重复杂, 范数大"];
        D ==> G["无法泛化"];
        E ==> H["学习底层规则"];
        E ==> I["权重简单, 范数小"];
        J["权重衰减 (Weight Decay)"] -- "作为核心驱动力" --> K{"引导寻找简单解 (奥卡姆剃刀)"};
        K -- "偏好" --> E;
        D -- "在K的压力下, 长时间训练后跃迁" --> E;
    end

    subgraph "影响因素 (Influencing Factors)"
        L["更多训练数据"] --> M{"加速 Grokking"};
        J --> M;
        N["更少数据噪声"] --> M;
        O["运算对称性"] --> M;
        A -- "受...影响" --> M
    end

    subgraph "意义与启发 (Implications)"
        E -- "证明" --> P["神经网络能够'理解'抽象规则"];
        P -- "提供新思路" --> Q["模型可解释性 & 科学发现"];
    end

    style A fill:#FFC300,stroke:#333,stroke-width:4px
    style B fill:#f9f7d8,stroke:#333,stroke-width:2px
    style D fill:#FFB6C1,stroke:#333,stroke-width:2px
    style E fill:#90EE90,stroke:#333,stroke-width:2px
    style J fill:#C39BD3,stroke:#333,stroke-width:2px
    style M fill:#AED6F1,stroke:#333,stroke-width:2px
    style P fill:#A3E4D7,stroke:#333,stroke-width:2px
    style Q fill:#A3E4D7,stroke:#333,stroke-width:2px
</Mermaid_Diagram>

Content:
hi there today we'll look at groing, hi there today we'll look at groing, generalization Beyond overfitting on, generalization Beyond overfitting on, generalization Beyond overfitting on, small algorithmic data sets by Alia, small algorithmic data sets by Alia, small algorithmic data sets by Alia, power Yuri BDA Harry Edwards Igor, power Yuri BDA Harry Edwards Igor, power Yuri BDA Harry Edwards Igor, babushkin and Vidant misra of open AI on, babushkin and Vidant misra of open AI on, babushkin and Vidant misra of open AI on, High level this paper presents a, High level this paper presents a, High level this paper presents a, phenomenon that the researchers call, phenomenon that the researchers call, phenomenon that the researchers call, grocking where a neural network will, grocking where a neural network will, grocking where a neural network will, generalize all of a sudden uh after, generalize all of a sudden uh after, generalize all of a sudden uh after, having after weigh the point of, having after weigh the point of, having after weigh the point of, overfitting on a data set so you train, overfitting on a data set so you train, overfitting on a data set so you train, the network it completely overfits on a, the network it completely overfits on a, the network it completely overfits on a, data set training loss is is down, data set training loss is is down, data set training loss is is down, training accuracy is 100% but it doesn't, training accuracy is 100% but it doesn't, training accuracy is 100% but it doesn't, generalize at all to the validation set, generalize at all to the validation set, generalize at all to the validation set, and then when you continue training the, and then when you continue training the, and then when you continue training the, network at some point it will just snap, network at some point it will just snap, network at some point it will just snap, into over uh into, into over uh into, into over uh into, generalizing on these data sets that, generalizing on these data sets that, generalizing on these data sets that, they're researching to a like a 100%, they're researching to a like a 100%, they're researching to a like a 100%, generalization so 100% accuracy on the, generalization so 100% accuracy on the, generalization so 100% accuracy on the, validation set this is extremely, validation set this is extremely, validation set this is extremely, interesting and as you can see the paper, interesting and as you can see the paper, interesting and as you can see the paper, has been presented at a workshop at iair, has been presented at a workshop at iair, has been presented at a workshop at iair, 2021 which means that it is not yet it's, 2021 which means that it is not yet it's, 2021 which means that it is not yet it's, sort of work in progress so there is, sort of work in progress so there is, sort of work in progress so there is, still a lot of unclear things about this, still a lot of unclear things about this, still a lot of unclear things about this, phenomenon it's a as I understand it a, phenomenon it's a as I understand it a, phenomenon it's a as I understand it a, phenomenological paper that just, phenomenological paper that just, phenomenological paper that just, presents look here is something, presents look here is something, presents look here is something, interesting that we found and I think, interesting that we found and I think, interesting that we found and I think, it's pretty cool so we'll dive into the, it's pretty cool so we'll dive into the, it's pretty cool so we'll dive into the, paper we'll look at this phenomenon they, paper we'll look at this phenomenon they, paper we'll look at this phenomenon they, do dig into it a little bit uh into, do dig into it a little bit uh into, do dig into it a little bit uh into, what's happening here and try to come up, what's happening here and try to come up, what's happening here and try to come up, with some, with some, with some, explanation so the basic premise of, explanation so the basic premise of, explanation so the basic premise of, grocking is the graph you see on the, grocking is the graph you see on the, grocking is the graph you see on the, left right here now it is a little bit, left right here now it is a little bit, left right here now it is a little bit, pixel is but I hope you can still see, pixel is but I hope you can still see, pixel is but I hope you can still see, what's happening the red part is uh the, what's happening the red part is uh the, what's happening the red part is uh the, training accuracy and on the x-axis you, training accuracy and on the x-axis you, training accuracy and on the x-axis you, have number of optimization steps and, have number of optimization steps and, have number of optimization steps and, this is a log scale so that's important, this is a log scale so that's important, this is a log scale so that's important, to to see this is a a log scale for, to to see this is a a log scale for, to to see this is a a log scale for, training steps in this direction now the, training steps in this direction now the, training steps in this direction now the, training accuracy naturally after a few, training accuracy naturally after a few, training accuracy naturally after a few, steps it shoots up to 100% we we'll get, steps it shoots up to 100% we we'll get, steps it shoots up to 100% we we'll get, to what data sets these things are in a, to what data sets these things are in a, to what data sets these things are in a, second but it's important to see the, second but it's important to see the, second but it's important to see the, network can in fact fit the training, network can in fact fit the training, network can in fact fit the training, data extremely well and it it just, data extremely well and it it just, data extremely well and it it just, overfits however the validation accuracy, overfits however the validation accuracy, overfits however the validation accuracy, it if you can see it there is a little, it if you can see it there is a little, it if you can see it there is a little, bump here but then it goes It goes down, bump here but then it goes It goes down, bump here but then it goes It goes down, again almost um I don't know whether we, again almost um I don't know whether we, again almost um I don't know whether we, should even regard this as a little bump, should even regard this as a little bump, should even regard this as a little bump, that's actually happening however it it, that's actually happening however it it, that's actually happening however it it, just stays it stays down it stays down, just stays it stays down it stays down, just stays it stays down it stays down, and then after you can see orders of, and then after you can see orders of, and then after you can see orders of, magnitude more steps this is 10 to the 2, magnitude more steps this is 10 to the 2, magnitude more steps this is 10 to the 2, 10 to the 3r 10 to the 4th 10 to the, 10 to the 3r 10 to the 4th 10 to the, 10 to the 3r 10 to the 4th 10 to the, 5ifth steps it shoots up and it starts, 5ifth steps it shoots up and it starts, 5ifth steps it shoots up and it starts, to generalize as well, to generalize as well, to generalize as well, this is very interesting because um you, this is very interesting because um you, this is very interesting because um you, know this essentially means you you keep, know this essentially means you you keep, know this essentially means you you keep, on training uh for a long time and when, on training uh for a long time and when, on training uh for a long time and when, all hope is lost still the network at, all hope is lost still the network at, all hope is lost still the network at, some point will will generalize now why, some point will will generalize now why, some point will will generalize now why, is this happening um and as I understand, is this happening um and as I understand, is this happening um and as I understand, it it's not the case often that the, it it's not the case often that the, it it's not the case often that the, network like drops down again out of, network like drops down again out of, network like drops down again out of, generalization though I haven't I, generalization though I haven't I, generalization though I haven't I, haven't actually seen this investigated, haven't actually seen this investigated, haven't actually seen this investigated, like if they run for 10 to the I don't, like if they run for 10 to the I don't, like if they run for 10 to the I don't, know how many steps but it seems like, know how many steps but it seems like, know how many steps but it seems like, once the network is generalizing is uh, once the network is generalizing is uh, once the network is generalizing is uh, has training accuracy of 100% it doesn't, has training accuracy of 100% it doesn't, has training accuracy of 100% it doesn't, fall out of that again so the question, fall out of that again so the question, fall out of that again so the question, is how does this happen like what What's, is how does this happen like what What's, is how does this happen like what What's, Happening Here uh why is this happening, Happening Here uh why is this happening, Happening Here uh why is this happening, why is it all of a sudden and what makes, why is it all of a sudden and what makes, why is it all of a sudden and what makes, it work and for that it's a bit uh, it work and for that it's a bit uh, it work and for that it's a bit uh, important to understand a very related, important to understand a very related, important to understand a very related, phenomenon in fact a connected probably, phenomenon in fact a connected probably, phenomenon in fact a connected probably, phenomenon called the double descend, phenomenon called the double descend, phenomenon called the double descend, phenomenon in deep learning the double, phenomenon in deep learning the double, phenomenon in deep learning the double, descent phenomenon graph looks somewhat, descent phenomenon graph looks somewhat, descent phenomenon graph looks somewhat, similar in that the premise is that on, similar in that the premise is that on, similar in that the premise is that on, the x-axis you have the number of, the x-axis you have the number of, the x-axis you have the number of, parameters uh in a network so the number, parameters uh in a network so the number, parameters uh in a network so the number, of parameters in a neural network and, of parameters in a neural network and, of parameters in a neural network and, then on the on the Y AIS you have let's, then on the on the Y AIS you have let's, then on the on the Y AIS you have let's, say, say, say, loss okay or actually let's say let's, loss okay or actually let's say let's, loss okay or actually let's say let's, say accuracy I'm not sure L most of, say accuracy I'm not sure L most of, say accuracy I'm not sure L most of, these plots for the double descent, these plots for the double descent, these plots for the double descent, phenomenon are actually loss so if you, phenomenon are actually loss so if you, phenomenon are actually loss so if you, consider the training loss, consider the training loss, consider the training loss, um as you increase the number of, um as you increase the number of, um as you increase the number of, parameters in your neural network you, parameters in your neural network you, parameters in your neural network you, will fit the data better and better the, will fit the data better and better the, will fit the data better and better the, training data so you get a curve that, training data so you get a curve that, training data so you get a curve that, goes something like this and then it, goes something like this and then it, goes something like this and then it, just stays at zero right so there's zero, just stays at zero right so there's zero, just stays at zero right so there's zero, training loss um as you increase the, training loss um as you increase the, training loss um as you increase the, number of parameters these every point, number of parameters these every point, number of parameters these every point, on this line is a neural network with a, on this line is a neural network with a, on this line is a neural network with a, given number of parameters that has just, given number of parameters that has just, given number of parameters that has just, been optimized to convergence okay, been optimized to convergence okay, been optimized to convergence okay, that's important to remember on the left, that's important to remember on the left, that's important to remember on the left, here we saw a graph during optimization, here we saw a graph during optimization, here we saw a graph during optimization, on the right here is a graph of many, on the right here is a graph of many, on the right here is a graph of many, different networks all of which have, different networks all of which have, different networks all of which have, been trained to uh convergence now what, been trained to uh convergence now what, been trained to uh convergence now what, you see with the validation loss in this, you see with the validation loss in this, you see with the validation loss in this, case so if you look at the validation, case so if you look at the validation, case so if you look at the validation, loss it might at some point it might, loss it might at some point it might, loss it might at some point it might, come down with the training loss right, come down with the training loss right, come down with the training loss right, and then in the classic fashion of, and then in the classic fashion of, and then in the classic fashion of, machine learning you as the number of, machine learning you as the number of, machine learning you as the number of, parameters go up you start to sort of, parameters go up you start to sort of, parameters go up you start to sort of, overfit the validation loss goes up, overfit the validation loss goes up, overfit the validation loss goes up, again uh because you start overfitting, again uh because you start overfitting, again uh because you start overfitting, you start memorizing the training data, you start memorizing the training data, you start memorizing the training data, set and then at a point where pretty, set and then at a point where pretty, set and then at a point where pretty, much the number of parameters equal the, much the number of parameters equal the, much the number of parameters equal the, number of training data points like the, number of training data points like the, number of training data points like the, number of let's just call this n then, number of let's just call this n then, number of let's just call this n then, you have again like a really crappy, you have again like a really crappy, you have again like a really crappy, validation loss because you're just, validation loss because you're just, validation loss because you're just, remembering the training data however if, remembering the training data however if, remembering the training data however if, you increase your parameter beyond that, you increase your parameter beyond that, you increase your parameter beyond that, point so if you scale up your neural, point so if you scale up your neural, point so if you scale up your neural, networks even more the validation loss, networks even more the validation loss, networks even more the validation loss, will come down again and actually end up, will come down again and actually end up, will come down again and actually end up, at a lower Point than if you were on, at a lower Point than if you were on, at a lower Point than if you were on, this place over here if you had not, this place over here if you had not, this place over here if you had not, enough parameters so there is a point, enough parameters so there is a point, enough parameters so there is a point, Beyond, Beyond, Beyond, overfitting uh where you have more, overfitting uh where you have more, overfitting uh where you have more, parameters than data points and interest, parameters than data points and interest, parameters than data points and interest, interestingly for neural networks it is, interestingly for neural networks it is, interestingly for neural networks it is, the case that it happens that they can, the case that it happens that they can, the case that it happens that they can, achieve generalization in fact better, achieve generalization in fact better, achieve generalization in fact better, generalization with, generalization with, generalization with, overparameterization than comparable, overparameterization than comparable, overparameterization than comparable, underparameterized uh models Which flies, underparameterized uh models Which flies, underparameterized uh models Which flies, in the face of of all statistics and, in the face of of all statistics and, in the face of of all statistics and, whatnot but we know this phenomenon, whatnot but we know this phenomenon, whatnot but we know this phenomenon, exists okay so uh we we knew that um, exists okay so uh we we knew that um, exists okay so uh we we knew that um, things like this can happen like the, things like this can happen like the, things like this can happen like the, training loss can be perfect and still, training loss can be perfect and still, training loss can be perfect and still, we can have generalization right the, we can have generalization right the, we can have generalization right the, groing phenomenon is a phenomenon, groing phenomenon is a phenomenon, groing phenomenon is a phenomenon, where I'm I'm going to guess I'm going, where I'm I'm going to guess I'm going, where I'm I'm going to guess I'm going, to guess the the the creators of the, to guess the the the creators of the, to guess the the the creators of the, double descend phenomenon haven't looked, double descend phenomenon haven't looked, double descend phenomenon haven't looked, quite as far in order to I guess they, quite as far in order to I guess they, quite as far in order to I guess they, simply ran training to convergence for a, simply ran training to convergence for a, simply ran training to convergence for a, number of steps and then they they they, number of steps and then they they they, number of steps and then they they they, looked at the validation loss so I guess, looked at the validation loss so I guess, looked at the validation loss so I guess, they would have stopped somewhere in, they would have stopped somewhere in, they would have stopped somewhere in, between here between 10 to the 3r and 10, between here between 10 to the 3r and 10, between here between 10 to the 3r and 10, to the 4th steps This research here is, to the 4th steps This research here is, to the 4th steps This research here is, simply what happens if we like let it, simply what happens if we like let it, simply what happens if we like let it, run for a really long time then this, run for a really long time then this, run for a really long time then this, shoots up uh as well and it seems like, shoots up uh as well and it seems like, shoots up uh as well and it seems like, it seems like for a lot of conditions, it seems like for a lot of conditions, it seems like for a lot of conditions, you you can you can do this so now it's, you you can you can do this so now it's, you you can you can do this so now it's, worth looking at what kind of data sets, worth looking at what kind of data sets, worth looking at what kind of data sets, we are we are interested in here the, we are we are interested in here the, we are we are interested in here the, data sets are synthetic data sets in, data sets are synthetic data sets in, data sets are synthetic data sets in, this paper the synthetic data sets are, this paper the synthetic data sets are, this paper the synthetic data sets are, binary operation tables so so here the, binary operation tables so so here the, binary operation tables so so here the, data sets we consider are binary, data sets we consider are binary, data sets we consider are binary, operation tables of the form a um and, operation tables of the form a um and, operation tables of the form a um and, then here this is like some sort of an, then here this is like some sort of an, then here this is like some sort of an, binary operation a let's just call it, binary operation a let's just call it, binary operation a let's just call it, multiplied a multiplied by b equals c, multiplied a multiplied by b equals c, multiplied a multiplied by b equals c, where a B and C are discrete symbols, where a B and C are discrete symbols, where a B and C are discrete symbols, with no internal structure and the, with no internal structure and the, with no internal structure and the, circle is a binary operation examples of, circle is a binary operation examples of, circle is a binary operation examples of, binary operations include addition, binary operations include addition, binary operations include addition, composition of permutations byari, composition of permutations byari, composition of permutations byari, polinomial and many many more in fact, polinomial and many many more in fact, polinomial and many many more in fact, they have some examples I think down, they have some examples I think down, they have some examples I think down, here so here you see some examples like, here so here you see some examples like, here so here you see some examples like, addition and multiplication but also, addition and multiplication but also, addition and multiplication but also, more complicated things like a polom, more complicated things like a polom, more complicated things like a polom, that you then um that you then uh do, that you then um that you then uh do, that you then um that you then uh do, modulo a prime number a division modulo, modulo a prime number a division modulo, modulo a prime number a division modulo, a prime number and so on so the way you, a prime number and so on so the way you, a prime number and so on so the way you, the way you create a data set is you, the way you create a data set is you, the way you create a data set is you, construct a table and in the table you, construct a table and in the table you, construct a table and in the table you, have a number of these, have a number of these, have a number of these, symbols and then you Define binary, symbols and then you Define binary, symbols and then you Define binary, operations by simply filling in that, operations by simply filling in that, operations by simply filling in that, table okay so if this were I don't know, table okay so if this were I don't know, table okay so if this were I don't know, like a plus a plus b and a and b are are, like a plus a plus b and a and b are are, like a plus a plus b and a and b are are, numbers then right A plus b is C if a is, numbers then right A plus b is C if a is, numbers then right A plus b is C if a is, 1 B is 2 C is 3 and so on um but you can, 1 B is 2 C is 3 and so on um but you can, 1 B is 2 C is 3 and so on um but you can, Define this as many different things uh, Define this as many different things uh, Define this as many different things uh, a lot of the experiments in this paper, a lot of the experiments in this paper, a lot of the experiments in this paper, are of the group S5 which is the group, are of the group S5 which is the group, are of the group S5 which is the group, group of all permutations of five, group of all permutations of five, group of all permutations of five, elements which I think has like so this, elements which I think has like so this, elements which I think has like so this, is a a group with 120 elements so your, is a a group with 120 elements so your, is a a group with 120 elements so your, table would here be 120 by, table would here be 120 by, table would here be 120 by, 120 and the operation would be the sort, 120 and the operation would be the sort, 120 and the operation would be the sort, of um composition of permutation so, of um composition of permutation so, of um composition of permutation so, every permutation of five elements, every permutation of five elements, every permutation of five elements, composed with another permutation gives, composed with another permutation gives, composed with another permutation gives, you yet another permutation of five, you yet another permutation of five, you yet another permutation of five, elements so you can just construct this, elements so you can just construct this, elements so you can just construct this, this table and then what you do is you, this table and then what you do is you, this table and then what you do is you, just simply cross out a few things in, just simply cross out a few things in, just simply cross out a few things in, the table so you say okay here I'm just, the table so you say okay here I'm just, the table so you say okay here I'm just, going to cross out a few things and this, going to cross out a few things and this, going to cross out a few things and this, is what the network should predict right, is what the network should predict right, is what the network should predict right, I'm going to train the network on the, I'm going to train the network on the, I'm going to train the network on the, data that I have and I'm going to, data that I have and I'm going to, data that I have and I'm going to, predict the cells that I crossed out, predict the cells that I crossed out, predict the cells that I crossed out, this way you can exactly measure how, this way you can exactly measure how, this way you can exactly measure how, good the network is right there is no, good the network is right there is no, good the network is right there is no, noise effectively in the data um it's, noise effectively in the data um it's, noise effectively in the data um it's, all very well defined and a human goes, all very well defined and a human goes, all very well defined and a human goes, about this with I guess with sort of a, about this with I guess with sort of a, about this with I guess with sort of a, logical mind they try to figure out like, logical mind they try to figure out like, logical mind they try to figure out like, ah what's the rule what's the rule a, ah what's the rule what's the rule a, ah what's the rule what's the rule a, neural network can simply remember the, neural network can simply remember the, neural network can simply remember the, training data but then it will not, training data but then it will not, training data but then it will not, generalize to the hidden Fields because, generalize to the hidden Fields because, generalize to the hidden Fields because, it cannot memorize those so if a neural, it cannot memorize those so if a neural, it cannot memorize those so if a neural, network generalizes here it also kind of, network generalizes here it also kind of, network generalizes here it also kind of, means that it must have somehow learned, means that it must have somehow learned, means that it must have somehow learned, the rule and this this is pretty, the rule and this this is pretty, the rule and this this is pretty, interesting so there are number of, interesting so there are number of, interesting so there are number of, quantities to keep in mind um the the, quantities to keep in mind um the the, quantities to keep in mind um the the, three quantities are first of all what's, three quantities are first of all what's, three quantities are first of all what's, the operation uh because there are more, the operation uh because there are more, the operation uh because there are more, and less complicated things for these, and less complicated things for these, and less complicated things for these, networks to learn just from the kind of, networks to learn just from the kind of, networks to learn just from the kind of, difficulty the complexity of the, difficulty the complexity of the, difficulty the complexity of the, operation itself second of all is the, operation itself second of all is the, operation itself second of all is the, data set size or the size of the binary, data set size or the size of the binary, data set size or the size of the binary, table itself in this case it's 120 by, table itself in this case it's 120 by, table itself in this case it's 120 by, 120 um and the third one is how many, 120 um and the third one is how many, 120 um and the third one is how many, things are left away so how large is the, things are left away so how large is the, things are left away so how large is the, training data fraction the fraction of, training data fraction the fraction of, training data fraction the fraction of, the table that is filled in for the, the table that is filled in for the, the table that is filled in for the, network to learn all of these three, network to learn all of these three, network to learn all of these three, things are going to play a crucial role, things are going to play a crucial role, things are going to play a crucial role, in this in this grocking phenomenon and, in this in this grocking phenomenon and, in this in this grocking phenomenon and, when and how it appears for example here, when and how it appears for example here, when and how it appears for example here, you see um they they, you see um they they, you see um they they, have trained neural networks on this S5, have trained neural networks on this S5, have trained neural networks on this S5, group gr right the permutations of, group gr right the permutations of, group gr right the permutations of, groups of five elements until they reach, groups of five elements until they reach, groups of five elements until they reach, generalization so they simply run it and, generalization so they simply run it and, generalization so they simply run it and, they measure how long does it take a, they measure how long does it take a, they measure how long does it take a, network to reach, network to reach, network to reach, 99% validation accuracy or higher right, 99% validation accuracy or higher right, 99% validation accuracy or higher right, that's that's the thing on the left is, that's that's the thing on the left is, that's that's the thing on the left is, essentially um you know the answer would, essentially um you know the answer would, essentially um you know the answer would, be something like between 10 to the 5, be something like between 10 to the 5, be something like between 10 to the 5, and 10 to the six right, and 10 to the six right, and 10 to the six right, okay so and they measure this as a, okay so and they measure this as a, okay so and they measure this as a, function of you might not be able to, function of you might not be able to, function of you might not be able to, read this but it says training data, read this but it says training data, read this but it says training data, fraction okay how much of the training, fraction okay how much of the training, fraction okay how much of the training, data is filled in and you can pretty, data is filled in and you can pretty, data is filled in and you can pretty, clearly see if I just give it like here, clearly see if I just give it like here, clearly see if I just give it like here, 20% of training data there are even some, 20% of training data there are even some, 20% of training data there are even some, runs that do not generalize in this, runs that do not generalize in this, runs that do not generalize in this, number of steps now would they, number of steps now would they, number of steps now would they, generalize if you were to optimize for, generalize if you were to optimize for, generalize if you were to optimize for, even longer who knows honestly but you, even longer who knows honestly but you, even longer who knows honestly but you, can see that as soon as you give like, can see that as soon as you give like, can see that as soon as you give like, 30% of the training data the runs in, 30% of the training data the runs in, 30% of the training data the runs in, general do generalize but they take, general do generalize but they take, general do generalize but they take, something like um here yeah 10 to the 5, something like um here yeah 10 to the 5, something like um here yeah 10 to the 5, number of steps to do so and then as you, number of steps to do so and then as you, number of steps to do so and then as you, increase the training date to fraction, increase the training date to fraction, increase the training date to fraction, uh this snap to the generalization, uh this snap to the generalization, uh this snap to the generalization, happens faster and faster you can see, happens faster and faster you can see, happens faster and faster you can see, right here as you give more training, right here as you give more training, right here as you give more training, data uh it goes faster and faster until, data uh it goes faster and faster until, data uh it goes faster and faster until, it generalizes and the generalization, it generalizes and the generalization, it generalizes and the generalization, happens as I understand it yeah fairly, happens as I understand it yeah fairly, happens as I understand it yeah fairly, like quickly like it it doesn't, like quickly like it it doesn't, like quickly like it it doesn't, generalize because it remembers the, generalize because it remembers the, generalize because it remembers the, training data and this always happens as, training data and this always happens as, training data and this always happens as, I understand it in a fairly similar, I understand it in a fairly similar, I understand it in a fairly similar, number of steps um but then at some, number of steps um but then at some, number of steps um but then at some, later point it just kind of snaps and, later point it just kind of snaps and, later point it just kind of snaps and, completely generalizes to the uh, completely generalizes to the uh, completely generalizes to the uh, validation set and this is this is, validation set and this is this is, validation set and this is this is, really interesting so we know that the, really interesting so we know that the, really interesting so we know that the, more training data we have around the, more training data we have around the, more training data we have around the, better right that's one recognition, um then the other the other thing is, um then the other the other thing is, they try to figure out okay um which, they try to figure out okay um which, they try to figure out okay um which, parts of the optimization algorithm, parts of the optimization algorithm, parts of the optimization algorithm, are are making this grocking phenomenon, are are making this grocking phenomenon, are are making this grocking phenomenon, happen and here they figure out that uh, happen and here they figure out that uh, happen and here they figure out that uh, weight decay in fact is one of the is, weight decay in fact is one of the is, weight decay in fact is one of the is, one of the big drivers of this so if, one of the big drivers of this so if, one of the big drivers of this so if, they add weight Decay to the algorithm, they add weight Decay to the algorithm, they add weight Decay to the algorithm, and they try a lot of different things, and they try a lot of different things, and they try a lot of different things, they try full batch versus mini batch, they try full batch versus mini batch, they try full batch versus mini batch, with Dropout without Dropout uh, with Dropout without Dropout uh, with Dropout without Dropout uh, modulating the learning rate and so on, modulating the learning rate and so on, modulating the learning rate and so on, but weight Decay seems to be one of the, but weight Decay seems to be one of the, but weight Decay seems to be one of the, biggest uh contributors to this groing, biggest uh contributors to this groing, biggest uh contributors to this groing, uh phenomenon to the fact or to how fast, uh phenomenon to the fact or to how fast, uh phenomenon to the fact or to how fast, these networks generalize you can see, these networks generalize you can see, these networks generalize you can see, that the network generalizes much sooner, that the network generalizes much sooner, that the network generalizes much sooner, uh if you have weight Decay turned uh up, uh if you have weight Decay turned uh up, uh if you have weight Decay turned uh up, than not also they make the observation, than not also they make the observation, than not also they make the observation, that that uh if you have symmetric, that that uh if you have symmetric, that that uh if you have symmetric, operations uh if your binary operation, operations uh if your binary operation, operations uh if your binary operation, is symmetric then also the grocking, is symmetric then also the grocking, is symmetric then also the grocking, phenomenon happens much faster than if, phenomenon happens much faster than if, phenomenon happens much faster than if, you have like nonsymmetric operations, you have like nonsymmetric operations, you have like nonsymmetric operations, this might just be a function of these, this might just be a function of these, this might just be a function of these, networks which if you if you have like, networks which if you if you have like, networks which if you if you have like, something like a Transformer uh you know, something like a Transformer uh you know, something like a Transformer uh you know, it it's it's sort of kind of invariant, it it's it's sort of kind of invariant, it it's it's sort of kind of invariant, to to the symmetry so it might like, to to the symmetry so it might like, to to the symmetry so it might like, essentially one data point is sort of, essentially one data point is sort of, essentially one data point is sort of, two data points in Disguise if it's, two data points in Disguise if it's, two data points in Disguise if it's, symmetric or or there's only half as, symmetric or or there's only half as, symmetric or or there's only half as, much stuff to learn uh you choose, much stuff to learn uh you choose, much stuff to learn uh you choose, whatever you you want to interpret this, whatever you you want to interpret this, whatever you you want to interpret this, as but I think yeah this is not as, as but I think yeah this is not as, as but I think yeah this is not as, important as the weight Decay and why do, important as the weight Decay and why do, important as the weight Decay and why do, I highlight this um I highlight this, I highlight this um I highlight this, I highlight this um I highlight this, because Al down here you can see they, because Al down here you can see they, because Al down here you can see they, analyze then um they analyze the results, analyze then um they analyze the results, analyze then um they analyze the results, of a Network that has learned to, of a Network that has learned to, of a Network that has learned to, generalize uh like this so so on the, generalize uh like this so so on the, generalize uh like this so so on the, right you see a t projection of the, right you see a t projection of the, right you see a t projection of the, output layer weights from a network, output layer weights from a network, output layer weights from a network, trained on modular addition so this is x, trained on modular addition so this is x, trained on modular addition so this is x, + y modulo 8 I think the lines show the, + y modulo 8 I think the lines show the, + y modulo 8 I think the lines show the, result of adding eight to each element, result of adding eight to each element, result of adding eight to each element, the colors show the residue of each, the colors show the residue of each, the colors show the residue of each, element modulo 8 so if you do the tne, element modulo 8 so if you do the tne, element modulo 8 so if you do the tne, projection you can see the lines are, projection you can see the lines are, projection you can see the lines are, obviously drawn by the authors but you, obviously drawn by the authors but you, obviously drawn by the authors but you, can see there are structures where if, can see there are structures where if, can see there are structures where if, you go along the line right here they, you go along the line right here they, you go along the line right here they, colored essentially this is always, colored essentially this is always, colored essentially this is always, adding eight adding eight adding eight, adding eight adding eight adding eight, adding eight adding eight adding eight, so there are structures where um this, so there are structures where um this, so there are structures where um this, the rule for generating the data is, the rule for generating the data is, the rule for generating the data is, clearly present in the data itself uh, clearly present in the data itself uh, clearly present in the data itself uh, sorry in the in the Network's weights, sorry in the in the Network's weights, sorry in the in the Network's weights, this gives you a strong indication that, this gives you a strong indication that, this gives you a strong indication that, the network has not only just remembered, the network has not only just remembered, the network has not only just remembered, the data somehow but has in fact, the data somehow but has in fact, the data somehow but has in fact, discovered the rule behind the data and, discovered the rule behind the data and, discovered the rule behind the data and, we have never incentivized the networks, we have never incentivized the networks, we have never incentivized the networks, to learn these rules that's the wild, to learn these rules that's the wild, to learn these rules that's the wild, point there are there are architectures, point there are there are architectures, point there are there are architectures, where you try to specifically make tell, where you try to specifically make tell, where you try to specifically make tell, the network look there there is a rule, the network look there there is a rule, the network look there there is a rule, behind this I want you to figure out the, behind this I want you to figure out the, behind this I want you to figure out the, rule you can maybe do symbolic, rule you can maybe do symbolic, rule you can maybe do symbolic, regression or um I don't know like like, regression or um I don't know like like, regression or um I don't know like like, you can try to build an internal graph, you can try to build an internal graph, you can try to build an internal graph, of and reason over it no no no we just, of and reason over it no no no we just, of and reason over it no no no we just, train neural networks right here and it, train neural networks right here and it, train neural networks right here and it, turns out out that these networks can, turns out out that these networks can, turns out out that these networks can, learn these, learn these, learn these, rules so why do I relate this to the, rules so why do I relate this to the, rules so why do I relate this to the, double descent phenomenon in the double, double descent phenomenon in the double, double descent phenomenon in the double, descent phenomenon um it is assumed or, descent phenomenon um it is assumed or, descent phenomenon um it is assumed or, I've I've heard the authors of these, I've I've heard the authors of these, I've I've heard the authors of these, papers uh speak about their their kind, papers uh speak about their their kind, papers uh speak about their their kind, of hypothesis why this happens and this, of hypothesis why this happens and this, of hypothesis why this happens and this, is a bit mixed with my my hypothesis as, is a bit mixed with my my hypothesis as, is a bit mixed with my my hypothesis as, well uh they speak of for example weight, well uh they speak of for example weight, well uh they speak of for example weight, Decay being one possible explanation so, Decay being one possible explanation so, Decay being one possible explanation so, they say if I have a bunch of of data, they say if I have a bunch of of data, they say if I have a bunch of of data, points let's say I have a bunch of data, points let's say I have a bunch of data, points let's say I have a bunch of data, points right here right and I want to do, points right here right and I want to do, points right here right and I want to do, regression on them well if I just do, regression on them well if I just do, regression on them well if I just do, linear regression I have one line right, linear regression I have one line right, linear regression I have one line right, it's fairly robust right it's fairly, it's fairly robust right it's fairly, it's fairly robust right it's fairly, flat it's fairly robust because it's, flat it's fairly robust because it's, flat it's fairly robust because it's, just one parameter now if I start to add, just one parameter now if I start to add, just one parameter now if I start to add, parameters right I get maybe I get to a, parameters right I get maybe I get to a, parameters right I get maybe I get to a, point where I have a good number of, point where I have a good number of, point where I have a good number of, parameters you know this this polom, parameters you know this this polom, parameters you know this this polom, maybe kind of like this still fairly, maybe kind of like this still fairly, maybe kind of like this still fairly, robust right you can see how it might, robust right you can see how it might, robust right you can see how it might, generalize to to new data then right so, generalize to to new data then right so, generalize to to new data then right so, this the blue one will be somewhere here, this the blue one will be somewhere here, this the blue one will be somewhere here, the dark blue one would be somewhere, the dark blue one would be somewhere, the dark blue one would be somewhere, here where the the validation loss, here where the the validation loss, here where the the validation loss, actually goes down with the training, actually goes down with the training, actually goes down with the training, loss but then when I add when I keep, loss but then when I add when I keep, loss but then when I add when I keep, adding data points uh sorry parameters, adding data points uh sorry parameters, adding data points uh sorry parameters, then you know classically I'll start you, then you know classically I'll start you, then you know classically I'll start you, know my my overfitting right here and, know my my overfitting right here and, know my my overfitting right here and, this it will not generalize to any point, this it will not generalize to any point, this it will not generalize to any point, that might be in between like one here, that might be in between like one here, that might be in between like one here, or so there will just go up so the green, or so there will just go up so the green, or so there will just go up so the green, would correspond to the point where I, would correspond to the point where I, would correspond to the point where I, just start to interpolate the training, just start to interpolate the training, just start to interpolate the training, data but then what happens if I go on if, data but then what happens if I go on if, data but then what happens if I go on if, I make even higher order pols or higher, I make even higher order pols or higher, I make even higher order pols or higher, order neural networks well at that point, order neural networks well at that point, order neural networks well at that point, at least these authors argue do I have, at least these authors argue do I have, at least these authors argue do I have, another, another, another, color this one they argue that you get, color this one they argue that you get, color this one they argue that you get, like a polinomial that or or or a curve, like a polinomial that or or or a curve, like a polinomial that or or or a curve, Pro that yes it has a lot of parameters, Pro that yes it has a lot of parameters, Pro that yes it has a lot of parameters, but it it uses these parameters such, but it it uses these parameters such, but it it uses these parameters such, that it can be sort of smoothly, that it can be sort of smoothly, that it can be sort of smoothly, interpolate the training data and this, interpolate the training data and this, interpolate the training data and this, curve is quite complicated in terms of, curve is quite complicated in terms of, curve is quite complicated in terms of, the number of numbers you need to, the number of numbers you need to, the number of numbers you need to, describe it but it uses the fact that it, describe it but it uses the fact that it, describe it but it uses the fact that it, has a lot of freedom you know it can, has a lot of freedom you know it can, has a lot of freedom you know it can, choose to be however it wants as long as, choose to be however it wants as long as, choose to be however it wants as long as, it interpolates the training data right, it interpolates the training data right, it interpolates the training data right, yet it chooses to be smooth because of a, yet it chooses to be smooth because of a, yet it chooses to be smooth because of a, combination of GD training it and of, combination of GD training it and of, combination of GD training it and of, weight Decay so the weight Decay would, weight Decay so the weight Decay would, weight Decay so the weight Decay would, prevent any of these numbers from, prevent any of these numbers from, prevent any of these numbers from, getting too big and therefore for, getting too big and therefore for, getting too big and therefore for, getting like super out of whack curve uh, getting like super out of whack curve uh, getting like super out of whack curve uh, so the weight Decay would in fact smooth, so the weight Decay would in fact smooth, so the weight Decay would in fact smooth, the curve and that makes the model, the curve and that makes the model, the curve and that makes the model, generalize really well because the, generalize really well because the, generalize really well because the, smoothness now is reasonably generalizes, smoothness now is reasonably generalizes, smoothness now is reasonably generalizes, to training data points that are in, to training data points that are in, to training data points that are in, between like this data point is still, between like this data point is still, between like this data point is still, fairly well represented by the purple, fairly well represented by the purple, fairly well represented by the purple, curve in fact it's better than the the, curve in fact it's better than the the, curve in fact it's better than the the, dark blue curve in this particular case, dark blue curve in this particular case, dark blue curve in this particular case, uh so you can see that the authors here, uh so you can see that the authors here, uh so you can see that the authors here, argue that weight Decay might be an, argue that weight Decay might be an, argue that weight Decay might be an, important contributor to why, important contributor to why, important contributor to why, overparameterized networks generalize, overparameterized networks generalize, overparameterized networks generalize, and it's interesting that the the these, and it's interesting that the the these, and it's interesting that the the these, grocking uh the authors of the grocking, grocking uh the authors of the grocking, grocking uh the authors of the grocking, phenomenon paper here find the same, phenomenon paper here find the same, phenomenon paper here find the same, thing they say okay if we use weight, thing they say okay if we use weight, thing they say okay if we use weight, Decay the grocking appears to happen, Decay the grocking appears to happen, Decay the grocking appears to happen, much, much, much, faster um if is this I don't know what, faster um if is this I don't know what, faster um if is this I don't know what, exactly they call grocking I'm just, exactly they call grocking I'm just, exactly they call grocking I'm just, going to call grocking this whenever the, going to call grocking this whenever the, going to call grocking this whenever the, validation loss uh snaps all of a sudden, validation loss uh snaps all of a sudden, validation loss uh snaps all of a sudden, from 0er to 100 on these these data sets, from 0er to 100 on these these data sets, from 0er to 100 on these these data sets, now again these are algorithmic data, now again these are algorithmic data, now again these are algorithmic data, sets so you know we don't know what, sets so you know we don't know what, sets so you know we don't know what, happens I think they they do make, happens I think they they do make, happens I think they they do make, experiments when they they noise some of, experiments when they they noise some of, experiments when they they noise some of, the data so um they they have some noise, the data so um they they have some noise, the data so um they they have some noise, in there and I think they find that if, in there and I think they find that if, in there and I think they find that if, they add noise then uh it's way more, they add noise then uh it's way more, they add noise then uh it's way more, difficult um I'm not sure though maybe, difficult um I'm not sure though maybe, difficult um I'm not sure though maybe, I'm confusing papers here um but what, I'm confusing papers here um but what, I'm confusing papers here um but what, what might be happening right here right, what might be happening right here right, what might be happening right here right, this is it's interesting because um what, this is it's interesting because um what, this is it's interesting because um what, might be happening is that by imposing, might be happening is that by imposing, might be happening is that by imposing, this, this, this, smoothness um and the, smoothness um and the, smoothness um and the, overparameterization we're sort of, overparameterization we're sort of, overparameterization we're sort of, biasing these networks to find like, biasing these networks to find like, biasing these networks to find like, Simple Solutions right so if, Simple Solutions right so if, Simple Solutions right so if, at if I have just very few training data, at if I have just very few training data, at if I have just very few training data, points if most of the cells here are, points if most of the cells here are, points if most of the cells here are, blacked out right the simplest solution, blacked out right the simplest solution, blacked out right the simplest solution, is simply to remember the training data, is simply to remember the training data, is simply to remember the training data, however as I get more and more training, however as I get more and more training, however as I get more and more training, data points right uh that give me more, data points right uh that give me more, data points right uh that give me more, and more information about a potential, and more information about a potential, and more information about a potential, underlying rule it becomes simpler for, underlying rule it becomes simpler for, underlying rule it becomes simpler for, me to Simply to understand the, me to Simply to understand the, me to Simply to understand the, underlying rule than to remember the, underlying rule than to remember the, underlying rule than to remember the, training data it's it's more it's more, training data it's it's more it's more, training data it's it's more it's more, difficult to remember the training data, difficult to remember the training data, difficult to remember the training data, than simply to learn the rule so what, than simply to learn the rule so what, than simply to learn the rule so what, might be happening here is that as I, might be happening here is that as I, might be happening here is that as I, train and this is always training here, train and this is always training here, train and this is always training here, the training happens always on the same, the training happens always on the same, the training happens always on the same, data right you you simply uh sample the, data right you you simply uh sample the, data right you you simply uh sample the, same things over and over again train on, same things over and over again train on, same things over and over again train on, it I think what might be happening is, it I think what might be happening is, it I think what might be happening is, that you kind of jump around in your, that you kind of jump around in your, that you kind of jump around in your, optimization procedure you can see there, optimization procedure you can see there, optimization procedure you can see there, there are some bumps in the training, there are some bumps in the training, there are some bumps in the training, accuracy here so you kind of Jump Around, accuracy here so you kind of Jump Around, accuracy here so you kind of Jump Around, jump around that's a song no um so you, jump around that's a song no um so you, jump around that's a song no um so you, jump around a bit and and in your in, jump around a bit and and in your in, jump around a bit and and in your in, your loss landscape there there might be, your loss landscape there there might be, your loss landscape there there might be, many of these local Minima where you in, many of these local Minima where you in, many of these local Minima where you in, fact uh remember the training data, fact uh remember the training data, fact uh remember the training data, perfectly so you kind of jump around a, perfectly so you kind of jump around a, perfectly so you kind of jump around a, bit between them right you remember the, bit between them right you remember the, bit between them right you remember the, training data perfectly and then one of, training data perfectly and then one of, training data perfectly and then one of, them is just you remember the training, them is just you remember the training, them is just you remember the training, data as well well now this is you, data as well well now this is you, data as well well now this is you, remember the training data as well, remember the training data as well, remember the training data as well, however the solution is just so much, however the solution is just so much, however the solution is just so much, simpler that you stay there this is not, simpler that you stay there this is not, simpler that you stay there this is not, a good way of visualizing it so it must, a good way of visualizing it so it must, a good way of visualizing it so it must, be something like here are the Minima, be something like here are the Minima, be something like here are the Minima, where here are the Minima where this is, where here are the Minima where this is, where here are the Minima where this is, the training just the the loss on the, the training just the the loss on the, the training just the the loss on the, data however there is another loss and, data however there is another loss and, data however there is another loss and, that's the loss on like the for example, that's the loss on like the for example, that's the loss on like the for example, the weight Decay loss and the weight, the weight Decay loss and the weight, the weight Decay loss and the weight, Decay loss is you know it's it's pretty, Decay loss is you know it's it's pretty, Decay loss is you know it's it's pretty, good all of these things but then for, good all of these things but then for, good all of these things but then for, one of them it's just like because that, one of them it's just like because that, one of them it's just like because that, solution is so much simpler so you're, solution is so much simpler so you're, solution is so much simpler so you're, going to choose you're going to jump, going to choose you're going to jump, going to choose you're going to jump, around between those Minima jump around, around between those Minima jump around, around between those Minima jump around, until you know once you reach this one, until you know once you reach this one, until you know once you reach this one, this loss right here that comes on top, this loss right here that comes on top, this loss right here that comes on top, of this is just so much lower that, of this is just so much lower that, of this is just so much lower that, you're going to you're going to stay, you're going to you're going to stay, you're going to you're going to stay, there and it's like wow I found such an, there and it's like wow I found such an, there and it's like wow I found such an, easy solution um I'm not going to go out, easy solution um I'm not going to go out, easy solution um I'm not going to go out, again so yeah now the big question is of, again so yeah now the big question is of, again so yeah now the big question is of, course how and why does something like, course how and why does something like, course how and why does something like, SGD plus weight Decay plus potential, SGD plus weight Decay plus potential, SGD plus weight Decay plus potential, other drivers of smoothness in these, other drivers of smoothness in these, other drivers of smoothness in these, models how and why do they correspond to, models how and why do they correspond to, models how and why do they correspond to, Simplicity of solutions right because, Simplicity of solutions right because, Simplicity of solutions right because, Simplicity of solutions is something, Simplicity of solutions is something, Simplicity of solutions is something, that kind of we humans have built in, that kind of we humans have built in, that kind of we humans have built in, like okay what's the rule behind this, like okay what's the rule behind this, like okay what's the rule behind this, what's the rule it's essentially, what's the rule it's essentially, what's the rule it's essentially, assuming that there is a simple rule, assuming that there is a simple rule, assuming that there is a simple rule, trying to find it because it make our, trying to find it because it make our, trying to find it because it make our, life much easier it's a simple, life much easier it's a simple, life much easier it's a simple, explanation for what's happening the, explanation for what's happening the, explanation for what's happening the, interesting part is that weight Decay or, interesting part is that weight Decay or, interesting part is that weight Decay or, something similar something that's, something similar something that's, something similar something that's, happening in these neuron networks is, happening in these neuron networks is, happening in these neuron networks is, essentially doing the same thing even, essentially doing the same thing even, essentially doing the same thing even, though we don't tell it to do it so, though we don't tell it to do it so, though we don't tell it to do it so, understanding this I think is going to, understanding this I think is going to, understanding this I think is going to, be uh quite an important um quite an, be uh quite an important um quite an, be uh quite an important um quite an, important task for the near future and, important task for the near future and, important task for the near future and, also maybe maybe we're not exactly right, also maybe maybe we're not exactly right, also maybe maybe we're not exactly right, with the way Decay maybe there is some, with the way Decay maybe there is some, with the way Decay maybe there is some, other constraint that we can impose that, other constraint that we can impose that, other constraint that we can impose that, encourages Simple Solutions in in the, encourages Simple Solutions in in the, encourages Simple Solutions in in the, way we care about Simplicity even more, way we care about Simplicity even more, way we care about Simplicity even more, and you know once we have that um the, and you know once we have that um the, and you know once we have that um the, it's it's like you know there this, it's it's like you know there this, it's it's like you know there this, age-old argument do these things, age-old argument do these things, age-old argument do these things, actually understand anything well in, actually understand anything well in, actually understand anything well in, this case I'm sorry but if you have, this case I'm sorry but if you have, this case I'm sorry but if you have, found this solution with the rule uh, found this solution with the rule uh, found this solution with the rule uh, essentially built into the networks of, essentially built into the networks of, essentially built into the networks of, the into the weights of the neural, the into the weights of the neural, the into the weights of the neural, network you can say well the network has, network you can say well the network has, network you can say well the network has, in fact learned the rule behind this, in fact learned the rule behind this, in fact learned the rule behind this, binary operations so you know who are we, binary operations so you know who are we, binary operations so you know who are we, to say these networks don't understand, to say these networks don't understand, to say these networks don't understand, anything at that point and also it gives, anything at that point and also it gives, anything at that point and also it gives, us the opportunity to you know train, us the opportunity to you know train, us the opportunity to you know train, these networks and then from the, these networks and then from the, these networks and then from the, structures of their latent spaces we, structures of their latent spaces we, structures of their latent spaces we, might in fact parse out the rules of, might in fact parse out the rules of, might in fact parse out the rules of, data we don't know yet so we let the, data we don't know yet so we let the, data we don't know yet so we let the, networks fit and we parse we parse the, networks fit and we parse we parse the, networks fit and we parse we parse the, underlying maybe physical laws maybe um, underlying maybe physical laws maybe um, underlying maybe physical laws maybe um, social social phenomena we pars them out, social social phenomena we pars them out, social social phenomena we pars them out, from the underlying uh data oh yeah here, from the underlying uh data oh yeah here, from the underlying uh data oh yeah here, okay there is an appendix where they, okay there is an appendix where they, okay there is an appendix where they, they list binary uh operations they have, they list binary uh operations they have, they list binary uh operations they have, tried out um models uh optimizations so, tried out um models uh optimizations so, tried out um models uh optimizations so, yeah they use a Transformer with two, yeah they use a Transformer with two, yeah they use a Transformer with two, layers for attention heads um so it's, layers for attention heads um so it's, layers for attention heads um so it's, not a it's not a big thing and also the, not a it's not a big thing and also the, not a it's not a big thing and also the, data sets aren't aren't super, data sets aren't aren't super, data sets aren't aren't super, complicated but it's pretty cool to see, complicated but it's pretty cool to see, complicated but it's pretty cool to see, uh this phenomenon now again on if if we, uh this phenomenon now again on if if we, uh this phenomenon now again on if if we, have real world data bigger networks, have real world data bigger networks, have real world data bigger networks, noisy data um it's not going to it's not, noisy data um it's not going to it's not, noisy data um it's not going to it's not, going to happen as drastically and also, going to happen as drastically and also, going to happen as drastically and also, they say as you increase the size of the, they say as you increase the size of the, they say as you increase the size of the, data set where is that as you increase, data set where is that as you increase, data set where is that as you increase, the size of the data set um then this, the size of the data set um then this, the size of the data set um then this, phenomenon is harder and harder so if, phenomenon is harder and harder so if, phenomenon is harder and harder so if, the entire data set is bigger uh the the, the entire data set is bigger uh the the, the entire data set is bigger uh the the, grocking phenomenon I guess it's it's, grocking phenomenon I guess it's it's, grocking phenomenon I guess it's it's, more tough to see and also here is the, more tough to see and also here is the, more tough to see and also here is the, experiment I mentioned where you have, experiment I mentioned where you have, experiment I mentioned where you have, several outliers so noisy data points, several outliers so noisy data points, several outliers so noisy data points, and as you um so this is the fraction of, and as you um so this is the fraction of, and as you um so this is the fraction of, correctly labeled data points so as you, correctly labeled data points so as you, correctly labeled data points so as you, increase the number of correctly labeled, increase the number of correctly labeled, increase the number of correctly labeled, data points you can see the grocking, data points you can see the grocking, data points you can see the grocking, happens in more often or to a better, happens in more often or to a better, happens in more often or to a better, validation accuracy than not so well you, validation accuracy than not so well you, validation accuracy than not so well you, can I don't know if you can read this, can I don't know if you can read this, can I don't know if you can read this, but, but, but, um yeah the these these down here they, um yeah the these these down here they, um yeah the these these down here they, have too many outliers so with too many, have too many outliers so with too many, have too many outliers so with too many, outliers either the validation accuracy, outliers either the validation accuracy, outliers either the validation accuracy, just stays at zero or it just turns up, just stays at zero or it just turns up, just stays at zero or it just turns up, like quite, like quite, like quite, late okay that's that's it here is an, late okay that's that's it here is an, late okay that's that's it here is an, example of one of these binary operation, example of one of these binary operation, example of one of these binary operation, tables that is a little bit larger I, tables that is a little bit larger I, tables that is a little bit larger I, don't know if it's one of the, don't know if it's one of the, don't know if it's one of the, 10020 uh sized ones but this is, 10020 uh sized ones but this is, 10020 uh sized ones but this is, something that would be presented to the, something that would be presented to the, something that would be presented to the, network and they, network and they, network and they, say they say what we invite the reader, say they say what we invite the reader, say they say what we invite the reader, to guess which operation is represented, to guess which operation is represented, to guess which operation is represented, here well have fun dear dear, here well have fun dear dear, here well have fun dear dear, reader um yeah all right so this was it, reader um yeah all right so this was it, reader um yeah all right so this was it, from me for the grocking paper as I said, from me for the grocking paper as I said, from me for the grocking paper as I said, this seems like it's work in progress I, this seems like it's work in progress I, this seems like it's work in progress I, think it's pretty cool work in progress, think it's pretty cool work in progress, think it's pretty cool work in progress, it uh raises a lot of questions and um I, it uh raises a lot of questions and um I, it uh raises a lot of questions and um I, think yeah I think it's it's pretty cool, think yeah I think it's it's pretty cool, think yeah I think it's it's pretty cool, I wonder how this happened like like how, I wonder how this happened like like how, I wonder how this happened like like how, how did how did people find this they, how did how did people find this they, how did how did people find this they, just forget to turn off their computer, just forget to turn off their computer, just forget to turn off their computer, and in the morning they came back and, and in the morning they came back and, and in the morning they came back and, they're like whoopsy doopsy generalized, they're like whoopsy doopsy generalized, they're like whoopsy doopsy generalized, though if you if you know if you build, though if you if you know if you build, though if you if you know if you build, these kinds of data sets I guess you, these kinds of data sets I guess you, these kinds of data sets I guess you, have something in mind already yeah in, have something in mind already yeah in, have something in mind already yeah in, any case that was it for me tell me what, any case that was it for me tell me what, any case that was it for me tell me what, what you think is going on in neural, what you think is going on in neural, what you think is going on in neural, networks or is there like is there like, networks or is there like is there like, networks or is there like is there like, a super easy aom's razor explanation, a super easy aom's razor explanation, a super easy aom's razor explanation, that I'm missing um I don't know tell me, that I'm missing um I don't know tell me, that I'm missing um I don't know tell me, what you think I'll see you next time, what you think I'll see you next time, what you think I'll see you next time, bye-bye
