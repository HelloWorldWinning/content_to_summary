Timestamp: 2025-11-19T10:51:39.657748
Title: Finally: Grokking Solved - It's Not What You Think
URL: https://youtube.com/watch?v=SRfJQews1AU&si=CUwvdoWbrNWCKa1v
Status: success
Duration: 27:02

Description:
好的，这是根据您提供的文本内容和要求生成的摘要分析。

### **核心思想摘要**

#### **1. Grokking现象回顾**
*   **定义**：Grokking是一种机器学习现象，指模型在训练数据上达到100%准确率（过度拟合）后，经过非常长时间的额外训练，其在从未见过的验证数据上的性能会突然从机会水平飙升至近乎完美。
*   **问题**：在此研究之前，人们不理解为什么Grokking会发生，尤其是为什么在性能提升前会有一段漫长的、看似无效的训练平台期。

#### **2. Grokking延迟的核心原因：一个多阶段的数值问题**
该研究揭示，Grokking的延迟并非神秘的学习过程，而是一个由数值不稳定导致的“学习陷阱”。
*   **第一阶段：懒惰训练与过度拟合**
    *   模型首先通过“懒惰”的方式，即死记硬背来完全拟合训练数据，达到100%的训练准确率。

*   **第二阶段：朴素损失最小化 (Naive Loss Minimization)**
    *   在过度拟合后，模型为了继续降低损失函数（如交叉熵损失），进入了一种“朴素”或“无效”的优化模式。
    *   它不再学习数据中有意义的、可泛化的特征，而是简单地通过**按比例放大其输出（Logits）的数值**来降低损失。这种操作在数学上能让损失值变小，但并未带来任何真正的学习。

*   **第三阶段：Softmax崩溃 (Softmax Collapse)**
    *   持续放大Logits会导致其数值变得极端巨大，超出了计算机浮点数（如float32）的表示精度。
    *   这种精度限制导致了**数值不稳定**，使得计算出的梯度几乎为零。
    *   当梯度消失时，模型的学习过程完全停止，陷入一个无法更新权重的状态，这被称为“Softmax崩溃”。这个阶段解释了Grokking前漫长的性能平台期。

#### **3. 触发Grokking的关键**
*   **打破僵局**：Grokking的突然出现，正是在模型通过某种方式**克服了这种数值不稳定性**之后发生的。一旦摆脱“Softmax崩溃”的陷阱，模型才能重新开始学习真正具有泛化能力的特征。
*   **潜在方法**：研究发现，通过降低输入数据的维度，可以迫使模型更早地进行泛化学习，从而提前触发Grokking。这为未来主动、快速地实现Grokking提供了思路。

---

### **核心结论（一句话总结）**
Grokking之所以会延迟出现，根本原因在于模型过度拟合后会陷入一种通过放大logits来进行“伪优化”的数值不稳定状态（即Softmax崩溃），导致学习完全停滞，直到该状态被打破后真正的泛化学习才会开始。

---

### **内容的总览框架**
该内容提出了一个解释Grokking延迟现象的因果链框架：从模型过度拟合的**“懒惰训练”**状态，演变为一种只放大logits而不学习新特征的**“朴素损失最小化”**阶段，最终因浮点数精度限制导致**“Softmax崩溃”**和学习停滞，而克服这一数值瓶颈则是提前触发Grokking的关键。

---

### **概念关系图 (Mermaid)**
<Mermaid_Diagram>
graph TD
    subgraph "第一阶段: 过度拟合"
        A["训练开始"] --> B["模型记忆训练数据"]
        B --> C["达到100%训练准确率 (过度拟合)"]
    end

    subgraph "第二阶段: 学习停滞 (核心瓶颈)"
        C -- "进入" --> D{"朴素损失最小化 (Naive Loss Minimization)"}
        D -- "表现为" --> E["持续放大Logits以降低损失"]
        E -- "导致" --> F["数值超出浮点数精度限制"]
        F -- "导致" --> G["梯度消失"]
        G -- "导致" --> H["Softmax 崩溃"]
        H -- "导致" --> I["学习完全停滞 (漫长平台期)"]
    end

    subgraph "第三阶段: Grokking 涌现"
        I -- "通过某种机制被打破" --> J["克服数值不稳定"]
        J -- "使得" --> K["模型开始学习有意义的泛化特征"]
        K -- "最终实现" --> M["验证集准确率飙升 (Grokking)"]
    end

    style A fill:#A2D5F2,stroke:#333
    style B fill:#A2D5F2,stroke:#333
    style C fill:#FAD02E,stroke:#333,stroke-width:2px
    style D fill:#FFB6B9,stroke:#B22222,stroke-width:2px
    style E fill:#FFD6D9,stroke:#333
    style F fill:#FFD6D9,stroke:#333
    style G fill:#FFD6D9,stroke:#333
    style H fill:#E57373,color:#FFFFFF,stroke:#B22222,stroke-width:3px
    style I fill:#BDBDBD,color:#FFFFFF,stroke:#333
    style J fill:#C8E6C9,stroke:#333,stroke-width:2px
    style K fill:#E1F5FE,stroke:#333
    style M fill:#9575CD,color:#FFFFFF,stroke:#4527A0,stroke-width:3px
</Mermaid_Diagram>

Content:
hello Community you're not going to, hello Community you're not going to, believe it we have an explanation why, believe it we have an explanation why, believe it we have an explanation why, grogging is happening look at this, grogging is happening look at this, grogging is happening look at this, finally and yeah I'm not talking here, finally and yeah I'm not talking here, finally and yeah I'm not talking here, about your particular company who called, about your particular company who called, about your particular company who called, the marketing name of their models gr or, the marketing name of their models gr or, the marketing name of their models gr or, anything like it I'm talking here about, anything like it I'm talking here about, anything like it I'm talking here about, the technology phenomenon that we have, the technology phenomenon that we have, the technology phenomenon that we have, in our transform architecture so purely, in our transform architecture so purely, in our transform architecture so purely, technical that happens with every, technical that happens with every, technical that happens with every, Transformer so 7 months ago my good I've, Transformer so 7 months ago my good I've, Transformer so 7 months ago my good I've, shown you here multiple videos where we, shown you here multiple videos where we, shown you here multiple videos where we, examine grogging grogging is that if you, examine grogging grogging is that if you, examine grogging grogging is that if you, have your accuracy and you have your, have your accuracy and you have your, have your accuracy and you have your, training data set your training data set, training data set your training data set, training data set your training data set, saturates you have overfitting of your, saturates you have overfitting of your, saturates you have overfitting of your, llm but then nothing happens and you, llm but then nothing happens and you, llm but then nothing happens and you, have a real bad performance on your, have a real bad performance on your, have a real bad performance on your, validation data set but you know what if, validation data set but you know what if, validation data set but you know what if, you just let it run and run and run long, you just let it run and run and run long, you just let it run and run and run long, after the overfitting of the training, after the overfitting of the training, after the overfitting of the training, set sets in suddenly there is a, set sets in suddenly there is a, set sets in suddenly there is a, performance increase and now validation, performance increase and now validation, performance increase and now validation, data set accuracy goes up to almost 100%, data set accuracy goes up to almost 100%, data set accuracy goes up to almost 100%, And this is great if you could achieve, And this is great if you could achieve, And this is great if you could achieve, this, this, this, immediately imagine we would have a, immediately imagine we would have a, immediately imagine we would have a, system that is for unseen training data, system that is for unseen training data, system that is for unseen training data, achieving close to 100% And in this, achieving close to 100% And in this, achieving close to 100% And in this, video part three I showed you here the, video part three I showed you here the, video part three I showed you here the, gr llm versus direct system how we can, gr llm versus direct system how we can, gr llm versus direct system how we can, optimize this and we even had a deep, optimize this and we even had a deep, optimize this and we even had a deep, dive into the layer architecture when, dive into the layer architecture when, dive into the layer architecture when, the retrieval heads in our AI agents, the retrieval heads in our AI agents, the retrieval heads in our AI agents, were discovered covered and we had here, were discovered covered and we had here, were discovered covered and we had here, causal competition Pathways after the, causal competition Pathways after the, causal competition Pathways after the, gring happens but we never could explain, gring happens but we never could explain, gring happens but we never could explain, why gring happens in the first place or, why gring happens in the first place or, why gring happens in the first place or, how we could ignite it earlier and now, how we could ignite it earlier and now, how we could ignite it earlier and now, today we have new Research into grag and, today we have new Research into grag and, today we have new Research into grag and, this is beautiful so let me give you now, this is beautiful so let me give you now, this is beautiful so let me give you now, finally the reason but gring happens, finally the reason but gring happens, finally the reason but gring happens, late and suddenly because our llm first, late and suddenly because our llm first, late and suddenly because our llm first, gets stap into a lazy training regime, gets stap into a lazy training regime, gets stap into a lazy training regime, where the memorization effect is the, where the memorization effect is the, where the memorization effect is the, dominant force of learning then the, dominant force of learning then the, dominant force of learning then the, system gradients align here with a, system gradients align here with a, system gradients align here with a, particular direction I'm going to, particular direction I'm going to, particular direction I'm going to, explain in a minute this has the fact, explain in a minute this has the fact, explain in a minute this has the fact, that we scale now the loges from every, that we scale now the loges from every, that we scale now the loges from every, layer to the extremes which in turn, layer to the extremes which in turn, layer to the extremes which in turn, leads here to the perfect softmax, leads here to the perfect softmax, leads here to the perfect softmax, collapse this means our layers and our, collapse this means our layers and our, collapse this means our layers and our, llms encounter a numerical instability, llms encounter a numerical instability, llms encounter a numerical instability, phase stopping each and every meaningful, phase stopping each and every meaningful, phase stopping each and every meaningful, learning process of the llm, learning process of the llm, learning process of the llm, now if you are not immediately familiar, now if you are not immediately familiar, now if you are not immediately familiar, and understand 100% of this hey this, and understand 100% of this hey this, and understand 100% of this hey this, video is for, video is for, video is for, you so let's start because me it took go, you so let's start because me it took go, you so let's start because me it took go, some time to understand really what is, some time to understand really what is, some time to understand really what is, the root cause of this I really want to, the root cause of this I really want to, the root cause of this I really want to, go here that everybody's able to, go here that everybody's able to, go here that everybody's able to, understand why groing is happening and, understand why groing is happening and, understand why groing is happening and, how you can achieve finally close to, how you can achieve finally close to, how you can achieve finally close to, 100% performance of your llm it all, 100% performance of your llm it all, 100% performance of your llm it all, starts here with the numbers that we use, starts here with the numbers that we use, starts here with the numbers that we use, in our calculation so we use floating, in our calculation so we use floating, in our calculation so we use floating, Point numbers and they have a certain, Point numbers and they have a certain, Point numbers and they have a certain, name of significant bits which Define, name of significant bits which Define, name of significant bits which Define, the Precision which we do here the, the Precision which we do here the, the Precision which we do here the, matrix multiplication or the tenser, matrix multiplication or the tenser, matrix multiplication or the tenser, computation and the concept of a, computation and the concept of a, computation and the concept of a, significant of antisa is here in a, significant of antisa is here in a, significant of antisa is here in a, scientific notation of course and you, scientific notation of course and you, scientific notation of course and you, can use any base not just the base 10 we, can use any base not just the base 10 we, can use any base not just the base 10 we, in binary have the base two or whatever, in binary have the base two or whatever, in binary have the base two or whatever, base you like to go with so in joural we, base you like to go with so in joural we, base you like to go with so in joural we, have our number and this is the way we, have our number and this is the way we, have our number and this is the way we, can build it our significant our base, can build it our significant our base, can build it our significant our base, and exponent here is everything, and exponent here is everything, and exponent here is everything, explained for you but let's have here a, explained for you but let's have here a, explained for you but let's have here a, real com real example here so counting, real com real example here so counting, real com real example here so counting, now the significant digit in a, now the significant digit in a, now the significant digit in a, significant key to remember is that a, significant key to remember is that a, significant key to remember is that a, significant in its normalized forms, significant in its normalized forms, significant in its normalized forms, includes here the digits before the, includes here the digits before the, includes here the digits before the, decimal points always a non-zero digit, decimal points always a non-zero digit, decimal points always a non-zero digit, in most representation simply assumed to, in most representation simply assumed to, in most representation simply assumed to, be one in the binary floating Point, be one in the binary floating Point, be one in the binary floating Point, format and it is not sored explicitly, format and it is not sored explicitly, format and it is not sored explicitly, and then we have the digit of the, and then we have the digit of the, and then we have the digit of the, decimal points which are part of the, decimal points which are part of the, decimal points which are part of the, representation really easy example 1 2 3, representation really easy example 1 2 3, representation really easy example 1 2 3, 56 we can have these in a scientific, 56 we can have these in a scientific, 56 we can have these in a scientific, notation like this and you see the, notation like this and you see the, notation like this and you see the, significant has six significant digits 1, significant has six significant digits 1, significant has six significant digits 1, 2 3 4 5, 2 3 4 5, 2 3 4 5, 6 look at the next, 6 look at the next, 6 look at the next, example this is here a beautiful number, example this is here a beautiful number, example this is here a beautiful number, and the significant has nine significant, and the significant has nine significant, and the significant has nine significant, digits however we can write this in a, digits however we can write this in a, digits however we can write this in a, scientific notation in this form and now, scientific notation in this form and now, scientific notation in this form and now, you already have a clue what's going to, you already have a clue what's going to, you already have a clue what's going to, happen yeah if the limited significant, happen yeah if the limited significant, happen yeah if the limited significant, has for example only six, has for example only six, has for example only six, digits we are limited simply by the, digits we are limited simply by the, digits we are limited simply by the, Precision in which we do our, Precision in which we do our, Precision in which we do our, mathematical, mathematical, mathematical, calculations because I just showed you, calculations because I just showed you, calculations because I just showed you, here we have with this number nine, here we have with this number nine, here we have with this number nine, significant digits but let's say we only, significant digits but let's say we only, significant digits but let's say we only, go with six digits so six significant, go with six digits so six significant, go with six digits so six significant, digits then this would be approximately, digits then this would be approximately, digits then this would be approximately, and stored in our calculation as this, and stored in our calculation as this, and stored in our calculation as this, number so we lose, number so we lose, number so we lose, information now if you have a look at, information now if you have a look at, information now if you have a look at, the base 10 representation of this let's, the base 10 representation of this let's, the base 10 representation of this let's, go to the extrem and let's have here, go to the extrem and let's have here, go to the extrem and let's have here, look this is here the base value if you, look this is here the base value if you, look this is here the base value if you, want and then we have here this, want and then we have here this, want and then we have here this, particular value for the correction for, particular value for the correction for, particular value for the correction for, the gradients if you calculate here our, the gradients if you calculate here our, the gradients if you calculate here our, gradient descent if we calculate back, gradient descent if we calculate back, gradient descent if we calculate back, propagation we have to do all of this, propagation we have to do all of this, propagation we have to do all of this, calculations if you not see it, calculations if you not see it, calculations if you not see it, immediately look this is the same number, immediately look this is the same number, immediately look this is the same number, like this one and this has the same, like this one and this has the same, like this one and this has the same, number like this one and now we just add, number like this one and now we just add, number like this one and now we just add, those two numbers and you say of course, those two numbers and you say of course, those two numbers and you say of course, this is it yeah but imagine we only, this is it yeah but imagine we only, this is it yeah but imagine we only, calculate with six significant digits, calculate with six significant digits, calculate with six significant digits, and I have shown you here in green the, and I have shown you here in green the, and I have shown you here in green the, six significant, six significant, six significant, digits so you see, digits so you see, digits so you see, absolutely given out the mathematical, absolutely given out the mathematical, absolutely given out the mathematical, Precision in which we calculate our, Precision in which we calculate our, Precision in which we calculate our, mathematical T of calculation we lose, mathematical T of calculation we lose, mathematical T of calculation we lose, information so due to the small number, information so due to the small number, information so due to the small number, of digits for the significant which is, of digits for the significant which is, of digits for the significant which is, six in this case the result now is not, six in this case the result now is not, six in this case the result now is not, this correct mathematical results but, this correct mathematical results but, this correct mathematical results but, now stored in our llm done by, now stored in our llm done by, now stored in our llm done by, mathematics the result is not this only, mathematics the result is not this only, mathematics the result is not this only, with our six significant, with our six significant, with our six significant, digits and yes you guessed it this is, digits and yes you guessed it this is, digits and yes you guessed it this is, the root cause why we have here after, the root cause why we have here after, the root cause why we have here after, you reach a 100% overfitting of your, you reach a 100% overfitting of your, you reach a 100% overfitting of your, training data why the system, training data why the system, training data why the system, collapses let's have a look at this in, collapses let's have a look at this in, collapses let's have a look at this in, detail it is so important and I will, detail it is so important and I will, detail it is so important and I will, again repeat this they have here a new, again repeat this they have here a new, again repeat this they have here a new, notation what we call a soft Max, notation what we call a soft Max, notation what we call a soft Max, collapse in the new network in the, collapse in the new network in the, collapse in the new network in the, Transformer layers so let's have task is, Transformer layers so let's have task is, Transformer layers so let's have task is, simple we have a problem you just, simple we have a problem you just, simple we have a problem you just, identifies three classes and we are, identifies three classes and we are, identifies three classes and we are, working here with float 32 which have 24, working here with float 32 which have 24, working here with float 32 which have 24, bits of the, bits of the, bits of the, significant so the logic here for our, significant so the logic here for our, significant so the logic here for our, simple three class identification have, simple three class identification have, simple three class identification have, now a particular samples and let's say, now a particular samples and let's say, now a particular samples and let's say, they are minus 10 minus 5 and 20 and you, they are minus 10 minus 5 and 20 and you, they are minus 10 minus 5 and 20 and you, can clearly see 20 is here the dominant, can clearly see 20 is here the dominant, can clearly see 20 is here the dominant, number now now this soft Max function, number now now this soft Max function, number now now this soft Max function, would normalize now these exponentiated, would normalize now these exponentiated, would normalize now these exponentiated, values into a probability distribution, values into a probability distribution, values into a probability distribution, so what we get here from our logic if we, so what we get here from our logic if we, so what we get here from our logic if we, do simply the softmax calculation we, do simply the softmax calculation we, do simply the softmax calculation we, would get this, would get this, would get this, and you say yeah that's absolutely fine, and you say yeah that's absolutely fine, and you say yeah that's absolutely fine, no this is insignificant this is, no this is insignificant this is, no this is insignificant this is, insignificant and this is, insignificant and this is, insignificant and this is, it yeah but wait we calculate the loss, it yeah but wait we calculate the loss, it yeah but wait we calculate the loss, function and the loss function gives us, function and the loss function gives us, function and the loss function gives us, here our gradient so the loss for this, here our gradient so the loss for this, here our gradient so the loss for this, simple example with the assumption that, simple example with the assumption that, simple example with the assumption that, the class three or 20 or dominant clause, the class three or 20 or dominant clause, the class three or 20 or dominant clause, or dominant Valu is here the true Clause, or dominant Valu is here the true Clause, or dominant Valu is here the true Clause, would be really small approaching zero, would be really small approaching zero, would be really small approaching zero, and if we calculate now the loss, and if we calculate now the loss, and if we calculate now the loss, function here, function here, function here, we see that our gradient goes to, we see that our gradient goes to, we see that our gradient goes to, zero and you might say yeah, and let me repeat this our gradient goes, and let me repeat this our gradient goes, to zero this mean the system is stuck, to zero this mean the system is stuck, to zero this mean the system is stuck, the system doesn't move anymore because, the system doesn't move anymore because, the system doesn't move anymore because, if there's not a gradient in a, if there's not a gradient in a, if there's not a gradient in a, gravitational potential the system does, gravitational potential the system does, gravitational potential the system does, not know how to further optimize the, not know how to further optimize the, not know how to further optimize the, equations because the gradient is zero, equations because the gradient is zero, equations because the gradient is zero, and zero is, and zero is, and zero is, stop and this is the reason why we have, stop and this is the reason why we have, stop and this is the reason why we have, this long period of 100% training data, this long period of 100% training data, this long period of 100% training data, overfitting but not, overfitting but not, overfitting but not, learning another time real example, learning another time real example, learning another time real example, floating Point calculations again we go, floating Point calculations again we go, floating Point calculations again we go, with a Precision of float 32 so as, with a Precision of float 32 so as, with a Precision of float 32 so as, 32 24 bit significant calculation let's, 32 24 bit significant calculation let's, 32 24 bit significant calculation let's, take this let's go with this this - 10, take this let's go with this this - 10, take this let's go with this this - 10, this- 5 and 20 now let's calculate X of, this- 5 and 20 now let's calculate X of, this- 5 and 20 now let's calculate X of, 10 x of minus 5 they're real small, 10 x of minus 5 they're real small, 10 x of minus 5 they're real small, numbers I've given you a little bit more, numbers I've given you a little bit more, numbers I've given you a little bit more, mathematical Precision those are the, mathematical Precision those are the, mathematical Precision those are the, numbers here and then we have our, numbers here and then we have our, numbers here and then we have our, dominant Factor X20 and this is 485, dominant Factor X20 and this is 485, dominant Factor X20 and this is 485, million, 165,00 n and, 165,00 n and, whatever so what we do, whatever so what we do, whatever so what we do, now remember when we calculate now soft, now remember when we calculate now soft, now remember when we calculate now soft, Max what we do in the denominator we, Max what we do in the denominator we, Max what we do in the denominator we, have a submission function over the, have a submission function over the, have a submission function over the, exponent here of our zets of our where, exponent here of our zets of our where, exponent here of our zets of our where, is it, is it, is it, Logics so this is exactly what we did, Logics so this is exactly what we did, Logics so this is exactly what we did, this are now the exponential values of, this are now the exponential values of, this are now the exponential values of, our zets so we sum over that and then, our zets so we sum over that and then, our zets so we sum over that and then, soft Max is exactly and this is here, soft Max is exactly and this is here, soft Max is exactly and this is here, exactly what we, exactly what we, exactly what we, divide and now you say soft Max is this, divide and now you say soft Max is this, divide and now you say soft Max is this, no no, problem again we are looking here to, problem again we are looking here to, dominant factor is here the entropy loss, dominant factor is here the entropy loss, dominant factor is here the entropy loss, the cross entropy loss, the cross entropy loss, the cross entropy loss, function this is what gives us here the, function this is what gives us here the, function this is what gives us here the, direction for the development of the, direction for the development of the, direction for the development of the, system so if the softmax output becomes, system so if the softmax output becomes, system so if the softmax output becomes, close to, close to, close to, 01 so the output is a valid probability, 01 so the output is a valid probability, 01 so the output is a valid probability, distribution this is why we do softmax, distribution this is why we do softmax, distribution this is why we do softmax, and the cross entropy laws for the, and the cross entropy laws for the, and the cross entropy laws for the, correctly s classified samples, correctly s classified samples, correctly s classified samples, approaches, approaches, approaches, zero so this means the radient now of, zero so this means the radient now of, zero so this means the radient now of, our cross entropy loss function with, our cross entropy loss function with, our cross entropy loss function with, respect to the weights is zero it stand, respect to the weights is zero it stand, respect to the weights is zero it stand, still is, still is, still is, inactivity is, inactivity is, inactivity is, that and this is what we do not want to, that and this is what we do not want to, that and this is what we do not want to, happen if we are not even in a local, happen if we are not even in a local, happen if we are not even in a local, minimum if you are not really just soft, minimum if you are not really just soft, minimum if you are not really just soft, Max function transform here a vector of, Max function transform here a vector of, Max function transform here a vector of, logic this mean our unnormalized score Z, logic this mean our unnormalized score Z, logic this mean our unnormalized score Z, into a probability distrib this is why, into a probability distrib this is why, into a probability distrib this is why, we do soft Max simple formula and, we do soft Max simple formula and, we do soft Max simple formula and, crucially this new research now, crucially this new research now, crucially this new research now, demonstrate that during the training, demonstrate that during the training, demonstrate that during the training, phase after reaching our 100% training, phase after reaching our 100% training, phase after reaching our 100% training, accuracy not Evolution the training, accuracy not Evolution the training, accuracy not Evolution the training, accuracy of the data set of the training, accuracy of the data set of the training, accuracy of the data set of the training, the model tends to increase now the, the model tends to increase now the, the model tends to increase now the, magnitude of the logits particular for, magnitude of the logits particular for, magnitude of the logits particular for, the cor Closs and you say what and I, the cor Closs and you say what and I, the cor Closs and you say what and I, said what does this, said what does this, said what does this, mean you tell me that the loss function, mean you tell me that the loss function, mean you tell me that the loss function, continues to optimize here, continues to optimize here, continues to optimize here, itself for a system, itself for a system, itself for a system, configuration that is not inducing any, configuration that is not inducing any, configuration that is not inducing any, new learning, new learning, new learning, exactly you know what it does it only, exactly you know what it does it only, exactly you know what it does it only, increases the magnitude of our logits, increases the magnitude of our logits, increases the magnitude of our logits, Z I want this I want to bring this point, Z I want this I want to bring this point, Z I want this I want to bring this point, over because this is so, over because this is so, over because this is so, important so when the Logics become very, important so when the Logics become very, important so when the Logics become very, large they can cause the gradients from, large they can cause the gradients from, large they can cause the gradients from, a large fraction of the sample to become, a large fraction of the sample to become, a large fraction of the sample to become, zero effectively stopping the learning, zero effectively stopping the learning, zero effectively stopping the learning, process and preventing here the llm to, process and preventing here the llm to, process and preventing here the llm to, be able to generalization of knowledge, be able to generalization of knowledge, be able to generalization of knowledge, to unseen data so this is the core, to unseen data so this is the core, to unseen data so this is the core, mechanism that the new research paper, mechanism that the new research paper, mechanism that the new research paper, identifies as a soft mix, identifies as a soft mix, identifies as a soft mix, collapse and this is a crucial element, collapse and this is a crucial element, collapse and this is a crucial element, in understanding gring because this, in understanding gring because this, in understanding gring because this, happens before croing takes place and, happens before croing takes place and, happens before croing takes place and, this happens by our llms do not reach, this happens by our llms do not reach, this happens by our llms do not reach, 100% simple here potential X and y- axis, 100% simple here potential X and y- axis, 100% simple here potential X and y- axis, and let's say this is here the absolute, and let's say this is here the absolute, and let's say this is here the absolute, minimum and this would be here our, minimum and this would be here our, minimum and this would be here our, potential form and we are here we here, potential form and we are here we here, potential form and we are here we here, with our optimization of the loss, with our optimization of the loss, with our optimization of the loss, function and here suddenly our gradient, function and here suddenly our gradient, function and here suddenly our gradient, becomes, becomes, becomes, zero so there's no direction to go all, zero so there's no direction to go all, zero so there's no direction to go all, the complete entropy loss optimization, the complete entropy loss optimization, the complete entropy loss optimization, that we have in our llm to learn new, that we have in our llm to learn new, that we have in our llm to learn new, knowledge converges to zero so we stay, knowledge converges to zero so we stay, knowledge converges to zero so we stay, here we don't learn anything new we are, trapped so this softare collapse is now, trapped so this softare collapse is now, a key obstacle to the generalization of, a key obstacle to the generalization of, a key obstacle to the generalization of, knowledge and that overcoming it is now, knowledge and that overcoming it is now, knowledge and that overcoming it is now, the major step that allowing gring to, the major step that allowing gring to, the major step that allowing gring to, happen so interesting before gring we, happen so interesting before gring we, happen so interesting before gring we, have a numerical mathematical, have a numerical mathematical, have a numerical mathematical, instability in our Transformer, instability in our Transformer, instability in our Transformer, calculations and investigating know the, calculations and investigating know the, calculations and investigating know the, root cause of of the softare collops the, root cause of of the softare collops the, root cause of of the softare collops the, Autos found beyond the point of, Autos found beyond the point of, Autos found beyond the point of, overfeeding the gradients strongly align, overfeeding the gradients strongly align, overfeeding the gradients strongly align, now with what they call here a, now with what they call here a, now with what they call here a, particular minimization direction on the, particular minimization direction on the, particular minimization direction on the, potential well and they call this here, potential well and they call this here, potential well and they call this here, the naive loss minimization Direction, the naive loss minimization Direction, the naive loss minimization Direction, the, nlmd why is this problematic again the, nlmd why is this problematic again the, model reduces the loss not through the, model reduces the loss not through the, model reduces the loss not through the, learning and more meaningful, learning and more meaningful, learning and more meaningful, representation but by scal scaling the, representation but by scal scaling the, representation but by scal scaling the, logits and I will again explain what, logits and I will again explain what, logits and I will again explain what, scaling the logit really means but the, scaling the logit really means but the, scaling the logit really means but the, effect is that it delays the, effect is that it delays the, effect is that it delays the, generalization of the knowledge and, generalization of the knowledge and, generalization of the knowledge and, leads here to complete systemwide, leads here to complete systemwide, leads here to complete systemwide, numeric, numeric, numeric, instability this means that scaling of, instability this means that scaling of, instability this means that scaling of, the logic explains the delay in the, the logic explains the delay in the, the logic explains the delay in the, generalization characteristics of saking, generalization characteristics of saking, generalization characteristics of saking, and why the learning stops of our, and why the learning stops of our, and why the learning stops of our, llm so let's have a look at this, llm so let's have a look at this, llm so let's have a look at this, scaling of the logic referred to a, scaling of the logic referred to a, scaling of the logic referred to a, specific type of transformation applied, specific type of transformation applied, specific type of transformation applied, to the logic Vector Z where the, to the logic Vector Z where the, to the logic Vector Z where the, individual components are multiplied by, individual components are multiplied by, individual components are multiplied by, a, a, a, constant and the new research identifies, constant and the new research identifies, constant and the new research identifies, that the upd of the network weights have, that the upd of the network weights have, that the upd of the network weights have, a particular mathematical effect that is, a particular mathematical effect that is, a particular mathematical effect that is, equally of changing the logic via logits, equally of changing the logic via logits, equally of changing the logic via logits, VIA a constant value, VIA a constant value, VIA a constant value, C this is it this is what you have to, C this is it this is what you have to, C this is it this is what you have to, wrap your mind around and say, wrap your mind around and say, wrap your mind around and say, why so the answer is that the scaling, why so the answer is that the scaling, why so the answer is that the scaling, factor C is applied to each element of, factor C is applied to each element of, factor C is applied to each element of, the logic Vector Z, the logic Vector Z, the logic Vector Z, individually and researcher argue that, individually and researcher argue that, individually and researcher argue that, after the model achieves the 100%, after the model achieves the 100%, after the model achieves the 100%, training set accuracy the gradiate, training set accuracy the gradiate, training set accuracy the gradiate, updates begins to align with what they, updates begins to align with what they, updates begins to align with what they, call the naive loss minimization, call the naive loss minimization, call the naive loss minimization, Direction and if you want a simple, Direction and if you want a simple, Direction and if you want a simple, mathematical formulation is, mathematical formulation is, mathematical formulation is, here and we have here the scaling with a, here and we have here the scaling with a, here and we have here the scaling with a, factor with a constant C so our logits Z, factor with a constant C so our logits Z, factor with a constant C so our logits Z, are now simply scaled with this, are now simply scaled with this, are now simply scaled with this, Factor now this particular nlm direction, Factor now this particular nlm direction, Factor now this particular nlm direction, in a mathematical space corresponds now, in a mathematical space corresponds now, in a mathematical space corresponds now, to parameter updates in a specific way, to parameter updates in a specific way, to parameter updates in a specific way, that have the effect of scaling the, that have the effect of scaling the, that have the effect of scaling the, logic meaning the updates are such that, logic meaning the updates are such that, logic meaning the updates are such that, if we take it a complete function it, if we take it a complete function it, if we take it a complete function it, results simply here in a multiplicative, results simply here in a multiplicative, results simply here in a multiplicative, Factor, again this formula here states that, again this formula here states that, applying an update to the model, applying an update to the model, applying an update to the model, parameters and the directions specified, parameters and the directions specified, parameters and the directions specified, here by the change is equivalent to, here by the change is equivalent to, here by the change is equivalent to, scaling the original output of the new, scaling the original output of the new, scaling the original output of the new, network by a positive constant so, network by a positive constant so, network by a positive constant so, nothing is, nothing is, nothing is, learned this D here is a specific, learned this D here is a specific, learned this D here is a specific, Direction in a parameter space that the, Direction in a parameter space that the, Direction in a parameter space that the, new research identify as the a floss, new research identify as the a floss, new research identify as the a floss, minimization Direction it's a parameter, minimization Direction it's a parameter, minimization Direction it's a parameter, updated the gradient start to favor, updated the gradient start to favor, updated the gradient start to favor, after the training data is correctly, after the training data is correctly, after the training data is correctly, classified because it decreases the the, classified because it decreases the the, classified because it decreases the the, loss at the expense of increasing here, loss at the expense of increasing here, loss at the expense of increasing here, the soft mix function so the update is, the soft mix function so the update is, the soft mix function so the update is, dependent on the current parameter State, dependent on the current parameter State, dependent on the current parameter State, F and the function f represents complete, F and the function f represents complete, F and the function f represents complete, neural network function it's a complex, neural network function it's a complex, neural network function it's a complex, mathematical function parameterized by, mathematical function parameterized by, mathematical function parameterized by, Theta that takes an input X and produces, Theta that takes an input X and produces, Theta that takes an input X and produces, the output logit Z before applying your, the output logit Z before applying your, the output logit Z before applying your, the soft mix, the soft mix, the soft mix, functions so when we scale the logit by, functions so when we scale the logit by, functions so when we scale the logit by, constant factor C greater than one the, constant factor C greater than one the, constant factor C greater than one the, probabilities will be increased for the, probabilities will be increased for the, probabilities will be increased for the, class so interestingly the soft maxor, class so interestingly the soft maxor, class so interestingly the soft maxor, class gets closer to one and the cross, class gets closer to one and the cross, class gets closer to one and the cross, entropy loss function gets lower, look so when they call this nlm a na, look so when they call this nlm a na, form of an optimization you immediately, form of an optimization you immediately, form of an optimization you immediately, understand why because the model is not, understand why because the model is not, understand why because the model is not, any more improving its ability to learn, any more improving its ability to learn, any more improving its ability to learn, to differentiate between the three, to differentiate between the three, to differentiate between the three, Clause problems in that the optimization, Clause problems in that the optimization, Clause problems in that the optimization, methodology is now in a naive function, methodology is now in a naive function, methodology is now in a naive function, if you want simply scaling all of its, if you want simply scaling all of its, if you want simply scaling all of its, output value to further decrease its, output value to further decrease its, output value to further decrease its, loss but this makes no sense this is, loss but this makes no sense this is, loss but this makes no sense this is, just a mathematical operation to reduce, just a mathematical operation to reduce, just a mathematical operation to reduce, the loss without any learning, the loss without any learning, the loss without any learning, function so if we think about here Z1 Z, function so if we think about here Z1 Z, function so if we think about here Z1 Z, 2 and Z3 that I showed you in an example, 2 and Z3 that I showed you in an example, 2 and Z3 that I showed you in an example, this is achieved by scaling all the, this is achieved by scaling all the, this is achieved by scaling all the, weights along the current direction XY Z, weights along the current direction XY Z, weights along the current direction XY Z, which has the effect of scaling the, which has the effect of scaling the, which has the effect of scaling the, logits therefore what's happening now, logits therefore what's happening now, logits therefore what's happening now, after it is 100% overfitting is just a, after it is 100% overfitting is just a, after it is 100% overfitting is just a, wrong form of optimization a naive form, wrong form of optimization a naive form, wrong form of optimization a naive form, of optimization that is just a, of optimization that is just a, of optimization that is just a, mathematical trick to make the number, mathematical trick to make the number, mathematical trick to make the number, bigger but you do not enable the, bigger but you do not enable the, bigger but you do not enable the, learning to happen with a loss, learning to happen with a loss, learning to happen with a loss, function this is the beautiful paper we, function this is the beautiful paper we, function this is the beautiful paper we, have here Department of computing, have here Department of computing, have here Department of computing, Imperial College of London grogging at, Imperial College of London grogging at, Imperial College of London grogging at, the edge of numerical stability before, the edge of numerical stability before, the edge of numerical stability before, we get a numerical instability in our, we get a numerical instability in our, we get a numerical instability in our, Transformer Network published January 8, Transformer Network published January 8, Transformer Network published January 8, 2025 beautiful have a GitHub code, 2025 beautiful have a GitHub code, 2025 beautiful have a GitHub code, everything is there for you and they, everything is there for you and they, everything is there for you and they, show that the software collops prevents, show that the software collops prevents, show that the software collops prevents, here the onset of the croing and that, here the onset of the croing and that, here the onset of the croing and that, mitigating here the software collops, mitigating here the software collops, mitigating here the software collops, leads to gring even without, leads to gring even without, leads to gring even without, regularization give me five minutes and, regularization give me five minutes and, regularization give me five minutes and, I explain this in detail so, I explain this in detail so, I explain this in detail so, investigating now the root cause of this, investigating now the root cause of this, investigating now the root cause of this, as see the authors say we find that, as see the authors say we find that, as see the authors say we find that, beyond the point of overfeeding the, beyond the point of overfeeding the, beyond the point of overfeeding the, gradient strongly aligns with what we, gradient strongly aligns with what we, gradient strongly aligns with what we, call Da floss minimization Direction and, call Da floss minimization Direction and, call Da floss minimization Direction and, therefore no learning is happening in, therefore no learning is happening in, therefore no learning is happening in, the system if you want to see this in a, the system if you want to see this in a, the system if you want to see this in a, formula it's really easy to read I mean, formula it's really easy to read I mean, formula it's really easy to read I mean, this is how I read the paper before I, this is how I read the paper before I, this is how I read the paper before I, made here this simplified explanation in, made here this simplified explanation in, made here this simplified explanation in, this video for you so here you have the, this video for you so here you have the, this video for you so here you have the, soft Max cross entropy laws here you, soft Max cross entropy laws here you, soft Max cross entropy laws here you, have the softmax collops described here, have the softmax collops described here, have the softmax collops described here, the SC loss becomes zero exactly as we, the SC loss becomes zero exactly as we, the SC loss becomes zero exactly as we, said this is the softare copse and the, said this is the softare copse and the, said this is the softare copse and the, naive loss minimalization if you do a, naive loss minimalization if you do a, naive loss minimalization if you do a, dysfunction you see exactly here that, dysfunction you see exactly here that, dysfunction you see exactly here that, the function here of the newal network, the function here of the newal network, the function here of the newal network, here is more or less equivalent to, here is more or less equivalent to, here is more or less equivalent to, multiplying this with a constant factor, multiplying this with a constant factor, multiplying this with a constant factor, C so this is now interesting so the, C so this is now interesting so the, C so this is now interesting so the, whole learning process that is based on, whole learning process that is based on, whole learning process that is based on, the entropy loss optimization of this, the entropy loss optimization of this, the entropy loss optimization of this, loss function is now, loss function is now, loss function is now, just a scaling functionality but not a, just a scaling functionality but not a, just a scaling functionality but not a, learning, learning, learning, functionality and there's more and they, functionality and there's more and they, functionality and there's more and they, say we find it under large class of, say we find it under large class of, say we find it under large class of, models to what transform architecture, models to what transform architecture, models to what transform architecture, does it apply namely those that, does it apply namely those that, does it apply namely those that, demonstrate here a positive, demonstrate here a positive, demonstrate here a positive, homogenity when training Beyond 100%, homogenity when training Beyond 100%, homogenity when training Beyond 100%, training accuracy the direction of the, training accuracy the direction of the, training accuracy the direction of the, weight is indeed an llm Direction and, weight is indeed an llm Direction and, weight is indeed an llm Direction and, they have here this definition of this, they have here this definition of this, they have here this definition of this, positive homogeneity function here where, positive homogeneity function here where, positive homogeneity function here where, exactly this formula applies L, exactly this formula applies L, exactly this formula applies L, corresponds to the number of layers and, corresponds to the number of layers and, corresponds to the number of layers and, they say many new network Architects, they say many new network Architects, they say many new network Architects, just such as theu MLPs and to transform, just such as theu MLPs and to transform, just such as theu MLPs and to transform, architecture without bios terms are, architecture without bios terms are, architecture without bios terms are, indeed positively homogeneous or, indeed positively homogeneous or, indeed positively homogeneous or, approximately homogeneous in the case of, approximately homogeneous in the case of, approximately homogeneous in the case of, the Transformer network if you want to, the Transformer network if you want to, the Transformer network if you want to, learn more about this I would recommend, learn more about this I would recommend, learn more about this I would recommend, you this paper it's an old paper but, you this paper it's an old paper but, you this paper it's an old paper but, it's a brilliant paper from chingua, it's a brilliant paper from chingua, it's a brilliant paper from chingua, University this is from end of December, University this is from end of December, University this is from end of December, 202 20 this is version four already and, 202 20 this is version four already and, 202 20 this is version four already and, it's about gradient descent maximization, it's about gradient descent maximization, it's about gradient descent maximization, the margin of homogeneous newal Network, the margin of homogeneous newal Network, the margin of homogeneous newal Network, and they study implicit regularization, and they study implicit regularization, and they study implicit regularization, of the gradient descent algorithm so if, of the gradient descent algorithm so if, of the gradient descent algorithm so if, you want to have a mathematical Deep, you want to have a mathematical Deep, you want to have a mathematical Deep, dive I love this paper, dive I love this paper, dive I love this paper, great so what happens after the llm, great so what happens after the llm, great so what happens after the llm, reaches now here the 100% training data, reaches now here the 100% training data, reaches now here the 100% training data, set accuracy the analine dynamic pushes, set accuracy the analine dynamic pushes, set accuracy the analine dynamic pushes, the model to scale only it's large it by, the model to scale only it's large it by, the model to scale only it's large it by, scaling the weights so this indeed, scaling the weights so this indeed, scaling the weights so this indeed, reduces the loss in the cross entropy, reduces the loss in the cross entropy, reduces the loss in the cross entropy, but at the expense of learning, but at the expense of learning, but at the expense of learning, meaningful representation increasing the, meaningful representation increasing the, meaningful representation increasing the, magnitude of the weights increasing the, magnitude of the weights increasing the, magnitude of the weights increasing the, numerical instability of the soft Max, numerical instability of the soft Max, numerical instability of the soft Max, and finally leading to the softmax, and finally leading to the softmax, and finally leading to the softmax, collapse so here we have the model is, collapse so here we have the model is, collapse so here we have the model is, now trapped by this particular training, now trapped by this particular training, now trapped by this particular training, Dynamics this optimization here of the L, Dynamics this optimization here of the L, Dynamics this optimization here of the L, function served us so well well for so, function served us so well well for so, function served us so well well for so, many years but there's also the dark, many years but there's also the dark, many years but there's also the dark, side to this it can trap our model and, side to this it can trap our model and, side to this it can trap our model and, prevent further learning and prevent, prevent further learning and prevent, prevent further learning and prevent, further, further, further, grocking interestingly if you read the, grocking interestingly if you read the, grocking interestingly if you read the, literature they found some interesting, literature they found some interesting, literature they found some interesting, points and the auor said we investigate, points and the auor said we investigate, points and the auor said we investigate, that by decreasing the dimensionality of, that by decreasing the dimensionality of, that by decreasing the dimensionality of, the, the, the, input the data becomes harder to, input the data becomes harder to, input the data becomes harder to, memorize for the llm thereby removing, memorize for the llm thereby removing, memorize for the llm thereby removing, the delay in the generalization onset, the delay in the generalization onset, the delay in the generalization onset, with, with, with, gring so this is now fascinating this is, gring so this is now fascinating this is, gring so this is now fascinating this is, the next hint what we have to do to, the next hint what we have to do to, the next hint what we have to do to, achieve grogging much earlier on they, achieve grogging much earlier on they, achieve grogging much earlier on they, say this confirms our hypothesis that, say this confirms our hypothesis that, say this confirms our hypothesis that, showing us the input representations are, showing us the input representations are, showing us the input representations are, decreased in the, decreased in the, decreased in the, dimension overfitting is prevented by, dimension overfitting is prevented by, dimension overfitting is prevented by, our llms and our llms generalize earlier, our llms and our llms generalize earlier, our llms and our llms generalize earlier, and the gring phase is earlier and this, and the gring phase is earlier and this, and the gring phase is earlier and this, is exactly what we wanted, is exactly what we wanted, is exactly what we wanted, however in our complex multi- jump, however in our complex multi- jump, however in our complex multi- jump, reasoning architecture of real complex, reasoning architecture of real complex, reasoning architecture of real complex, problems we don't want here to decrease, problems we don't want here to decrease, problems we don't want here to decrease, the dimensionality of the input because, the dimensionality of the input because, the dimensionality of the input because, we need the complexity to really span, we need the complexity to really span, we need the complexity to really span, here the the input space if you want but, here the the input space if you want but, here the the input space if you want but, this gives us a hint what we have to do, this gives us a hint what we have to do, this gives us a hint what we have to do, and you're not going to believe, and you're not going to believe, and you're not going to believe, it having these insights now the order, it having these insights now the order, it having these insights now the order, come up with two new ideas so we need, come up with two new ideas so we need, come up with two new ideas so we need, and they develop a mechanism to escape, and they develop a mechanism to escape, and they develop a mechanism to escape, this trap to start learning again a real, this trap to start learning again a real, this trap to start learning again a real, meaningful representation that allows, meaningful representation that allows, meaningful representation that allows, for a really excellent generalization of, for a really excellent generalization of, for a really excellent generalization of, up to 100% accuracy on the de data, up to 100% accuracy on the de data, up to 100% accuracy on the de data, set isn't this beautiful I had to wait 7, set isn't this beautiful I had to wait 7, set isn't this beautiful I had to wait 7, month to finally understand what is, month to finally understand what is, month to finally understand what is, happening before for groing and why, happening before for groing and why, happening before for groing and why, groing is delayed so, groing is delayed so, groing is delayed so, much so now now we are in the position, much so now now we are in the position, much so now now we are in the position, to say okay I understand what's, to say okay I understand what's, to say okay I understand what's, Happening Here and Now I understand what, Happening Here and Now I understand what, Happening Here and Now I understand what, is happening here, is happening here, is happening here, now and now understanding this you know, now and now understanding this you know, now and now understanding this you know, what we're going to do we're going to, what we're going to do we're going to, what we're going to do we're going to, bring here the onset of gring real close, bring here the onset of gring real close, bring here the onset of gring real close, here to the beginning so that we do not, here to the beginning so that we do not, here to the beginning so that we do not, have to waste our time for, thousand and, have to waste our time for, thousand and, have to waste our time for, thousand and, thousand and 10,000 more optimization, thousand and 10,000 more optimization, thousand and 10,000 more optimization, steps or we don't have to waste the, steps or we don't have to waste the, steps or we don't have to waste the, energy we don't have to waste the time, energy we don't have to waste the time, energy we don't have to waste the time, we don't have to waste whatever, we don't have to waste whatever, we don't have to waste whatever, computation we can build now our groing, computation we can build now our groing, computation we can build now our groing, alms much earlier onet and I can tell, alms much earlier onet and I can tell, alms much earlier onet and I can tell, you I have two system that I gred here, you I have two system that I gred here, you I have two system that I gred here, in the old times and I hold on to these, in the old times and I hold on to these, in the old times and I hold on to these, two gred llms here like it's the I don't, two gred llms here like it's the I don't, two gred llms here like it's the I don't, know the crown jewels here of the, know the crown jewels here of the, know the crown jewels here of the, British Empire because those system have, British Empire because those system have, British Empire because those system have, performance that is simply an accuracy, performance that is simply an accuracy, performance that is simply an accuracy, that is simply, that is simply, that is simply, amazing so this opens up now complete, amazing so this opens up now complete, amazing so this opens up now complete, new avenues to improve it but of course, new avenues to improve it but of course, new avenues to improve it but of course, this will be then the content of the, this will be then the content of the, this will be then the content of the, video of tomorrow I hope you enjoyed, video of tomorrow I hope you enjoyed, video of tomorrow I hope you enjoyed, here the introduction to grogging the, here the introduction to grogging the, here the introduction to grogging the, first explanation why grogging happens, first explanation why grogging happens, first explanation why grogging happens, what is a softmax collapse and yeah I, what is a softmax collapse and yeah I, what is a softmax collapse and yeah I, can think now of four methods to, can think now of four methods to, can think now of four methods to, overcome these probabilities to, overcome these probabilities to, overcome these probabilities to, circumvent and he at numerical, circumvent and he at numerical, circumvent and he at numerical, instability but you know what there's, instability but you know what there's, instability but you know what there's, another interesting question that I have, another interesting question that I have, another interesting question that I have, to run some tests now they are of course, to run some tests now they are of course, to run some tests now they are of course, if people do not have the computer, if people do not have the computer, if people do not have the computer, infrastructure they go down to an 8bit, infrastructure they go down to an 8bit, infrastructure they go down to an 8bit, quantization of the, quantization of the, quantization of the, llms and now the main question, llms and now the main question, llms and now the main question, is is it possible to do the quantization, is is it possible to do the quantization, is is it possible to do the quantization, in a way in a form that we can ignite, in a way in a form that we can ignite, in a way in a form that we can ignite, grow rocking, grow rocking, grow rocking, earlier this is done and is shown here, earlier this is done and is shown here, earlier this is done and is shown here, by the researcher here for a floating, by the researcher here for a floating, by the researcher here for a floating, 16 but if we do simplification by, 16 but if we do simplification by, 16 but if we do simplification by, quantization of our models and now it, quantization of our models and now it, quantization of our models and now it, depends heavily how we quantize it what, depends heavily how we quantize it what, depends heavily how we quantize it what, value we quantize at what stage in the, value we quantize at what stage in the, value we quantize at what stage in the, training we set in our quantization, training we set in our quantization, training we set in our quantization, models so I think this is now absolutely, models so I think this is now absolutely, models so I think this is now absolutely, fascinating if you want to bring this, fascinating if you want to bring this, fascinating if you want to bring this, down to, down to, down to, smaller model small language model how, smaller model small language model how, smaller model small language model how, we can cope with this onset early onset, we can cope with this onset early onset, we can cope with this onset early onset, of grogging to reach a 100% performance, of grogging to reach a 100% performance, of grogging to reach a 100% performance, I mean this would be amazing but let's, I mean this would be amazing but let's, I mean this would be amazing but let's, do the first step now part two is coming, do the first step now part two is coming, do the first step now part two is coming, up and I show you multiple solution how, up and I show you multiple solution how, up and I show you multiple solution how, we can improve and have immediately, we can improve and have immediately, we can improve and have immediately, gring immediately an amazing performance, gring immediately an amazing performance, gring immediately an amazing performance, for our llm I hope you enjoyed it and if, for our llm I hope you enjoyed it and if, for our llm I hope you enjoyed it and if, you are a subscriber of my channel you, you are a subscriber of my channel you, you are a subscriber of my channel you, will get immediately notified when part, will get immediately notified when part, will get immediately notified when part, two becomes available
