Timestamp: 2025-11-26T06:39:08.935351
Title: Stanford CS25: V2 I Introduction to Transformers w/ Andrej Karpathy
URL: https://youtube.com/watch?v=XfpMkf4rD6E&si=ShI2gXJ_on86rhCe
Status: success
Duration: 1:11:40

Description:
好的，这是从讲座文本中提炼出的核心思想、框架和概念图。

### **核心思想摘要**

#### **一、 历史背景与演进**
1.  **Transformer 之前的时代**：在 Transformer 出现之前，主流方法如特征工程（繁琐且效果不佳）和循环神经网络（RNN/LSTM）受限于长序列信息丢失和“编码器瓶颈”问题，难以处理复杂的上下文依赖。
2.  **关键转折点**：2017 年的论文《Attention Is All You Need》是颠覆性的，它大胆地删除了 RNN 的循环结构，证明仅依靠“注意力机制”就能构建出性能更优越的模型。
3.  **注意力机制的起源**：其灵感源于机器翻译中“软对齐”的思想，允许模型在生成输出时，能够“回顾”并重点关注输入序列中的相关部分，如同人类翻译时在源句和目标句之间建立联系。

#### **二、 Transformer 核心架构解析**
1.  **核心隐喻：通信与计算**
    *   **通信阶段 (Communication)**：通过**自注意力 (Self-Attention)** 机制实现。序列中的每个元素（Token）都生成一个查询（Query）、一个键（Key）和一个值（Value）。通过计算查询与所有键的相似度，模型可以决定应该从其他元素那里“接收”多少信息，并对相应的值进行加权求和，从而更新自身表示。这本质上是一个在节点间动态传递信息的过程。
    *   **计算阶段 (Computation)**：通过**前馈神经网络 (Feed-Forward Network / MLP)** 实现。在信息交换后，每个元素独立地通过一个简单的多层感知机进行非线性变换，以深化和提炼信息。
2.  **关键组件**
    *   **多头注意力 (Multi-Head Attention)**：并行运行多个独立的注意力“头”，让模型能同时从不同的表示子空间中学习信息，增强了模型的表达能力。
    *   **位置编码 (Positional Encoding)**：由于自注意力机制本身是无序的，位置编码为模型注入了关于 Token 在序列中位置的信息，这对于理解语言至关重要。
    *   **残差连接 & 层归一化**：这两个组件是训练深度 Transformer 模型的关键，它们能有效防止梯度消失或爆炸，稳定训练过程。

#### **三、 主要模型变体与跨领域应用**
1.  **三大架构**
    *   **仅编码器 (Encoder-Only)**：如 BERT，所有 Token 之间可以自由通信，非常适合需要深度理解上下文的任务，如文本分类和情感分析。
    *   **仅解码器 (Decoder-Only)**：如 GPT，采用因果（掩码）注意力，每个 Token 只能关注它之前的 Token，因此非常适合生成任务，如文本续写和对话。
    *   **编码器-解码器 (Encoder-Decoder)**：原始 Transformer 架构，用于处理输入和输出序列不同的任务，如机器翻译。
2.  **通用应用范式**：Transformer 的核心优势在于其灵活性。它可以将任何数据——无论是**计算机视觉**中的图像块 (Patches)、**语音处理**中的频谱图切片，还是**强化学习**中的状态-动作序列——都统一视为一组 Token，然后应用相同的注意力机制进行处理。

#### **四、 Transformer 为何如此高效？**
1.  **硬件亲和性与并行化**：与 RNN 必须顺序处理的特性不同，Transformer 可以并行处理序列中的所有 Token，这完美契合了现代 GPU 的大规模并行计算能力，极大地提升了训练效率。
2.  **通用性与灵活性**：它将所有输入都视为一个“集合”，并通过自注意力机制动态地学习元素间的关系，摆脱了卷积网络等对数据（如图像）欧几里得结构的依赖，使其成为一个极其通用的框架。
3.  **强大的上下文学习能力 (In-Context Learning)**：Transformer 可以在其前向传播过程中，表现出类似“元学习”的能力。通过在提示 (Prompt) 中提供几个示例，模型能“即时”学会新任务，而无需更新权重，仿佛在内部模拟了某种形式的快速优化过程。

---

### **核心观点 (一句话总结)**

Transformer 的核心是一种通用、可并行的计算架构，它将注意力机制作为一种灵活的通信协议，使其能够高效地统一处理各种模态的数据，并通过强大的上下文学习能力展现出近乎“即时编程”的潜力。

---

### **整体框架 (Overarching Framework)**

本次讲座的整体框架将 Transformer 定位为人工智能领域的一次**范式转移**。它从历史演进的角度出发，揭示了从 RNN/LSTM 的“瓶颈”到 Transformer “注意力即一切”的突破。其核心框架将 Transformer 解构为一种通用的、基于**集合 (sets)** 的计算模型，其中**“自注意力”是灵活的通信机制**，**“前馈网络”是独立的计算单元**。这种设计不仅使其能够跨越模态（文本、视觉、语音）统一处理信息，更因其高度并行化的特性，完美契合了现代硬件（GPU），并展现出惊人的上下文学习（In-Context Learning）能力，使其近乎成为一种可通过自然语言“编程”的通用计算机。

---

### **概念关系图 (Mermaid Conceptual Map)**
<Mermaid_Diagram>
graph TD
    subgraph "历史演进 & 理论基础"
        A["前Transformer时代: RNN/LSTM"] -- "存在局限性 (长程依赖, 瓶颈)" --> B["革命性论文: 'Attention Is All You Need' (2017)"];
        B -- "提出核心思想" --> C["'注意力'是唯一所需机制"];
    end

    subgraph "Transformer 核心架构 (The Architecture)"
        D["输入: Tokens + 位置编码"];
        subgraph "Transformer Block (重复N次)"
            E["通信阶段: 多头自注意力 (Multi-Head Self-Attention)"];
            F["计算阶段: 前馈神经网络 (Feed-Forward Network)"];
            G["辅助结构: 残差连接 & 层归一化"];
            E --> F;
            F -- "通过残差/归一化" --> H((Block输出));
        end
        D --> E;
        H --> I["最终输出 (例如: Logits)"];
    end

    subgraph "关键特性 & 影响力"
        J["高度并行化 (GPU友好)"];
        K["通用灵活 (处理集合而非序列)"];
        L["强大的上下文学习 (In-Context Learning)"];
        M["成为AI领域事实上的标准架构"];
    end

    C --> E;
    I -- "展现出" --> J;
    I -- "展现出" --> K;
    I -- "展现出" --> L;
    L --> M;

    subgraph "主要模型变体与应用"
        N["编码器-解码器 (翻译)"];
        O["仅编码器 (BERT - 理解)"];
        P["仅解码器 (GPT - 生成)"];
        Q["跨领域应用: 视觉(ViT), 语音(Whisper), 强化学习"];
    end

    I -- "衍生出" --> N;
    I -- "衍生出" --> O;
    I -- "衍生出" --> P;
    K -- "促成" --> Q;

    style A fill:#FFDDC1,stroke:#333,stroke-width:2px;
    style B fill:#C1FFD7,stroke:#333,stroke-width:2px;
    style C fill:#C1E7FF,stroke:#333,stroke-width:2px;
    style D fill:#F9F7D8,stroke:#333;
    style E fill:#ADD8E6,stroke:#333,stroke-width:2px;
    style F fill:#ADD8E6,stroke:#333,stroke-width:2px;
    style G fill:#E6E6FA,stroke:#333;
    style I fill:#F9F7D8,stroke:#333;
    style J fill:#FFFFCC,stroke:#333;
    style K fill:#FFFFCC,stroke:#333;
    style L fill:#FFFFCC,stroke:#333;
    style M fill:#90EE90,stroke:#333,stroke-width:2px;
    style N fill:#FFB6C1,stroke:#333;
    style O fill:#FFB6C1,stroke:#333;
    style P fill:#FFB6C1,stroke:#333;
    style Q fill:#D3D3D3,stroke:#333;
</Mermaid_Diagram>

Content:
Transformers United V2., was held at Stanford, about robots that, this picture might suggest., deep learning models, the world by storm, the field of AI and others., language processing, been applied all over, learning, biology, robotics, of videos lined up for you, speakers, talks, presenting, transformers, different fields and areas., learn from these videos., ado, let's get started., introductory lecture., blocks of transformers., introducing the instructors., temporary deferral from the PhD, robotics startup, Collaborative, some general purpose robots, robotics and building FSG, in reinforcement learning, remodeling, and I, publications in robotics, and other areas., so nice to [INAUDIBLE].., a first-year CS PhD here., CMU and undergrad at Waterloo., anything involving language, recently, I've, vision as well as [INAUDIBLE], for fun, a lot of music, a lot on my Insta, YouTube, guys want to check it out., starting a Stanford piano club, feel free to email, bodybuilding, and huge fan, and occasional gamer., about myself, I just, say that I'm super, sorry-- to teach this., time I was offered., really great group of speakers, for this offering., that you're all here, really fun quarter together., the most outspoken student, become an instructor next year, have a few minutes., in this class is, first of all, work, how they, just beyond NLP, are pretty [INAUDIBLE], AI machine learning., interesting directions, just an introductory., the basics of transformers, the self-attention mechanism, more on models like BERT, to get started., presenting the attention, with this one paper., Vaswani et al in 2017., of transformers., the prehistoric error, like RNM, LSDMs, mechanisms that didn't work, explosion of transformers, using it for everything., quote from Google., increased every time, after 2018 to 2020, of transformers, a bunch of other stuff, 2021 was the start, got a lot of genetic modeling, Codex, GPT, DALL-E, or a lot of things, the startup in '23., like ChatGPT, Whisperer, without splitting up, so once there were RNNs., models, LSTMs, GRU., were good at encoding history, didn't encode long sequences, at encoding context., the last word in the text, dot, dot, dot., the context for it, attention mechanism, if they're just using LSDMs, are good at is, also context prediction, like a word like it, property attention, possible activations., than existing mechanisms., we were on the verge of takeoff., the potential of transformers, long sequence problems, AlphaFold, offline RL., zero-shot generalization., tasks and applications, images from language., it feels like [INAUDIBLE].., talk on transformers, going from 2021 to 2022, the version of [INAUDIBLE], unique applications, art, music, storytelling., these new capabilities, logical reasoning, get human enlightenment, reinforcement learning, to perform really good., mechanisms for controlling, lot of also, a lot, areas like diffusion models., spaceship, and we are all, of more applications, and it'll be great, transformers also up there., understanding and generation., everyone is interested in, see a lot of models, also, finance, business., see GPT author a novel, long sequence modeling., models are still, or something like that., generalize much more, generalized agents, a multi-input predictions, see more of that, too., domain specific models., a GPT model, let's, a DoctorGPT model., LawyerGPT model that's, that are trained on everything., more niche models that, mixture of experts, can think this is a--, consult an expert, model for your different needs., of missing ingredients, is external memory., see this with the models, inflections are short-lived., memory, and they, to remember or store, you want to fix., computation complexity., quadratic over the sequence, it and make it faster., want to do is we, controllability of these models, models can be stochastic., control what sort of outputs, experienced the ChatGPT, different output each time., a mechanism that controls, our state of art language, human brain works., surge, but we still, how they can make more informed., the invites to come to class., I'll just walk over., 10 hours on the slides, talk about transformers., first two over there., talk about those., just to simplify the lecture, a little bit of context, class even exist., historical context., you guys about this., saw Lord of the Rings., roughly 2012, the full course, wouldn't even say, about, but back then, it, use if you were serious., OK to use, I think., you even realize, potentially entering, when I was working specifically, pipeline's looked like this., classify some images, think this is representative., in the paper describing, of kitchen sink, features, descriptors., to a poster session, vision conference, favorite feature descriptor, ridiculous, and you, one you should incorporate, you would extract all of them, put an SVM on top., [? Spar ?] SIFT histograms, histograms, textiles, geometry specific histograms., complicated code by themselves., everywhere and running it, it also didn't work., it represents the prediction, like this once in a while, just shrug your shoulders, once in a while., looking for a bug., every single chunk of AI, separate vocabulary, papers, those papers, paper, and you're like, of speech tagging, and tactic parsing, was completely different., read papers, I would, changed a little bit, and colleagues basically, scale a large neural network, get very strong performance., a lot of focus on algorithms., neural nets scale very well., about compute and data, actually did copy paste, networks pop up everywhere, vision, and NLP, and speech, the same kind of modeling, you start reading papers there, for example, to sequence paper, back to in a bit., papers, and you're like, OK, it starts to read things, the barrier to entry, the big deal is, came out in 2017, kits and the neural networks, literally the architectures, architecture that you, everything seemingly., unassuming machine translation, to transformer architecture., is that you can just basically, and use it everywhere., the details of the data, data, and how you feed it in., caricature, but it's, first order statement., even more similar looking, just using transformer., was remarkable to watch, the last decade., interesting is I think, that we're maybe converging, the brain is doing, homogeneous and uniform, sheet of your cortex., the details are changing, hyperparameters, and your visual cortex, looks very similar., converging to some kind, learning algorithm here., is interesting and exciting., where the transformer came, application of neural networks, language modeling, case, the next word, allows you to build, using multi-layer perceptron, and predicted the probability, fourth word in a sequence., good at this point., started to apply this, sequence to sequence paper, pretty influential, here was OK, we, words and predict the fourth., go from an English sentence, was OK, you can, in English and arbitrary number, so how do you, that can process, LSDM, and there's basically, covered by the slack, by this., encoder LSDM on the left, one word at a time, of what it has read., a conditioning vector, goes chonk, chonk, word in a sequence, French or something like that., this, that people identified, and tried to resolve, this encoder bottleneck., that we are trying to condition, a single vector, encoder to the decoder., too much information, in a single vector, looking around for ways, the encoder bottleneck as it, us to this paper, by Jointly Learning, the abstract, "in this paper, of a fixed length vector, improving the performance, encoder-decoder architecture, this by allowing, automatically soft search, that are relevant to predicting, having to form, segments exclusively.", back to the words that, using this soft search., decoding in the words, are decoding them, look back at the words, attention mechanism proposed, is the first time that I saw, that comes from the encoder, of the hidden states, of this sum come, on these compatibilities, state as you're decoding, generated by the encoder., time that really you, is the current modern equations, first paper that I saw it in., that there's a word, know, to call this mechanism., into the details of the history, here, Dzmitry, I, correspondence with him, sent him an email., is really interesting., with the soft attention, the heart of the transformer?, back this massive email, which, from that email., how he was looking for a way, between the encoder and decoder., about cursors that, that didn't quite work out., day, I had this thought, to enable the decoder, to put the cursor in the source, by translation exercises, my middle school involved., between source and target, this was kind of interesting, English speaker, in this machine translation, then led to transformer., search a softmax, of the [INAUDIBLE] states., my great excitement, the very first try.", interesting piece of history., that the name of RNN search, better name attention came, of the final passes, is All You Need, Search is All You Need, Bengio to thank, better name, I would say., that's the history, thought was interesting., 2017, which is Attention, component, which, just one small segment, bidirectional RNN, RNN, All You Need paper is saying, delete everything., work very well, keep attention., this paper actually is usually, are very incremental., they show that it's better., Attention is All You, things at the same time., in a very unique way, very good local minimum, really a landmark paper, remarkable and, I think, work behind the scenes., just keep attention., operates over sets--, to this in a second--, encode your inputs, the notion of space by itself., residual network structure, with multi-layer perceptrons., came from a different paper., of multiple heads of attention, like a fairly good set, to this day are used., multi-layer perceptron goes up, a bit more detail--, a number of papers, kinds of little details, sticks because this is actually, knowledge that didn't stick, of the layer norms, version where here you, the multiheaded attention feed, before instead., layer norms, but otherwise, that you're seeing today, architecture from 5 years ago., is working on it, remarkably resilient, real interesting., that, I think, in positional encoding., different rotary and relative, changes, but for the most part, interesting paper., the attention mechanism., it is not similar to the ways, presented before., way of how I see it., kind of like the communication, and the transformer, communication phase, which, attention, and the computation, multilayered perceptron, phase, it's, dependent message, as OK, forget everything, translation, everything., directed graphs., are storing a vector., about the communication, vectors talk to each other, phase later is just, basically acts on every node, talk to each other, some simple Python--, basically to create, of using attention, private data vector, as private information, key, a query, and a value., by linear transformation, the things that I am--, things that I'm looking for?, the things that I have?, things that I will communicate?, have your graph that's, random edges, when you actually, what's happening is, nodes individually, and you're at some node, query vector q, which, some graph, and this, via this linear transformation, inputs that point to this node, are the things that I have, interact by dot product, by doing dot product, unnormalized weighting, of the information in the nodes, the things I'm looking for., that with softmax, 1, you basically just, now sum to 1 in our probability, weighted sum of the values, to get interestingness or like, normalize it, and then, flow to me and update me., each node individually., message passing scheme, the transformer., vectorized batched way, also interspersed with layer, to make the training behave, happening in the attention, on a high level., phase of the transformer, then, scheme happens, then in every layer in series, weights each time., multi-headed attention goes., encooder-decoder models, terms of the connectivity, OK, all these tokens that, we want to condition on, connected to each other., they communicate fully, their features., because we are, language model, we, communication for future tokens, the answer at this step., decoder are fully connected, states, and then they, everything that is decoding., this triangular structure, message passing scheme, a little bit careful because, here with the decoder, from the top of the encoder., in the encoder, looking at each other, each other many, many times., out what's in there, looking only at the top nodes., message passing scheme., more of an implementation, any questions about this., and multi-headed attention, advantage of [INAUDIBLE]??, multi-headed attention, so, just this attention scheme, multiple times in parallel., independent applications, scheme basically just, multiple times, the query, key, and value., it like in parallel, I'm, different kinds of information, all in the same node., copy-paste in parallel., copy-paste but in series., it's self-attention, is that the node here, this is really self-attention, these nodes produces, from this individual node., you have one cross-attention, the queries are still, but the keys and the values, function of nodes that, I'm trying to decode some--, for certain things, the values in terms, that could answer my queries, nodes in the current decoding, top of the encoder., have already seen all, many times cannot broadcast, terms of information., the self-attention is--, and self-attention, and the values come from., are produced from this node, external source like an encoder, same mathematical operations., message passing [INAUDIBLE], of these nodes is a token., a very good picture of it, represent the third word, and in the beginning, embedding of the word., think through this analogy, [INAUDIBLE] nodes, basically the vectors., and then maybe I'll, to the graph., go to-- let me now go to, in mind, at least, concrete implementation, that is very minimal., over the last few days, GPT-2 on open web text., implementation that reproduces, provide it enough compute--, for 38 hours or something, remember correctly., can take a look at it., briefly step through it., decoder-only transformer., it's a language model., next word in the sequence, in the sequence., we train on this, fake Shakespeare., a Tiny Shakespeare, my favorite toy datasets., Shakespeare, concatenate it, file, and then, language models on it, Shakespeare, if you like, need to do is we, a sequence of integers, natively process--, into transformer., encoding is done is, in the simplest case, integer, and then instead of "hi, this sequence of integers., single character as an integer, sequence of integers., it all into one, one-dimensional sequence., have a single document., multiple independent documents, is create special tokens, those documents, end of text tokens, to create boundaries., don't have any modeling impact., transformer is supposed, that the end of document, should wipe the memory., of data just mean, one-dimensional sequence, of this sequence., Then the block size indicates, that your transformer will, is 8, that means, to eight characters of context, character in a sequence., how many sequences in parallel, as large as possible, advantage of the GPU, So in this example, independent example, small chunk of the sequence, inputs and the targets, contained in a single 4, itself, the target is 58., the sequence 47, 58, the target is 51 and so on., of examples that score by 8, individual examples, a transformer, the batches are learned, the time dimension here along, trained on in parallel., is more like B times T., context grows linearly, make along the T direction, that the model will learn from, a decoder-only model, an encoder because there's no, in some other external, a sequence of words that, I'm going slightly faster, have taken 231 or something, pass, we take these indices, identity of the indices, lookup table., index into a lookup table of, up embedding, and pull out, transformer by itself, process is set natively., encode these vectors, have both the information, its place in the sequence from 1, about what and where, so the token embeddings, are just added exactly as here., optional dropout, just contains, and their positions, blocks of transformer., into what's block here., this is just a series, there's a layer norm, decoding the logits, integer in a sequence, the output of this transformer, core language model head., encode all the words, sequence of blocks, layer to get the probability, the next character., the targets, which, the targets are just, by one in time--, into a cross entropy loss., negative log likelihood, what's here in the blocks., applied sequentially, mentioned, this communicate, phase, all the nodes, so these nodes are basically, is 8, then we are, nodes in this graph., in this graph., to only by itself., by the first node and itself., to by the first two nodes, residual pathway and x., and then the self-attention, these eight nodes communicate., mind that the batch is 4., this is also applied--, nodes communicating, them individually communicating, the batch dimension, of course., anywhere luckily., changed information, the multi-layer perceptron., missing the cross-attention, decoder-only model., this step here, attention, and that's, communicate phase., forward, which is the MLP, a bit later., fairly straightforward., processing on each node, representation at that node., two-layer neural net, which is just, or something like that., anything too crazy there., causal self-attention part, the meat of things, because of the batching, of how you mask the connectivity, you can't obtain, from the future when, away the information., if I'm the fifth position, token coming into the input, third, second, and first, out what is the next token., in the next element, the answer is at the input., information from there., is all tricky, the forward pass, keys, and values based on x., queries, and values., computing the attention, multiplying the piece., parallel for all the queries, in all the heads., that there's also, is also done all in parallel, dimension, the time dimension, and you end up, and it's all really confusing., it later and convince yourself, doing the right thing., batch dimension, the head, time dimension, features at them., all the batch elements, for all, all the time elements, you earlier, which is query, and what this is doing, attention between the nodes, communicate to be negative, negative infinity, and so negative infinity will, that those elements be zero., to basically end up, between these nodes, optional, matrix multiply v is basically, according to the affinities, weighted sum of the values, is doing that weighted sum., contiguous view, complicated and batched, tensors, but it's really not, optional drop out, back to the residual pathway., communication phase here., this transformer., infinite Shakespeare., we start with a sum token, in this case, you, new line as the start token., only to yourself, single node, and you, distribution for the first word, for the first character, back the character, it as an integer., the second thing., position, and this, add the positional encodings, goes in the transformer, now communicates, and it's identity., plugging it back., block size, which is eight, because you can never, eight in the way you've, context until eight., generate beyond eight, because the transformer only, in time dimension., in the [INAUDIBLE] setting, size or context length, will be 1,024 tokens or 2,048, usually like BPE tokens, or WorkPiece tokens., different encodings., think, [INAUDIBLE].., expand the context size, because the attention, [INAUDIBLE] case., an encoder instead of a decoder, do is this [INAUDIBLE], mask the attention, communicate to each other, allowed, and information, encoder here, just delete., will use attention, this encoder might store say, and they are all, other going up the transformer., implement cross-attention, encoder-decoder transformer, transformer or a GPT., cross-attention in the middle., self-attention piece where all, piece, a cross-attention piece, cross-attention, we need, the top of the encoder., more line here, cross-attention instead of a--, it instead of just pointing, cross-attention line here., lines because we, come from x but the keys, from the top of the encoder., basic code information, encoder, strictly, simple modifications, talk that you have, model like BERT, encoder-decoder model, like machine translation., it using this language modeling, aggressive, and you're just, [INAUDIBLE] in the sequence., slightly different objectives., the full sentence, allowed to communicate fully., classify sentiment or something, the next token in the sequence., slightly different, denoising techniques., these constraints on it, if I fully follow., to look at this analogy, you can interpret, we do the communicate, of eight in my example, connect from left to right., connect-- usually, as a function of the data, a single example where, changes dynamically, connectivity is fixed., and you're training a BERT, tokens you want, decoder-only model, thing, and if you, then you have, know much more about this, like if you ran [INAUDIBLE], [INAUDIBLE] but then you also, one or more of [INAUDIBLE]----, hard to say, so that's, interesting because like, yeah, see like the path, path internally., didn't look like a transformer., which have lots of this., like this, but there's, kind of in a ResNet., very much like this, use layer norms in ResNets, they can be batch norms., a ResNet, and they, block in addition, MLP block, which, speaking deconvolution, but I think, is just like a typical weights, is kind of interesting, work is not there, you this transformer., out 5 years later, everyone's trying to change it., that it's like a package, which I think is really, to paper authors, unaware of the impact, would have at the time., actually, it's unfortunate, that changed everything, it's like question marks, random machine translation, doing machine translation., going to happen., people read it today, confused potentially., tweets at the end, have renamed it, of like, well, I'll get to it., good question as well., I certainly don't, modeling approach., weird to sample a token, some ways, some hybrids, an example, which, really cool, or we'll, the sequences later but still, like an up and coming modeling, find much more appealing., go chunk, chunk, chunk, I do a better draft two., a diffusion process., the [INAUDIBLE], have the edge rates, it by the values, [INAUDIBLE] it., within graph neural networks, networks like a confusing term, yeah, previously, everything is a graph neural, is a graph neural network, the transformer operates over, by edges in a direct way., representation, and then, yeah., I still have 30 slides., DE, I think, it basically, with random weights, your dimension size grows, the variance grows., become the one half vector., control the variance, in a good range for softmax, an initialization thing., have been applied, and the way this was done, ridiculous ways, computer vision person, and they make sense., with VITs as an example is, it up into little squares., squares, literally, transformer, and that's, and so the transformer, case, really know where, positionally encoded, a lot of the structure, to approach it that way., simplest baseline, images into small squares, individual nodes actually, transformer encoder, talking to each other, entire transformer., here would be like nine., just take your melSpectrogram, slices and you feed them, this, but also Whisper., copy-paste transformer., you just chop up melSpectrogram, transformer and then pretend, you take your states, actions, experience in environment, it's a language., the sequences of that, that for planning later., so we were briefly, how you can plug them in., AlphaFold, computationally, say about transformers, they're very flexible, example from Tesla., that takes an image, about the image., question is, how do you, trivial like say, I, information that I, the outputs to be informed by., sensors like Radar., information, or a vehicle type, feed information into a ComNet?, it's much easier, whatever you want, you chop it, feed it in with a set, self-attention figure out, should communicate., apparently works., and throw it into the mix, nets from this burgeon, where previously you, to conform to the Euclidean, you're laying out the compute., actually kind of, space if you think about it., everything is just sets., flexible framework, into your conditioning set., self-attended over., from that perspective., transformers so effective?, example of this comes, I encourage people to read., Few-Shot Learners., renamed this a little bit., something like transformers, learning or meta-learning., them really special., that they're working with, context, and I'm, example of many., asking questions about it., context in the prompt, and the answers., of question-answer, question-answer, question-answer, and so on., to have to leave soon, huh?, interesting is basically, given in a context, is that the transformer, learn in the activations, gradient descent, fine-tuning fashion., give an example and the answer, using gradient descent., transformer internally, doing something, gradient, some kind, weights of the transformer, they go into, OK, loop with stochastic gradient, of the intercontext learning., the transformer is reading, outer loop is the training, there's some training, of the transformer, a sequence that, like gradient descent., papers that hint at this, in this paper, called the draw operator., raw operator is implemented, and then they show, things like ridge regression, hinting that maybe there, like gradient-based learning, of the transformer., impossible to think through, gradient-based learning?, pass, and then update., a ResNet, right, to the weights., random set of weights, and update your weights, pass, update the weights., so much more hand-wavey, papers are trying, be potentially possible., tweets I just copy-pasted here, general consumption, and hypey a little bit., architecture is so interesting, it became so popular., simultaneously optimizes, think, are very desirable., transformer is very, able to implement, potentially functions, optimizable thanks, connections, layer nodes, extremely efficient., but the transformer, computational graph, network, which, of the parallelism of GPUs., was designed very deliberately, work like neural GPU, well, which is really just, nets that are efficient on GPUs, constraints of the hardware, very interesting way, I probably would have called--, transformer a general purpose, computer instead of attention, in hindsight called that paper., is very general purpose, so, in terms of GPU usage, gradient descent and trains, other hype tweets here., read them later., is maybe interesting., are special purpose computers, specific task, GPT, reconfigurable at runtime, language programs., given as prompts, by completing the document., personally to computer., powerful computer, by gradient descent., later, but that's for now., scale up the training set, neural net like a transformer, kind of general purpose, way to look at it., a single text sequence, sequence in the prompt., is both powerful, enough, very hard data set, purpose text computer., interesting way to look at it., is [INAUDIBLE] how, more efficient or [INAUDIBLE], a bit of that., RNNs in principle, arbitrary programs., statement to some extent, probably expressive, and that they can implement, efficient because they, as a compute graph, thin compute graph., the neurons and you looked--, neurons interconnectivity, try to visualize them., long graph and that's bad., for optimizability, exactly know why, is when you're backpropagating, make too many steps., shallow wide graph, and so, a very small number of hops., residual pathways, flow very easily., these layer norms, all of those activations., not too many hops, supervision to input, flows through the graph., done in parallel, have to go from first word, then third word., every single word, parallel, which is kind of a--, really important because all, talked about but extremely, learning scale matters., network that you can train it, extremely important., on the current hardware, it with multiple modalities, data as different token, take your image, them up into patches., thousand tokens or whatever., but I don't actually, representation of radar., chop it up and enter it., encode it somehow., needs to know, special token of that to--, are what's slightly, representation, and it's, information would also, token that can be learned., those before really--, but you can positionally, encoding means you can, the coordinates, that, but it's better, the position., that is always, there, it just adds on it., trainable by background., the [INAUDIBLE].., seems like they're sometimes, understand your question., positional encoders, inductive bias or something, out in location always, the network in some way., intuition is good, enough data, usually, it is a bad thing., knowledge when you, knowledge in the data, usually productive., on what scale you want., data, then you actually, data, then actually, you do, much smaller data set, then, are a good idea, bias coming from your filters., is extremely general, mess with the encodings, encode [INAUDIBLE] and fix it, to the attention mechanism, is chopped up into patches, to this neighborhood., the attention matrix, you don't want to communicate., play with this, attention is inefficient., for example, layers, in little patches, communicate globally., kinds of tricks like that., in more inductive bias., the inductive biases, from the core transformer., and the interconnectivity, out in the positionally--, this for computation., papers on this now if not more., to keep track of., browser, which is-- oh, like 200 open tabs., even sure if I want, like that [INAUDIBLE], actually like even more, the context length fixed, somehow use a scratch pad., you will teach the transformer, in [INAUDIBLE] hey, remember too much., a start scratch pad, want to remember, and then, with whatever you want., when it's decoding, special objects, start scratch pad, whatever it puts, and allow it to attend over it., transformer just dynamically, to use other gizmos and gadgets, its memory that way, to use a notepad, right., keep it in your brain., brain is like the context line, give it a notebook., notebook, and read from it, plug in another transformer., like there was more than just, I did see a [INAUDIBLE] event., size was just moved., the internals of ChatGPT., you think about architecture, one's a personal question., to work on next?", working on things like nanoGPT., slightly from computer vision, vision-based products, do, which I rewrote to nanoGPT., GPTs, and I mean, like ChatGPT, I think, in a product fashion, of people feel it, something like a Google plus, think is more interesting., a round of applause?
