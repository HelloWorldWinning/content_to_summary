Timestamp: 2025-07-01T18:14:55.719570
Title: 失败即资产---AI时代的开发思路 BV1kLgSzQEFS
URL: https://b23.tv/KtW47jw
Status: success
Duration: 16:14

Description:
好的，以下是根据您提供的文本提炼的核心思想和总结：

**核心结论句 (Core Point):**
构建成功的 AI 代理需要一个持续迭代的反馈循环，该循环由用户数据和严格的评估驱动，最终的用户是最终的评判者。

**总体框架 (Overarching Framework):**
数据飞轮 (Data Flywheel) - 一个持续改进的过程，从数据收集、分析、评估，到迭代改进和部署，周而复始。

**核心思想总结 (Summary Outline):**

1.  **构建AI代理的挑战:**
    *   构建好的AI代理以及使非技术用户能够构建代理的平台非常困难。
    *   主要挑战在于AI的非确定性和用户行为的不可预测性。
    *   初始原型只是开始，真正的挑战在于发布后。

2.  **核心过程：数据飞轮:**
    *   一旦产品上线，责任就转向构建数据飞轮。
    *   飞轮流程：收集反馈 -> 理解用法/故障 -> 构建评估/功能 -> 产品改进 -> 更多用户 -> 更多数据。

3.  **数据收集 (Data Collection):**
    *   **代码埋点 (Instrumentation):** 确保记录详细信息 (完整的 traces, 工具调用, 错误, 前后处理步骤)，以便调试和重现。使运行可重复 (mock 副作用)。
    *   **显性用户反馈 (Explicit Feedback):** 高信号，但用户参与度低 (点赞/点踩)。应在正确上下文 (如代理运行完成后) 更积极地请求反馈。
    *   **隐性用户反馈 (Implicit Feedback):** 低门槛、低成本。例子包括：用户开启代理、复制响应、对话中的不满信号 ("别偷懒"、重复问题)、甚至咒骂。
    *   **使用LLM识别/分组痛点 (LLM for Frustration Detection):** 需要大量调优。
    *   **传统用户指标 (Traditional Metrics):** 分析流失用户等，从数据中提炼信号。

4.  **数据分析 (Data Analysis):**
    *   需要LLM Ops软件 (购买或自建) 来理解复杂的代理运行过程。
    *   自建工具有价值：提供特定领域上下文理解，易于将故障转化为评估用例 (一键转化)。
    *   **规模化理解:** 聚合、聚类故障模式和交互类型，发现问题工具和模式，指导产品路线图。
    *   **使用推理LLM解释故障 (LLM for Root Cause Analysis):** 给定 trace 和上下文，LLM能帮助找到或指向问题的根源。

5.  **构建评估 (Building Evals):**
    *   借鉴测试金字塔概念，评估分为不同类型：
        *   **单元评估 (Unit Evals):** 预测 n+1 状态，简单断言 (工具调用、参数、关键词、完成状态)。最易添加，适合针对性解决特定故障 (hill climbing)。**注意：**易对现有模型过拟合，惩罚不同但有效的路径，难以反映整体性能。
        *   **轨迹评估 (Trajectory Evals):** 评估完整的端到端运行过程 (最终状态、工具调用、产物)。设置较难 (尤其涉及副作用需模拟环境)，运行较慢，但回报高。
        *   **LLM判官 (LLM as a Judge):** 使用LLM进行评分/比较。需谨慎确保其判断准确性并避免偏见。
        *   **基于规则的评分 (Rubrics-Based Scoring):** LLM判官结合人类手工定义的、针对具体运行的评估标准。
    *   不同评估类型的用途：
        *   LLM判官/基于规则：系统能力高层概览，模型基准测试。
        *   轨迹评估：捕捉多轮交互标准。
        *   单元评估：调试特定故障。

6.  **部署与验证 (Deployment & Verification):**
    *   **不要痴迷于内部评估分数 (Don't Obsess Over Metrics):** 高分不一定代表好产品，可能意味着数据集无趣或过拟合。
    *   **数据集划分 (Dataset Pools):** 回归数据集 (确保不破坏现有用例)、目标数据集 (挑战高难度用例)。
    *   **最终目标是用户满意度 (User Satisfaction is the Ultimate Goal):** 用户是最终的评判者，不要只看分数而忽视用户体验 ("vibes")。
    *   **终极验证方法：A/B测试 (AB Testing):** 将一小部分流量导向新版本，监控真实用户反馈和业务指标 (激活、留存)。这是做出最明智决策的基础。

**Mermaid 概念图:**

```mermaid
<Mermaid_Diagram>
graph LR
    A("Zapier 代理")
    B("非确定性")
    C("用户行为")
    D("数据飞轮")
    E("用户满意度 (最终目标)")

    subgraph "数据收集"
        direction LR
        F("代码埋点 (Traces)") --> G("显性反馈")
        F --> H("隐性反馈")
        F --> I("传统指标")
        G --> D
        H --> D
        I --> D
    end

    subgraph "数据分析"
        direction LR
        J("LLM Ops 软件") --> K("数据聚合/聚类")
        J --> L("LLM识别根因")
        K --> D
        L --> D
    end

    subgraph "构建评估"
        direction LR
        M("单元评估") --> N("迭代改进")
        O("轨迹评估") --> N
        P("LLM判官") --> N
        Q("基于规则的评分") --> P
        M --> D
        O --> D
        P --> D
        Q --> D
    end

    subgraph "部署与验证"
        direction LR
        R("内部评估分数 (参考)") --> S("A/B 测试 (最终验证)")
        N --> S
        S --> E
    end

    A --> B
    A --> C
    B --> D
    C --> D

    D --> F
    D --> J
    D --> M

    N --> R
    R --> E
    S --> E

    style A fill:#ADD8E6,stroke:#333,stroke-width:2px
    style B fill:#FFB6C1,stroke:#333,stroke-width:1px
    style C fill:#FFB6C1,stroke:#333,stroke-width:1px
    style D fill:#FFFFCC,stroke:#333,stroke-width:2px
    style E fill:#90EE90,stroke:#333,stroke-width:3px,color:#333
    style F fill:#D3D3D3,stroke:#333,stroke-width:1px
    style G fill:#D3D3D3,stroke:#333,stroke-width:1px
    style H fill:#D3D3D3,stroke:#333,stroke-width:1px
    style I fill:#D3D3D3,stroke:#333,stroke-width:1px
    style J fill:#D3D3D3,stroke:#333,stroke-width:1px
    style K fill:#D3D3D3,stroke:#333,stroke-width:1px
    style L fill:#D3D3D3,stroke:#333,stroke-width:1px
    style M fill:#D3D3D3,stroke:#333,stroke-width:1px
    style O fill:#D3D3D3,stroke:#333,stroke-width:1px
    style P fill:#D3D3D3,stroke:#333,stroke-width:1px
    style Q fill:#D3D3D3,stroke:#333,stroke-width:1px
    style N fill:#D3D3D3,stroke:#333,stroke-width:1px
    style R fill:#D3D3D3,stroke:#333,stroke-width:1px
    style S fill:#D3D3D3,stroke:#333,stroke-width:1px

    linkStyle 3 stroke:#FFA500,stroke-width:2px
    linkStyle 4 stroke:#FFA500,stroke-width:2px
    linkStyle 5 stroke:#FFA500,stroke-width:2px
    linkStyle 6 stroke:#FFA500,stroke-width:2px
    linkStyle 7 stroke:#FFA500,stroke-width:2px
    linkStyle 8 stroke:#FFA500,stroke-width:2px
    linkStyle 9 stroke:#FFA500,stroke-width:2px
    linkStyle 10 stroke:#FFA500,stroke-width:2px
    linkStyle 11 stroke:#FFA500,stroke-width:2px
    linkStyle 12 stroke:#FFA500,stroke-width:2px
    linkStyle 13 stroke:#FFA500,stroke-width:2px
    linkStyle 14 stroke:#FFA500,stroke-width:2px
    linkStyle 15 stroke:#FFA500,stroke-width:2px
    linkStyle 16 stroke:#FFA500,stroke-width:2px
    linkStyle 17 stroke:#FFA500,stroke-width:2px
    linkStyle 18 stroke:#FFA500,stroke-width:2px
    linkStyle 19 stroke:#FFA500,stroke-width:2px
    linkStyle 20 stroke:#008000,stroke-width:3px
    linkStyle 21 stroke:#008000,stroke-width:3px
    linkStyle 22 stroke:#008000,stroke-width:3px
</Mermaid_Diagram>
```

Content:
 Yeah, and brief introduction to Zapier agents. I believe many of you know what Zapier is. This automation software, a lot of boxes, arrows, essentially about automating your business processes. Agents is just, well, more agent-ing alternative to Zapier. You describe what you want. We propose a bunch of tools that trigger, and we enable that. And hopefully we enable your, we automate your whole business processes. And a key lesson that we have after those two years is that building good AI agents is hard. And building good platform to enable non-technical people to build AI agents is even harder. That's because AI is non-deterministic, but on top of that your users are even more non-deterministic. They are going to use your products in a way that you cannot imagine up front. So if you think that building agents is not that hard, you probably have this kind of picture in mind. You probably stumbled upon this library called Blank Chain. You pull some examples, tutorial, you pick the prompt, pull a bunch of tools. You chatted with this solution, and they thought, well, it's actually kind of working, all right? So let's deploy it, and let's collect some profit. Turns out the reality has a surprising amount of detail, and we believe that building probabilistic software is a little bit different than building traditional software. The initial prototype is only a start. And after you ship something to your users, your responsibility switches to building the data flywheel. So once your user starts using your product, you need to collect the feedback. You're starting to understand the usage patterns, the failures, so then you can build more evils, build an understanding of what's failing, what are the use cases. As you're building more evils and firm features, probably your product is getting better, so you're getting more users, and there are more failures, and you have to build more features, and on and on and on. So yeah, it forms this data flywheel. But starting with the first step, okay, yeah. So starting from the beginning, how do you start collecting actionable feedback? Backing up for just a second, the first step is to make sure you're instrumenting your code, right? Which you probably already are doing. Whether you're using brain trust or something else, they all offer an easy way to get started, like, just tracing your completion calls. And this is a good start, but actually, you also want to make sure that you're recording much more than that in your traces. You want to record the two calls, the errors from those two calls, the pre- and post-processing steps. That way, it would be much easier to debug what went wrong with the run. And you also want to strive to make the run repeatable for eval purposes. So for instance, if you log data in the same shape as it appears in the run time, it makes it much easier to convert it to an eval run later, because you can just populate the inputs and expected outputs directly from your trace for free. And this is especially useful as well for two calls, because if your two call produces any side effects, you probably want to mock those in your eval. So you get all that for free if you're recording them in your trace. Okay, great. So you've instrumented your code, and you started getting all this raw data from your runs. Now it's time to figure out what runs to actually pay attention to. Explicit user feedback is really high signal, so that's a good place to start. Unfortunately, not many people actually click those classic thumbs up and thumbs down buttons. So you've got to work a bit harder for that feedback. And in our experience, this works best when you ask for the feedback in the right context. So you can be a little bit more aggressive about asking for the feedback, but you're in the right context. You're not bothering the user before that. So for us, one example of this is once an agent's finished running, even if he was just a test run, we show a feedback call to action at the bottom, right? Did this run do what you expected? Give us the feedback now. And this small change actually gave us a really nice bump in feedback submissions, surprisingly. So thumbs up and thumbs down are a good benchmark, a good baseline, but try to find these critical moments in your user's journey where they'll be most likely to provide you that feedback, either because they're happy and satisfied or because they're angry and they want to tell you about it. Even if you work really hard for the feedback, explicit feedback is still really rare. An explicit feedback that's detailed and actionable is even harder, because people are just not that interested in providing feedback generally. So you also want to mind user interaction for implicit feedback. And the good news is there's actually a lot of low-hanging fruit possibilities here. Here's an example from our app. Users can test an agent before they turn it on to see if everything's going OK. So if they do turn it on, that's actually a really strong positive implicit feedback, right? Copying a model's response is also good implicit feedback, even opening eyes doing this for Chatsupiti. And you can also look for implicit signals in the conversation. Here the user is clearly letting us know that they're not happy with the results. Here they're telling the agent to stop slacking around, which is clearly implicit negative feedback, I think. Sometimes the user sends a follow-up message that is mostly rehashing what it asked the previous time to see if the L-M interprets that phrasing better. That's also also good implicit negative feedback. And there's also a surprising amount of cursing. Recently we had a lot of success as well using an L-M to detect and group frustrations. And we have this weekly report that we post in our Slack. But it took us a lot of tinkering to make sure that L-M understood what frustration means in the context of our products. So I encourage you to try it out, but expect a lot of tinkering. You should also not forget to look at more traditional user metrics, right? There's a lot of stuff in there for you to mind implicit signal too. So find what metrics your business cares about and figure out how to track them. Then you can distill some signal from that data. You can look for customers, for example, that churned in the last seven days and go look at their last interactions with your product before they left, and you're likely to find some signal there. Okay. So I have rolled data. But now I'll let the industry experts speak. Why is it in the start? Yeah. Yeah. Or the meeting should continue until everyone looks at their data. Okay. But how actually you're going to do that? So we believe that the first step is to either buy or build L-M up software with a both. You're definitely going to need that to understand your agent runs, because one agent run is probably multiple L-M calls, multiple database interactions, two calls, rest calls, whatever. Each one of them can be source of failure, and it's really important to piece together this whole story and understand what caused this cascading failure. Yeah. I said we are doing both, because I believe, by coding your own internal tooling, it's really, really easy right now with cursor and cloud card, and it's going to pay you massive dividends in the future for two reasons. First of all, it gives you an ability to understand your data in your own specific domain context. And the second of all, it also, you should be also able to create a functionality to turn every single interacting case or every failure into an eval with the minimal amount of fraction. So whenever you see something interesting, there should be like a one-click to turn it into an eval, it should become your instinct. Once you understand what's going on on a singular run basis, you can start understanding things at scale. So now you can do feedback aggregations, clustering, you can bucket your failure modes, you can bucket your interactions, and then you're going to starting to see what kind of tools are failing the most, what kind of interactions are the most problematic. That's going to almost create for your automatic roadmap. So you'll know where to apply your time and effort to improve your product the most. Doing anything else is going to be a sub-optimal strategy. Something that we are also experimenting with is using reasoning models to explain the failures. Turns out that if you give them the trace, output input instructions and anything you can find, they are pretty good at finding the root cause of a failure. Even if they are not going to do that, they are probably going to explain you the whole run or just direct your attention into something that's really interesting and might help you find the root cause of the problem. Cool. So now you have a good short list of failure modes you want to work on first. It's time to start building out your evolves. And we realize over time that there are different types of evolves, and the types of evolves that we want to build can be placed into this hierarchy that resembles the testing pyramid for those of you that know that. So with unit tests like evolves at the base, end-to-end evolves or trajectory evolves, how we like to call them in the middle, and the ultimate way of evaluating using AB testing with stage roles at the top. So let's talk a bit about those. I was trying to use unit test evolves. We are just trying to predict the n plus one state from the current state. So these work great when you want to do simple assertions. For instance, you could check whether the next state is a specific tool call or if the tool call parameters are correct, or if the answer contains a specific keyword, or if the agent determined that it was done, all the good stuff. So if you are starting out, we recommend focusing on unit test evolves first because these are the easiest to add. It helps you build a muscle of looking at your data, spotting problems, creating evolves that reproduce them, and then just focusing on fixing them. Beware of turning every positive feedback into an evolve. We found that unit test evolves are best for hill climbing specific failure modes that you spot in your data. So now unit test evolves are not perfect, and we realized that ourselves, we realized we had over indexed on unit test evolves when the new models were coming out that were objectively stronger models, but they were still performing worse in our internal benchmarks, which was weird. And because the majority of our evolves were so fine-grained, this made it really hard to see the forest for the trees when benchmarking new models. There was always a lot of noise when we tried comparing them. Like when you're looking at a single trace, it's easy to kind of go through the trace and understand what's happening, but when you need to kind of look at it from, I don't know how to play against it, when you want to look at it through an aggregation of many traces, then it starts getting difficult to understand what's happening. Why are so many of these passing and some of these are regressing? Yeah, so we realized that maybe machine can help us. It turns out in that previous video when I was investigating one experiment inside brain trust, there's a lot of looking at the screen trying to figure out what went wrong, and we are like, hey, maybe we can just give this old data to, once again, a reasoning LLM, and compare the models for us. It turns out that with brain trust MCP and reasoning model, you can just ask it to, hey, look at this run, look at this run, and tell me what's actually different about the new model that we are going to deploy. In this case, it was Gemini Pro versus Claude, and what the reasoning model found was actually really, really good. It found that Claude is like a decisive executor, whereas Gemini is really yapping a lot. It's asking follow-up questions. It needs some positive affirmations, and it's sometimes even hallucinating about JSON structures. So, yeah, it helped us a lot. It also surfaces a problem with unit test evals a lot, which is different models have different ways of trying to achieve the same goal. And unit test evals are penalizing different paths. They are like hard-coded to only follow one path. And, yeah, our unit test evals were overfitting to our existing models that are actually collecting using that model. So, what we started experimenting with is trajectory evals. Yeah, instead of grading just one iteration of an agent, we let the agent run to the end state. And we are not grading just the end state, but we are also grading all the tool calls that were made along the way and all the artifacts that have been generated along the way. This can be also paired with LLM as a judge. We are just going to speak about it later. Yeah, but they are not free. I think they have really high return on investment, but they are much harder to set up, especially if you are evaluating runs that have tools that call side effects, right? When you are running an evil, you definitely don't want to send an email on behalf of the customer once again, right? So, we had a fundamental question whether we should demog environment or not. And we decided that we are not going to mock the environment because otherwise you are going to get data that is just not reflecting the reality. So, what we started doing is just mirroring users environment and crafting a synthetic copy of that. Also, they are much slower, right? So, they can sometimes take up to an hour, so it's not pretty great. And we are also learning a bit more into LLM as a judge. This is when you use LLM to create or compare results from your boss. And it's tempting to lean into them for everything, but you need to make sure that the judge is judging things correctly, which can be surprisingly hard. And you also have to be careful not to introduce subtle biases, right? Because even small things that you might overlook might end up influencing it. Later, we have also been experimenting with this concept of rubrics-based scoring. We use LLM to judge the run. But each row in our dataset has a different set of rubrics that were handcrafted by a human and described in natural language. What specifically about this run should the LLM be paying attention to for the score? So, one example of this, did the agent react to an unexpected error from the calendar API and then tried again? So, to sum it up, here is our experimental model of the types of e-vals that we build for separate agents. We use LLM as a judge or rubrics-based e-vals to build a high-level overview of your system's capabilities. And these are great for benchmarking new models. We use trajectory e-vals to capture multi-term criteria, and we use unit tests like e-vals to debug specific failures. He'll climb them. But beware of overfitting with these. Yeah, in a couple of closing thoughts, don't obsess over metrics. Remember that when a good metrics become a target, it sees us to be a good target. So, when you're close to achieving 100% score on your evil data set, it's not meaning that you're doing a good job. It's actually meaning that your data set is just not interesting, right? Because we don't have AGI yet, so it's probably not true that your model is that good. Something that we're experimenting with lately is dividing the data set into two pools, into the regressions data set, to make sure that we are making any changes. We are not breaking existing use cases for the customers. Also, the aspiration of data set, of things that are extremely hard, for instance, like nailing 200 tools calls in a row. And lastly, the thickest step back. What's the point of creating e-vals in the first place? Your goal isn't to maximize some imaginary number in a lab-like setting. Your end goal is user satisfaction. So the ultimate judge are your users. You shouldn't be optimizing for the biggest scores for the e-vals and completely disregard the vibes. So that's why you think the ultimate verification method is an A-B test. Just take a small proportion of your traffic, let's say 5%, and root it to the new prompt. Monitor the feedback, check your metrics like activation, user retention, and so on. Based on that, probably you can make the most educated guess instead of being in the lab and optimizing this imaginary number. That's all. Thank you. Thank you.
