Timestamp: 2025-07-17T09:06:58.909900
Title: OpenAI内部揭秘：ChatGPT的诞生、挑战与未来 BV1hMu4zNEBV
URL: https://b23.tv/HGISJQp
Status: success
Duration: 1:07:17

Description:
以下是对文本内容的总结：

**1. 提纲和结构化总结：**

*   **引言**
    *   本次播客访谈Andrew Maine与OpenAI的Mark Chen（首席研究官）和Nick Turley（ChatGPT负责人）深入探讨。
    *   主要围绕ChatGPT的早期发展、OpenAI的产品哲学、图像生成和代码工具的突破，以及AI对未来技能和科学进步的影响。

*   **ChatGPT的诞生与初期成功**
    *   **名称的意外性：** 最初计划命名为“chat with GPT 3.5”，因发布前夜的简化决策而定名为“ChatGPT”。
    *   **病毒式传播：** 发布前内部对其“是否足够好”存在疑虑，但发布后用户反响极其热烈，从D2被日本Reddit用户发现到D4被认定将改变世界。
    *   **运营挑战：** 初期服务器频繁宕机、资源短缺，促使团队专注于稳定服务，体现“研究预览”到“产品化”的转变。

*   **OpenAI的产品理念与文化**
    *   **迭代与“接触现实”：** 强调快速部署、获取用户反馈的重要性，认为这是改进模型性能和确保安全的有效方式。
    *   **用户反馈学习：** 通过用户点赞/踩等信号优化模型行为，例如处理初期模型出现的“谄媚性”问题，体现对问题的迅速响应。
    *   **透明化与安全：** 提倡公开AI行为规范（spec），允许用户在一定范围内调整模型偏好，并区分对待不同风险层级的安全问题（例如：生物武器与日常使用）。
    *   **“放权”而非限制：** 倾向于先允许有益用例，再研究和解决潜在危害，而非默认禁用。

*   **图像生成（Image Gen/DALL-E 3）的突破**
    *   **超预期成功：** 图像生成质量（特别是“单次生成完美匹配”和“风格迁移”）的提升带来巨大价值，甚至比ChatGPT更早吸引印度等新用户群体。
    *   **从娱乐到实用：** 起初以为主要用于娱乐（如动漫风格），但用户发现其在家居设计、信息图表、书籍插画等专业实用场景的巨大潜力。

*   **代码与工具（Codex）的演进**
    *   **双重范式：** 区分实时代码生成（如IDE补全）和“代理式编码”（Agentic Coding），后者允许模型长时间思考和解决复杂任务（如PR单元）。
    *   **内部采用与效能：** OpenAI内部广泛使用代码工具，显著提升工程师的生产力，并将其视为衡量产品价值的重要指标。
    *   **“品味”的挑战：** 尽管代码具有可验证性，但“良好代码”的定义包含“品味”和真实世界工程实践（测试、文档、协作），这仍是AI需要学习的领域。

*   **AI的未来与人类所需技能**
    *   **AI作为“超级助手”：** 展望AI将成为口袋里的智能体——导师、顾问、软件工程师，具备更长的记忆和异步工作流能力（允许用户等待复杂任务完成）。
    *   **赋能与民主化：** AI将使更多人能处理原本无法完成的任务，如医疗咨询（民主化第二意见）、教育（个性化辅导），而非取代专家。
    *   **未来人类技能：**
        *   **好奇心：** 核心，探索未知、提出正确问题的能力。
        *   **自主性：** 主动发现问题并解决问题的驱动力。
        *   **适应性：** 快速学习新事物、适应快速变化环境的能力。
        *   **委托与协作：** 学习如何将任务委托给AI，并与AI协作。
    *   **“推理”能力的推动：** 模型日益增强的推理能力（如“思考过程”）将加速科学（物理、数学）领域的发现和进步。

*   **嘉宾个人使用偏好**
    *   Mark：利用Deep Research模型在社交前预习信息。
    *   Nick：使用语音功能整理思路、生成待办事项，认为语音交互潜力巨大。

**2. 核心观点：**

人工智能的未来在于通过不断迭代的、以用户为中心的通用模型，将智能作为超级助手融入日常生活和专业领域，赋能个体，并加速科学发现，但这也要求人类培养好奇心、适应性和委托能力。

**3. 总体框架：**

以用户为中心的迭代部署与通用人工智能的持续赋能

<Mermaid_Diagram>
graph LR
    subgraph "播客主题 Podcast Topics"
        A["播客"] --> B("ChatGPT的早期")
        A --> C("OpenAI的文化与产品理念")
        A --> D("图像生成产品")
        A --> E("代码与工具产品")
        A --> F("AI的未来与人类技能")
    end

    subgraph "ChatGPT核心特性与挑战 ChatGPT Core & Challenges"
        B --> B1("名称由来：GPT-3.5对话")
        B --> B2("发布前内部犹豫")
        B2 --> B3("意外的病毒式传播")
        B3 --> B4("早期运营挑战 (宕机/资源)")
        B4 --> C1("推动迭代部署")
    end

    subgraph "OpenAI核心理念 OpenAI Core Philosophy"
        C1("迭代部署") --> C2("频繁接触现实")
        C1 --> C3("用户反馈")
        C3 --> C4("产品持续演进")
        C2 --> C4
        C1 --> C5("透明化 (公开规范)")
        C1 --> C6("AI安全与风险管理")
        C6 --> C7("平衡用户自由")
        C7 --> C4
    end

    subgraph "主要AI产品 Main AI Products"
        D --> D1("Image Gen (DALL-E 3)")
        D1 --> D2("高质量单次生成")
        D1 --> D3("强大风格迁移")
        D1 --> D4("应用: 娱乐用途")
        D1 --> D5("应用: 实用场景 (设计/图表)")
        E --> E1("代码模型 (Codex)")
        E1 --> E2("演进: 代理式编码")
        E2 --> E3("解决复杂任务")
        E1 --> E4("内部广泛使用")
        E4 --> E5("提升工程师生产力")
    end

    subgraph "AI的未来与人类赋能 Future AI & Human Empowerment"
        F --> F1("愿景: 超级AI助手")
        F1 --> F2("特性: 长记忆")
        F1 --> F3("特性: 异步工作流")
        F1 --> F4("赋能: 医疗/教育")
        F1 --> F5("赋能: 日常任务")
        F1 --> F6("促进: 科学进步")
        F6 --> F7("核心能力: 推理")
        F7 --> F6
        F --> F8("未来所需人类技能")
        F8 --> F9("好奇心")
        F8 --> F10("自主性")
        F8 --> F11("适应性")
        F8 --> F12("委托与协作能力")
        F4 --> F8
        F5 --> F8
    end

    C4 --> F1
    E5 --> F1
    D5 --> F1

    style A fill:"#D8BFD8",stroke:"#333",stroke-width:2px,color:"#333";
    style B fill:"#FFFACD",stroke:"#333",stroke-width:1px,color:"#333";
    style C fill:"#FFE4B5",stroke:"#333",stroke-width:1px,color:"#333";
    style D fill:"#FAFAD2",stroke:"#333",stroke-width:1px,color:"#333";
    style E fill:"#FAFAD2",stroke:"#333",stroke-width:1px,color:"#333";
    style F fill:"#F0F8FF",stroke:"#333",stroke-width:1px,color:"#333";

    style B1 fill:"#FFDAB9",stroke:"#333",stroke-width:1px,color:"#333";
    style B2 fill:"#FFDAB9",stroke:"#333",stroke-width:1px,color:"#333";
    style B3 fill:"#90EE90",stroke:"#333",stroke-width:1px,color:"#333";
    style B4 fill:"#FFB6C1",stroke:"#333",stroke-width:1px,color:"#333";

    style C1 fill:"#ADD8E6",stroke:"#333",stroke-width:1px,color:"#333";
    style C2 fill:"#ADD8E6",stroke:"#333",stroke-width:1px,color:"#333";
    style C3 fill:"#ADD8E6",stroke:"#333",stroke-width:1px,color:"#333";
    style C4 fill:"#98FB98",stroke:"#333",stroke-width:1px,color:"#333";
    style C5 fill:"#ADD8E6",stroke:"#333",stroke-width:1px,color:"#333";
    style C6 fill:"#FFB6C1",stroke:"#333",stroke-width:1px,color:"#333";
    style C7 fill:"#ADD8E6",stroke:"#333",stroke-width:1px,color:"#333";

    style D1 fill:"#ADD8E6",stroke:"#333",stroke-width:1px,color:"#333";
    style D2 fill:"#90EE90",stroke:"#333",stroke-width:1px,color:"#333";
    style D3 fill:"#90EE90",stroke:"#333",stroke-width:1px,color:"#333";
    style D4 fill:"#FFFACD",stroke:"#333",stroke-width:1px,color:"#333";
    style D5 fill:"#90EE90",stroke:"#333",stroke-width:1px,color:"#333";

    style E1 fill:"#ADD8E6",stroke:"#333",stroke-width:1px,color:"#333";
    style E2 fill:"#90EE90",stroke:"#333",stroke-width:1px,color:"#333";
    style E3 fill:"#ADD8E6",stroke:"#333",stroke-width:1px,color:"#333";
    style E4 fill:"#90EE90",stroke:"#333",stroke-width:1px,color:"#333";
    style E5 fill:"#98FB98",stroke:"#333",stroke-width:1px,color:"#333";

    style F1 fill:"#ADD8E6",stroke:"#333",stroke-width:1px,color:"#333";
    style F2 fill:"#FFFACD",stroke:"#333",stroke-width:1px,color:"#333";
    style F3 fill:"#FFFACD",stroke:"#333",stroke-width:1px,color:"#333";
    style F4 fill:"#90EE90",stroke:"#333",stroke-width:1px,color:"#333";
    style F5 fill:"#90EE90",stroke:"#333",stroke-width:1px,color:"#333";
    style F6 fill:"#98FB98",stroke:"#333",stroke-width:1px,color:"#333";
    style F7 fill:"#ADD8E6",stroke:"#333",stroke-width:1px,color:"#333";
    style F8 fill:"#FFFACD",stroke:"#333",stroke-width:1px,color:"#333";
    style F9 fill:"#B0E0E6",stroke:"#333",stroke-width:1px,color:"#333";
    style F10 fill:"#B0E0E6",stroke:"#333",stroke-width:1px,color:"#333";
    style F11 fill:"#B0E0E6",stroke:"#333",stroke-width:1px,color:"#333";
    style F12 fill:"#B0E0E6",stroke:"#333",stroke-width:1px,color:"#333";
</Mermaid_Diagram>

Content:
 Hello, I'm Andrew Maine and this is the opening eye podcast. My guests today are Mark Chen, who is the Chief Research Officer at Opening Eye and Nick Turley, who is the head of chat GPT. We're going to be talking about the early viral days of chat GPT. We're going to talk about image gin, how opening eye looks at code and tools like Codex, what kind of skills they think that we might need for the future and we're going to find out how chat GPT got its totally normal name. Even half of research doesn't know what those three letters stand for. You're going to have an intelligence in your pocket that it can be your tutor, your advisor, your software engineer. There's a real decision the night before. Can we actually watch this thing? First off, how did Opening Eye decide on that awesome name? I was going to be in chat with GPT 3.5 and we had a late night decision to simplify. Wait, wait, wait, wait, wait, wait. Say that again. I was going to be chat with GPT 3.5, which rolls off the tongue even more nicely. That's, and you said that was a late night decision, meaning like weeks before you finally decided what to call it. Right, right, right. No, weeks before we hadn't started on the project. Oh goodness. But, yeah, I think we realized that that would be hard to pronounce and came up with a great name instead. So that was the night before. Roughly. Not even the day before. It was all kind of a blur at that point. I would imagine a lot of that was a blur. And I remember here, I remember being in a meeting when we talked about the low key research preview, which like really was like we really thought like, oh, this is because it's, it was 3.5. 3.5 was a model that had been out for months. And from a capabilities point of view, when you just look at the e-vow, you're like, yeah, it's the same thing, but we just put the interface in here and made it so you didn't have to prompt as much. And then chat JPG comes out. And when was the first sign that this thing was blowing up? I'm curious for everyone. I was there slightly on recollection of that, that era because it was a very confusing time. But for me, Day 1 was sort of, you know, is the dashboard broken classic like the logging campy, right? Day 2 was like, oh, weird. I guess like Japanese Reddit users discovered this thing. Maybe it's like a local phenomenon. Day 3 was like, okay, it's going viral. It's definitely going to die off. And then by day four, you're like, okay, it's going to change the world. Mark, did you have any expectation about that? About no, honestly, I mean, we've had so many launches, so many previews over time. And yeah, this one really was something else. We had to take off for it. It was huge. And yeah, my parents just stopped asking me to go work for Google. Wait, wait, wait a second. Up until chat, GPT, your parents were asking like what you're doing here. Yeah, no, I mean, they just never heard of OpenAI. I think for many years, thought AGI was this pie in the sky thing and I wasn't having a serious job. So it was a real revelation for them. Yeah. What was your job title at the time? I think just member of technical staff. Member technical staff. Yeah. And then that will open now your head of research. I guess so, yeah. So all right. Yeah, actually on the GPT name, I think even half of research doesn't know what those three letters stand for. It's kind of funny. Like half of them think it's generative pre-training. Half of them think it's generative pre-trained transformer. And what is it? It's the latter. Okay. All right. Yeah, those people, they don't know the name of it. Yeah. It is, it's weird how just a silly name like that, all of a sudden becomes a thing. Would you see that with like, you know, Google, Yahoo, Kleenex, things like that, Xerox? And sometimes they were, some of those were names by intention. And this was really just a silly sort of name. For me, the moment that I felt like after watching the launch, watching it accelerate, I knew what was going to happen. And then what it did was when it was on South Park. And remember that when South Park made fun of the name and that was the first time I'd watched South Park and let's just say a while. And that episode, I still think it's magic. Yeah. It was obviously profound to watch and see, you know, something you help make show up in pop culture. But there's the punchline in the end where it's like, oh, this was co-written by Chad GPT. And they took that off though. I think I think in later episodes, because it used to say, I think written by like Trey Parker and that Chad GPT and then no, it was. And then I think later, I think they may have pulled that off at some point. I don't remember like, I strongly feel that you shouldn't have to give credit to it. Yeah, that wasn't that's what I was using. I had to give credit to Chad GPT for every aspect of my life. Well, might as well say Chad GPT maybe with Andrew. So it's easy for prep for your interviews. You know, one of my co-producers, Justin, probably uses it. I haven't asked him yet because I'd like to think that he's hand crafting every single question that we're thinking about here. But I am sure you say it was a bit of a blur. I'll tell you, like a standout moment for me, the launch of Chad GPT was, I don't know if you remember this, but the Christmas party. And we'd had several weeks of Chad GPT out there and Sam Altman went up and said, Hey, it's been exciting to watch this, but the internet being the internet. And I think we all felt this way. It's going to die down. Spoiler alert, it did not die down. And it just kept accelerating. What were the things you had to do internally to sort of keep this thing up and running as more people wanted to use it? We had quite a few constraints. And for those of you who remember, you know, I think you guys remember, Chad GPT was down all the time in the beginning. And that was, yeah, we'd said, Hey, there's a research preview, no guarantees. You maybe goes down. But the minute you had people loving and using this thing, that didn't feel super good. So, you know, people were certainly working around the clock to keep the side up. I remember, you know, we obviously ran out of GPUs. We ran out of database connections. We had, you know, we're getting rate limited in some of our providers. Nothing was really set up to run a product. So in the beginning, we just built this thing. We called it the fail well. And it would just tell you kind of nicely that the thing was down and made a little poem, I think was generated by GPT three about being down and sort of tongue in cheek. Yeah, that got us through the winter break because we did want people to have some sort of a holiday. And then when we came back, we were like, okay, this is clearly not valuable. You can't just go down all the time. And eventually we got to something we could serve everyone. Yeah. And I think, you know, the demand really speaks to the generality of Tatcha BT, right? We had the ceases that Tatcha BT embodied what we wanted in AGI, just because it was so general. And I think, you know, you're seeing that demand ramp just because people are realizing, you know, any use case that I want to give or to throw to the model, it can handle. We were kind of known as the company working on AGI. And I think prior to Tatcha BT, the API was certainly the first time we had a public offering where people could go use it and do it. But then it was more for developers and stuff. And I think that as long as people were sort of thinking AGI, that seemed to be the point at which people thought these models would be useful. But we saw GPT three, we saw that that was useful. And then we saw that we can do other things are useful. Was everybody at OpenAI on board with Chatsh EPT being useful or being ready to launch? Yeah, I don't think so. You know, even the night before, I mean, there's this very famous story at OpenAI of you know, Ilya taking 10 cracks at the model, you know, 10 tough questions. And my recollection is maybe only on five of them. He got answers that he thought were acceptable. And so there's a real decision the night before. Do we actually launch this thing? Is the world actually going to respond to this? And I think it just speaks to when you build these models in house, you so rapidly adapt to the capabilities. And it's hard for you to kind of put yourself in the shoes of someone who hasn't kind of been in this model training loop and see that there is real magic there. Yeah. Yeah. Yeah, I think to build on that, like the Condoracy internally about, you know, is this thing good enough to launch? I think it was humbling, right? Because it's just a reminder of how wrong we all are when it comes to AI. That's why, you know, frequent contact with reality is so important. Could you elaborate more on that contact with reality? What does that mean? Yeah. I mean, when you think about iterative climate, one way I like to frame it is, you know, there's no point everyone agrees where it's suddenly useful. And I think usefulness is this big spectrum. And so, you know, there's not one capability level or one bar that you meet. And suddenly, you know, the model is useful for everyone. Were there any hard decisions about what to include or what to focus? On we were very, very principled on Chatsubite to not balloon the scope. You know, we were adamant to get feedback and data as quickly as we could. I'm always in slack telling you that a lot of things that didn't add this. I remember actually there was a lot of controversy about the UI side. For example, we didn't launch with history and then we thought we would probably want that. And, you know, guess what? That was the first request. I also think there's always the question like, can we train an even better model? Like, you know, with two weeks more time, I'm glad we did. And I didn't because, you know, we, I think got a ton of feedback as we did. So, yeah, there was a ton of this scope discussions and, you know, the holidays were coming up. So I think we had this kind of natural forcing function for getting something out. Yeah, there's this this habit of things that if it's going to come after a certain point November, it's not going to come out like February. Yeah, there's a sort of window where things would fall on either side. Well, that would be the classic once in a big tech company. I think we're definitely a bit more flexible in the we should. I felt like one of the big impacts was once people are using it, it felt like the rate of these things improving was tremendous. I don't know if that was something that we really had in a calculus. We could certainly think about training on the larger site more data, scaling compute, but then the idea of actually having them, the signal you would get from that many people using it. Yeah. I think over time, you know, feedback really has become an integral part of how we build the product. And it's also becoming an integral part of safety. And so you always feel the time cost of losing that on feedback. You know, you can deliberate in a vacuum, right? Are they going to respond to this better? Are they going to respond to that better? But it's just not a substitute for just bringing it out there, right? I think our philosophy is let the models have contact with the world. And if you need to revert something, that's fine. But I think there's really no substitute for this fast feedback. And it's become one of the big levers for how we improve model performance too. It's sort of funny. Like, I feel like we started with shipping these models in a way that is more similar to hardware, where you make like one launch very rarely and it has to be right. And you know, you're not going to update the thing and then you're going to work on the next big project. And it's capital intensive and timelines are long. And over time, and I think chat to be tea was kind of the beginning. It's looking more like software to me where you make this week when updates, you have a kind of a constant pace that will can adopt something doesn't work. You pull it back and you sort of lower the stakes in doing that and you lower that you increase the empiricism. And of course, just operationally to you can innovate faster in a way that is more and more in touch with what you just want. Yeah, one of the examples we had of that was the model becoming too obsequious or sycophantic. Could you explain what happened there where that was where people all of a sudden say, Hey, it's telling me I've got a hundred ninety IQ and I'm the most handsome person in the world, which I had no problem with personally. But other people did. And what was going on there? Yeah. So I think one important thing is we rely on user feedback to the models, right? And it's this very complicated mix of reward models, which we use in a procedure we call our late chat, right? Using human feedback to use our relative, improve the models. Yeah. Just like a brief example, what that would mean. Yeah. Yeah. So I think one way to think about it is, you know, when a user enjoys a conversation, you know, they provide some positive signal. Yeah. Yeah. A thumbs up, for instance. And we train the model to prefer to respond in a way that would elicit more thumbs up, right? And this may be obvious in retrospect, but stuff like that, if balanced incorrectly, can lead to the model being more sycophantic, right? You can imagine users might want that kind of that feeling of, you know, a model saying good things about them. But I don't think it's a very good long term outcome. And actually, when we look at kind of our response to sycophancy and the rollout that resulted there, I think there were a lot of good points about it. You know, this was something that was flagged just by a small fraction of our power users. It wasn't something that a lot of people who generally use the models notice. And I think we really picked that out fairly early. We responded to it, I think with the appropriate level of gravity. And yeah, I think it just shows that, you know, we really do take these issues quite seriously and we want to intercept them very early. Yeah. It felt like there was maybe 48 hours since the model came out and then Joanne Zhang had a response explaining exactly what happened. And I think that that's the, that's the hard part. How do you navigate that? Because the problem with social media is you were basically monetized by engagement time. You want to keep people on there longer so you can show them more ads. And certainly the more people use chat, you could obviously, there's a cost to open any ideas, maybe use it once and say around forever, but that's not practical. How do you weigh that? The idea of making people happy with what they're getting versus making the model, you know, be broadly more useful than just pleasing. I feel very lucky and this regard because we have a product that's very utilitarian and people use it to either achieve things that they do know how to do, but don't feel like doing faster or with less effort or they're using it to do things that they couldn't do at all. You know, first example is maybe, you know, writing an email that you've been dreading. Second example might be, you know, running a data analysis that you didn't have to know how to do in Excel, you know, true story. So, so, you know, those are very utilitarian things and fundamentally, as you improve, you actually spend less time on the product, right? Because, you know, ideally it takes less turns back and forth, or maybe you actually delegate to the eyes so you're not in the product at all. So for us, you know, time spent, it's very much not the, not the, not the thing we optimized for. You know, we do care about your long-term retention, because we do think that's a sign of value. If you're coming back three months later, that's what it means we did something right. But what that means is, you know, I always say, show me the incentive and I'll share the outcome. We have, I think, the right fundamental incentives to build something great. That doesn't mean we'll always get it right. The sycophancy events were really, really important and good learning for us. And I'm proud of how we acted on it. But fundamentally, I think we have the right, the right setup to build something awesome. So that brings up the challenge. I wonder how you'd navigate that is that. One of the things early on when, you know, Chatsibu came out, there was like the allegations, it's woke, it's woke and people are trying to promote some sort of like agenda from. And my argument always been like, you train a model on, you know, kind of on corporate speak, you know, average news and a lot of academia, that's going to kind of follow into that. And I remember Elon Musk was very critical about it. And then when he trained the first version of Groc, it did the same thing. And then he's like, Oh, yeah, when you trained it on this sort of thing and did that. And internally, it opened and either discussions about how do we make the model not try to push you, not try to steer you? Could you go a little bit how you try to make that work? Yeah. So I think at its courts, a measurement problem, right? And I think it's actually bad to downplay these kind of concerns because they are very important. Right. And we need to make sure that the model, the default behavior that you get is something that's centered, that, you know, it doesn't reflect bias on the political spectrum or in many other, you know, axes of bias. And at the same time, you know, you do want to allow the user the capability to, you know, if you want to talk to a reflection of something with more conservative values, to be able to steer that a little bit, right? Or liberal values, right? And so I think the thing is you want to make sure that defaults are meaningful and they're centered and that's a measurement problem. And you also want to give ability, some flexibility, right, within bounds to steer the model to be a persona that you want to talk to. I think that's right. Um, I think, you know, in addition to neutral default, ability to bring your own values to some extent, I think, you know, being transparent about the whole thing is, I think, really, really important. I'm not a fan of secret system messages that, you know, try to like, you know, hack the model into saying or not saying something. Um, what we've tried to do is publish our spec. So you can go look at, you know, if you're getting certain model behavior, is that a bug, um, you know, is it violation of our own synod spec? Or is it actually in the spec, in which case, you know, who to criticize and who doesn't, who do you lot? Or is it just under specified in the spec, in which case that allows us to improve it and add more specificity into that document? So I sort of publishing the rules of the AI that it's supposed to be following. Um, I think that's an important step to have more people contribute to the conversation than just the people inside of a banana. So we're talking about like the system prompt, the part of the instruction that the model gets before the user puts the input. And well, I think it's one that yeah. Um, this is the problem is one way to steer the model, but you know, it goes much deeper into that, right? Yeah, we have a very large document that, um, outlines across a bunch of different behavior categories, how we expect the model to behave. And just to give you an example here, right? Um, you can imagine if there's someone who comes in with just like a incorrect belief, just a factually incorrect kind of a point of view. Um, how should the model interact with that user? Right. And should it reject that point of view outright or should it collaborate with the user on kind of figuring out what's, what's true together? And you know, we take that ladder point of view. And I think there are a lot of very subtle decisions like this, which we put a lot of time in. Yeah, that's a hard one because I think. Some things that you can test for and you can try to figure out advance, but when you're trying to figure out how an entire culture is going to adopt something that's challenging, like if I was some of those convinced that the world was flat, you know, like how much should the model push back against me? And some people are like, oh, it should push it back all the way, but it's okay. What if you're one religion or not another and. Yeah, it turns out rational people and well, many people can disagree on how, you know, the model should be able to see instances and you're not always going to get it right, but you can be transparent and what approach we took. You can allow users to customize it. And I think, you know, this is our approach. I'm sure there's, you know, ways we can improve on it. But I think between transparent and the open about how we're trying to tackle it. We can we can get you back. How are you thinking about as people start to use these models more and more, regardless of whether or not that's some dial you're trying to turn, it's just the more useful it becomes, the more people want to use it. You know, there was a time when nobody wanted a cell phone and now we can't get away from them. And how are you thinking about relationships people are forming with with their systems? Obviously, you know, we, you know, I mentioned this earlier. This is a technology you have to study. Yeah, designed in a static way to do XYZ. It's it's highly empirical. So, you know, as people adopt and the way that they use the product, it's something that we we need to go understand and and and act on as well. I've been observing this trend with interest where I think, you know, increasing number of people, especially Gen Z and, you know, younger populations are coming to chat to be to use a thought partner. And I think in many cases, that's really helpful and beneficial because you've got someone to brainstorm on a relationship question. You've got someone to brainstorm on a, you know, a professional question or something else. But in some cases, it can be harmful as well. And I think detecting the scenarios and first and foremost, having the right model behavior is very, very important to us. So actively monitoring and in some ways, it's one of those problems we're going to have to grapple with because with any technology that becomes ubiquitous, it's going to be dual use people are going to use it for all this awesome stuff. And people are going to use it in ways that, you know, we wish they didn't. And we have some responsibility to make sure that we handle that with the appropriate gravity. I find myself having longer conversations with it. I like the memory function. I like the fact you can turn it off. You don't want. And I think about like, you know, what's this going to be two years from now, three years from now when it has a much longer memory, much more context with this. I like the idea to have these sort of like, you know, memento anonymous modes too, or it's not going to store this. But I kind of wonder how much you've been thinking about two years, three years down the road. What's that going to be like when chat, you know, is way more about you. Yeah. I mean, I think memory is just such a powerful feature. In fact, it's one of the most requested features when we talk to people externally. It's like, this is the thing I really want to pay more for. And I think, you know, you like in it to, if you've ever kind of had a personal assistant, you know, you know, I'm not. Well, you do need to build a not relatable work. Sorry, guys. I'm sorry. Yeah. But you know, it's just like it's kind of in any kind of relationship that you have with a person, right? You, you build up context with them over time. Um, and I think just the more they know about you, right, the richer the relationship, the more, you know, um, it can also help you, right? You can work together to collaborate on tasks together. I do become self conscious of the fact that like it knows everything about me when I'm grumpy. And I've, I've, I've argued with it recently, by the way. That's good. Yeah. You should be able to argue with it. Yeah. You understand a lot about yourself and having a thing to argue with. And I think you spare others of that experience, which can also be beneficial. But don't argue on math and science. You're not going to win this. Yeah. No, I think increasingly very unlikely. Yeah. Uh, yeah, I think memory is cool. I, I, I, in two marks point, it's been part of our vision for a long time, because, you know, we said we were going to build a super assistant before we really knew what that meant. Um, chat to be to you. It was sort of the early demonstration to that idea. But if you kind of think about, you know, real world intelligences, you know, even they are not particularly useful on their first day. Um, and I think being able to solve that problem, begin to solve that problem has been profound to your earlier question, though, you know, it, it really does feel like, you know, if you fast forward a year or two, a chat GPT or things like it are going to be your most valuable account by far. It's been, you know, so much about you. And that's why I think given people ways to talk with this thing, you know, in private is very important. Um, we make, you know, this like temp chat thing varies, like literally on the home screen, because we think it's, you know, increasingly important to let you talk about stuff sort of off the record too. So, um, it's an interesting question. And, and, um, I think privacy and AI is going to be, be an interesting one. Um, for the next couple of years. Oh, and a switch here's talk about another release, which again, kind of caught people by surprise and blow up was image Jen. And, uh, I was here for Dolly, Dolly two, and then, then Dolly three came out and I thought Dolly three, I thought was a very capable model, but it seemed like it preferred a certain kind of image and a lot of the utility and the capabilities for variable binding was sort of kind of hidden away. And then image Jen was kind of just this breakthrough moment that it caught me off guard. How did you guys feel about the launch of that? Yeah, honestly, it caught me off guard too. Um, and this really props to the research team, you know, um, gave in particular to a ton of work here, um, Kenji, many others on the table. It did phenomenal work. And, um, I think it really spoke to this thesis that when you get a model, just good enough that in one shot, it can generate an image that fits your prompt. That's going to create immense value. And I think we never quite had that before, right? Um, that you just get the perfect generation. Oftentimes on the first, first try. Um, and I think that's something very powerful, you know, like, uh, people don't want to pick the best out of a grid. I think, uh, yeah, you just got very good prompt falling and, you know, this great style transfer to, right? Um, this ability to kind of put images, um, as context for the models of to modify and to change and the fidelity that you could do that with. Um, I think that was really powerful for people. I think, I think this image and experience, um, it was just kind of another mini chat to be T moment, um, all over again, where, you know, you have kind of this, you've been staring at this for a while. You're like, yeah, it's going to be cool. I think people really like it. Um, but you kind of, you know, you're launching like 20 different things. And then suddenly the world is going crazy in a way that you, you kind of only find out, um, by shipping. Like I remember distinctly, you know, we had like 5% of the Indian internet population try, um, image in over the weekend. And I was like, wow, we're reaching new types of users who we wouldn't even have thought, you know, who might not have thought of using chat. That's really cool. And, um, to my point, I think a lot of this is, um, because there's just this continuity where something suddenly works so well and truly the way you expected, um, where I think it blows people's minds. You know, and I think we're going to have those moments and other modalities to, you know, I think voice, you know, it hasn't quite passed the touring test yet, but I think the minute it does, people are going to, um, I think find that immensely powerful and valuable, you know, the video is going to have its own moment where it starts meeting the expectations that users have. So I'm really excited about the future because I think there's so many of these magical moments coming that are really going to transform people, um, people's lives and also you change, um, sort of chat to be these relevance for people because, um, you know, there's, um, I've always felt like this text people and there's image people and like some of them are a little bit different. Um, and now they're all using the product and discovering the value. Um, across the board, the moment when it launched, I think it kind of illustrated the, the problem that been with image models before. And, you know, when Dolly came out, it was super exciting because you're like, I'm, I'm like doing pictures of space monkeys and all these sorts of things. The moment you try to do a really complex image, and that's the phrase I brought up before, which is variable binding, you start to see these things drop off. And that was when I realized, oh, there's going to be a challenge for other image systems that don't have kind of the scale and the compute of like a GPT four under the hood. And now was it just, was it basically that like taking like a GPT four scale model and say now you do images that they'd break through? Well, I think there are a lot of different parts of research that made, made this such a big success. Right. I think, um, with a complicated multi step pipeline, it's never just one thing. Right. It's like very good post training. It's very good training. Um, and, um, I think it's just all of that coming together. Right. Um, variable binding definitely was one thing that we paid a lot of attention to. I think one thing about the image, the launch is, is a launch that was very deep. I think people, you know, they started by working on, you know, creating anime versions themselves, but you realize when you play with it more, you know, the infographics, they work. Oh, yeah. Like you actually create charts, you can call them a book panels. Yeah. You can mock up when your home would look like, exactly. Yeah. For sure. And it's like, I've heard all these things from from users that are like completely surprising about that. We did the, the, the podcast set up by literally taking some photos of chairs in the room and just put it in there and saying, create a better setup. And it was amazing. So we've seen kind of a lot of the, you know, there was a lot of the anime style images, which kind of like for summary, it was just sort of the weird thing where it was just just better than what we'd seen before. And I don't think anybody is ready to be really surprised by an image model in that way. I think obviously internally and externally, what were some of the things that surprised you or some of the new things you saw people doing? Yeah. I mean, I'll tell you a quick story there too, because, um, you know, up until the day of the launch, we're trying to figure out what's the right use case to showcase, you know, like, uh, and I think I'm so glad we ended up on kind of anime styling. It's just everyone looks good as an animated. That's true. I mean, it's funny, with original chat, GBT, I thought it would be strictly utilitarian product and then the surprise of people is it for fun. In this case, it was sort of the opposite where I was like, okay, this is going to be really cool for me. And it was people are going to like have fun with this thing. But then I was like really surprised by all the genuinely useful ways of using image gen, whether or not it's planning your home project. Um, as I mentioned earlier, um, you know, of, um, you're doing a construction, you want to see what things would look like if, you know, you had this remodel or this furniture or whatever, um, to, um, you're working on a slide deck, um, for this important, um, presentation and you just want to have really useful consistent illustrations, um, that are on topic and, um, and, and get it. So, so I, I really have been, been kind of personally surprised by the utility in this case because I knew it would be fun. Uh, that was not a question. Yeah. I think I, you said to generate a tier list of AI companies and it, but it opened me at the top. Yeah. You win model. Um, what did post training? Yeah. Yeah. That's just happened. You know, who knew, um, what has been the thinking in its change? Because I remember originally with Dolly, the idea of like, okay, we have to be a lot of very controlled about what it can do, what it can't do. Originally, I remember we first launched, you couldn't do people, which was not a very useful model. And then finally was trying to roll back. How much of that was cultural shift? How much that was the technological ability to control for things and how much of that was just saying we've got to push the norms. I would say it was both cultural shift and improvement in our ability to control things, the culture shift, you know, I'm, I'm not going to, I think when I joined OpenAI, there was a lot of conservatism around, you know, what capabilities we should give users, maybe for good reason. The technology was really new. Um, a lot of us were new to working on it. And, you know, if you're going to have a bias, you know, biasing towards safety and being careful, it's not a bad, you know, in DNA to have. But I think over time we learned that there's so many positive use cases that you, um, effectively prevent when you make arbitrary restrictions in the model. What about faces? Why not? Why can't I make any face I want? Um, so this is a good example of, of, um, a, you know, capability that that's got pros and cons and you can air on one side or the other. But, you know, when we, um, first shipped, um, image uploads, um, into chat, GBT, uh, we had some debates, you know, about what, what capabilities do you allow? This is where are you conservative? And I think one debate that we had is like, do we upload, allow the upload of images with faces or rather when you upload an image that contains a face, do you, um, you should we just like gray out the face because you avoid so many problems, right? Yeah. You can make inferences about people based on, on their face. Um, you could say mean things to people based on, um, their face. Um, um, and, and you know, you would just take a giant shortcut and all the early issues if you didn't allow that. But, um, I've always felt we need to, um, air on the side of freedom and we need to do the hard work. And I think in this case, you know, there's so many valid ways, you know, if I want feedback on makeup or on my haircut or anything like that, um, I want to be able to talk to chat to you about it. That those are valuable and benign use cases. And I would prefer to allow and then study, you know, where does that, um, fall short? Where is that harmful? And then iterate from there. It was taking a default stance on disallowed. And I think that's one of those ways in which our stance and posture has changed a bit over time in terms of where we set, you know, where we start. Yeah, we were very good. I think imagining worst case scenarios. What if I use this, these faces to evaluate hires for a company or whatever, but also it's like, Hey, it's just eczema. Like, you know, there's a lot of utility there. And honestly, I think there are certain demands of, of AI safety where worst case scenario thinking is very appropriate. So I think that is an important way of thinking about risk when it comes to certain forms of risks that are existential or even just very, very bad. You know, we have the preparedest framework, which helps us reason through some of those things. You know, um, can the AI let you make a bioweapon? It's good to think about the worst case there. It can be really, really bad. So you kind of have to have that way of thinking in the company and you have to have certain topics where you think about safety in that way. But you can't let that kind of thing spill over onto other domains of safety where the stakes are lower because you end up, I think, making very, very, um, conservative decisions that block out many valuable use cases. So I think being sort of principled about different types of safety on different time horizons and with different levels of stakes is very important for us. I think I want to blunt mode sometimes and just because like right now where it actually roasts you. Well, I mean, like, yeah, because I'll ask the model like, with the voice in speech out model, be like, do I sound tired? And it's like, well, you know, I don't really want to, you know, and I'll be like, yeah, you know, just trying to get it to be honest. You know, I think there's many cultures that would prefer a blender chat. Yeah. Very much on the radar. Yeah, just to piggyback off Nick's answer. I think it's the iterative deployment that gives us the confidence, right, to push towards user freedom. And we've had many cycles of this. We know what users can and can't do. And that gives us the confidence to want with the restrictions that we do. One of the other capabilities, one of the other generative capabilities that's been very interesting has been code. And I remember early on GPD three, we saw that all of a sudden it could spit out entire react components. And we saw that, oh, wow, there's some utility there. And then we went, we actually trained a model more specifically on code. And that led to we had code X and we had code interpreter now, code X is somehow back. And, you know, new, new form, same name, but the capability to keep increasing. And we've seen code work its way first into VS code via co-pilot. And then cursor and then I win surf, which I use all the time now. What, how much pressure has there been in the code space? Because I'd say that if we ask people who made the top code model, we might get different answers. Yeah. And I think it reflects that when people talk about coding, they're talking about a lot of different things. Right. I think there's coding in a specific paradigm. Like if you pull up an ID and you want to kind of get a completion on a function, that's very different from, you know, agentic style coding. You know, you ask, you know, I want this PR and, you know, and I think we've done a lot of focus. Could you, I'm trying to, can you pack a little bit of waving by agentic coding? Yeah. Yeah. So I think when you, you draw a distinction between more kind of real time response models, you can think of chat to be, to first order as you ask a prompt and then you get a response fairly, fairly quickly. And a more agentic style model where you give it a fairly complicated task. You let it work in the background. And after some amount of time, it comes back to you with what it thinks is something close to the best answer. Right. And I think we see increasingly that the future will look like more of a sync, kind of, you know, where you're asking it very difficult hard things. And you're letting the model think and reason and come back to you with really the best version of what it can come back with. And we see the evolution of code in that way too. I think eventually we do see a world where you'll kind of give very high level description of what you want. And the model will take time and it'll come back to you. And so I think our first launch codecs really reflects that kind of paradigm where we are giving it PR units of fairly heavy work that encapsulate, you know, a new feature or, you know, a big bug fix. And we want the model to spend a lot of time thinking about how to accomplish this thing. Rather than kind of give you a faster response. And you had your question, you know, there's coding is such a giant space. There's so many different angles at it. Kind of like talking about knowledge work or something incredibly broad, which is why I don't think there's one winner and I think there's one best thing. I think there's so many options and I think developers are the lucky ones because they have so many choices right now. And I think that's fundamentally exciting for us too. But to my point, I think this agentic paradigm has been particularly exciting for us. One framing I often use when thinking about product here is I want to build products that have the properties such that, you know, the model gets 2x better. Product gets 2x more useful. And I think, yeah, chat to be has been a wonderful thing because I've heard a long time. I think that was true. But I think as we look at, you know, smarter and smarter models, I think there's some limit to people's desire to talk to like a PhD student versus, you know, they might value other attributes of the model, like its personality and, you know, what it can actually do in the real world. But experiences like codecs, I think they create the right body such that we can drop in, you know, smarter and smarter models and it's going to be quite transformative because you get the interaction paradigm right where people can specify this task, give the model time, and then get a result back. So I'm really excited where it's going to go. It's an early research preview, but just like with chat GBT, we felt like it would be beneficial to get feedback as early as possible and excited where we're going to take it. I was using Sonnet a lot, which I love. I think Sonnet for coding is fantastic, but with 04 Mini Medium setting in Windsurf, I found was great. I found once I started using that, I was really happy because won the speed, everything goes like that. And I think that, and I think they're very good reasons why people like other models, and I don't want to get a comparison. But I found out for me for the kinds of tasks I was using, this was the first time. I was very happy you guys put that out there. Absolutely. Yeah. And, you know, we feel like there's still a lot of low-hanging, if you're in code, it is a big focus for us. And I think we'll find in the near future, you'll find many more good options for the right code model tailored for your use case. Yeah. I find often, if I just need a quick answer, like, I don't write something in Dart, which is a 4.1 and say, but yeah, something bigger. I think that's going to be the harder part is because, yeah, these EVALS are some ways saturated, but also everybody has their own criteria that we look at. And that's going to be kind of a question to sort of see, how are we going to adapt to all that? Right. Yeah. I mean, specifically in code, right? I think there's more beyond, did it get you the right answer? With code, you know, people care about the style of the code. They care about, you know, however, both it was in the comments. It cares about, you know, how much proactive work did the model do for you, right? On other functions. And so I think, you know, there's a lot to get right. And users often have very different preferences here. Yeah. It's funny. I used to, you know, people used to ask me, what domains are going to be transformed by, you know, fastest. And I used to say, you know, it's code because like similar to math and other things, it's very, very, verifiable and decibel. And I think those are the domains that are particularly great to do. RL on, and you know, they're therefore going to see all this, this awesome, you know, agentic stuff just suddenly work. I still think that's true. But the thing that surprised me about code is that, you know, there is still so much of an element of taste in terms of what makes good code. And there's, you know, there's a reason that, you know, people train to be a professional software engineer. It's not because there are a queue gets better because they, they're rather because they learn, you know, how to build software inside an organization. What does it mean to write good tests? What does it mean to write good documentation? How do you respond when someone disagrees with your code? Those are all actual elements of being a real software engineer that we're going to have to teach these models to do. So I expect progress to be fast. And I still think code has a ton of nice properties that make it very ripe. For the gentic products, but, but, but I do think it's very interesting to agree that, you know, the, the, the, the element of taste and style and real world software engineering matters. It's interesting too, because with chat CPT and, and the other models, you're kind of dealing with having to bridge the divide between consumer and pro. I open up chat CPT and I tell my friends like, oh, yeah, because I'll plug it into whatever code model I'm working because I can actually connect it to there. And I think about, you know, well, that's a very different use case. A lot of other people, although I've shown people like how to go in and use, you know, an IDE and actually have it just write documents for you and create folders and stuff, which people don't realize like, yeah, you can do that. You can have chat CPT actually control and do that, which is cool. But then you think about like, okay, we've got a tab now for images. There's the codecs tab. So if I want to connect to GitHub and have it work through there, and there's a Sora into there. So it's kind of interesting to see how all of these things are coalescing into there. How do you differentiate between a consumer feature, a professional feature, and maybe like an enterprise feature? Look, we build a very general purpose technology and it's going to be used by a whole range of folks and unlike many companies, which have this kind of founding user type and then they use technology to solve that user's problems, we do start off with the technology, observe who finds value in it and then iterate for them. Now with codecs, our goal was very much to build for professional software engineers, knowing though that there's sort of a splash zone where I think a lot of other people will find value in it and we'll try to make it accessible for those people as well. There are a lot of opportunities to target non-engineers and personally really motivated to create a world where, you know, or help build a world where anyone can make software. codecs is not that product, but you could imagine this product existing over time. But, you know, as a general principle, it's really hard to predict exactly who the target user is until we made some of these general purpose technologies available because it gets back to the empiricism I was talking about. We just never exactly know where the value is going to land. Yeah, and I think even to dig deeper into that, assuming like, you know, you could have a person who's mostly using Tatchu BU for coding, right? But 5% of the time, you know, they might just want to talk to the model or like, I represent at the time they just want a really cool image, right? And so I think, you know, there are certainly archetypes of people who use the models, but in practice, we see that people want this exposure to different capabilities. Yeah. With codecs and watching the launch of that, it kind of struck me. There are some tools you see that there's a lot of excitement about because there's a lot of internal demand for that. How much are you using it internally? Are tools like that? More and more. Okay. I'm really excited to just say internal adoption. It's everything from, you know, exactly what you'd expect people using codecs to offload the tests to, you know, we have a, um, analyst workflow that we'll look at, you know, logging errors and automatically flag them and slack people about it. So there's all these, these ways that we're, we've actually heard some people are using it as a to-do where like future tasks they're, they're hoping to do, they're starting to fire off codecs tasks. So this is the perfect type of thing that I think you can, you can talk to it internally. And, and, you know, I, I'm very excited about, you know, the leverage that engineers are going to get out of a tool like this. I think it's going to allow us to move faster with, with the people we have and make each engineer that we hire, you know, like 10 times more productive. So, so in some ways internal usage is a very good predictor of what we want to take. Yeah. I mean, we don't want to ship something to other people that we don't find value in ourselves. And I think, you know, leading up to the launch. Laundry buddy. Laundry buddy. Laundry buddy. The essential partner. Okay. Sorry. Sorry. I mean, yeah, we, I mean, we had some power users though that, you know, hundreds of PRs a day that they were generating personally, right? So I think, you know, there are people internally finding a lot of utility from what we're building. Also, the, if you think about internal adoption, it's also a good reality check because, you know, people are busy, you know, adopting new tools and tools takes some activation energy. So actually, the thing you find when you try to talk with things internally is, is, is some of their reality component of how long it takes people to actually adjust to a new workflow. And it's been, it's been humbling to, to watch, right? So, so I think you learn both about the technology, but you also learn about some of the adoption patterns when you're trying to get a bunch of busy people to change the way they're echoed. As you build these tools, internally, people have to learn how to use them and are having to adapt. And there's a lot of question now about kind of what kind of skills do people need in the future? You know, what kind of skills do you look for on your teams? I thought about this a lot. Hiring is hard, especially if you want to have a small team that is very, very good and humble and able to move fast, etc. And I think curiosity has been the number one thing that I look for. And it's actually my advice to students when they ask me, what do I do in this world where everything's changing? Because I mean, for us, there's so much that we don't know. There's a certain amount of humility you have to have about building on this technology. Because you don't know what's valuable. You don't know what's risky until you really study and go deep and try to understand. And when it comes to working with AI, which, you know, we obviously do a lot, not just in code, but in kind of every facet of our work, it's asking the right questions that is the bottleneck, not necessarily getting the answer. So I really fundamentally believe that we need to hire people who are deeply curious about the world and what we do. I care a little bit less about their experience in AI. Mark presumably feels a bit different about that one. But for the product side, it's being curiosity that I've found the most, the best predictor of success. No, I mean, even on research, I think increasingly less, we index on you have to have a PhD in AI, right? I think this is a field that people can pick up fairly quickly. I also came into the company as a resident without much formal AI training. And I think correlated to what Nick said, I think one important thing is for our new hires to have agency, right? Opening has a place where you're not going to get so much of a, oh, here's today, you're going to do thing one, thing to think three. It's really about being kind of driven to find, hey, here's the problem. No one else is fixing it. I'm just going to go dive in and fix it. And also adaptability, right? It's a very fast-changing environment. That's just the nature of the field right now. And you need to be able to quickly figure out what's important and pivot what you need to do. The agency thing is real. I think we often get asked for how does up and you keep shipping and it feels like you're pushing something out every week or something like that. It's funny because it never feels to me. I always feel like we could be going even faster. But I think fundamentally, we just have a lot of people with agency who can ship that comes to product, that comes to research, that comes to policy. Shipping can mean different things. We all do very different things at OpenAI. But I think the ratio of people who can actually do things and the lack of red tape, except where it matters in a couple areas where I think red tape is very, very important. But I think that is what makes OpenAI very unique and it obviously affects the type of people who we want to hire too. I was brought into the company because I was originally given access to GPD3 and I just started showing all these use cases for it and making videos every week for it. And I was annoying people, I'm sure. It was exciting. It was exciting time. I described it to people like, they, I think they built a UFO and I get to play with it. And then I make it hover and like, oh, you made it hover. I'm like, well, they built it. I just pressed the button and got to do that. But that was just what I found very empowering was the fact that I'm self-taught. I learned to code by Udemy courses and stuff and then to be a member of the engineering staff and be told just go do stuff. Nothing too critical. I didn't break anything anybody. And that's good to know that that kind of spirit is still there. And I think that is part of the reason why OpenAI is able to ship even though it was like 150, 200 people worked on GPD4. I think people forget about that. Totally. And honestly, this is how even chat2pt, this is how it came together. We had a research team. They'd been working for a while on instruction following and then the successor to that and post draining these models to be good at chat. But the product effort came together as a hackathon. I remember distinctly, we said, like, who's excited to go build consumer products? And we had all these different people. Like we had a guy from the super computing team who was like, I'll make an iOS app. I've done that in the past life where we had a researcher who wrote some back-end code. And it was this convergence of people who were excited to do stuff. And I think the ability to do so. And I think that's how you get the next chat2pt is running an organization where that is possible and continues to be possible at this scale. Hackathons were my favorite thing because one, being a performer in love and show and tell. But it was just neat to be able to see things that you knew were going to be a product or something later on. Because when you're playing with the technology that's advanced and all that, do you guys still do them? Yeah, absolutely. We've had some fairly recently. And they are typically times last week. Can't say what it was about. But it was an exciting thing. Sure. It's how you find out what's possible. I'm excited to hear that. I do have a question which is how much as it grows, again, when I started, I think 150 people on the company, now there's 2000 and now I see a video with Sam talking to Johnny I and how much is that going to change the character, the spirit of bringing in all this out. I think all the outside ex-martis has been great. We've seen this great run of products. But do you see it changing the culture? Well, I mean, I think probably in the right way. It's like, I think when we look at AI, we don't think of it as some fairly narrow thing. And we've always been in thought by just the potential and all the different things you could build with AI. And yeah, to next point, right? This is why we're able to ship so quickly. These people imagine all these different possibilities. They imagine the future with AI and they try to bring it about. And I think these are facets of that imagination. It's like, what does AI look like if you imagine the AI first device, for instance? When you go from 200 to 2000, you'd think a lot would change. And in some ways it has. But I think people often underestimate the number of things that we're doing. I always feel like being at opening it feels much closer to being in a university where you've got this common reason to being there. But everyone's doing something different and you'll sit down at dinner at lunch and you'll talk to someone and learn about their thing. And you're like, wow, that's so cool that you're doing that. And so it feels much smaller because I think of the broad range of things we're doing and therefore each individual effort, whether or not that's something like chat, GPT, or something like Sora or it's an etc. It's actually staffed in a very, very conservative and lean way that continues to keep people very autonomous and make sure they have resources, etc. So I think it's partly that that has made it feel very, very similar in the good ways to see when I start here. We talked a bit about one of the things you look for is curiosity and Mark said that's helpful too. If I'm somebody outside of AI, okay, if I'm 25 or I'm 50 and I'm looking at the advancement of technology and maybe have it a little bit of fear because I see copywriting is one of the things that chat GPT got great at. Writing code is great. I personally have the opinion that we'll never have enough people creating code because there's more things code can do in the world than we can imagine. And even if it places the copy, my wife showed me the other day on her skin block or sunblock lotion bottle showed me on her sunblock lotion bottle like some very funny copy about like the ingredients. I said, oh, this is not a place I expected to see this, but that's one of the tiny little places that all of a sudden that you can put more thought into it. That being said, I know that I'm a bit of an optimist because I see all these opportunities are places to go in there. What advice do you give people, you know, whatever point they are in life about preparing for or adapting to and being part of the future? You know, I like how Mark just looked right. Oh, I can go. Okay. I'm jumping right now. Yeah. I think the important thing is you have to really lean into using the technology, right? And you have to see how your own capabilities can be enhanced, how you can be more productive, more effective by using the technology. I fundamentally do think that the way this is going to evolve is you still have your human experts. But what I helps the most is the people who don't have that capability at a very advanced level. Right. So if you imagine, right, like, as these models get much better at health care advice, they're going to help people who don't have access to care the most, right? Imension ratio, right? It's not producing an alternative for, you know, experts or, you know, professional artists, allowing people like me and Nick to create creative expressions, right? And so I think it's kind of rising the tide that allows people to be competent and effective at a lot of things all at once. And I think that's kind of how we're going to see a lot of these tools with strapped people. The world's going to change a lot. And I think truly everyone has a moment with the, I does something that they considered sacred and human. I know a guy they got vested in our felt or threatened about his achievements and code, unabilities. Well, that happened for me a long time. I know someone else. Oh, yeah. I mean, yeah, it's definitely better than me at a lot of code problem solving for sure. Yeah, right. So I think it's deeply human to feel some level of all respect and maybe even fear. And I think to Mark's point, be actually using this thing, can demissify it. I think we all grew up or, you know, learned about the word AI in a world where I'm in something pretty different from what we have today, you've got these algorithms that, you know, try to sell you things, try to do things, or you've got movies, you know, where the ethics over, etc. And like that term means so many things to different people that I'm entirely unsurprised that, you know, there's fear. So actually using the thing is I think the best way to have a grounded conversation about it. And then I think from there, the best way to prepare. I think there's some degree to which you need to understand the products and keep up sure. But I think things like prompt engineering or sort of understanding the intricacies of the CI, they're kind of not the right direction. I think sort of, this fundamental human thing's like learning how to delegate that is incredibly important, because increasingly, you know, you're going to have an intelligence in your pocket that it can be your tutor, can be your advisor, can be your software engineer. It's much more about you understanding yourself and the problems you have and how someone else might help than a specific understanding of AI. So I think that's going to be important. Curiosity, I mentioned earlier, I think asking the right questions, you'll get you only get what you put in, right? That's important. And I think fundamentally being ready to learn new things. I think the more you learn, understand how to pick up new topics and domains, etc. The more you're going to be prepared for a world where the nature of work is shifting much faster than it has ever shifted before. So I'm prepared that my job in product is going to look different or not exist at all. But I am looking forward to picking up something new. And I think as long as you bring that perspective, you're well set up to leverage AI. I think we sometimes over index on, you know, sometimes certain jobs go away because like, you know, we don't really need a lot of, you know, typewriter repair people anymore, right? And then certain kinds of coding jobs are probably going to go away. But like I said, I think there's way more opportunity for coders or people to create code, however it's done. And you mentioned like the health field. And that's one of the things I hear people like, Oh, when, you know, when we replace everything with AI, like, well, I mean, I would be very happy having an AI diagnosed me operate on me and probably do everything else. But I do want somebody there to talk me through the procedure and hold my hand. But also, I want people asking questions like, like, you know, every day, take a bunch of vitamins. It's just the right time of day to take it. You know, I can't bother my doctor with all these silly little questions. I really don't think you end up displacing doctors. You end up displacing not going to the doctor. You end up democratizing the ability to get a second opinion. Very few people have that resource or know to take advantage of your resource like that. You end up bringing Medicare into pockets of the world where that is not readily available. And you end up helping doctors gain confidence. You know, I think I often heard from doctors that, you know, they already talked to existing colleagues to get a second opinion. In some cases, that's not possible. And I think you'd be surprised by the number of doctors that use chat to be teeth. Now, on things like medicine, there's work to make the model really, really good. And we're excited to do that work. There's also work to prove that the model is really good because I think you're not going to trust that until there's some degree of sort of legitimacy. And then there's work to explain the areas where the model might not be good because increasingly, once it gets to human and then human level performances, it's hard to frame exactly where it will fall short, which is also hard to sort of reckon with. But nonetheless, I think that opportunity is one of the things that gets me up in the morning. Education might be the other one. And I think there's a tremendous opportunity to help people. What do you think is going to surprise us the most in the next year to 18 months? How does he think it's going to be the amount of research results that are powered, even in some small way by the models that we've built? And one of the kind of quiet things that's taken the field by Storm is the ability of the models to reason. And you already see some research. I'm going to make you explain what you say reason. So this fits into that. I want you to reason through the question. Yeah. Yeah. Think out loud. Exactly. Your traces. Yeah, this, this really fits into this agentic paradigm that we were talking about earlier. And the way that the models approach solving a problem that takes some time to solve is that it reasons through it, much like your, I might, right? If I give you a very complicated. I think you reason probably much better than I do. I mean, I think I I'm flattered with a yeah, like a complicated puzzle, right? You might think to yourself, for instance, this is a crossword puzzle, right? Like you might think through all the different alternatives and what's consistent, you know, is this row kind of consistent with that column. And you're searching through a lot of alternatives. You're backtracking a lot. We're trying a lot of hypotheses. And, and then at the end, right, you come up with a well formed answer. And so the models are getting a lot better at that. And that's what's powering a lot of the advancements in math and science encoding. So this has reached a level where, today, in many research papers, people are using O3 almost as a sub routine, right? There's sub problems within the research problems we're trying to solve, which are just fully automated and solved through plugging into a model like O3. I've seen this in several physics papers, talk to physicists even where they're like, wow, like, I had this expression that I couldn't simplify, but O3 made headway on it. And, and these are coming from some of the best physicists in the country. So I think you're going to see that happen more and more and more and more. And we're going to see just acceleration in progress in fields like physics and mathematics. It's a hard one to beat because, you know, I would swap many things we do in exchange for making it true, you know, significant, you know, scientific advancement. But I think we can, we, we can have multiple of these things. I think for me, it's, it's the fact that any well described problem that is intelligence constrained, I think will be solved in products. And I think we're fundamentally just limited by our ability to do that. So what I mean is like, you know, in companies in the enterprise, there are so many problems that are fundamentally hard that the models are not smart enough to do yet, with software engineering, with running data analyses, whether or not it is providing amazing customer support. There's all these problems that the models fall short at today that are very, very easy to describe and evaluate. And I think we'll make tremendous progress at those. On the consumer side, these problems exist too. They're a bit harder to find just because consumers are worse at telling us exactly what they want. That's the nature of building consumer products. But I think it's very, very worthwhile where, you know, there's many hard things we do in our personal life, whether or not it's doing taxes, whether or not it's planning a trip, whether or not it's searching for a high consideration purchase, whether or not that's a house or a car or a piece of clothes. All of those things are problems where we need just a little bit more intelligence and the right form factor. So I think the other thing that's going to happen in the next year and a half is you'll see a different form factor in AI evolve. I think chat is still incredibly useful interaction model. And I don't think it's going to go away. But increasingly, you're going to see more of these sort of asynchronous workflows. Coding is just one example. But for consumers, it might be sending this thing off to go find you the perfect pair of shoes or to go leave and plan a trip or to go finish your taxes. And I think that's going to be exciting. And we're going to think of AI a little bit differently than just a chatbot. One of my favorite examples, both from a utility point of view capability and then UI was deep research. And deep research is probably the best example we maybe have of probably a gintic sort of model use right now, because it used to be you would ask for a model to tell you about a topic. You would you would either get the data or just do a big search the internet and then it would just summarize all that where deep research will go find some set of data, look at it, ask a question, then go find some new data and come back to it and keep going on. And I think the first time I used other people use it like, wow, this is taking a while. And then you added a UI change so I can actually go away and go do something else. And then the lock screen on my phone will show me this is working, which was a paradigm shift. And I talked to Sam here about that. And Sam said that was a surprise to him was the fact that people would be willing to wait for answers. And now I've seen a new metric for models as how long a model can spend trying to solve a problem, which is a good metric if it ultimately solves it. And that's has this been an update to you and how you think about these things, the idea of like, oh, we don't just want and I guess you talked about this before about agentic, any idea that it's not just give me the answer. It's like, take your time, get back to me. I think, you know, to build a super assistant, you got to relax constraints. Like today you have a product that is, you know, entirely synchronous, you have to initiate everything. That's just not the maximally best way to help people. Like if you think about a real world intelligence that you might get to work with, it has to be able to go off and do things over a long period of time. It has to be able to be proactive. So I think there's like, we're sort of in this process of relaxing a lot of the constraints on the product and on the technology to better mimic a very, very helpful entity. The ability to go do five minute tasks, you know, five hour tasks, eventually five day tasks is like a very, very fundamental thing that I think is going to unlock a different degree of value in the product. So I've actually not been that surprised that people are willing to do that. Like I don't really want to be sitting around waiting for my co-worker either. And I think if the value is there, I'd gladly be doing other stuff and come back. Yeah. And we really don't do it just because, right? We do it out of necessity. The model needs that time to solve the really hard coding problem or the really hard math problem. And it's not going to do it with less time, right? You can think about this as I give you some kind of brain teaser, right? Your quick answer is probably like the intuitive wrong one. And you need that actual time to kind of work throughout the cases to like, are there any gotchas here? And I think it's that kind of stuff that ultimately makes robust agents. We've seen kind of there's like the paper of the moment where somebody comes out and says, I found a blocker. And I remember there was one a month or so ago, and they said models couldn't solve certain kinds of problems. And it wasn't hard to figure out a prompt that you could train into a model and it could solve those kinds of problems. And we had a new one that talked about how they would fail at certain kinds of problem solving ones. And that was kind of quickly, I think debunked by showing that the paper kind of had flaws in there. But there are limitations. There are things that there might be some blockers and things are things we don't know are going to be there. I think brittleness is one of the things there is a point where models can only spend so much time solving a problem. We're probably at a point we're only having the model, maybe two systems watch each other. And we have to think about how a third system stops, the wait for things to break down. But do you see kind of any blockers between here and where I'm getting the models they're going to be solving, doing things like coming up with interesting scientific discoveries? I mean, I think there are always technical innovations that we're trying to come up with. Fundamentally, we're in the business of producing simple research ideas that scale. And the mechanics of actually getting that to scale are difficult. It's a lot of engineering, a lot of research to kind of figure out how to kind of pre-pass a certain roadblock. I think those are always going to exist. Every layer of scale gives you new challenges and new opportunities. So fundamentally approaches the same, but we're always encountering new small challenges that we have to overcome. Just to build on that, I mean, the other business we're in is in building great product with great products with these models. And I think we shouldn't underestimate the challenge and amount of discovery needed to really bring these ever intelligent models into the right environment, whether or not that's giving them the right sort of action space and tools, whether or not that's really being proximate to the problems that are hardest, understanding those and bringing the eye there. So I think there's the technical answer. But I think there's also the real world deployment. And I think that always has challenges that are very, very hard to predict yet worthwhile and part of our mission to do this all. All right. Last question. And I'll begin. It's what's your favorite use or tip for Chachi PT? Mine is I take a photograph of a menu and I'm like, it helped me plan a meal or whatever. I'm trying to stick to a diet or whatever. So yeah, really want that use case. But I've been trying it for wine lists. And that is my eval on multimodality. It's delicious. Really? It keeps embarrassing me with like hallucinated wine recommendations. And I go order it and they're like, never heard of this. So I'm glad yours works. But for me, that's the that's still useful. I mean, maybe the wine lenses too dense. That was a problem. That was a problem. The operator was it like originally was the vision models that too much dense text. It just loses his placement. Yeah. I mean, speaking to deep research, I love using deep research. And you know, when I go meet someone new, when I'm going to talk to someone about AI, right, I just preflight topics, right? I think the model can do a really good job of texturizing who I am, who I'm about to meet and what things we might find interesting. And I think it really just helps with that whole process. Very cool. I'm a voice believer. I it's still got, I don't think it's entirely mainstream yet because it's got it's got many little things that all head up. But for me, you know, half of the value of voice is actually just having someone to talk to you and forcing yourself to articulate yourself. And I find that to sometimes be very difficult to do in writing. So on my way to work, I'll use it to process my own thoughts. And like with some luck, and I think this works most days, I'll have the restructured list of two dudes by the time I actually get there. So voice for me, it needs to be the thing that, you know, I both love using and want to see improve next year.
