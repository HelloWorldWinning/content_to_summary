Timestamp: 2025-07-21T11:26:33.082882
Title: 从Token到思想，大模型到底理解“概念”了吗？ BV1NPgwzREzH
URL: https://b23.tv/DOd17ZW
Status: success
Duration: 5:56

Description:
**摘要**

本文基于杨立坤对大型语言模型（LLM）的长期质疑，详细阐述了他联合署名论文《从头肯到思想》的核心观点、研究方法与主要发现。该论文通过信息论方法，对比了LLM与人类在概念形成、语义结构及信息压缩与意义保留上的异同，并为杨立坤“LLM难以孕育真正理解”的论点提供了坚实的理论依据。

---

**一、 引言：杨立坤对主流LLM技术路线的长期质疑**
*   **核心观点：** Meta首席AI科学家杨立坤（Yann LeCun）认为，主流LLM模型通过预测下一词元来生成文本的模式，本质上无法孕育出真正的理解、推理或质量。他将此视为学术路线的派系之争，此前缺乏实证支持。
*   **新进展：** 其与他人共同署名的论文《从头肯到思想》（2023年6月发表），为这一批判提供了坚实的理论依据。

**二、 论文《从头肯到思想》的核心探讨**
*   **主旨问题：** 人类在接收信息时会自动进行压缩和意义平衡。那么，LLM能否像人类一样，在压缩与意义之间取得平衡，还是仅在海量语料上进行统计性“语和”（组合）？
*   **研究方法：**
    *   **理论基础：** 引入信息论方法，特别是“率失真理论”（Rate Distortion Theory）和“信息瓶颈原理”（Information Bottleneck）。其核心思想是在压缩数据时，需要牺牲多少信息才能保留关键语义。
    *   **实验数据：**
        *   **人类方面：** 使用三套经典的心理学数据集（Roch和Macrochi实验），涉及1049个物品和34个概念类别，研究人类如何分类及判断典型性。
        *   **LLM方面：** 选取数十种主流模型（从0.5B到Llama 70B，包括Bot、Gima、Fight、Mischirton等），提取其静态词嵌入（embedding），并应用聚类分析。
*   **三大核心研究问题 (RQ)：**
    *   **RQ1：** LLM形成概念的类别与人类的一致性？
    *   **RQ2：** LLM内部的语义结构是否也能表现出典型性（如“知更鸟”比“蝙蝠”更像“鸟”）？
    *   **RQ3：** LLM和人类在压缩信息与保留意义之间的平衡方式有何不同？

**三、 论文主要发现与结果**
*   **RQ1 (类别对齐结果)：**
    *   LLM能分对大类，其聚类结果与人类在大方向上高度一致。
    *   尤其是编码器模型（如Bot/BERT）表现出色，甚至优于许多大型生成模型。这表明LLM在宏观上能区分概念。
*   **RQ2 (细节理解与典型性结果)：**
    *   LLM普遍未能捕捉到细节的典型性（如人类认为“知更鸟”比“蝙蝠”更像“鸟”）。
    *   通过计算物品嵌入与对应类别词嵌入的相似度，并与人类典型性打分进行相关性分析，发现LLM的相关性普遍很低，甚至为负。这说明LLM知道这是一类，但不知道哪个成员最典型。
*   **RQ3 (压缩与意义平衡结果)：**
    *   **LLM：** 在平衡压缩与失真度方面更高效，以更少的复杂度、更低的失真形成了紧凑的分类。LLM追求的是统计效率，倾向于“为压缩而压缩”。
    *   **人类：** 分类更混杂，压缩度不高，但保留了大量的上下文和语义细节。人类追求的是灵活性和语境敏感性。

**四、 论文结论与对AI研究的启示**
*   LLM这种“为压缩而压缩”的做法，尽管高效，但难以捕捉人类认知中原型的模糊边界、以及对上下文意义的理解。
*   未来的AI研究应考虑改进LLM的训练目标，不再仅仅优化损失函数，而是加入信息瓶颈、语义保证度等指标，使其思维方式更接近人类。

**核心观点 (一句话总结)：**
大型语言模型在追求统计效率和数据压缩的同时，未能像人类一样有效捕捉概念的典型性、模糊边界及语境化意义，这限制了其对真实世界的理解能力。

**总体框架：**
该内容围绕“大型语言模型（LLM）与人类认知在信息处理（特别是概念形成、语义理解及信息压缩与意义保留）上的对比分析”这一框架展开，以杨立坤的批判性视角为引导，通过一项具体实证研究（《从头肯到思想》论文）来验证和深化其观点。

---
<Mermaid_Diagram>
graph TD
    A["杨立坤对LLM的质疑"] --> B{"LLM本质限制: 预测性而非理解性"};

    B --> C["《从头肯到思想》论文 (2023年6月)"];

    subgraph "论文核心探讨"
        C --> D{"主旨问题: LLM能否平衡压缩与意义?"};
        D --> E[("三大核心研究问题 (RQ)")];
        E --> E1("RQ1: 概念类别一致性");
        E --> E2("RQ2: 语义结构典型性");
        E --> E3("RQ3: 压缩与意义平衡方式");
        D --> F["研究方法"];
        subgraph "研究方法 Methodology"
            F --> F1["信息论方法"];
            F1 --> F1a["率失真理论"];
            F1 --> F1b["信息瓶颈原理"];
            F --> F2["实验数据"];
            F2 --> F2a["人类心理学数据"];
            F2 --> F2b["数十种LLM模型数据"];
        end
    end

    subgraph "主要发现 Findings"
        E1 --> G1("RQ1结果: 宏观类别与人类一致");
        E2 --> G2("RQ2结果: 缺乏典型性理解");
        E3 --> G3a("RQ3结果: LLM高效压缩 (统计效率)");
        E3 --> G3b("RQ3结果: 人类灵活保留 (语境敏感)");
    end

    G1 --> H["论文结论"];
    G2 --> H;
    G3a --> H;
    G3b --> H;

    H --> I["核心区别: LLM重压缩, 人类重意义"];
    I --> J["未来AI研究方向: 改进训练目标, 融入语义保证"];
    J --> K["从词元拼接工具到有思想的理解者"];

    style A fill:#A2D2FF,stroke:#369,stroke-width:2px,color:#333;
    style B fill:#FFB6C1,stroke:#E60000,stroke-width:2px,color:#333;
    style C fill:#90EE90,stroke:#008000,stroke-width:2px,color:#333;
    style D fill:#FFFFCC,stroke:#FFD700,stroke-width:1px,color:#333;
    style E fill:#D3D3D3,stroke:#666,stroke-width:1px,color:#333;
    style E1 fill:#F0F8FF,stroke:#B0C4DE,stroke-width:1px,color:#333;
    style E2 fill:#F0F8FF,stroke:#B0C4DE,stroke-width:1px,color:#333;
    style E3 fill:#F0F8FF,stroke:#B0C4DE,stroke-width:1px,color:#333;
    style F fill:#ADD8E6,stroke:#6495ED,stroke-width:1px,color:#333;
    style F1 fill:#E0FFFF,stroke:#87CEEB,stroke-width:1px,color:#333;
    style F1a fill:#F0F8FF,stroke:#B0C4DE,stroke-width:1px,color:#333;
    style F1b fill:#F0F8FF,stroke:#B0C4DE,stroke-width:1px,color:#333;
    style F2 fill:#E0FFFF,stroke:#87CEEB,stroke-width:1px,color:#333;
    style F2a fill:#F0F8FF,stroke:#B0C4DE,stroke-width:1px,color:#333;
    style F2b fill:#F0F8FF,stroke:#B0C4DE,stroke-width:1px,color:#333;
    style G1 fill:#D8F8D8,stroke:#8BC34A,stroke-width:1px,color:#333;
    style G2 fill:#FFDDDD,stroke:#F44336,stroke-width:1px,color:#333;
    style G3a fill:#D7EAF7,stroke:#2196F3,stroke-width:1px,color:#333;
    style G3b fill:#F7D7EA,stroke:#9C27B0,stroke-width:1px,color:#333;
    style H fill:#FFD700,stroke:#DAA520,stroke-width:2px,color:#333;
    style I fill:#FFA07A,stroke:#FF4500,stroke-width:2px,color:#333;
    style J fill:#C6E0B4,stroke:#4CAF50,stroke-width:2px,color:#333;
    style K fill:#ADD8E6,stroke:#369,stroke-width:2px,color:#333;
</Mermaid_Diagram>

Content:
大家多年以来 人工智能领域的三继头记就是Mata的首席科学教杨立坤首席AI科学教杨立坤杨立坤他一直对主流LLL模型背后的技术路线他是词玩意态度的他跟我说 目前智慧规模型核心任务就是通过预测下一次来生成文本这种模式在本质上没有办法运遇出真正的质量无论这个模型规模这么大都没有办法来负质真正的理解 推理或者类似这个跟大多数的其他的学术路线是有明显的不一样的所以他的观点的长期被认为是学术路线的派系之真但是他缺乏真正的实证资质那最近他有一篇共同署名的认文叫做《从头肯到思想》就是LM跟人类如何在压缩与以意上面之间传很好像为他的批判找到了坚实的理论的依据那么就起来看看这篇2005年6月发表的这篇论文我们首先从这个问题的本身说起人类在接收信息的时候会自动地进行与以下说比如说看照一只这个尾或者一只这个道理这个原内容能够生成但是他理解扭类概念就像人类一样他能够进行压缩和意义的平衡吗还是只是在海量的语料下面进行一个统计的语和呢所以这个作者为了三个核心的问题来探讨大迷团就是所谓的RQ1,RQ2和RQ3这RQ1就是LM形成概念的类别他跟人类的一致吗RQ2就是说LM内部的这个语一结构是否也能表现出点形性比如只跟腰比脱腰更像腰RQ3就是LM和人类在压缩信息跟保留意义之间的平衡方式你有什么不同呢那这篇认文呢他用的是信息的方法来见误引入了两个经济的理论第一个叫做苏罗亚和市征理论就Rade Distortion Syria第二个呢是信息平坚原理就Information bottleneck他的核心思想就是压束数据的时候要牺牲多少的信息才能保留关键的语意在能类这个方面作者用的是三套经典的心理学数据结包括Roch和Macrochi的实验数据都是关于人类怎么进行分类为什么觉得更像是抹类东西的研究总共有1049个物品34个概念的类比在RLM方面做着选了几十种模型从比较小的千万0.5B到拉马70B都有还包括Bot, Gima,Fight,Mischirton主流的这些模型价格他们在集取抹个模型静态苏罗这个引摆令苏罗词的指挥啊不是这个动态数捉然后用这个Keyman实际应据类看模型能不能自动这个深层的类比啊跟人类这个类比能不能对得上那首先看第一个实验RQ1的结果也就是所谓的类比这个对齐那LLM能分对这个大类他发现这个大模型的这个据类啊和人类在这个大方向上还是挺对的尤其是Bot这种编码器的模型甚至比许多的大模型表现还好这个说明什么呢说明LLM在红关上面他能够分轻大概能够知道杀发是杀发自根扬是扭累接下来就是RQ2就是细节理解上面结果做着发现呢这个模型呢能不能捕捉到随时最典型的这个任务上面发现是跟扬比如说人会觉得自根扬比编辅更像扬那LLM呢做着算了每个物品的引摆令和他对应的词的引摆令直接的相思度夸算的相思度再和人类的典型的打分做这个Botman的这个相关就会发现这个相关性普遍都很低有些甚至是负的这也就是说大模型知道这是一类但是并不知道哪个最典型那RQ3的结果就是压缩磨手是这个意义那做得用前面的两个性讯模型来对比了压缩和试真度合成一个打分的指标L结果发现LLM在这个平风体积下更高效他用呢更少的复杂度更低的试真形成的比较紧凑的这个风类而人类这个风类呢更混杂压缩的不紧但是保留了很多的上下温和语意的这个细节也就是说LLM追求的是统计的效率人类这些觉得是灵活性和语境的敏感性所以这个人文最后强调LLM这种为了压缩压缩的做法虽然非常的高效但是很难够普出到人类认知中的远行模糊的编剑上下温的这个意义未来的AI研究呢可以考虑改进LLM的训练的目标不再仅仅只是优化那个Loss方形能不能加落性些平景与一保证度等指标让他们看起来更像人类的这个思维的这种方式所以人类在意的是是不是这个有意义LM在意的是不是能够压缩所以从偷肯到缩的中间还差的情愿所以我们要继续的探索把AI从迟的拼直工具变成有思想的理解着好 我们今天的分享就到这里我们下期视频再见
