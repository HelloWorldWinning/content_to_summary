Timestamp: 2025-07-09T20:59:25.805748
Title: 强化学习无法让模型学会新的思考能力  基座模型决定能力上限  只用一个训练数据RL也能炼出推理 BV1LsGxzLE63
URL: https://www.bilibili.com/video/BV1LsGxzLE63/?spm_id_from=333.1007.tianma.8-3-25.click&vd_source=0eeb7ad2c1a37164e848fbfa306683ca
Status: success
Duration: 48:15

Description:
好的，以下是对原文的总结：

**1. 核心观点总结 (一句话):**

当前大型语言模型的推理能力主要源于预训练阶段形成的BaseModel，后续的强化学习微调更多是提升了现有能力的采样效率和分布，而非引入了全新的推理模式，但蒸馏可能可以引入新模式。

**2. 贯穿的框架 (Overarching Framework):**

文章通过比较大模型的不同训练阶段（预训练、SFT、RLHF）及其对模型“推理能力”（或称思维链生成能力）的影响，探讨了模型能力的真实来源。核心框架是：**大型模型推理能力主要基础来源于预训练阶段，后训练（特别是RLHF）的作用在于优化现有能力的表现（效率、分布），而非从零开始构建新的推理能力，而蒸馏可能是一个引入新能力的方式。**

**3. 大纲式总结:**

*   **引言与研究问题:**
    *   介绍背景论文（作者黄高团队，高关注度）。
    *   核心问题：如何高效生成大型模型思维链（COT Generation / Trinity with Few-shot/Low-Compute），避免使用“Reasoning”一词。
*   **背景：对COT和RLHF的再审视:**
    *   引用Positioning Paper (Banjo et al., "Chain of Thought is not an explainerability")：质疑大模型COT的真实性(Unfaithful)，认为它不反映内部推理过程，不应过度依赖。
    *   传统训练流程：Pretrain → SFT (BaseModel) → RLHF (用于Alignment, COT)。
    *   RLHF作用质疑：多篇工作表明RLHF主要产生Distribution Shift，未根本改变BaseModel。BaseModel在RLHF前已具备良好COT能力（如Echo Chamber, Understanding RLHF等工作所示）。
*   **核心论文发现 (系统研究BaseModel能力):**
    *   系统验证BaseModel已具备推理能力，RLHF未引入新的*根本性*思考模式。
    *   使用Pass@K指标衡量模型内在能力：大K时，BaseModel表现往往优于RLHF模型。
    *   RLHF使模型推理能力范围(Scope)变窄。
    *   RLHF模型的思维路径已包含在BaseModel可生成的路径中。
    *   结论：大模型推理能力存在*上限*，由BaseModel决定。RLHF主要提高对*已知可解决*问题的*采样效率*，而非学习新问题。
*   **替代方法：蒸馏:**
    *   与RLHF不同，蒸馏(Distillation)可以通过学习教师模型的数据来引入新的推理模式。
*   **基于BaseModel能力的高效后训练方法:**
    *   利用BaseModel已有能力，可以在RL训练中提高效率。
    *   **单样本RLHF (1-shot RLHF):** 仅使用一个样本进行RL训练，通过模型在训练过程中的探索（受Entropy Loss/Temperature鼓励），可达到与多样本训练相似的泛化效果（Post-saturation Generalization）。
    *   **少量关键Token RLHF (Few-token RLHF):** 识别COT中的High-Entropy (关键决策点，分叉) 和 Low-Entropy token。RLHF主要影响High-Entropy token的Entropy（提高）。只使用High-Entropy/Focal token训练RL可提高效率和性能，并解释了RL泛化优于SFT记忆（SFT降低高熵Token Entropy）。
*   **总结与未来展望:**
    *   重申BaseModel已具强大推理能力，RLHF主要优化现有能力。
    *   为改进RLHF提供方向：探索与环境互动，引入新经验数据。
    *   未来拓展：将这些发现和方法应用于特定领域（低资源、安全、医疗金融等），寻找领域内Focal Tokens。

**4. Mermaid 概念图:**

<Mermaid_Diagram>
graph TD
    A["大模型能力来源"] --> B["预训练阶段"]
    B --> C["BaseModel"]
    A --> D["后训练/对齐"]
    C --> E["推理能力 / COT"]
    D --> F["RLHF"]
    D --> G["SFT"]
    D --> H["蒸馏"]

    I["COT的质疑 (Banjo等)"] --> E
    J["BaseModel已有能力证据 (Echo Chamber, Understanding RLHF等)"] --> C
    J --> E

    subgraph "核心论文发现"
        C --"通过Pass@K衡量"--> K["BaseModel能力强"]
        F --"主要影响"--> L["RLHF影响"]
        L --> M["提高采样效率"]
        L --> N["能力范围变窄"]
        L --> O["采样BaseModel已有路径"]
        L --"不引入"--> P["新推理模式"]
        C --"决定"--> Q["能力上限"]
    end

    subgraph "不同后训练方法作用"
        F --> M
        F --> N
        F --> O
        F --x P
        H --> R["引入新推理模式"]
        G --"降低关键Token熵"--> S["SFT: 记忆化/泛化弱"]
    end

    subgraph "高效利用BaseModel能力的方法"
        C --> T["高效训练方法"]
        T --> U["单样本RLHF"]
        T --> V["少量关键Token RLHF"]
        V --> W["关注高熵关键Token"]
    end

    K --> Q
    Q --> F
    P --> R
    O --> F
    W --> V

    style A fill:#F9F7D8,stroke:#333,stroke-width:2px,color:#333;
    style B fill:#ADD8E6,stroke:#333,stroke-width:1px,color:#333;
    style C fill:#FFFFCC,stroke:#333,stroke-width:1px,color:#333;
    style D fill:#ADD8E6,stroke:#333,stroke-width:1px,color:#333;
    style E fill:#C3E6CB,stroke:#333,stroke-width:1px,color:#333;
    style F fill:#FFB6C1,stroke:#333,stroke-width:1px,color:#333;
    style G fill:#FFD700,stroke:#333,stroke-width:1px,color:#333;
    style H fill:#90EE90,stroke:#333,stroke-width:1px,color:#333;
    style I fill:#D3D3D3,stroke:#333,stroke-width:1px,color:#333;
    style J fill:#D3D3D3,stroke:#333,stroke-width:1px,color:#333;
    style K fill:#FFFFCC,stroke:#333,stroke-width:1px,color:#333;
    style L fill:#FFB6C1,stroke:#333,stroke-width:1px,color:#333;
    style M fill:#C3E6CB,stroke:#333,stroke-width:1px,color:#333;
    style N fill:#FF6347,stroke:#333,stroke-width:1px,color:#333;
    style O fill:#FFB6C1,stroke:#333,stroke-width:1px,color:#333;
    style P fill:#FF6347,stroke:#333,stroke-width:1px,color:#333;
    style Q fill:#FFFFCC,stroke:#333,stroke-width:1px,color:#333;
    style R fill:#90EE90,stroke:#333,stroke-width:1px,color:#333;
    style S fill:#FFD700,stroke:#333,stroke-width:1px,color:#333;
    style T fill:#B0C4DE,stroke:#333,stroke-width:1px,color:#333;
    style U fill:#B0C4DE,stroke:#333,stroke-width:1px,color:#333;
    style V fill:#B0C4DE,stroke:#333,stroke-width:1px,color:#333;
    style W fill:#B0C4DE,stroke:#333,stroke-width:1px,color:#333;

    linkStyle 5 stroke:#FF0000;
    linkStyle 8 stroke:#FF0000;
    linkStyle 10 stroke:#FF0000;
    linkStyle 11 stroke:#008000;

</Mermaid_Diagram>

Content:
大家好欢迎来到一阶音口的今天我们来录一篇2025年5月份上线的文章这篇文章来自于清华大学和上海交通大学先说一下这篇文章的作者这个作者黄高他是我们之前录的那篇Slow Zero也就是用Rain First Learning进行Self Play那篇文章的作者另外黄高还有一篇非常有名的工作他是当年DanceNet的一作如果你对Company Vision预的工作比较熟悉的话当时ResNet提出来之后有一系列后续的改进的工作这篇DanceNet就是ResNet后续一篇非常重要的改进工作另外这篇工作在短短的不到两个月的时间就已经有112个Settation大家还是非常认可这篇工作的所以今天我就想来给大家录一下这篇工作那我还擅长TopDong的方式先给大家一些我的理解这样帮助大家对这篇Paper有一个宏观的认识然后我们再具体去看一下这篇Paper的一些细节那我在理解Paper的时候喜欢把不同的Paper都给放到一个框架去思考和理解并且把这些Paper所解决的不同的问题都给互相的串起来联系起来这样不光帮助我去把握整个科研领域的脉诺也能帮助我去做一些托展的思考这样能够将思想去继续延伸去解决我自己科研领域的问题所以我初来的把这篇Paper所要解决的问题归达为一个COT generation或者说是Trinity with FacingC的问题也就是说我们在使得大元模型产生这些思维链的时候如何能够比较高效地用少量的样本或者是用少量的算地让这些大元模型去产生思维链那我这里没有太使用Resinning这个词原因是因为最近有不少的工作提出现在大元模型产生的这些思维链其实并不能真正地被看成像人类那样的Resinning过程所以我们在看待这些大元模型Resinning能力的时候可能要小心一点所以我更多的是喜欢把它称为思维链Rush Band-9就在全部角和其他的一些大老门发表了一篇文章标题叫做Channel StartIt's not a explainerability这篇文章主是一个Positioning Paper所谓Positioning Paper就是要提出了一个观点这些观点的证明材料主要来自于其他的一些Paper所以这篇Paper并没有什么实验那Ban9在这篇Paper并提出的一个观点就是我们现在常用的大元模型产生的这种思维链其实是Mise Leading也就是有误导性的我们并不能从过思维链来推断出模型是如何得到这些解论的也就是思维链有可能是Faceful并不真实的大模型类伯思考的过程跟人类并不一样这个思维链和最后得到的结果可能并没有直接的关系所以Ban9在这篇Positioning Paper里面主要就是呼吁大家不要过度地去使用这些思维链尤其是在一些比较敏感的领域比如说遗聊法律或者一些自动化的系统里面因为这些思维链是不准的另外就是对于一些下游的任务尽量要采取一些严格的方法去验证这个思维链的真实性那因为这是一篇Positioning Paper主要的观点就是这个这些类人我就不给大家展开了感兴趣的可以自己去看传统的大便模型训练的方法一般都是先做Pretrene然后再做SFT这篇Positioning Paper是把Pretrene SFT阶段产生的模型叫做Basemode然后一般的方法是会在用RingFurse Merlin在称一波那RingFurse Merlin一般起到的作用是做Alignment还有就是现在常用的产生思维链所以可能大家会觉得这个思维链是通过RingFurse Merlin这一步产生的但其实在领域内已经有不少的工作了提到RingFurse Merlin主要的作用是一个Dissubusion Shift也就是它不会反的Mentaly改变Basemode最后的输出只不过Basemode输出的Dissubusion做一个比较小的SFT那具体这个是什么意思后面会给大家详细的讲解这篇Positioning Paper观察到的一个现象就是在Basemode里面其实它就已经能够产生比较好的COT了只不过这个Basemode的COT和经过RingFurse Merlin训练之后的COT它的Dissubusion是有些不一样的这篇Positioning Paper就用Puzz at K这个指标去横量Basemode和RL训练之后的ModeCOTDissubusion有哪些不一样那我一直有一个观点我认为KM0与没有那种使头里崩喉子出来的研究当然也包括这篇文章其实早在这篇文章之前就已经有一些之前的工作已经讨论过这种Basemode已经具有很好的产生4A电的能力给大家简单的录几篇相关的工作那这篇Paper我觉得跟前面的工作做的不一样的就是这篇Paper单独把这个点凌出来然后做了一些比较系统的实验相对来说比较全面的去讨论的这样的一个问题当然这篇Paper也有一些他自己独特的inside比如说他就提出来COTDissubusioning Furse Merlin没有引入新的Resonin Patent但是Diviseration也就是真流可以引入一些新的PATT那基于这篇Paper和前序的这篇Paper的讨论如果Basemode就已经有比较好的思维量能力了那很容易的就会想到那我们在Trinity这些Basemode的时候如何能够提高思维量产生的这种效率那这里又有两篇后序的Paper一篇Paper就是只用一个以藏头就能够使模型在文芬阶段产生思维量另外一篇Paper提出来那我们可以用Hi-Anti-V的Token去训练大学模型使得他产生思维量的这个效率更高当然这篇Paper还很多那我觉得之所以这些后序的工作能够展开都是基于前面这些工作的一个结论也就是说这些思维量在Basemode里面已经都存在了这样的话你只需要很少量的样本或者是一些比较信息量大的Token去做reference能力就可以从某种程度上换行这个Basemode产生思维量的能力或者是又倒这种Dissolution Shift所以这个就是我在月堵这篇Paper的时候产生的一个思维倒徒业所以我就会按照这样的一个思维去给大家一起读一下这篇Paper以及相关的一些Paper来请大家跟着我这个思维一起走那我们直接开始那我们就先来看一下前任的一些工作这篇Paper来自于哈佛大学在2025年4月份上显这篇Paper取了一个非常有意思的Title叫做IcoChamber坐着这里把Iao的PoseTrinity比预成一个IcoChamber也就是说IaoPoseTrinity其实是在放大在Pricinine阶段大远模型所学到了那些比黑面那我们这里简单的看一下这个图就可以了坐着这里画了一个思维图左半部分是Pricinine右半部分是PoseTrinity在Pricinine阶段按照不同的比例去混合这个训练数据然后做了一些不同的版本在这些不同的版本上面去做于训练这样才会产生一个BaseModel然后他们就会做意外类传经过Pricin的BaseModel能看到这些书出它其实是有一个相对来说比较平均的Title Views也就是这些书出跟他Pricinine里面的数据是有一定的关系的比如说红色的对应的就是这里的于训练数据但是经过PoseTrinity也就是RL的后训练之后用这篇Piper的原话来说就是这个Model会最后converge到了一个Dominant Output的Dissubption其实在Pricinine阶段就已经存在了也就是这里的红色在Pricinine阶段就已经存在了只不过Dissubption产生了一些Shift我们再来看另外一篇Piper叫做Andestanding R10LikeTrinityA Critical Perspective在Depsyc R10的Dissubption就是Perspective在Depsyc R10的Piper上线之后也就是大概在2005年12分的时候有很多的Piper进行跟进这篇Piper就在R10的Piper之后大概一两个月上线我觉得这篇Piper可能是跟我们今天要录的那篇Piper非常接近的一篇这篇Piper其实分了两个部分它第一部分是引入了一个新的GRPO的改进版叫DoctorGRPO因为这部分跟我们今天的主题不太相关所以我就不在这里展开了那第二部分主要是研究了一下R10Trinity它到底给BaseModel带来了一些什么变化我们来看一下这篇Piper这里的TakeWaste我觉得有几个比较重要的结论第一个结论就是TipPiper所研究的所有的BaseModel在R10数年之前其实都已经拥有比较强的数学解体能力了关键的方法就是使用一些弹并这所谓的弹并其实就是一些ProM也就是说你不需要通过RL方法仅仅是在BaseModel里面使用一些简单的ProM就可以让模型去解数学体了第二个TakeWaste就是像千万2.5这一类的模型甚至不需要使用ProM就能得到很好的结果了所以这里做着推测是因为他们在Prytrin的阶段使用了一些Crossing Answering的Tax也就是说在Prytrin阶段就已经引入SFT的一些数据了第三个可能跟我们今天要录这篇Pry果比较相关的几乎在所有的BaseModel里面他们都能发现这个AHA模糊如果大家还记得的话Depsyc R1里面首先体出了这种所谓的AHA模糊是在RainforceModel里春年的过程当中产生的这篇Pry果就提到其实在Depsyc的BaseModel也就是Waste3里面就已经能够看到这样的一些AHA模糊这个表展现的就是前面所提到的Template所谓的Template其实就是比如说R1或者是千万系列所使用的ProM也就是告诉模型要回答一些问题然后把SKAL过程放到Fing这个Tax里面然后把答案放到Answer这个Tax然后做着就用上面提到的那些ProM去测试了一些当前主流的BaseModel大家现在所看到的这些模型全部都是BaseModel没有经过RainforceModel里春年的然后这里主要展示了模型的三个能力一个是QuestionAnswer的能力一个是Explorationability还有一个是Self-Rexaction我们先来看一下QuestionAnswer的能力首先能看到即使不用Template也就是灰色的这个Base这个模型基本上也能回答一些问题唯一的一个例外就是Depsyc V3Base这个模型可能是一个纯粹的BaseModel也就是仅仅经过陪春的一个模型所以在不给Template的时候也就是这个灰线Depsyc V3Base这个模型QuestionAnswer的能力是很差的但是一旦提供一些Template这个模型QuestionAnswer的能力就标升了所以坐着在这里用这个图表达了一个意思就是在这些BaseModel里面对绝大多数的模型来说即使不使用Template这个模型已经具备一定的QuestionAnswer所以它是一个BelieT当然使用了Template之后对于模型些模型来说它的QuestionAnswer的BelieT是有所提高的接下来坐着看的是模型的这种探索能力而这里坐着就采用了PathEdit也就是我们今天要录的那篇Paper所采用的一个指标PathEditK让模型尝试8次只要有一次答案是正确的就认为这个模型尝试答案是正确的所以这个指标主要就是看模型它是否有能力在给定足够多的尝试次数之后可以正确的回答这个问题所以这里很亮的是模型探索的一个能力然后很做表这里的是Template这不同的颜色代表的是不同的Basemodel对于某一些模型比如说上面的这些它的探索能力是比较好的PathEdit的值接近于0.8左右这就是说明即使在没有经过Ruinference能力尘力这些Basemodel它已经具有正确回答这些问题的能力了最后再来看一下Self Reflaction也就是R1Paper所提出的Harmelmen作者在这里用的一个方法就是去数这个思维列里面Keyword它出现的次数比如说经常会看到的weight等等这样的词然后类似的也能观察到对于某一些这些Basemodel即使在没有经过Ruinference能力它已经具有一定的Self Reflaction能力了可以看到这个Count其实并不低而且随着Templature有一定的上升趋势这里做的就举了一个直观的例子对于DipsickV3Base也就是没有经过Ruinference能力吹的Dipsick模型可以看到给定这样的一个数学体它已经能产生Self Reflaction的一些碎链比如说ahaI can use this to getBla-bla-bla还有最近点weightI'm overthinkingLet's try again还有这里的Let's check if we made the error所以这些结果合起来就表明其实在这个Basemodel里面这个模型已经具备了I'll春完之后所展现的那些能力给大家介绍完前人的工作再来看这篇Paper其实整体速度就非常的清楚了那这篇Paper在我看来突出的贡献就是把Basemodel已经具有Rezoning Capacity这样一个观点单独凝出来进行了细织的研究这里的一个词Incentivise已经快被大家用烂了这个词本身的意思是激励的意思最早来自于OpenEye的一个报告Don't teach Incentivise然后Depsyc R1在他的PaperTitle里面也用到了Incentivise这个词几个很多的Paper都演用了这个词那这篇Paper讨论的一个核心的问题就是我们常说的大原模型的Rezoning Ability到底是从哪里来比如说像Depsyc R1Rezoning的capacity可能是从WinFurseMalering训练出来了那这篇Paper就提出了一个观点其实在Basemodel也就是在进行WinFurseMalering训练之前这个模型就已经有Rezoning的capacity了我们来快速地看一下这个Abstract这样大家有一个非常High-level的认识知道这篇Paper到底在讲什么这里提到的WinFurseMaleringWinFurseWireFileboard Rewards就只现在常用的大原模型RL训练的那套框架比如说在R1的Paper里面他们利用数学或者编程这类的问题进行RL训练因为数学或者编程的问题最后的结果非常容易验证所以这样的WinFurseMalering就叫做WireFileboard Rewards也就是它的架励是比较容易产生比较容易验证的那可能一般的认为RLVR是使得现在的大原模型产生4WL的一个重要的手段但是这篇Paper就提出了一个方法他们用这种PASS SDK的指标去比较了一下不同的大原模型在MAS CODE,VGREESIDENT这一系列的BUNCHMARK上面他们的性能这个PASS SDK的意思就是说让这个大原模型尝试K一次只要有一次打完是正确的就认为这个大原模型产生的打完是正确的这就好比我进行3分球的比赛如果让你尝试100次只要有一次你能投进一个3分球说明你是有这个能力投进一个3分球的但如果你还是一个小孩可能比例不够球没有办法人那么远即使让你尝试1000次一万次你可能也投不进一次所以作者就用这种PASS SDK的方法是去评价一个模型它到底有没有这样的能力那如果在尝试K一次当中只要有一次成功那说明这个模型是有这个能力的通过这一系列的实验作者就得出了一个比较有意思的计论他认为当前的2RLVR这里的我后面就简称为2RL方法其实并没有根本上的为打远模型引入一些新的思考的饭式原始是因为当把这个K也就是尝试的次数增大的时候发现这种Base Model也就是没有经过RL训练的模型反而会比经过RL训练的模型这个PASS SDK的指标要更好一些而且他们要做了一些有趣的实验比如说他们就发现经过RL训练的大远模型它的Resetning capacityBondery要更加窄的一些具体什么意思后面会具体的介绍另外他们发现RL训练过的模型它产生的这种Resetning PASS其实已经包括在了Base Model所能产生的这种桑碰一几句Bershen里面所以他们就提出了一个观点这种大远模型的Resetningability其实是有一个上限的这个上限就是有Base Model所决定的最后他们还做了一些有意思的实验比如说他们发现通过真流也就是Destilation反而能够引入一些新的Resetning pattern所以这边配合核心就是想表达一个观点我们现在常用的RL训练方法其实并没有从本质上为大远模型引入一些思考的能力这些思考能力在进行RL训练之前Base Model就已经有了所以这就为继续改进当前的RL训练的方法就提供了一些的线索也许我们可以提出一些新的RL方法通过能和环境的互动去收集更多的Experience Data从而是鼓励模型能够去探索产生一些新的思考饭式总的来说这篇配合就讲了这样的一个故事我们接下来再来具体的看一些结果那图一这里主要是展示了一个试一图这边展示的是Base Model然后这边展示的是Rainforcement Link称过之后的 Model就这里想用一个Trace Search的过程去表达Base Model和RL模等之后这种Sompling的方式是发生的变化这里黑色的实现表示是模型在进行Sompling虚线表示模型没有进行Sompling可以看到在Base Model里面这个模型Somp的方式是比较广经过RL训练之后这个模型Somp的方式更窄了一些并且大家注意就是RLSomp的PASS其实已经存在于Base Model里面了所以作者这里主要想表达了一个意思就是经过RL训练之后这个模型Sompling的Efacency是提高了也就是这些没有太多Reward的这些地方模型就不进行Sompling但是这只是对于某一类问题比如说Problem AR而言但是对于另外一些问题比如说Problem B这些旁之里面它有可能会有一些正确的案子但是因为RL训练的原故导致模型只搜索右边这些分支所以就使得正确的答案搜索不到了所以作者在这里就想表达通过RL训练之后一个不好的地方就是使得Reedsending Capacity的Scope是有所减少的我们再来看一下右边这个图这个千万2.57B是一个Base Model在Base Model上面使用JRPO进行训练这里的不同的STEP表示的是训练的过程所以如果我们先看K.1的时候随着这个线往上走也就是训练的过程不断的加深这个PostK也就是Carage是在不断的增高的但是当这个K增加到256的时候大家看这个趋势是反过来的随着训练的不断加深这个Carage是在不断下降的所以这个图展示的其实就是这个意思也就是Refresue Learning因为去掉了这些Such areaScope of Resinting Capacity是有所降低的我们再来看一下Figure 2作者把这个任务分成了三类以类是这种数学的任务第二类是编程类的任务第三类就是视觉相关的任务这每一行代表的是一个数学的版权然后上面分别是三个千万系列的模型以及一个Lama模型然后绿线表示的是Base Model红线表示的是RL称果的模式纵紫表在这里都是Pascade也就是模型它的Refresue Learning Capacity通过这所有的图我们都能看到一个趋势在K比较小的时候RL的模式基本上都是比Base Model要好也就是红线要比绿线好但是随着这个K增大这个Base Model往往都会超过RL模式所以这个图想表明的一个解论就是当K增大的时候Base Model是逐渐逼近甚至是超过RL训练的模式因为当K很大的时候其实评价的就是这个模型内在的Visual能力其实Base Model是有一定Visual能力的并且这种Visual能力比RL模的还要更大一些更强一些我们再来看一下在Code Generation也就是编程类位地上面是不是也有类似的线线左边这两个图表示的就是在编程问题上面的结果然后这里展示的是一个全文Estruct Model和一个CodeRR1 Model在这篇配复里面把所有没有经过RL训练的模型都叫做Base Model所以这个模糊虽然是一个Estruct Model但因为它没有经过RL所以也把它认为是一个Base Model这个CodeR1是经过RL训练的然后LisDome也可以观察到当随着K增大的时候这个Base Model的Cardwich是在不断的升高甚至会超过RL模糊Lis的带另外一个数据机上也能看到对于这些V9Resign的Banchmark也观察到类似的结论而这里就不再坠说了这里有一个很重要的问题我想给大家提一下也就是这里的对COT进行验证因为这篇Paper我觉得某种程度上来说其实有一个裸机上的漏洞因为它把这个大于模型FinalAnser的准确度和COT的好坏直接画上的等号在前面给大家提到的Rosbando那篇PositionPaper里面就提到COT和模型的FinalAnser之间其实没有直接的英国关系也就是很可能会出现模型FinalAnser是对的但是COT是错当我们在算模型的PASS atK的时候本质上看的是FinalAnser的准确性但其实这篇Paper的作者再用PASS K也就是FinalAnser的准确性来推断模型它思维链是否正确这里就很可能有一个问题就是PASS K虽然很高但它对应的COT有可能是错的这个等号到底能不能画是一个很大的问题所以在这里提到它们是通过人工的方法去检查了一部分的COT比如对于VGEO REASONY它们检查了最难的问题对数据的问题它们也抽查了一些思维链基本的解论就是在它们抽查的这些思维链里面至少有一个是正确的那总的来说这篇Paper通过这种人工抽查的方式从某种程度上去验证COT的正确性但我还是觉得这个地方是否能画等号是值得怀疑的我是比较倾向于ReasonBandjo那篇PASS K的Paper所提出的观点不能检单的认为FinalAnser是对的思维链就一定是对的接下来做了在这里就做了一些Depenesis也就是深度的分析它们做的第一个分析是在RL创过的模型里面这个ReasonYmpath也就是思考的路径其实已经存在于BaseModel了我们直接来看右边的这个图这个图代表的是千万二零二五齐弊它的准确度可以简单的理解为这是一个模型准确度的Distribution所以很做标是准确度但是把它切成了不同的interval比如说这里是零然后零到零点一一次类推一直到一然后两个做目标代表的是frequency和percentive这里有几个重要的结果一个结果就是可以看到经过RL春格的模型也就是这个粉红色的法对于那些在BaseModel里面准确度本来就还不错的情况也就是大概这样的一些区域经过RL训练之后这些就被进一步的增强了所以acresy就集中到了零点就和一这个区域之类但是同时也观察到在acresy零的这个地方RL的这个BAR也很高这个Distribution其实是在亡两边线其实这个图可以这么理解acresy比较低的对应的是比较难的那些题目这些acresy比较高的接触于一的是那些比较简单的题目这样的话在BaseModel那些本来它就可以很好解决的那些题目经过RL训练之后对于这些简单的题目这个模型是更加的自信更加的准确把它解决掉但是也有一个另外一个极端就是对你先比较难的问题如果这个BaseModel本身是不会的话那么这个RL并没有起太大的作用不会仍然是不会并且因为这个BAR更高了所以对于那些之前可能还会但是不是那么准确的题目现在也不会了这也就是前面所提到的经过RL训练之后这个模型的reasoning capacity变得更窄更 narrow了所以作者在这里提出的一个解论就是这个RL的改善并不是因为RL能够帮助模型解决更多的新的问题而只是让模型的这个Sompalient efficiency更高了也就是我们前面看到的The Tracerge的那个图这个Sompalient efficiency的改善是在于那些BaseModel已经能解决的问题上了所以接下来作者又做了一个分析看了一下BaseModel和RL训练之后的模型这个SolverProblem它之间是否有重点我们来看一下这个Table4和Table5这里比较的就是BaseModel和RL训练过的模糊它能解决的问题也就是SolverProblems它的那个index和RL它能解决的这些问题它的这个index可以看到是上面这个BaseModel所能解决问题的index的一个自己上面是在数学问题上下面是在编程问题上面所以这就说明RL的训练并没有让这个模型学会解决的问题只不过还是在解决之前就会解决的问题接下来做了一个Plexity and Nanus6这个Plexity简单的说就是模型生成一段话它的Properability这个只是越低越好因为它有一个复好在前面越低说明模型生成这么一段话的可能性越高这里做的一个实验是这么一个意思比如说让RL训练过的模型产生一个答案然后把这个答案的文字放到BaseModel里面通过这个BaseModel 头肯的Properability去计算Plexity也就是看一下RL模型产生的文字这个BaseModel有多大的概率能产生相同的文字这个值也是越低越好所以通过这个BoxPro的这个我们能看到BaseModel 产生RL训练模型对应的那些Response它对你的Plexity是非常低的这个第一是相对于BaseModel 产生自己的Response 它的Properability而言所以这就间接的说明这个RL的模型它产生的答复其实已经包含在了BaseModel所产生的答复里面从Lykerhood的角度来讲是这样的所以坐着在这里就做了一个简单的总结这里主要提了三点第一点就是他们观察到这个可以被RL模型所取的问题其实已经可以被BaseModel所解决了RL模型所带来的改善主要是做更高效的Sompaling所以RL并不能帮助模型去学习解决新的问题第二点就是经过RL训练之后这个模型它的Resoning Carriage其实变得更窄了一些相对于BaseModel第三点经过RL训练之后的模型Resoning Pass思考路径其实已经在BaseModel里面了所以最后坐着就得到了一个解论我觉得也是这篇文章的核心解论就是RL并没有给模型带来访的满头里New Resoning Carpacity并没有从本质上让模型学会一些新的思考饭式这些思考的能力饭式其实已经在BaseModel里面了所以坐着认为这个BaseModel其实就提供了一个AppleBound从当前的RL训练方法来看它是不可能超过BaseModel所提过的这个AppleBound然后坐着接下来又看了一下不同的RL算法是否也能观察到类似的现象最上面的这个是一个BaseModel下面是不同的训练方法这个图是在不同的训练级上面有印斗卖和Otof斗卖这个图主就是比较的K.1也就是图最左点和K.256也就是最右边这个差距所以通过这个图基本上我们也能看得到相似的结论就是这个差距跟算法没有本质的关系所以前面观察到的那些现象或者是结论很可能跟具体的RL算法没有关系很可能就是现在所使用的RL的这整套思路或者是饭式导致了观察到的这样一个现象为什么现在RL训练会导致出现这些现象呢其实我们只观上想一想也很容易理解我就以加PO这个图来举例吧这是因为在RL训练的时候这个PolicyModel也就是大圆模型本身要产生很多的思维量或者是答案也就是这里的O1一直到OG然后根据这些Sumphold的结果产生Reword如果有正确的答案那么就给Reword所以这里面其实隐含了一个前提也就是PolicyModel或者我们直接就说BasedModel它必须先能产生正确的Jectory或者是正确的思考路径你才有可能在后面计算出Poditive的Reword你才有可能后面计算Otwenditch去更新模型的参数那换一个角度如果这个BasedModel它不能很好的产生正确的Rezoning Pass那么这个里面就没有导致产生正确答案的路径了所以你后面就不可能产生Poditive的Reword了也就是说你不可能用RL去训练一个这样的模型因为你得不到Poditive的Reword这就好比一个老师教一个学生这个学生也有基本的能力它能听说读写能和你交流作为老师你只需要引导它就可以了如果一个学生缺乏基本的能力听说读写都不会跟它沟通都是一个问题那么你就没有办法去引导它所以这样的模型你是没办法用RL进行训练的所以从只是个晚上也很容易理解这篇Paper所提出的这样的观点那一个潜在的解决思路就是像最近RichSatan所写的Aero of Gisbeganance一样让这个模型去跟环境互动然后在这个互动的固定当中产生很多的Observation或者是Experience或者新的经验知识这有可能是一个潜在的解决方向那既然ARO不能引入新的Rewording Pass的话那还有别的什么方法吗坐着在这里就看了一下Disturation我们来看一下这个图上面黑线这个就是Disturation所对应的模型然后仔线是做的InstructFan Tuning这个红线是ARO然后横众做标跟之前的图类似对于ARO我们能看到当K变大的时候这个BaseModel也就是这个率线其实要超过这个红线的所以就表明了Receiving CapacityBaseModel要比ARO model要更大一些但是如果我们看Disturation的这个模式的话很有意思它一直在ARO和BaseModel之上哪怕当K特别大的时候Disturation对应的模式也要比BaseModel和ARO model这个Carage要更好一些所以通过这简单的这个图坐着就想说Disturation其实是可以引入新的Receiving Pattern从直观上也比较容易理解因为它没有ARO在训练过程当中产生的那些Royal这些Royal来自于大圆模型本身ARO训练就是使用这些Royal计算Reward所以如果BaseModel不会产生新的Receiving Pattern那自然ARO训练之后这些Royal的里面也不会有新的Recenting Pattern所以训练之后的模型也不会有新的Recenting Pattern但是Disturation本身是做的一个Suprise Learning也就是这些训练的数据来自于一个更强大的Recenting model所以这个训练数据里面包含了新的Recenting Pattern就是BaseModel是没有见过的所以Disturation是可以引入新的Recenting Pattern所以直观上也是比较容易理解所以通过前面的工作我们就能得到一个解论BaseModel已经有很强大的Recenting Pattern那接下来我们自然就会想到一个问题如果BaseModel已经这么强大了我们再进行Recenting Pattern Learning的时候如何利用这个特性呢这篇Paper就从这个角度做了一些有趣的实验它这里主要提出来了就是如果想提高模型的Recenting能力的话Recenting Pattern Learning的时候其实只需要一个存在一个帐篷我们直接来看一下这个Fig1它这里的OneShout就是只有一个存在一个帐篷然后派表示的是不同的一个帐篷比如说派1派13Tusha的就是两个一个帐篷然后这里的虚线表示的是之前的那种在大量数据上面训练的RL左边是一个数据级右边是在6个数据级上面平均的结果我们来看一下这个OneShout的结果虽然在训练的早期收链比较慢还有一些帮品但经过足够长的时间训练之后最后只用一个E-Zampo训练所达到的效果和使用更多数据所训练所达的效果基本是一样的在MAS500这个数据上是这样的在6个Banchenmark上平均的结果也能观察到这样的一个现象这里的E-Zampo到底是一个什么E-Zampo呢做了这里就举了一个例子这个问题其实是一个物理的问题大概是给定一些压强和速度等物理量要算一下这个风速做了在这里给了两个比较有意思的结论第一个就是其实这个问题给的标准答案是有点问题的正确的标准答案应该是12.7但其实数据里面给的是12.8有一个45入的错误第二个就是这个Base Model其实已经基本上能解决这个问题了只不过在最后的答案是错的但是中间的思考过程是基本上对的那你有可能会问在一个数据上面训练难道不会overfitting吗所以坐着在这里就做了一个实验然后这两个图分别对的是两个不一样的训练数据很做标就是训练的过程综做标就是acresy这个蓝色的线就是春领的acresy可以看到在训练刚开始这个模型就已经overfitting了因为在训练数据上面acresy到了100%但是在测试数据上面这个acresy还是在不断地缓慢地提高的在另外一个数据上面也能观察到同样的线下当各的训练数据上面overfitting但是测试数据上面性能是在不断地提高虽然一个数据导致模型overfitting但是模型似乎还是在不断的学习和改善所以坐着这里就取了一个名词叫做post saturation 见到来自于神在保合之后仍然还在放话这个线像在一般的arrel训练也就是使用大量的数据进行arrel训练上面是观察不到的这个蓝色的部分是在大量训练数据上面的acresy这个trinning的acresy没有保合一直在提高但是taskedacresy确实屈于保合甚至有一点点下降坐着在这里就举了一个例子左边这一列表示的就是一个trinning的彰抗右边就是一个测试的彰抗然后这里表示的是随着新练的步骤增加这个模型它的答案会发生什么变化可以看到在刚开始的时候也就是stabling这个base model 答案是12.88也非常的接近正确答案然后在这一个例子上面训练500步的时候已经能得到正确答案了比较有意思的是当训练到1860步已经开始orfitting的时候这个模型的输出已经变得非常的奇怪也就是它自己的语言了但是答案是正确的比较有意思的是即使在1860步模型已经orfitting到这一个训练样本上去开始出现一些自己独特的语言但是在测试的数据上面1860步的时候这个模型完全是正常的甚至还提出来一个新的方法使用rational root theory这个去解决这个问题问题回答答案也是正确的所以这就是前面做者提到的post saturation 进入来自ation春季数据上面虽然已经satrate开始orfitting但是在测试几上面仍然在generalize这是一个很有意思的现象做者在这里就做了一些oblation的starly最后的结论就是这里观察到的post saturation 进入reization跟anthropy loss是有一定关系的他们发现如果不用anthropy loss也就是这里的一个浅蓝色的虚线当训练到达一定程度的时候这个acresy基本就有保合了但是如果加上anthropy loss并且让这个tamperature稍微大一点的话这个测试的acresy是在不断的提高的我觉得这就是前面我们所提到的在reinferstenerning的阶段需要这个base model去产生一些rowout也就是各种各样不同的答案那这个anthropy loss或者是这个比较大的tamperature就鼓励模型去产生一些不一样的解析思路所以虽然只有一个训练样本但因为二要训练过程当中模型仍然在做不同的尝试所以这些新的rowout让模型仍然可以学习所以它的测试acresy还是在不断提高的当然所有的这些都必须建立在这个base model是有很强的reinferstenerning可牌随体这个结论支主之上的前面那篇paper提供了一个思路是在reinferstenerning的时候只使用一个影藏搜让模型产生reasling的obility那这篇来自千万的paper我觉得就更有意思了可以只使用少数的token让模型通过reinferstenerning让模型产生reasling的能力那之所以能这么做这背后有一个观察作者发现在思维练的这些token里面其实有我养类token第一类作者把教做highandropy minority token这种就好像你在爬山的时候在那些比较关键的部阻有分差的时候对你的那些token也就这里红色所标是出来的那这些token就决定了下面思维练的深沉过程或者是走向另外一类token是lowandrype majoritytoken也就是说它的信息上比较小而且这类的token大量的存在这就好比在reasling parts上面是那些直路的点没有分差那这里做着就举了一个简单的例子比如说在解决一个数学问题1加1等于几那些比较关键的highandropy token就是这里的一些连接词but while这种是帮助模型在定义思练走向的那些token而像其他的比如说translate to base2等等都是lowandrype只需要follow前面深沉的那些token即可又把这个图展示的地片配合了核心思想如果我在进行rx练的时候只使用这些highandropy或者是falking tokens也就是分差的那些token效果会如何呢粉色的就是只使用这些falk tokens蓝色的使用所有的token可以看到在这两个数据级上面这个模型的性能都是有提升的我们接下来看一下这个图做着就化了一下磁云上面这些是highandropy token的思云下面是lowandrype token的思云可以看到那些比较重要的falk tokens基本都是一些连接词比如说supposed scenes actuallyfars还有最经典的wait等等对于那些lowandrype tokens就是这种比如说AB可能是取觉得只考signterm这些可能跟数学有关的一些符号然后左边这个图展示的是一个andrype分布所以做着在这里就用了一个8020入只是用highandropy token去训练模型然后做着在这里就观察到了几个有意思的现象ring for simulon里在模型的训练过程当中主要保留了模型已经存在的这种andrype pattern我们来看一下这个table one这里展示的就是based model和r训练之后的model随着训练的部数这个token overlapped ratio这里主要比较的是那些highandropy tokens可以看到随着这个训练的部阻的增加这个overlapped ratio基本上还是保持在一个比较大的就说明rl并没有很大的改变based model的andrype token的分布那r主要改变的是什么呢就是这些highandropy token做着在这里就展示了这样一个图很做标就是训练的过程做做标就是andrype这里的展示的是不同andrype token100%touch这就是andrype最大的那些token这就是andrype很低的那些token然后一次类推86是就是据中间的那些token随着rl的训练对于那些low andrype的token比如像这些这些基本是没有太大变化但是如果我们看这些highandropy的token它的andrype是在不断的增大的所以通过这个结果我们能看到rl训练其实改变的就是这些highandtype比较重要的那些token所以做着在这里就提出来那既然如此我就没必要用所有的token我就只使用那些focult token也就是比较重要的highandropy token就可以了所以这几个图展示的就是这样的一个结果可以看到随着训练的增加如果只使用focult token的话比使用所有tokenaccreasy要更高一些右边这个展示的是response lance相比较使用所有的token这个response lance会更长一些做着在这里把token又具体的分了一些类top10%的也就是那些安卓比最大的token然后以此类推bottom就是指安卓比特别小的那些百分之百就是使用所有的token如果使用bottom的那些信息量比较少的token进行训练的话这个模型的性能是没有使用highandropytoken训练性能要好的做着在这里还做了一个有趣的sgaling loss实验很做不要表示的是模型的残数纵作标是accreasy左右是两个不同的数据类这个栏线表示就是用所有的token进行训练红色表示的就是指使用focult token如果我们指使用少数的focult token的话这个性能会更好一些那如果这样的话我们在训练的时候就没必要使用所有的token指使用这些关键的token进行训练就可以了这样能够极大的结成训练的成本更容易的把模型给sgale up上去做着在discarction这部分也做了一些我觉得非常有意思的讨论第一个讨论就是为什么rl generalizesft memorize这是一篇应该是Google的非常有名的paper做着在这里提出来的一个解释就是在rl进行训练的时候这些focult tokens被rl给保留下来了甚至会增加这部分focult token的anthropy所以并没有太改变被smodel的这些reasoning pass但是对sft来说它在训练的时候会 push out你的log所以你可以理解为它是整体在改变这些token的分布所以这就会导致这些focult tokens的anthropy减少所以使得这个reasoning pass变得不是那么的灵火然后这里的灵火性可能就是一个放华性很重要的原因所以这就解释了为什么rl有很好的放华性但是sft却没有做着今天来又讨论的一下这个大约模型cout它这里面的token为什么是一些rl完准会批的token和anthropy的token混在一起做着认为这是因为大约模型是在大量的数据上面进行ptrin的这个ptrin就设置到了一些prire-lullet并且需要人类看到懂的这种语言的流畅性所以这就导致模型在产生这些思维链的时候就会出现这种hannthropy和lowanthropytoken混在一起的现象但这里有一个潜在的问题就是传统的这些wing-thrustman能力的方法它其实是有一个假设就是假设这个action的anthropy注意这个大约模型里面action其实就是头肯这个rl是要假设一个uniform也就是平均的actionanthropy分布所以这篇paper就针对这样的一个特点对rl的方法做了一些改进只是使用某一特定类型的token进行训练也取得了比较好的效果这一系列的paper我就给大家露完了这里给大家做一个小节这一系列的paper主要就是在讨论现在大约模型这个base model它其实已经有很强的reasoning capacity了这个rl具体它起了作用是什么它并没有对base model做很大的改动只是做了一些微小的disputant shift如果从头肯角度来讲的话它只是对那些hatchupy token做了一些改动那沿着这个思路后续有很多的paper开始跟进如何能够在rl这个框架下更高效地去训练模型产生cout比如说只使用一个e-zamp或者是只使用少量的头肯那我觉得如果你掌握了这整个领域在这一块的发展的慢落化那我们很容易就可以在进行外推做一些future那我的建议就是要结合你自己的个人兴趣和你自己的研究方向去做一些外推比如说如果你是做dormotite那你是否可以想一下在dormotite领域这个结论是否仍然成立是否在dormotitebase model里面reasoning capacity就已经存在是否在dormotite的这些问题里面也存在这种hatchupy或者是focking token的这种现象那如果你是做比如安全领域那对于安全领域的一些问题是否也有类似的这样的一些现象我们再用rl做alignment去加强模型的safety的时候是否某些safety相关的特性在base model里面就已经存在了而又或者你是在医疗金融等相关的领域这些领域里面数据缺乏那你是否可以套用这些方法使用少量的样本用rl去翻听你的模型你是否能找到你特定你领域内那些focking tokens用他们去进行rl为条那如果你读了这些paper有什么好的想法欢迎你来我们的社区和大家一起讨论社区的地址就是easyincode.comeasy和incode之间有一个很限今天的视频就到这里了谢谢大家
