Timestamp: 2025-04-09T16:01:06.883671
Title: Reinforcement Learning with Neural Networks: Essential Concepts
URL: https://youtube.com/watch?v=9hbQieQh7-o&si=aJ0EOKHb4OltDQgB
Status: success
Duration: 23:59

Description:
好的，下面是对提供的文本的总结，包括核心思想、整体框架、以及Mermaid图。

**总结**

**核心思想：**
强化学习中的策略梯度通过猜测理想输出值并结合奖励反馈，即使在没有预先确定的目标值的情况下，也能训练神经网络。

**整体框架：**

I. **引言：**
    *   强化学习用于在没有预先确定的理想输出值的情况下训练神经网络。
    *   策略梯度是一种强化学习算法。

II. **传统反向传播回顾：**
    *   传统反向传播依赖于已知的理想输出值来计算损失函数和梯度。
    *   梯度用于调整神经网络的参数（例如，偏差）。

III. **策略梯度的核心思想：**
    *   如果不知道理想的输出值，则通过猜测（guess）来代替。
    *   对猜测结果进行反馈（奖励，reward）来调整参数。

IV. **策略梯度的步骤：**
    *   A. **前向传播：**
        *   将输入值输入到神经网络中，得到一个概率输出。
        *   根据概率选择一个行为（例如，选择去哪个餐馆）。
    *   B. **猜测（Guess）：**
        *   猜测这次行动是正确的。基于这个猜测，计算理想输出值。
    *   C. **计算梯度（Calculate）：**
        *   计算基于猜测的输出值和神经网络实际输出值之间的差异。
        *   使用该差异计算梯度。
    *   D. **奖励（Reward）：**
        *   基于行动的结果，得到一个奖励值。
        *   如果行动是好的，奖励为正；如果行动是坏的，奖励为负。
    *   E. **更新梯度（Update）：**
        *   将梯度乘以奖励值，得到更新后的梯度。
        *   使用更新后的梯度来调整神经网络的参数。

V. **实例演示：**
    *   通过“去哪个餐馆吃薯条”的例子，详细说明策略梯度的每一步。
    *   例子中，神经网络根据饥饿程度输出选择餐馆的概率，策略梯度根据实际情况（薯条量是否符合预期）来调整神经网络的偏差。

VI. **奖励的重要性：**
    *   奖励用于纠正猜测中的错误。
    *   正奖励表示猜测正确，负奖励表示猜测错误。
    *   奖励的大小可以影响参数调整的幅度。

VII. **总结：**
    *   策略梯度允许在没有已知输出的情况下训练神经网络。
    *   猜测和奖励是策略梯度的关键组成部分。

<Mermaid_Diagram>
```mermaid
graph LR
    subgraph Data Input [数据输入]
    A[输入状态 (例如: 饥饿程度) ]:::input
    end

    subgraph Neural Network [神经网络]
    B[神经网络模型]:::model
    C[输出概率 (例如: P(去Norms), P(去Squatch))]:::output
    end

    subgraph Action Selection [行动选择]
    D[根据概率选择行动 (例如: 去 Norms)]:::action
    end

    subgraph Reward System [奖励系统]
    E[执行行动]:::execute
    F[观察结果 (例如: 薯条量)]:::observe
    G[评估行动 (奖励)]:::evaluate
    end

    subgraph Gradient Update [梯度更新]
    H[猜测理想输出]:::guess
    I[计算梯度]:::calculate
    J[应用奖励]:::apply
    K[更新模型参数 (例如: 偏差)]:::update
    end

    %% Define node styles
    classDef input fill:#f9f,stroke:#333,stroke-width:2px
    classDef model fill:#ccf,stroke:#333,stroke-width:2px
    classDef output fill:#ffc,stroke:#333,stroke-width:2px
    classDef action fill:#cff,stroke:#333,stroke-width:2px
    classDef reward fill:#cfc,stroke:#333,stroke-width:2px
    classDef gradient fill:#fcc,stroke:#333,stroke-width:2px

    %% Define directional flow with arrows
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> J
    C --> H
    H --> I
    I --> J
    J --> K
    K --> B

    %% Add text descriptions
    linkStyle 0,1,2,3,4,5,6,7,8,9,10 stroke-width:2px,stroke:#333;
```
</Mermaid_Diagram>



Content:
WEBVTT Kind: captions Language: en when you don't know take a guess and when you don't know take a guess and when you don't know take a guess and then see what happens and update your then see what happens and update your then see what happens and update your guess after that stack guess after that stack guess after that stack Quest hello I'm Josh Starmer and welcome Quest hello I'm Josh Starmer and welcome Quest hello I'm Josh Starmer and welcome to Stack Quest today we're going to talk to Stack Quest today we're going to talk to Stack Quest today we're going to talk about reinforcement learning with neural about reinforcement learning with neural about reinforcement learning with neural networks and we're going to cover the networks and we're going to cover the networks and we're going to cover the essential essential essential concepts this stack quest is brought to concepts this stack quest is brought to concepts this stack quest is brought to you by the letters A B and C a always B you by the letters A B and C a always B you by the letters A B and C a always B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B C C C Curious always B Curious always B Curious always B curious note this stack quest assumes curious note this stack quest assumes curious note this stack quest assumes that you are already familiar with the that you are already familiar with the that you are already familiar with the main ideas of how neural networks work main ideas of how neural networks work main ideas of how neural networks work as well as the basics of back as well as the basics of back as well as the basics of back propagation if not check out the quests propagation if not check out the quests propagation if not check out the quests or my book on AI or my book on AI or my book on AI now imagine it was snack time and we had now imagine it was snack time and we had now imagine it was snack time and we had to choose between going to Squatch's Fry to choose between going to Squatch's Fry to choose between going to Squatch's Fry Shack or Norm's Fry Hut for a delicious Shack or Norm's Fry Hut for a delicious Shack or Norm's Fry Hut for a delicious French fry snack there are a lot of ways French fry snack there are a lot of ways French fry snack there are a lot of ways we could pick one of the two places for we could pick one of the two places for we could pick one of the two places for a snack for example we could just flip a a snack for example we could just flip a a snack for example we could just flip a coin if we got heads we could go to coin if we got heads we could go to coin if we got heads we could go to squatches and if we got tails we could squatches and if we got tails we could squatches and if we got tails we could go to norms however just flipping a coin go to norms however just flipping a coin go to norms however just flipping a coin wouldn't take into account how hungry we wouldn't take into account how hungry we wouldn't take into account how hungry we were and some days we might be more were and some days we might be more were and some days we might be more hungry than others also flipping a coin hungry than others also flipping a coin hungry than others also flipping a coin doesn't take into account how many fries doesn't take into account how many fries doesn't take into account how many fries we get at each place for example we we get at each place for example we we get at each place for example we might usually get a lot of fries at might usually get a lot of fries at might usually get a lot of fries at norms and that would be great if we were norms and that would be great if we were norms and that would be great if we were really hungry but maybe less great if we really hungry but maybe less great if we really hungry but maybe less great if we were not so hungry it's also possible were not so hungry it's also possible were not so hungry it's also possible that Norm might give us a skimpy order that Norm might give us a skimpy order that Norm might give us a skimpy order of fries and if we were really hungry of fries and if we were really hungry of fries and if we were really hungry that would be a total bummer in contrast that would be a total bummer in contrast that would be a total bummer in contrast if we were not very hungry it might not if we were not very hungry it might not if we were not very hungry it might not be so be so be so bad likewise squatch might serve us a bad likewise squatch might serve us a bad likewise squatch might serve us a skimpy order of fries or a large one and skimpy order of fries or a large one and skimpy order of fries or a large one and depending on how hungry we were each depending on how hungry we were each depending on how hungry we were each order could be good or bad so it would order could be good or bad so it would order could be good or bad so it would be nice to have a way to decide which be nice to have a way to decide which be nice to have a way to decide which place to go to for a snack that took our place to go to for a snack that took our place to go to for a snack that took our hunger into account as well as the hunger into account as well as the hunger into account as well as the possibility that we could get a large or possibility that we could get a large or possibility that we could get a large or skimpy order of fries at each place to skimpy order of fries at each place to skimpy order of fries at each place to solve this problem we're going to use a solve this problem we're going to use a solve this problem we're going to use a neural network that takes our hunger as neural network that takes our hunger as neural network that takes our hunger as input and gives us the probability that input and gives us the probability that input and gives us the probability that we will go to norms porm as the output we will go to norms porm as the output we will go to norms porm as the output the only problem is that we need to the only problem is that we need to the only problem is that we need to train this one train this one train this one bias now usually when we want to train a bias now usually when we want to train a bias now usually when we want to train a neural network we would start with a neural network we would start with a neural network we would start with a training data set like this that has training data set like this that has training data set like this that has possible input values paired with their possible input values paired with their possible input values paired with their ideal output values with this type of ideal output values with this type of ideal output values with this type of training data we could train our neural training data we could train our neural training data we could train our neural network using standard back propagation network using standard back propagation network using standard back propagation however in this example we don't know in however in this example we don't know in however in this example we don't know in advance if norm or squatch will serve us advance if norm or squatch will serve us advance if norm or squatch will serve us a large or small order of fries and that a large or small order of fries and that a large or small order of fries and that means we don't know in advance what the means we don't know in advance what the means we don't know in advance what the output value should be josh if we don't output value should be josh if we don't output value should be josh if we don't know what the output value should be how know what the output value should be how know what the output value should be how can we train our neural network when we can we train our neural network when we can we train our neural network when we don't have known output values in a don't have known output values in a don't have known output values in a training data set we can still train our training data set we can still train our training data set we can still train our model using reinforcement learning there model using reinforcement learning there model using reinforcement learning there are lots of different reinforcement are lots of different reinforcement are lots of different reinforcement learning algorithms for training neural learning algorithms for training neural learning algorithms for training neural networks but in this video we'll focus networks but in this video we'll focus networks but in this video we'll focus on one called policy on one called policy on one called policy gradients gradients gradients bam note the goal of this stack quest is bam note the goal of this stack quest is bam note the goal of this stack quest is to provide you with a solid to provide you with a solid to provide you with a solid understanding of the essential concepts understanding of the essential concepts understanding of the essential concepts of how policy gradients work that said of how policy gradients work that said of how policy gradients work that said if you want to see all of the if you want to see all of the if you want to see all of the nitty-gritty mathematical details worked nitty-gritty mathematical details worked nitty-gritty mathematical details worked out step by step check out the follow-up out step by step check out the follow-up out step by step check out the follow-up quest now before we dive into quest now before we dive into quest now before we dive into reinforcement learning and policy reinforcement learning and policy reinforcement learning and policy gradients I want to do a quick review of gradients I want to do a quick review of gradients I want to do a quick review of how traditional back propagation works how traditional back propagation works how traditional back propagation works so we can understand its limitations and so we can understand its limitations and so we can understand its limitations and see how policy gradients overcome them see how policy gradients overcome them see how policy gradients overcome them so for now let's just assume that we so for now let's just assume that we so for now let's just assume that we have this training data and know the have this training data and know the have this training data and know the desired outputs in desired outputs in desired outputs in advance given this data we could run the advance given this data we could run the advance given this data we could run the input values through the neural network input values through the neural network input values through the neural network one at a time and quantify the one at a time and quantify the one at a time and quantify the differences between the output from the differences between the output from the differences between the output from the neural network and the ideal output neural network and the ideal output neural network and the ideal output values then if we wanted to we could values then if we wanted to we could values then if we wanted to we could calculate these differences for calculate these differences for calculate these differences for different values for the bias and draw a different values for the bias and draw a different values for the bias and draw a graph that shows how the differences graph that shows how the differences graph that shows how the differences change with different bias values then change with different bias values then change with different bias values then using a value for the bias we could using a value for the bias we could using a value for the bias we could calculate the derivative of the calculate the derivative of the calculate the derivative of the differences with respect to the bias and differences with respect to the bias and differences with respect to the bias and because the derivative the slope of this because the derivative the slope of this because the derivative the slope of this green line is negative we would know green line is negative we would know green line is negative we would know that the value for the bias that that the value for the bias that that the value for the bias that minimizes the differences and thus minimizes the differences and thus minimizes the differences and thus results in a better fitting model is results in a better fitting model is results in a better fitting model is larger than the current larger than the current larger than the current value in other words when the derivative value in other words when the derivative value in other words when the derivative the slope of this green line is negative the slope of this green line is negative the slope of this green line is negative then we need to shift the value for the then we need to shift the value for the then we need to shift the value for the bias to the right on the xaxis bias to the right on the xaxis bias to the right on the xaxis alternatively if we use this value for alternatively if we use this value for alternatively if we use this value for the bias to calculate the derivative the bias to calculate the derivative the bias to calculate the derivative then the derivative the slope of this then the derivative the slope of this then the derivative the slope of this green line would be positive telling us green line would be positive telling us green line would be positive telling us that the optimal value is less than the that the optimal value is less than the that the optimal value is less than the current value for the current value for the current value for the bias in other words when the derivative bias in other words when the derivative bias in other words when the derivative the slope of this green line is positive the slope of this green line is positive the slope of this green line is positive then we need to shift the value for the then we need to shift the value for the then we need to shift the value for the bias to the left on the xaxis bias to the left on the xaxis bias to the left on the xaxis in summary when the derivative is in summary when the derivative is in summary when the derivative is negative we need to increase the value negative we need to increase the value negative we need to increase the value for the bias by shifting it to the right for the bias by shifting it to the right for the bias by shifting it to the right and when the derivative is positive we and when the derivative is positive we and when the derivative is positive we need to decrease the value for the bias need to decrease the value for the bias need to decrease the value for the bias by shifting it to the by shifting it to the by shifting it to the left lastly the derivatives can left lastly the derivatives can left lastly the derivatives can correctly tell us which direction to correctly tell us which direction to correctly tell us which direction to shift the bias because the training data shift the bias because the training data shift the bias because the training data contains the ideal output values so this contains the ideal output values so this contains the ideal output values so this is how back propagation normally works is how back propagation normally works is how back propagation normally works bam however our problem is that we don't bam however our problem is that we don't bam however our problem is that we don't know in advance where we can know in advance where we can know in advance where we can consistently get a small or large order consistently get a small or large order consistently get a small or large order of fries and that means our data set of fries and that means our data set of fries and that means our data set can't have ideal output values and can't have ideal output values and can't have ideal output values and without the ideal output values we can't without the ideal output values we can't without the ideal output values we can't calculate the differences between them calculate the differences between them calculate the differences between them and the outputs from the neural network and the outputs from the neural network and the outputs from the neural network and without those differences we can't and without those differences we can't and without those differences we can't calculate these calculate these calculate these derivatives and without the derivatives derivatives and without the derivatives derivatives and without the derivatives we don't know if we should shift the we don't know if we should shift the we don't know if we should shift the bias value to the right or to the bias value to the right or to the bias value to the right or to the left josh what are we going to do well left josh what are we going to do well left josh what are we going to do well it turns out we don't actually need to it turns out we don't actually need to it turns out we don't actually need to know the ideal output values in advance know the ideal output values in advance know the ideal output values in advance to calculate the derivatives to calculate the derivatives to calculate the derivatives instead we can guess what the ideal instead we can guess what the ideal instead we can guess what the ideal values should be and use those guesses values should be and use those guesses values should be and use those guesses to calculate the to calculate the to calculate the derivatives derivatives derivatives bam yes now let's work through our bam yes now let's work through our bam yes now let's work through our example and see how reinforcement example and see how reinforcement example and see how reinforcement learning and policy gradients can use learning and policy gradients can use learning and policy gradients can use guessing to make this work we'll start guessing to make this work we'll start guessing to make this work we'll start by assuming we are not hungry and that by assuming we are not hungry and that by assuming we are not hungry and that means we'll plug 0. 0 into the neural means we'll plug 0. 0 into the neural means we'll plug 0. 0 into the neural network now we just do the math doop and the probability that we'll go doop and the probability that we'll go doop and the probability that we'll go to norms P norm is to norms P norm is to norms P norm is 0. 5 and that means that the 0. 5 and that means that the 0. 5 and that means that the corresponding probability that we will corresponding probability that we will corresponding probability that we will go to squatches P squatch is 1 minus P go to squatches P squatch is 1 minus P go to squatches P squatch is 1 minus P norm which equals norm which equals norm which equals 0. 5 in other words we can draw a line 0. 5 in other words we can draw a line 0. 5 in other words we can draw a line that is 0. 5 units long to represent the that is 0. 5 units long to represent the that is 0. 5 units long to represent the probability of going to squatches and probability of going to squatches and probability of going to squatches and then append another line that is 0. 5 then append another line that is 0. 5 then append another line that is 0. 5 units long to represent the probability units long to represent the probability units long to represent the probability of going to norms and we end up with a of going to norms and we end up with a of going to norms and we end up with a line that goes from 0 to line that goes from 0 to line that goes from 0 to 1 now to decide which place we will 1 now to decide which place we will 1 now to decide which place we will visit for a snack we pick a random visit for a snack we pick a random visit for a snack we pick a random number between 0 and 1 in this example number between 0 and 1 in this example number between 0 and 1 in this example we randomly pick the number we randomly pick the number we randomly pick the number 0. 2 and because 0. 2 is in the region 0. 2 and because 0. 2 is in the region 0. 2 and because 0. 2 is in the region that represents Squatch's fry shack that represents Squatch's fry shack that represents Squatch's fry shack we'll go we'll go we'll go there now comes the part where we make a there now comes the part where we make a there now comes the part where we make a guess if we make a guess that the guess if we make a guess that the guess if we make a guess that the correct thing to do is go to Squatches correct thing to do is go to Squatches correct thing to do is go to Squatches when our hunger is when our hunger is when our hunger is 0. 0 then ideally we would want the 0. 0 then ideally we would want the 0. 0 then ideally we would want the probability of going to Squatches P probability of going to Squatches P probability of going to Squatches P squatch to be one and that means that squatch to be one and that means that squatch to be one and that means that ideally we'd want the probability of ideally we'd want the probability of ideally we'd want the probability of going to norms P norm to be going to norms P norm to be going to norms P norm to be zero now given these ideal values that zero now given these ideal values that zero now given these ideal values that are based on our guess we can quantify are based on our guess we can quantify are based on our guess we can quantify the difference between the ideal value the difference between the ideal value the difference between the ideal value for P squatch for P squatch for P squatch 1. 0 and the actual value for P squatch 1. 0 and the actual value for P squatch 1. 0 and the actual value for P squatch 0. 5 that's based on the output from the 0. 5 that's based on the output from the 0. 5 that's based on the output from the neural network neural network neural network note in the follow-up stat quest adding note in the follow-up stat quest adding note in the follow-up stat quest adding reinforcement learning to neural reinforcement learning to neural reinforcement learning to neural networks mathematical details will show networks mathematical details will show networks mathematical details will show exactly how we quantify this difference exactly how we quantify this difference exactly how we quantify this difference spoiler alert we'll use cross entropy spoiler alert we'll use cross entropy spoiler alert we'll use cross entropy for now all you need to know is that we for now all you need to know is that we for now all you need to know is that we can quantify the difference between the can quantify the difference between the can quantify the difference between the P squatch derived from the guess 1. 0 O P squatch derived from the guess 1. 0 O P squatch derived from the guess 1. 0 O and the P squatch derived from the and the P squatch derived from the and the P squatch derived from the output from the neural network output from the neural network output from the neural network 0. 5 and that means we can calculate the 0. 5 and that means we can calculate the 0. 5 and that means we can calculate the derivative of the difference with derivative of the difference with derivative of the difference with respect to the bias that we want to respect to the bias that we want to respect to the bias that we want to optimize optimize optimize bam how exactly do we calculate this bam how exactly do we calculate this bam how exactly do we calculate this derivative that's a great question derivative that's a great question derivative that's a great question Squatch and we'll go through it step by Squatch and we'll go through it step by Squatch and we'll go through it step by step in the follow-up stat quest for now step in the follow-up stat quest for now step in the follow-up stat quest for now the most important thing is to know that the most important thing is to know that the most important thing is to know that the guess makes it possible anyway given the guess makes it possible anyway given the guess makes it possible anyway given our guess that going to Squatch's place our guess that going to Squatch's place our guess that going to Squatch's place is the best thing to do the derivative is the best thing to do the derivative is the best thing to do the derivative ends up being ends up being ends up being 0. 5 and if that guess is correct then 0. 5 and if that guess is correct then 0. 5 and if that guess is correct then the positive value for the derivative the positive value for the derivative the positive value for the derivative tells us to decrease the value for the tells us to decrease the value for the tells us to decrease the value for the bias and shift it to the left bias and shift it to the left bias and shift it to the left on the other hand if our guess is on the other hand if our guess is on the other hand if our guess is incorrect and we should have gone to incorrect and we should have gone to incorrect and we should have gone to norms norms norms instead then we'll want to do the exact instead then we'll want to do the exact instead then we'll want to do the exact opposite of what the derivative tells us opposite of what the derivative tells us opposite of what the derivative tells us to do in other words instead of to do in other words instead of to do in other words instead of decreasing the value for the bias we decreasing the value for the bias we decreasing the value for the bias we want to increase the value for the bias want to increase the value for the bias want to increase the value for the bias and shift it to the and shift it to the and shift it to the right but Josh how will we know if the right but Josh how will we know if the right but Josh how will we know if the guess is correct or not guess is correct or not guess is correct or not in other words how do we know if we in other words how do we know if we in other words how do we know if we should shift the bias to the left or the should shift the bias to the left or the should shift the bias to the left or the right the answer is simple squatch we right the answer is simple squatch we right the answer is simple squatch we just order some fries just order some fries just order some fries bam in this example when we order some bam in this example when we order some bam in this example when we order some fries we get this relatively small fries we get this relatively small fries we get this relatively small serving hey Squatch how come our order serving hey Squatch how come our order serving hey Squatch how come our order of fries is so small i couldn't resist of fries is so small i couldn't resist of fries is so small i couldn't resist and I ate some of your fries well this and I ate some of your fries well this and I ate some of your fries well this time I guess that's okay it's okay time I guess that's okay it's okay time I guess that's okay it's okay because our hunger level is because our hunger level is because our hunger level is 0. 0 meaning we're not hungry at all so 0. 0 meaning we're not hungry at all so 0. 0 meaning we're not hungry at all so it was actually a good thing that it was actually a good thing that it was actually a good thing that Squatch ate some of our Squatch ate some of our Squatch ate some of our fries in other words getting a small fries in other words getting a small fries in other words getting a small order of fries when we're not hungry order of fries when we're not hungry order of fries when we're not hungry means we made the correct guess and means we made the correct guess and means we made the correct guess and since we made the correct guess we set since we made the correct guess we set since we made the correct guess we set something called reward to 1. 0 something called reward to 1. 0 something called reward to 1. 0 in contrast if Squatch had given us a in contrast if Squatch had given us a in contrast if Squatch had given us a large order of fries then we wouldn't be large order of fries then we wouldn't be large order of fries then we wouldn't be hungry enough to eat all of the fries hungry enough to eat all of the fries hungry enough to eat all of the fries and that means we made the wrong guess and that means we made the wrong guess and that means we made the wrong guess so in this case we would set the reward so in this case we would set the reward so in this case we would set the reward to negative to negative to negative one note any positive reward will work one note any positive reward will work one note any positive reward will work for a correct guess and any negative for a correct guess and any negative for a correct guess and any negative reward will work for an incorrect guess reward will work for an incorrect guess reward will work for an incorrect guess and we'll go through an example with and we'll go through an example with and we'll go through an example with different values in just a bit but for different values in just a bit but for different values in just a bit but for now let's just set the reward to 1. 0 for now let's just set the reward to 1. 0 for now let's just set the reward to 1. 0 for a correct guess and negative 1. 0 for an a correct guess and negative 1. 0 for an a correct guess and negative 1. 0 for an incorrect incorrect incorrect guess okay I get that a positive or guess okay I get that a positive or guess okay I get that a positive or negative reward reflects if our guess is negative reward reflects if our guess is negative reward reflects if our guess is correct or not but how does that help us correct or not but how does that help us correct or not but how does that help us interpret the derivative that we interpret the derivative that we interpret the derivative that we calculated earlier calculated earlier calculated earlier we simply multiply the derivative by the we simply multiply the derivative by the we simply multiply the derivative by the reward to get an updated derivative that reward to get an updated derivative that reward to get an updated derivative that points us in the correct direction points us in the correct direction points us in the correct direction bam in this example we were not hungry bam in this example we were not hungry bam in this example we were not hungry and squatch gave us a small order of and squatch gave us a small order of and squatch gave us a small order of fries so we made the correct guess and fries so we made the correct guess and fries so we made the correct guess and the reward equals the reward equals the reward equals 1. 0 and multiplying the derivative 0. 5 1. 0 and multiplying the derivative 0. 5 1. 0 and multiplying the derivative 0. 5 by the reward 1. 0 we know means the by the reward 1. 0 we know means the by the reward 1. 0 we know means the updated derivative is the same as the updated derivative is the same as the updated derivative is the same as the original original original derivative and the positive updated derivative and the positive updated derivative and the positive updated derivative tells us to shift the bias to derivative tells us to shift the bias to derivative tells us to shift the bias to the left just like the original the left just like the original the left just like the original derivative in other words when we make derivative in other words when we make derivative in other words when we make the correct guess to begin with then the the correct guess to begin with then the the correct guess to begin with then the derivative that is based on that guess derivative that is based on that guess derivative that is based on that guess points us in the correct direction points us in the correct direction points us in the correct direction in contrast if we were not hungry but in contrast if we were not hungry but in contrast if we were not hungry but Squatch gave us a large order of fries Squatch gave us a large order of fries Squatch gave us a large order of fries then we would have made the wrong guess then we would have made the wrong guess then we would have made the wrong guess and the derivative would be telling us and the derivative would be telling us and the derivative would be telling us to do the opposite of what we should do to do the opposite of what we should do to do the opposite of what we should do however multiplying the derivative by a however multiplying the derivative by a however multiplying the derivative by a negative reward for making the wrong negative reward for making the wrong negative reward for making the wrong guess would flip things around and then guess would flip things around and then guess would flip things around and then the updated derivative would correctly the updated derivative would correctly the updated derivative would correctly tell us to shift the bias to the right tell us to shift the bias to the right tell us to shift the bias to the right in other words if we made the wrong in other words if we made the wrong in other words if we made the wrong guess to begin with then multiplying the guess to begin with then multiplying the guess to begin with then multiplying the derivative which is based on that guess derivative which is based on that guess derivative which is based on that guess by a negative number will flip things by a negative number will flip things by a negative number will flip things around and point it in the correct around and point it in the correct around and point it in the correct direction this is super important so let direction this is super important so let direction this is super important so let me repeat it when we multiply a me repeat it when we multiply a me repeat it when we multiply a derivative that is based on an incorrect derivative that is based on an incorrect derivative that is based on an incorrect guess by a negative reward we flip the guess by a negative reward we flip the guess by a negative reward we flip the direction and correct the direction and correct the direction and correct the mistake bam mistake bam mistake bam in summary when we multiply the in summary when we multiply the in summary when we multiply the derivative by the reward we end up with derivative by the reward we end up with derivative by the reward we end up with an updated derivative that tells us how an updated derivative that tells us how an updated derivative that tells us how to modify the bias to modify the bias to modify the bias correctly correctly correctly bam note before we move on I want to bam note before we move on I want to bam note before we move on I want to reiterate that the rewards don't have to reiterate that the rewards don't have to reiterate that the rewards don't have to be one and negative one for example if be one and negative one for example if be one and negative one for example if squatch didn't give us any fries at all squatch didn't give us any fries at all squatch didn't give us any fries at all then we could set the reward to two org then we could set the reward to two org then we could set the reward to two org -2 depending on how hungry we were if we -2 depending on how hungry we were if we -2 depending on how hungry we were if we were not hungry then the reward equals were not hungry then the reward equals were not hungry then the reward equals 2. 0 and the updated derivative will be 2. 0 and the updated derivative will be 2. 0 and the updated derivative will be twice what it was before but still twice what it was before but still twice what it was before but still pointing in the original pointing in the original pointing in the original direction and so the updated derivative direction and so the updated derivative direction and so the updated derivative would tell us to take a larger step in would tell us to take a larger step in would tell us to take a larger step in the same direction as before the same direction as before the same direction as before in contrast if we were hungry and we in contrast if we were hungry and we in contrast if we were hungry and we didn't get any fries then the reward didn't get any fries then the reward didn't get any fries then the reward equals equals equals -2. 0 and the updated derivative would -2. 0 and the updated derivative would -2. 0 and the updated derivative would tell us to take a larger step in the tell us to take a larger step in the tell us to take a larger step in the opposite direction of the original opposite direction of the original opposite direction of the original derivative derivative derivative bam now let's go back to our example bam now let's go back to our example bam now let's go back to our example where we were not hungry and squatch where we were not hungry and squatch where we were not hungry and squatch gave us a small order of fries so our gave us a small order of fries so our gave us a small order of fries so our guess was correct and the reward equal guess was correct and the reward equal guess was correct and the reward equal 1. 0 1. 0 1. 0 when we multiply the derivative 0. 5 by when we multiply the derivative 0. 5 by when we multiply the derivative 0. 5 by the reward the reward the reward 1. 0 we get the updated derivative 1. 0 we get the updated derivative 1. 0 we get the updated derivative 0. 5 we then plug the updated derivative 0. 5 we then plug the updated derivative 0. 5 we then plug the updated derivative into gradient descent to calculate the into gradient descent to calculate the into gradient descent to calculate the step step step size now in this example the learning size now in this example the learning size now in this example the learning rate is set to rate is set to rate is set to 1. 0 so the equation ends up being 1. 0 * 1. 0 so the equation ends up being 1. 0 * 1. 0 so the equation ends up being 1. 0 * the updated derivative the updated derivative the updated derivative 0. 5 and so the step size is equal to 0. 5 and so the step size is equal to 0. 5 and so the step size is equal to 0. 5 then we calculate the new value for 0. 5 then we calculate the new value for 0. 5 then we calculate the new value for the bias by subtracting the step size the bias by subtracting the step size the bias by subtracting the step size 0. 5 from the old bias value 0. 5 from the old bias value 0. 5 from the old bias value 0. 0 to get the new bias value 0. 0 to get the new bias value 0. 0 to get the new bias value 0. 5 now we plug the new bias term into 0. 5 now we plug the new bias term into 0. 5 now we plug the new bias term into our neural our neural our neural network bam now that we have updated the network bam now that we have updated the network bam now that we have updated the bias we have to get some more fries so bias we have to get some more fries so bias we have to get some more fries so we can keep optimizing it so we plug in we can keep optimizing it so we plug in we can keep optimizing it so we plug in our hunger level our hunger level our hunger level 0. 0 and when we do the math the 0. 0 and when we do the math the 0. 0 and when we do the math the probability that we will go to norms is probability that we will go to norms is probability that we will go to norms is now now now 0. 4 and that means that the probability 0. 4 and that means that the probability 0. 4 and that means that the probability we go to squatches 1 minus p norm has we go to squatches 1 minus p norm has we go to squatches 1 minus p norm has increased to increased to increased to 0. 6 and the probability that we go to 0. 6 and the probability that we go to 0. 6 and the probability that we go to norms has decreased to 0. 4 norms has decreased to 0. 4 norms has decreased to 0. 4 4 remember the last time our hunger was 4 remember the last time our hunger was 4 remember the last time our hunger was 0. 0 we went to squatches and we got what 0. 0 we went to squatches and we got what 0. 0 we went to squatches and we got what we wanted so it makes sense that given we wanted so it makes sense that given we wanted so it makes sense that given the same hunger as before we now have a the same hunger as before we now have a the same hunger as before we now have a higher probability of going to higher probability of going to higher probability of going to squatches now we pick a random number squatches now we pick a random number squatches now we pick a random number between 0 and 1 and we get 0. 9 between 0 and 1 and we get 0. 9 between 0 and 1 and we get 0. 9 and because 0. 9 is in the region that and because 0. 9 is in the region that and because 0. 9 is in the region that represents Norm's fry hut that's where represents Norm's fry hut that's where represents Norm's fry hut that's where we'll we'll we'll go now in order to update this bias we go now in order to update this bias we go now in order to update this bias we have to calculate the derivative and in have to calculate the derivative and in have to calculate the derivative and in order to calculate the derivative we order to calculate the derivative we order to calculate the derivative we have to make a guess that going to norms have to make a guess that going to norms have to make a guess that going to norms was the right thing to do and if going was the right thing to do and if going was the right thing to do and if going to norms is the right thing to do then to norms is the right thing to do then to norms is the right thing to do then ideally the probability of visiting ideally the probability of visiting ideally the probability of visiting norms should be norms should be norms should be 1. 0 making the guess allows us to 1. 0 making the guess allows us to 1. 0 making the guess allows us to quantify the difference between the quantify the difference between the quantify the difference between the output from the neural network output from the neural network output from the neural network 0. 4 and the ideal value 0. 4 and the ideal value 0. 4 and the ideal value 1. 0 and with that difference we can 1. 0 and with that difference we can 1. 0 and with that difference we can calculate the derivative with respect to calculate the derivative with respect to calculate the derivative with respect to the the the bias this time when we calculate the bias this time when we calculate the bias this time when we calculate the derivative we get0. 6 now we need to determine if we made the now we need to determine if we made the now we need to determine if we made the correct guess so that we know if we correct guess so that we know if we correct guess so that we know if we should shift the bias to the left or the should shift the bias to the left or the should shift the bias to the left or the right so we order some fries and norm right so we order some fries and norm right so we order some fries and norm gives us a huge pile of them normally gives us a huge pile of them normally gives us a huge pile of them normally this would be awesome unfortunately we this would be awesome unfortunately we this would be awesome unfortunately we aren't aren't aren't hungry and since we are not hungry the hungry and since we are not hungry the hungry and since we are not hungry the fact that Norm gave us a huge pile of fact that Norm gave us a huge pile of fact that Norm gave us a huge pile of fries means we made the wrong guess so fries means we made the wrong guess so fries means we made the wrong guess so the reward equals the reward equals the reward equals -1 now we multiply the derivative by the -1 now we multiply the derivative by the -1 now we multiply the derivative by the reward to get the updated derivative reward to get the updated derivative reward to get the updated derivative 0. 6 note the updated derivative 0. 6 is 0. 6 note the updated derivative 0. 6 is 0. 6 note the updated derivative 0. 6 is positive so that means we need to positive so that means we need to positive so that means we need to decrease the value of the bias some decrease the value of the bias some decrease the value of the bias some more we then plug the updated derivative more we then plug the updated derivative more we then plug the updated derivative into gradient descent to calculate the into gradient descent to calculate the into gradient descent to calculate the step size and remember the learning rate step size and remember the learning rate step size and remember the learning rate is set to 1. 0 0 so the step size ends up is set to 1. 0 0 so the step size ends up is set to 1. 0 0 so the step size ends up being being being 0. 6 then we calculate the new value for 0. 6 then we calculate the new value for 0. 6 then we calculate the new value for the bias by subtracting the step size the bias by subtracting the step size the bias by subtracting the step size 0. 6 from the old bias 0. 6 from the old bias 0. 6 from the old bias value. 5 to get the new bias value value. 5 to get the new bias value value. 5 to get the new bias value -1. 1 and now we plug the new bias term -1. 1 and now we plug the new bias term -1. 1 and now we plug the new bias term into our neural network into our neural network into our neural network bam now if we plug 0. 0 zero which means bam now if we plug 0. 0 zero which means bam now if we plug 0. 0 zero which means we are not hungry into the neural we are not hungry into the neural we are not hungry into the neural network the probability that we will go network the probability that we will go network the probability that we will go to norms is even smaller than it was to norms is even smaller than it was to norms is even smaller than it was before now it's before now it's before now it's 0. 2 and it makes sense that the 0. 2 and it makes sense that the 0. 2 and it makes sense that the probability of going to norms is getting probability of going to norms is getting probability of going to norms is getting smaller because so far each time we've smaller because so far each time we've smaller because so far each time we've plugged 0. 0 into the input we should plugged 0. 0 into the input we should plugged 0. 0 into the input we should have gone to squatches instead of norms have gone to squatches instead of norms have gone to squatches instead of norms and that means that the reinforcement and that means that the reinforcement and that means that the reinforcement learning algorithm we're using ordinary learning algorithm we're using ordinary learning algorithm we're using ordinary policy gradients is working double policy gradients is working double policy gradients is working double bam note so far we've only plugged 0. 0 bam note so far we've only plugged 0. 0 bam note so far we've only plugged 0. 0 for hunger into the neural network for hunger into the neural network for hunger into the neural network however in order to fully train the however in order to fully train the however in order to fully train the model we have to use values from 0 to 1 model we have to use values from 0 to 1 model we have to use values from 0 to 1 as inputs to the neural network and as inputs to the neural network and as inputs to the neural network and after doing lots of updates using all after doing lots of updates using all after doing lots of updates using all kinds of input values between 0 and 1 kinds of input values between 0 and 1 kinds of input values between 0 and 1 the value for the bias starts to hover the value for the bias starts to hover the value for the bias starts to hover around around around -10 and the fact that we are hovering -10 and the fact that we are hovering -10 and the fact that we are hovering around some value means we're done around some value means we're done around some value means we're done training now when we are not hungry and training now when we are not hungry and training now when we are not hungry and the input is the input is the input is 0. 0 the probability of going to norms is 0. 0 the probability of going to norms is 0. 0 the probability of going to norms is zero and that means when we're not zero and that means when we're not zero and that means when we're not hungry we'll always go to squatches hungry we'll always go to squatches hungry we'll always go to squatches that's because more often than not I eat that's because more often than not I eat that's because more often than not I eat most of your most of your most of your fries so when you're not hungry and fries so when you're not hungry and fries so when you're not hungry and don't want many fries come to my don't want many fries come to my don't want many fries come to my shack in contrast when we are hungry and shack in contrast when we are hungry and shack in contrast when we are hungry and the input value is the input value is the input value is 1. 0 the probability of going to norms is 1. 0 the probability of going to norms is 1. 0 the probability of going to norms is one and that means when we are hungry one and that means when we are hungry one and that means when we are hungry we'll always go to norms and that's we'll always go to norms and that's we'll always go to norms and that's because more often than not I don't eat because more often than not I don't eat because more often than not I don't eat your your your fries so when you're hungry and want a fries so when you're hungry and want a fries so when you're hungry and want a lot of fries come to my lot of fries come to my lot of fries come to my hut triple hut triple hut triple bam in summary reinforcement learning bam in summary reinforcement learning bam in summary reinforcement learning allows us to optimize a neural network allows us to optimize a neural network allows us to optimize a neural network when we don't have known outputs or when we don't have known outputs or when we don't have known outputs or target values in target values in target values in advance we start by using the neural advance we start by using the neural advance we start by using the neural network to decide where to go then we network to decide where to go then we network to decide where to go then we make the guess that wherever we ended up make the guess that wherever we ended up make the guess that wherever we ended up going was the right place to go for going was the right place to go for going was the right place to go for example if we went to Norms we would example if we went to Norms we would example if we went to Norms we would guess that going to Norms was the best guess that going to Norms was the best guess that going to Norms was the best option even though we might later option even though we might later option even though we might later discover that we should have gone to discover that we should have gone to discover that we should have gone to Squatches once we've made that guess we Squatches once we've made that guess we Squatches once we've made that guess we use it to calculate the derivative with use it to calculate the derivative with use it to calculate the derivative with respect to the parameter we want to respect to the parameter we want to respect to the parameter we want to optimize optimize optimize we then determine the reward associated we then determine the reward associated we then determine the reward associated with that guess and we multiply the with that guess and we multiply the with that guess and we multiply the derivative by the reward to correct for derivative by the reward to correct for derivative by the reward to correct for any error that we might have made when any error that we might have made when any error that we might have made when we made the original guess and that we made the original guess and that we made the original guess and that gives us the updated derivative and we gives us the updated derivative and we gives us the updated derivative and we plug the updated derivative into plug the updated derivative into plug the updated derivative into gradient descent to optimize the neural gradient descent to optimize the neural gradient descent to optimize the neural network network network bam now it's time for some shameless bam now it's time for some shameless bam now it's time for some shameless self-promotion if you want to review self-promotion if you want to review self-promotion if you want to review statistics and machine learning offline statistics and machine learning offline statistics and machine learning offline check out the StackQuest PDF study check out the StackQuest PDF study check out the StackQuest PDF study guides and my books the Stack Rest guides and my books the Stack Rest guides and my books the Stack Rest Illustrated Guide to Machine Learning Illustrated Guide to Machine Learning Illustrated Guide to Machine Learning and the StackQuest Illustrated Guide to and the StackQuest Illustrated Guide to and the StackQuest Illustrated Guide to Neural Networks and AI at Neural Networks and AI at Neural Networks and AI at stackquest. org there's something for stackquest. org there's something for stackquest. org there's something for everyone hooray we've made it to the end everyone hooray we've made it to the end everyone hooray we've made it to the end of another exciting Stack Quest if you of another exciting Stack Quest if you of another exciting Stack Quest if you like this Stack Quest and want to see like this Stack Quest and want to see like this Stack Quest and want to see more please subscribe and if you want to more please subscribe and if you want to more please subscribe and if you want to support Stat Quest consider contributing support Stat Quest consider contributing support Stat Quest consider contributing to my Patreon campaign becoming a to my Patreon campaign becoming a to my Patreon campaign becoming a channel member buying one or two of my channel member buying one or two of my channel member buying one or two of my original songs or a t-shirt or a hoodie original songs or a t-shirt or a hoodie original songs or a t-shirt or a hoodie or just donate the links are in the or just donate the links are in the or just donate the links are in the description below all right until next description below all right until next description below all right until next time quest on
