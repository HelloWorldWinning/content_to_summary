Timestamp: 2025-04-26T10:56:42.790292
Title: AI下一站：顶尖学者分享AI研究新机遇 ｜ 俞舟教授访谈_上 UndcL0uEmYs
URL: https://youtube.com/watch?v=UndcL0uEmYs&si=Bk7a2G5fSCPjR92N
Status: success
Duration: 23:08

Description:
好的，这是对提供的文本进行的提炼总结，包含概要、核心观点、框架和概念图：

**概要：**

1.  **NLP领域的发展和变化：**
    *   NLP领域因深度学习和大模型的出现经历了巨大变化，变得更受欢迎，吸引了更多人才和资源。
    *   尽管大模型带来了便利，但并不意味着NLP领域的研究已经结束，反而提供了更多新的研究方向和机会。
    *   行业界对大模型的理解可能存在误解，研究的重点在于解决问题的过程和算法的价值，而不仅仅是最终产品。

2.  **关于DeepSeek的看法：**
    *   DeepSeek在工程上的创新，例如降低成本和提高效率，具有重要价值，但其贡献主要在于工程优化而非算法突破。
    *   DeepSeek的成功在于将各种组件结合在一起，并公开细节，促进了整个社区的进步。

3.  **大模型的能力和涌现性：**
    *   真正领先的技术往往掌握在少数人手中，需要时间才能被广泛认可。
    *   大模型的能力来自于大规模的数据和训练，使其能够学习到数据中的模式和结构，并进行泛化和推理。
    *   涌现性是指大模型在达到一定规模后表现出的意想不到的能力，例如上下文学习，这与数据中的并行结构有关。

4.  **研究的价值和方向：**
    *   研究和实际应用之间存在差异，研究的重点在于解决问题的过程和算法的价值。
    *   研究应该关注未来的发展趋势，探索新的技术和应用，而不仅仅是解决当前的问题。
    *   衡量研究价值的标准是其是否具有创新性、是否解决了实际问题、以及是否对领域发展做出了贡献。

**核心观点：**

大模型的出现改变了NLP领域，但也带来了新的研究方向和机会，研究应关注算法价值、工程优化和未来趋势，真正领先的技术需要时间才能被广泛认可。

**总体框架:**

该内容以于州教授的视角，讨论了自然语言处理（NLP）领域在大语言模型（LLM）出现后的变化、机遇与挑战。讨论涉及LLM对研究方向的影响、DeepSeek等工程实践的意义、以及对LLM涌现能力的理解。强调了研究的价值在于解决问题的过程、算法的创新，以及对未来趋势的探索。

<Mermaid_Diagram>
```mermaid
graph LR
    subgraph NLP领域
        A[发展变化\n(更受欢迎，更多人才)]:::green --> B(研究方向新机遇):::blue
        B --> C{算法价值 & 工程优化};
        A --> D(大模型出现):::red
        D --> B
    end

    subgraph DeepSeek的意义
        E[工程创新\n(降低成本，提高效率)]:::green --> F(促进社区进步):::blue
        F --> C
        E --> G(工程优化为主):::orange
        G --> C
    end

    subgraph 大模型涌现能力
        H[大规模数据\n&训练]:::green --> I(学习模式与结构):::blue
        I --> J(泛化与推理):::blue
        I --> K(涌现上下文学习能力):::blue
        K --> I
    end

    subgraph 研究的价值与方向
        L[解决问题过程 & 算法价值]:::blue --> M(探索未来趋势):::blue
        M --> N{创新性, 解决问题, 贡献};
        N --> L
    end

    A --> L
    D --> H
    E --> M
    K --> M

    style A fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff
    style D fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff
    style H fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff
    style L fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff

    classDef green fill:#ccffcc,stroke:#333,stroke-width:2px
    classDef blue fill:#ccccff,stroke:#333,stroke-width:2px
    classDef red fill:#ffcccc,stroke:#333,stroke-width:2px
    classDef orange fill:#ffcc99,stroke:#333,stroke-width:2px
```
</Mermaid_Diagram>

**总结:**

当前NLP领域处于快速发展和变革时期，研究者应把握机遇，关注算法的本质，探索未来的方向，才能推动该领域的持续进步。


Content:
很多人觉得LP在大圆模型出来之后整个field就半天附近的变化了然后有的人可能觉得导致应用更广了然后有的人觉得这个field就完了就没什么好做的了能不能给大家一个能的感觉和你的一些想法或者是观察真正领先一个时代的技术都是掌握在少数人手中的不被大家认可是一个厂派慢慢的这个东西都会被人认可了这不是它一个很不一样的点就是它就公众上的这些创新然后就可以把架壳变得很低然后另一边的非常一肥身体应该需求都少了或者说是infrastrum本动机动盗但是他们只是一个很明显的appscation这一本工程相的东西你说的东西好像和他们很接近但是听起来又是research我觉得这两个是一个东西还是其实是会养比如dipsyc的这些东西把他们能发一个顶看嗯今天给大家带来两集跟哥伦比较大学于州教授的访谈大家知道我平时不专门做嘉宾介绍但是因为于州教授在我的这个访谈里边做的自我介绍太低调了我觉得没有办法表现出来它多厉害所以时候我就专门去做了一下research然后去讲一下于州教授的这个履力到底是什么样子希望这个例子也可以帮助大家双双多理解一下一个厉害的学者它的履力是什么样子的以及这样大家看视频的时候就知道了于州教授是多么的迁续和低调我在这里边简单讲两个点一个是于州教授的学术影响力一个是它的意见影响力于州教授的学术影响力是它的论文引用是8664也是指数高达478664引用虽然很高但是不是一个特别天文的数字但是我们要知道于州教授是2017年开始做教授的而且它的大部分引用大概近8000次都是来自于2020年之后这表明了它的研究方向是高度气和当前AI领域的热点所以说也是被广泛引用的而且它的实验识也非常厉害在2024年一年内就获得了ACL和NACL的两个结束论文讲这是在于员处理领域的两个顶会获得一个顶会已经很难得了在一年内同时获得两个顶会的这种成就更加丰富毛领角也就证明了它这个实验识研究的质量有多高除了引用和奖象以外在业界余教授还在2018年的时候就入选了服务司杂志科学领域的30位30岁以下亲英宝注意就是美国的服务司是有含金量的不是国内的那个特别人的特别然后在2018年的时候他带领团队赢得了亚马逊竞争系列的Alexa Price的挑战赛他们开发的社交聊天机器人也获得了千万级别的用户余教授还得到了很多公司比如说 IBM 所尼尔马腾讯等等的研究奖货尔教职奖也带到了业界对他工作的政客和追逐而且这段时间余教授一直在创业他现在创业的这个项目叫Archlex AI是致力于将前的AI Agents的技术运用在各个场景比如说AI销售所以说我们频道上业界大佬聊的很多但是像这种很快学术和业界的人聊的不多王盐志余教授是一个不可多的在学术界非常有影响力然后在业界也有很多成就的人而且他还去做创业公司我们的两集视频上集了学术下集了应用尤其上集的问题就是监狱言余教授已经在这里边学了这么久了他被待遇的模型是怎么颠覆的然后AI研究对AI应用到底有什么意义什么才是一个好的AI研究比如说Depthic是一个好的AI研究那下一部分就讲余教授在做AI应用的时候他有什么经验教训才能什么空然后他哪怕有这么强的学术背景有这么多的创业心理而且跟很多大公司合作过了他现在在创业Archlex AI的过程中又遇到什么样的挑战那如果像这么厉害的人都遇到这些挑战的话那我们每个人也应该都行不过一点对创业这些事情多一些精微置心还有一个点就是余教授跟我聊的时候我当时都没有注意到就是我教他怀孕但是我不知道他已经离生产这么县在我们访参结束之后的一个周之内余教授就去生二胎生完二胎以后马上又投入工作了这个访谈也是非常有意义的一个时间点那好这里就给大家带来各论比较大学的余教授拜拜拜拜大家好我叫余教我现在在各论比较大学做教授同时我也自己创业做了一个公司叫Archlex我们主要是做AI agent orchestration layer我们有OpenSource也有 Enterprise version也有一两个我们称之为 Vertical SAS帮助我们更好的实现我们的agents的validation然后我自己的背景是我在中国读的本科在浙江大学诸客争设计论计论机和音乐的双鞋位然后在本科的时候我就接触了AI做很多Mission Learning工作然后有一些Application包括有一些 Computer Vision Application也有一个Mission Learning就是Driven Machine Translation就是从英文发现到中文发现到英文这样然后本科必要以后我就直接到砍卖机没用读PhD然后之后我在PhD必要以后我就马上做了老师我现在在UC Davis做了三年老师然后Partime也在UC Berkeley做一些 Collaborative Research之后我就按照纽约在各米较大学做老师也做了有四年多了四年多了五年然后我一直都是做了泰国教学生的然后人家可能有时候问我这算哪个是你的Mager Conference Fan you或什么那我的Mager Conference Fan就是我们说的ACLSocily Journal of Computational Linguistics同时因为像我们做这种跟偏向于Departining或者是Large Language Model的组我们也经常会出现在像IClear,New York SadICML的这些会议里因为我们做的比较 General做Argorithms所以说我们也会投比如说CDR或者说甚至会投RabaticS会想Arroids等等所以说其实我们做Argorithms的话就是根据Application不同我们也会选择不同的Application Track因为说我也会有比如说DubDub Paper也会有Rexist Paper等等我们自己本来做了research就是做最早是做Covicational Agents然后现在Agents的话你可能想一下就说之前是只能说话现在可以做Action然后可以做更多interesting的Planning Argrance我们以前本来就是做各种RLR等等的Argorithm去做Sekential Decision然后我们现在做到Egentary Tax以后就有更多丰富的场景各种各样的Ustances然后我们就做了很多Argorithm为什么去做Efficiency去Lesset Better Large-English ModelBehavior等等然后这个Agentary Tax可以完成的更好完成的更快In general就是我们做的比较多的research然后我们就 spin off了一个我们的公司就是做 Open Source的这个Agentary GoArchistration Layer这样帮助大家可以更好的去用我们提供的这些图来更快的 build Agents这样不用重新做很多 engineering的工作Argorithm已经有了已经穿过了这样就可以提到一些based的这个 performance你说你之前做这个你在学书界应该也做了蛮久然后你之前做的在大圆模型之前的 是吧我们很早就开始做了就是以前只有我们身arting model没有nearline works的时候就开始做inoutie对 我是一年还是都 PhD的就是做 natural processing我也是本科的时候也是对 我本科的时候就是做我们身translation这样子NLP task我是建议系和元学系的双鞋位然后之后在下面就是读的NLP然后毕业了以后也一直是做这方面的研究做了有学界好年轻松快要是对刚才我们聊了我觉得有两个问题特别想问你就是你的这个双鞋公司Archist然后他对Agent然后你对这个field是什么理解的然后你在这里边做的工作是什么和你觉得就是大家平时要去学习或者说在这里边有什么带进去的问题然后另外一个问题也是非常地提交一下你是做这个NLP做了很久然后很多人觉得NLP在这个大圆模型出来之后就整个field就反天附地的变化了然后有的人可能觉得就是反而应用功广了然后有的人觉得这个field就完了就没什么好做的了因为之前所有需要被解决的问题下去还全都被解决对 我不知道你是能不能给大家一个怎么说呢就是你的感觉和你的一些想法或者是观察对 其实我们这个field怎么说呢就是大家都是被看到有特别多的development on very time然后可能我是觉得是一个好事就是我们变得越来越popular就比如说我们这个会议ACO然后我们刚去的时候我一年读的PHD的时候那就是几百个人的事情然后现在几千个人然后同样的像New York CVR都有一个报道式的增长从我的角度来讲是一个越来越大的community大家也放了特别的Roll resource然后因为这个特别exciting而TRAG了最好的学生来做我们这个事情所以说像大眼我觉得是一个非常坏事的事情我可以跟全世界最无效的人一起 work之后来说是一个非常exciting的事情当然同样的这这个竞争也是非常激烈因为这不仅是你学书界做这些事而这个industry也有特别多的D-Pockets也在做同样的事情所以你的竞争就特别激烈然后导致你需要就是实实跟心你自己的 knowledge同样你也会有这个压力是说在我们资源不同配置的情况下什么样子可以是你的一个pass forward但每个人在某一个时间段他都会有一个 extensive like questions就是我要怎么样重新reamment myself every three to five years但是我觉得总的来说是一个好事那就是几个这个法就说我是2017年理液的就是大概是在 neural network开始星期是在1415年开始那我们是比较早去到 neural network的这样子的工作的处然后当时也是因为这个 neural network带来了第一波的这个 AI boom大家看到这个 image recognition可以做的这么好就是这个可以商业化做落地然后很快大家都有一个就是叶姐觉得这个特别有意思然后想要照同样的 pht然后呢这个master和underbody也都想要上这样的课然后突然这个需求量就增多了那在 academia就突然就有了照NLP的指挥的这个机会就是如果你看2014年2015年这样子的指挥是几乎没有的就是非常肯排的可能长得一届也就照一到两个新老师这就是一个罗布一个课对 这就是个非常想想想是来因为我当时2011年的时候就是开始跟系统的做这种NLP的research的时候其实它的唯一的应用可能就是Google的这种 information search就要 information with Trilo是挺好的对 真正的大表说后面的 translation就 speech recognition其实还是有一个小的 gap然后之后到EC15年我这个 speech recognition的这个能力突然上升了我们新 translation能力也上升了然后 computer vision recognition也上升了很多然后就 open up to a lot of opportunities然后我们学书界也有这个不用然后工业界也有这个不用其实这两个之间稍微有一些得了也但是总的来说是整个方向就是变得招人更多然后大家也更容易在这里做投资所以说我们就是在2017年左右就是开始慢慢的这个教细就这么多了然后比如说像平时像我刚 PhDB的学生很难找到教细我们当时就非常幸运地就找到了教细然后又过了三年以后呢就是开始 deep learning慢慢就是越来越火然后更多的学校说哎 我也想要有一个做NLP的老师以前 top 30的学校并不一定每个学校都有一个教NLP的老师然后也是就 open up to the community那这样的话就有更多的新的老师来被招聘然后有一些老师也可以摸到更好的学校这样子总的来说就是一个新形象容量过程然后因为这个投入甚至于老师展规模特之后这个系列界投入就更多了那对于我们来说我们拿方订也更容易然后我们的学生也更容易拿到这个教细我们也更容易做参业所以说整个来说是一个好事我想 double click on这个就是二张跟二张跟二张之后便多了就会便多了这件事情因为就是把之前很多人会觉得之前有的人吧会觉得我甚至可能也是其中之一就是NLP所研究的问题都直接被解决了然后可能很多时候一个 PhD 的 research变成了一个 API call然后就行了好 那就容情况下研究NLP还有什么值得研究的指容感觉就是你要么就是说你研究大远模型的嘛要么你这NLP还有什么值得研究的呢或者说就是大家会不会注意必须全都到不大远模型哪里对所以这个其实是公园界或者是一般人的一个 misconception因为我们做 research不是说是一个 final product而是一个processing是一个怎么解决问题的过程对不代表说你一时做的一个 paper可能过了一年它没有再多的利用价值它这个就是没有用的因为更多的我们是做的是一个 algorithm这个 algorithm 和新的technology 可以做 combination不是说它就会完全覆盖我们原来的 algorithm 的这个能力也有很多东西呢就是很多大家觉得RG已经 solve了但其实它完全没有 solve因为它有很多 corner cases怎么做的更 efficient更 robust怎么可以做 reducing cost 等等但不同的应用环境比如说 multimodal fusion 等等需要很多东西是没有解决的大家可以看到的这些东西我们只是觉得是一个就非常地长的一些 similar当然像我的话我是一直有一种alternated事情比如说我最早是做我的 PhD 的这个 seed 希望situated awareness interactive intelligence system其实就是现在大家说的 multimodal agents对 直播我们当时因为确实是没有这些 API我所有的这个 module 我都在自己寄然后都是要在这个 super store所以说人家就说这个要花很多的投入很多的那些人来做这些事情那就是说做 research 完你就是不能说只是说现在能做的事你想要 once that further就是说我 as cm 2 3年以后这个东西会更完善的情况下我在这个基础上可以做什么样跟 futuristic 的 research那比如说以前我们的 speed recognition model都是自己写的computer vision model都是自己写的当然它有很多的 error它不过roversed不好但是它在这个大小的 system可以 explore 东西还是类似的那现在我只把这些原来自己写的比较 low quality 的 module换成了新的大家已经 package 好的 API那其实这个东西是整个的 research direction 没有 change只不过我觉得就是对新的写上来说这个 very 也降低了对 你能研究的东西变多了很多但是刚刚说的点我在问一个点就是你提到了这些算法本身的价值和没有被解决的问题比如说怎么样子跟 efficient quantar case 然后 caster 这些然后这就让我想到了最新的 deep seek然后 deep seek 它一个很不一样的点就是它在工程上的这些创新然后它可以把价格变得很低然后它全临变得非常 efficient用了这个应验需求更少或者说是 inference 成本更低的东西但是他们这是一个很明显的 application 追问然后是一个工程向的东西你说的东西好像和他们很接近但是听起来又是 research 追问的东西你觉得这两个是一个东西还是其实是不一样 research 其实是非常很 neutral对吧 efficiency 这些也是 research然后当然你 push 搬去就是说做stata of the art 也是 research它其实有不同种的方式从我们讲到来讲就是说这个东西是不是拿某的有没有人做过它是不是提供真实的价值这个是大家就是说就是评价 research 的 taste你刚刚说的这点就是我就说要 challenge一下给我 research 坑明了其实大家也有过很多 critik就是比如说 deep seek 所应用的这些东西他们是不拿某的 结果所知但是他们结合得非常好和应用得非常好最后是产生了价值那这种你觉得就是比如说 deep seek的这些东西把他们能发一个顶看去个这样的问题我觉得还是它其实有挺多的贡献 对吧但是我觉得现在有一些人可能夸张了它的贡献但是我觉得从我的假子来讲我是非常推荐sarsher research谈那个 deep seek的 explain基本上跟我的这个观点是 align我觉得它最重要的 contribution就是在说它重写了这个底层的价格让可以 training 这个 model pre training 的过程变得更 fsarsher 更神强这样的 奥奔奈法很多 possibility去用更低的成本去做更好的 model它本身也是一个 research然后几次 确实就是从argo 什么的层面来讲它的 contribution 没有那么大因为就是 M.O.E.A. 等等之前也都有人做过但是它的 contribution 再也说它把它的具体是怎么做一些的特别系这个有然后奥奔奈法一些它的 model 为啥等等它这个会促使整个坑厌力有更多的人可以做同样的 research同样的比如说它有一些比如说为 model 怎么设置等等确实有很多人也做包括我的主意也做这些类似的工作不是说 completely new但是同样的这些 components组合在一起也有它的意义而不是说到是破天津的时候它又却不断开户现在每一线都会在方向为啥对吧嗯然后我再从另外一个角度去问一个类似的问题那就是对 normal 的定义 hacer魔的数字的证软什么什么我觉得查分人带到了一个然后其实现在会造秀然后我们试解嘘就是比较 transport然后 transport当时出来的时候 hitting 它会直接开始 Ubuntuほど第二就是我记得好像OPEN-I说他们的GPG3然后去投conference 投点会然后点会不要然后那种就不投了对 这两个怎么看因为我从我也讲到来讲就是真正领先一个时代的技术都是掌握在少数人手中的所以说不被大家认可是一个厂派就慢慢的会这个东西都会被人认可只是需要时间远是Nobody Research 越不会被之前认可比如说像Transformer这样的工作它确实有刚开始Tension or Intense 也在很多运动里面就是挺意思的大家觉得然后慢慢的我们给它各种各样的Data 做Pretrination以后就是特别是GPG3的话我们看到Emergentability就对我们说都是非常Surprise因为我们很早因为走过全程我们很早就也自己Pretrination model嘛也自己做GPT2的Finding and等等当真正发现的GPG3的时候它跟GPT2的Pportformance完全不同还有一些Emergentability就是说Incontact Learning 等等是我们原来完全没有想到的这个点它本身所以真的对我们来说是一个非常Ground Breaking Research Research你觉得它有限的能力到底是为什么就是Emergence这个事情的就是我们知道它可能在什么节点永线比如说到了什么样的参数量或者说你就读了一些代码以后它永线了Incontact Learning但是为啥我们都是认为就是量变引起之变同样的人也是一样的嘛 对吧你就是小朋友它到某一个点它突然就离一下很多东西所以它就是需要这个过程也不说我们之前做很多就是比如说CD很明显就是Self Supervised Learning就是说我们不给它很多这种人工Purity Data而是让它就是是在这个既然环境中可以生成的这种Sinceletats就让它学要这些Tats它收集的成分比较低当的这些Tats足够多的时候它其实兴奋了一整个World 整个工作然后突然这个Model就许会了然后我们自己也做了很多工作去Emerwise就是说为什么比如说Incontact Learning是出现的我们有一篇ICML的Paper就是去年就是讲是什么东西Lustella这个Incontact Learning我们发现在很多的Pre-training的这股这个Corpice里面有一种我们称之为Parallel Structure的东西它比如说我们几个例子就是一个EM本身是有Structure它句子和句子之间也是有Structure您可能看不出来就是这个Running Text但是你如果自己分析是会发现的我们举一个比较简单的在中文的中美种修辞穿着肤、心、等等它是它这个Weportation比如说这种Parton它不只是在我们说的Sintice里面在Sementate的Email Described里面也有这在我们在分析里如果你把这些东西给它打了万了再去Train的话它就Incontact Learning的Belary就没有了就是说是它参考这个Model会把这个Model的东西给它打了万了它自己学会了Parallel于是它就有了Incontact Learning的Belary那我觉得这个地方就说说收一下吧所以说请你如果做一下几句话的总结就是Why are the large language model work at all你会怎么总结就是给一个小朋友解释这件事情的话你怎么解释就是Large language model会 work是因为大多模型、太多和大多可态的当它接受到足够多的信息我接受到足够多的Supervision以后它会自己展览wise它学到的支持这样的话它不简单的就是Copy原来它学到了东西更加会extrapple的一些东西好的好的办法解释那接下来就是关于研究之方面你还没有什么想聊的东西嗯从我讲的来讲就是说盈江和实际的落地是有很大的差异的这个现实生活中这模型它学到了东西它可能对于原写到就是不是一回事它有自己的逻辑我们现在的Explodit可能对它来说并不重要因为我觉得我已经有点autof my depth有很多东西可以做那什么东西值得做什么东西不值得做这件事情大家是怎么衡量的而实际上可以说到对这个NLP带来的你的比例方向什么当中的一些东西我没有什么特别本质的变化你说自己是最好的没有人信你因为所有人都在说自己是最好的
