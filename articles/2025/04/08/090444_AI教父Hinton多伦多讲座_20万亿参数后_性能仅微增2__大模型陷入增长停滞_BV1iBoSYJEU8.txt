Timestamp: 2025-04-08T09:04:44.327021
Title: AI教父Hinton多伦多讲座：20万亿参数后，性能仅微增2%，大模型陷入增长停滞 BV1iBoSYJEU8
URL: https://b23.tv/Bv71t28
Status: success
Duration: 26:28

Description:
好的，这是一份对文本的总结，包括要点、核心观点、整体框架、以及 Mermaid 图：

**核心要点总结：**

I. **人工智能与物理学的交汇：**

*   **相关性与相关函数：** 物理学中相关函数用于描述宇宙，例如粒子碰撞和气候数据分析。类似地，大型神经网络内部存在大量参数，可以尝试利用物理学中的相关函数来理解和解释这些复杂网络。
*   **物理学启发的进展：** 受限玻尔兹曼机（RBM）和缩放定律（scaling laws）等物理学概念推动了深度学习的发展。缩放定律，即网络规模或计算能力的提升与性能的特定幂律关系，受到了物理系统中类似行为的启发。
*   **变分方法（Variational Methods）：** 机器学习中使用的变分方法来源于物理学。变分方法可以帮助理解模型间的高效通信，例如，通过随机选择不同的编码方式（如“下雨了”或法语表达）进行信息传递，实际上可以提高通信效率。
*   **信息论与物理学：** 香农的信息熵理论已广泛应用于量子信息和物理系统的信息处理。

II. **人工智能的伦理与安全：**

*   **CERN for AI：** 类似于欧洲核子研究中心（CERN）的大型国际合作项目可能对人工智能的发展有益。然而，由于人工智能可能被用于开发致命性自主武器和网络攻击，国际合作面临政治阻碍。
*   **安全与理解的平衡：** 在追求人工智能理解的同时，必须关注其潜在的危险。
*   **模型权重公开的风险：** 公开大型基础模型的权重（例如 Meta 发布的模型）可能使网络犯罪分子更容易利用这些模型进行恶意活动。
*   **风险评估的偏差：** 人们普遍低估了未来可能发生的负面事件的概率。

III. **深度学习的未来：**

*   **缩放定律的局限性：** 随着计算能力和数据量的增加，大型语言模型（LLM）的性能提升可能正在经历收益递减。
*   **新思路的重要性：** 为了克服缩放带来的瓶颈，需要新的思路和工程技巧来推动深度学习的进一步发展。
*   **类脑计算（Brain-inspired Computation）：** 脑启发式的技术，例如基于时间差分的反向传播，可能在神经网络中起作用，尽管其效率不如数字计算机。

**核心观点：**

人工智能的发展潜力巨大，但也伴随着伦理和安全风险，我们需要在理解人工智能的同时，警惕其潜在的危害，并探索新的思路以推动其安全发展。

**整体框架：**

1.  人工智能与物理学的联系与启发
2.  人工智能的伦理与安全挑战
3.  人工智能的未来发展方向

<Mermaid_Diagram>
graph LR
    subgraph Physics[Physics Principles]
        A[Correlation Functions]:::physics
        B[Scaling Laws]:::physics
        C[Variational Methods]:::physics
        D[Information Theory]:::physics
        classDef physics fill:#f9f,stroke:#333,stroke-width:2px;
    end

    subgraph AI[Artificial Intelligence]
        E[Large Neural Networks]:::ai
        F[Deep Learning Advances]:::ai
        G[Model Communication]:::ai
        H[Quantum Information Processing]:::ai
        I[Ethical and Safety Concerns]:::ai
        J[Future AI Development]:::ai
        K[Brain-inspired Computation]:::ai
        classDef ai fill:#ccf,stroke:#333,stroke-width:2px;
    end

    subgraph Ethics[AI Ethics & Safety]
        L[International Collaboration (CERN for AI)]:::ethics
        M[Risk Assessment]:::ethics
        N[Weaponization Concerns]:::ethics
        O[Model Weight Release Risks]:::ethics
        classDef ethics fill:#ffc,stroke:#333,stroke-width:2px;
    end

    A --> E
    B --> F
    C --> G
    D --> H

    E --> I
    F --> I
    G --> I
    H --> I

    I --> L
    I --> M
    I --> N
    O --> N

    F --> J
    G --> J
    H --> J
    K --> J
    J --> I

    style I fill:#fcc,stroke:#333,stroke-width:2px;
</Mermaid_Diagram>


Content:
 This is where it becomes clear that I don't actually know any physics. Well, I hope to convince you to the contrary. So thanks so much for a really wonderful talk that I had a list of questions that I was thinking of asking you and then I'm tempted to just throw them all out and just go off what you said because it was really interesting. But let me start with one thing that you mentioned over and over is this idea of correlations and correlation functions being useful for either the learning procedure or for understanding what's happening. And in most areas of physics, correlation functions are how we describe the universe. Even the particle collisions that will go on at the Large Hadron Collider, you can think of them as correlation functions, these things called quantum fields, or what Professor Kushner studies with climate science, if you have a time series of data and it's random, some of the information you can get out of it is how much is whether at this time correlated with whether at that time. I'm wondering, we have these enormous trillion parameter models now. There's lots of numbers inside those things. Can we use any of this intuition for kind of physics inspired correlation functions to tease out any of the meaning or interpretability of what's happening inside those large networks? Probably, but I don't know how to do it. That is, many people say we never going to be able to trust these big neural networks until we understand how they're working. I think we may well never understand, indeed, how these big models are working. I mean, we program them, so we know roughly the architecture of the net, but how they work depends on what they learn from data. And when something with a trillion real value parameters makes a decision, it may be that there is no simple explanation of why I did that and the values of those trillion parameters. Now, I should say most people in the field think we can do better than that, but I'm not convinced we can do much better. So let me also ask, you know, you mentioned this idea of the restricted Boltzmann machine is like an enzyme that catalyzed some advancement in deep learning. And my understanding is there have been several of those physics inspired advances that have managed to do something pretty amazing. I think one example is this idea of scaling laws that as you increase the size of a network or you give it more and more computing time, it does better and better, but it does better and better in a particular way that physicists would call a power law and that if you make a certain kind of plot, it looks like a straight line. And my understanding is that that idea was inspired by that kind of behavior showing up all the time in physical systems, in condensed matter systems, in high energy systems. And the fact that this has happened, there are other examples. Several times suggest to me this is not an accident. Well physicists have thought a lot about things, right? And they're very smart. So like a lot of mathematics came from people thinking about physics. True. I think probably some of the new mathematics came from trying to figure out why the moon didn't fall into the earth and things like that. So it's not surprising that physicists are among the first people to come across fancy mathematical techniques. Well let me ask, where do you think the physics perspective can be most useful in understanding deep learning and maybe to contrast it with the perspective of say psychology or biology or traditional computer science? I wish I could answer that. So Ganguly at Stanford is an ex-physicist and has done very nice work on showing what's happening in the weights as models learn. So I mean my belief is insights from physicists are going to be useful. But let me be a politician and just change the subjects slightly and tell you a piece of physics that really is very useful. So people in machine learning use variational methods which come from physics. And I want to try and describe for the general public how you might use a variational method or how you get an insight from a variational method to understand how you could get efficient communication between models. So let's suppose you have two different ways of encoding exactly the same event and you want to communicate this. We have a sender and we have a receiver and the sender wants to communicate this event to the receiver using as few bits as possible, sending as few bits across the channel as possible. And so let's suppose you want to tell someone it's raining and being Canadian you can say it's raining or ill three. I don't speak French but I think that's right. I'm not a real Canadian. So a normal person would say well if it takes sort of 10 bits to say it's raining it takes 10 bits to say ill blue what you should do is just pick one and do it and it's going to take you 10 bits. But a physicist would say okay you've got two different ways of coding the same thing. What if we stochastically picked one of those ways? So we flip a coin and we pick one way. Now it looks like you don't win anything by doing that because you flipped your coin and you either say ill blue or you say it's raining and you still have to send 10 bits. But suppose that instead of flipping a coin what we do is we run a little random number generator to get our random bit and suppose that the receiver has the same random number generator. So now we're going to run the random number generator and decide whether to say it's raining or ill blue. The receiver can also run that random number generator and see which random bit we used and so actually we can communicate the random bit that we use for picking between these two codes. And so the receiver can actually get 11 bits of information. You can get the 10 bits of information that tell them whether it's raining or not and you can get another bit of information which might just be about a random number generator but instead of using a random number generator we can just take some other message we want to communicate. Suppose you're doing consulting on the side and you want to do something else at the same time you can communicate this other message one bit at a time by your choice between whether you say ill blue or it's raining. So it turns out that a stochastic choice is actually the right thing to do and if you look at a free energy function then the number of bits it takes to send the code is like the energy and the probabilities you're assigned to code is like the entropy and the best thing to do is use a Boltzmann distribution. So if you've got a code that's a bit longer than another code use that less often but you do occasionally use it and so what happens is results from statistical physics show up encoding theory like this if you've got alternative codes. It's actually better to use the code stochastic than to just pick the cheapest one. That's a little bit of physics showing up in. Which is you know it's a nice example of the kind of the confluence of fields you know the work that Shannon did on entropy has now found its way all into quantum information and the way that people think about information and physical systems. I want to riff a little bit on the subject of cooperation and message passing. So physics has this very long tradition of enormous worldwide collaborative projects of which CERN the European Organization for Nuclear Research is one of them. The history is really amazing you know kind of founded to induce cooperation after World War II but managed to do amazing things like discover the Higgs Boson which won the Nobel Prize a couple years ago and by putting a lot of very very smart people on a similar task was able to make amazing progress. It feels like we might need something like a CERN for AI nowadays. What would such a thing look like? How would people contribute to it? How might it be the organizing and what would it do? Given the current political situation it would clearly be in Europe. There's a big problem with it. So right now people are organizing a petition to say we should have a CERN for AI and I'm trying to decide whether I want to sign on to that. The main problem is AI is going to be useful for lethal autonomous weapons and all the countries that sell arms like the US and Russia and China and Israel and Britain are all going to want to develop their own lethal autonomous weapons and they're not going to want to collaborate. It's also good for cryptographic attacks and things like that. Cyber attacks. So I think there's a lot of reasons why it's going to be much harder to get countries to collaborate. Like imagine you try to set up CERN at a point where people thought okay we've got the atom bomb, we've got the H bomb and then there's some other kind of bomb we haven't quite got yet. Let's all collaborate to explore it. It wouldn't have worked like that. They had to believe that we got the H bomb now and doing this high energy physics probably isn't going to give us anything worse than that so we can collaborate. I think it will be a very good idea if we can make it happen but I think there's going to be a lot of political reasons why governments don't want to do it. Maybe we'll just ask one more question and then we'll get to some questions from the audience. The analogies between deep learning and big world changing weapons like the nuclear bombs, there's a lot of really interesting resonances there and I think one of them is the fact that the underlying physics that governs how atom bombs work is stuff that I teach in my graduate particle physics classes because it is part of how the world is described at the fundamental level but of course if you take that with the quantity of some material you can dig up somewhere and make something that is you know unimaginably destructive and I wonder how you think we should be balancing these desires for understanding this tool that is AI is going to do amazing things for good certainly in addition to all the things that it could do to make the world worse but how do we balance the understanding with the need for safety. Okay so there's one thing I think quite strongly which is you wouldn't make it so you could buy on the web fissile material because with nuclear weapons the difficult bit is getting the fissile material I mean it's all difficult but that's the most difficult bit that's very expensive and if you want to stop people having them you bomb the facilities and make the fissile material so you'd be crazy to release the fissile material so because maybe researchers say well I'd like some fissile material so I can play with bombs oh well we should give them it's unfair not to give it to researchers in universities when countries have it but it'd be crazy and meta started it but they now release the weights of these large foundation models and the point is the main constraint on what you can do with AI is that it takes a lot of money and a lot of data to train a large foundation model. Once you've got the weights of the model you can then use that model for doing all sorts of other things with not much training so I think they were completely crazy to make the weights public. People talked about it like it was open sourcing but it's not like open sourcing at all in open sourcing you release code and people look at that code and say wait a minute that line is a bit suspicious when you open source weights they don't look at it and say oh look this weights are a bit suspicious they just take those weights and they train them to do something else but they're starting from a very good starting point so that boat is sailed already but it was crazy to release the weights of these big models because that means any cyber criminal who can get his hands on a few hundred thousand dollars can retrain one of them. Sorry that's bad news right? Given that this stuff is already out there in the world do you think there are things that we can learn from it that will help us prevent some catastrophe that will come from the larger models that are trained after it? I'm quite pessimistic so my friend Jan Laker who works for meta is very optimistic and it turns out your personality has a big effect on what you predict the future is going to be like and it turns out I'm right so psychological research shows that if you take normal healthy people normal cheerful healthy Americans and you ask them to predict what's the chance that in the next five years you will remember if your immediate family will be seriously injured in a car accident. I don't know the action I can't remember the numbers but they'll say things like one percent and actually it's like ten percent and you ask them what's the chance that you or a member of your immediate family will get cancer in the next five years and again they're much too low they just underestimate the probability of these awful things happening. If you take paranormal schizophrenics they overestimate if you take mildly depressed people they get it just right. So maybe we have well it would be great to continue the conversation with some questions for the audience so yeah what do you go ahead and yeah. Really nice conversation. I don't know if we have a mic runner or not if not oh we do okay so questions from the audience and I see a hand raised in the back there. So all Dr. Hinton wonderful talk not only for the experts but also for the general public and would like to ask you specifically my name is Zim Rizvi there is a book also come and over recently Genesis Mr. Ksenja was also one of the authors and then other owners of. When you put your hand like that you put the mic the other way and I can't hear what you're saying. And there's a mention of an ethos based future development of AI where they talk about the human ethos being built as part of the whole regulatory framework and understanding it that you alluded to globally a corporation that need to be in place before we pursue the artificial intelligence to get to a point where the AGI is achieved or we go beyond that. We'd like to kind of know about like once there is a new tech comes there's always this debate about regulation and when that regulation is required and when it's too early about it and or we too late already for the artificial intelligence your answer to that will be appreciated. Okay so the first point is we're not going to be able to slow it down because it's got too many very good uses. I mean more or less all industries can make use of AI it's going to be very useful in healthcare it's going to be very useful in education it's going to be very useful in designing new materials which may be very helpful for things like climate change. So we're not going to be able to slow it down the question is are we going to be able to develop it safely and there seems to be not much political will to do that. People are willing to talk about things like discrimination and bias things they understand but most people still haven't understood that these things really do understand what they're saying we're producing these alien intelligences. For now we're in control but we're making them into agents so they get to do things in the world and they're very soon going to realize that a good thing to do to achieve your goals is to get more control and so I'm very worried that we're in a situation now where we'd like really strong sensible governments with intelligent thoughtful people running them and that's not what we've got. Okay go ahead yeah. Hi Jeff how are you? My question is about power laws and so there's been this theory or idea that as we scale more compute and more data we're going to have more performance models. I think Sam Altman mentioned that it's correlated with the log of the compute in the data but ever since the release of GPT-4 we've seen that every single update of LLMs has been unjurewhelming to a bit like we haven't seen the big jump that we saw from maybe GPT-3.5 to GPT-4 and most recently with the GROG 3 release there was a lot of hype about the amount of compute that went into it with yellow musk and xai but it was also not like that great model. I'm wondering if you think that we've sort of plateaued with regards to compute and data and that we need a different approach. I think it's quite reasonable to think we're getting diminishing returns from this scaling and basically you get a little bit better each time you double the amount of compute that's the log right so it's getting more and more expensive to get these small improvements. However scaling has taken us a long way we've got things that are really pretty smart now and new ideas are also going to take us further and new engineering tricks for making these more efficient going to take us further. So I'd be very surprised if it just plattos. People have been predicting for ages that it's just about to just plateau and it hasn't but it may be that we need more good ideas to make it keep developing. So if you look at Moore's Law for example, Moore's Law for as long as I've been almost as long as I've been alive and people are saying Moore's Law is about to come to an end and just computers just got faster and faster and faster and faster is it making smaller and smaller and smaller and then it got to the point where they were doing like the gigaflop and people said they're not going to get any faster it's the end of Moore's Law and then they went off at right angles and started getting more and more parallel and we got embedded GPUs and things and Moore's Law just kept on in fact even accelerated. So I think it's going to be like that I think scaling took us so far and scaling is maybe like making computers faster and faster but now making computers more and more parallel kept Moore's Law going for another 10 years or so maybe Moore and I think we're going to get new ideas that are going to make things work better. There's not so many smart people working on this but I can't believe there aren't good new ideas that are going to make it work better. Maybe one last question. Oh from the, can we actually, okay we'll have maybe in the back and final one to the front. Hi so the theory of active inference has sensory neurons running parallel to motor neurons and they're tightly coupled and you know the error signal in the sensory neurons activates a motor neuron to make some kind of correction which is, I mean that's about activations and not weights but I'm wondering is there something like back propagation possible here like if you have these actual parallel sets of neurons running is there something like that? Yes you can actually make a version of back propagation work in a brain. It just doesn't work as well as it does in a digital computer and the cerebellum is one place where people thought there may well be something like back propagation because you get this visual slip signal, you get a visual slip signal that can be used to train your vestibular apparatus which is much faster and so you can use visual slip to train vestibular and that's where you get the error signal for back propagation and people are speculated about whether something like back propagation goes on there but you can also do back propagation in a brain by using temporal differences. So back propagation initially seems very implausible because in the forward pass you're sending neural activations and in the backward pass you're sending neural sensitivities. The signal that's coming back is how sensitive is the error to a change in this neuron, a derivative. So that's completely different kind of information but you can use temporal derivatives to stand in for error derivatives. So you can have a system that has two passes and the difference between the activation in those two passes is your error derivative and the sum evidence the brain might be using temporal derivatives as error derivatives. So for example you have neurons that detect motion, you have neurons that detect location and then if you ask well how do you detect motion the obvious way to do it is to look at the difference in location over time but that's not how you do it at all. You have separate neurons that represent motion so why can't you use the difference of location neurons to represent motion and one possible answer is because you're using those temporal differences to represent error derivatives. So there's a whole paper by me and Yoshua Benjia and other people. I don't think Yoshua is an author of that. There's a paper by me and various people on using temporal differences as error derivatives and showing how you can implement a version of back propagation in a brain but the point is it doesn't work very well when you're trying to pack a lot of information into a few connections. It won't get information into bottlenecks. If you have big big generous neural networks with plenty of spectra, these techniques will work and will allow you to do something like back propagation but not as efficiently as in the digital computer. So it may be that the brain has ways of doing stuff like back propagation by using temporal differences but nobody really knows and it's been somewhat disappointing the attempts to understand what the brain is up to in perceptual learning. I don't think we've got there yet. Question from the Martin family. Thank you so much for the fantastic lecture. It was incredibly interesting. Just, I have so many questions. It's hard to choose one but we were just looking at image generation, not image recognition. I was wondering if there was some sort of relationship between the wake state and the sleep state. Like is image generation where the brain system in the wake state is generating the image based off of what it's learned? So I should emphasize that both machines were a nice idea about how you get a learning signal that used simple relationships that you get at thermal equilibrium is physical physics but I don't think there was actually going on unfortunately and they're not particularly good engineering. So I think it may well be that something like sleep-on learning is going on but not the most. Well we've come to the end of the evening so I really would love to thank Dr. Hinton again and Dr. Khan for a wonderful fireside chat and of course for the amazing lecture we heard in the first hour. So let's thank them again.
