Timestamp: 2025-04-18T08:55:12.348238
Title: The New Prompting Rules: How to Prompt Frontier LLM Models like Gemini 2.5, GPT 4.1 &amp; Claude 3.7
URL: https://youtube.com/watch?v=ZjzCJHjzoPs&si=OpFfLnoxi_Sz69-D
Status: success
Duration: 19:41

Description:
好的，这是根据您提供的文本生成的总结：

**总结：**

**一、核心要点：**

通过遵循OpenAI等机构提供的最新提示工程指南，并结合长上下文窗口的优势，可以显著提高大型语言模型（LLM）的输出质量和指令遵循能力，但仍需注意模型可能存在的幻觉问题。

**二、总体框架：**

1.  **引言：** 介绍视频主题，即最新的提示工程规则，并强调这些规则适用于多种前沿模型（如Anthropic的Claude和Google的Gemini）。
2.  **长上下文窗口的优势：**
    *   现代LLM拥有更大的上下文窗口（例如Gemini 2.5 Pro），可以处理更长的对话和更复杂的任务。
    *   更大的上下文窗口允许模型记住更早的上下文信息，从而提高对话的连贯性。
    *   长上下文窗口支持更长的思考链（Chain of Thought），即使模型本身不是专门的推理模型，也能进行一定程度的推理。
    *   可以容纳更大的工具输出，例如网页搜索或数据抓取的结果。
3.  **不再依赖提示词库：**
    *   得益于模型指令遵循能力的提升，不再需要购买包含“魔法技巧”的提示词库。
    *   更重要的是遵循模型提供者（如OpenAI）推荐的结构化方法。
4.  **指令遵循能力的提升：**
    *   新模型（如GPT-4.1）更好地遵循指令，可以指导模型执行步骤的顺序。
    *   可以指示模型以特定顺序输出内容。
5.  **分隔符和格式：**
    *   虽然大多数模型都能理解多种输出格式，但Markdown、JSON和XML是特别好的选择。
    *   XML在处理长上下文时表现良好，可以明确分隔元素。
    *   建议从简单的Markdown开始，根据需要添加XML。
    *   如果要获得重复性输出，可能仍然需要将模型响应分解为不同的提示。
6.  **否定词的使用：**
    *   新模型支持否定词，例如可以指示模型在不知道答案时回复“我不知道”。
    *   这有助于减少模型在信息提取等任务中的幻觉。
7.  **提示结构：**
    *   推荐的提示结构：角色和目标 -> 指令 -> 推理步骤 -> 输出格式 -> 示例 -> 上下文 -> 重复指令（可选）。
    *   在最终指令中，可以要求模型逐步思考（think step by step），以触发思考链推理。
8.  **幻觉问题：**
    *   即使使用最新的模型和提示技巧，仍然可能出现幻觉。
    *   需要评估模型并预料到可能出现的幻觉。

**三、 mermaid conceptual map:**

<Mermaid_Diagram>
graph LR
    subgraph "提示工程的核心概念"
    A[角色与目标] --> B(指令);
    B --> C{推理步骤};
    C --> D[输出格式];
    D --> E{示例};
    E --> F[上下文信息];
    F --> G(重复指令 - 可选);
    end

    subgraph "长上下文窗口的优势"
    H[更大的上下文窗口] -- 支持 --> I{处理复杂任务};
    I -- 提高 --> J[对话连贯性];
    I --> K[思考链推理];
    I --> L[容纳工具输出];
    end

    subgraph "指令遵循能力的提升"
    M[指令遵循能力] --> N(排序执行步骤);
    N --> O[特定顺序输出];
    end

    subgraph "格式选择"
    P[Markdown] --> R{简单提示};
    Q[XML] --> S{长上下文};
    T[JSON]
    R -- 适用于 --> P
    S -- 适用于 --> Q
    end

    subgraph "幻觉问题"
    U[模型幻觉] --> V(需要评估);
    V --> W(需要预期);
    end

    A -->X(遵循OpenAI指南);
    H -->X;
    M -->X;
    P -->X;
    Q -->X;
    T -->X;
    U -->X;
    X --> Y{提升模型输出质量};

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#f9f,stroke:#333,stroke-width:2px
    style G fill:#f9f,stroke:#333,stroke-width:2px
    style H fill:#ccf,stroke:#333,stroke-width:2px
    style I fill:#ccf,stroke:#333,stroke-width:2px
    style J fill:#ccf,stroke:#333,stroke-width:2px
    style K fill:#ccf,stroke:#333,stroke-width:2px
    style L fill:#ccf,stroke:#333,stroke-width:2px
    style M fill:#ddf,stroke:#333,stroke-width:2px
    style N fill:#ddf,stroke:#333,stroke-width:2px
    style O fill:#ddf,stroke:#333,stroke-width:2px
    style P fill:#eed,stroke:#333,stroke-width:2px
    style Q fill:#eed,stroke:#333,stroke-width:2px
    style T fill:#eed,stroke:#333,stroke-width:2px
    style R fill:#eed,stroke:#333,stroke-width:2px
    style S fill:#eed,stroke:#333,stroke-width:2px
    style U fill:#faa,stroke:#333,stroke-width:2px
    style V fill:#faa,stroke:#333,stroke-width:2px
    style W fill:#faa,stroke:#333,stroke-width:2px
    style X fill:#fff,stroke:#333,stroke-width:2px
    style Y fill:#aaf,stroke:#333,stroke-width:2px

</Mermaid_Diagram>

**四、补充信息：**

*   视频内容引用了OpenAI的GPT-4.1介绍和提示指南作为材料来源。
*   提到了ML Expert Pro的AI训练营，包括在线课程和直播互动环节。


Content:
WEBVTT Kind: captions Language: en hey everyone my name is Vaneline and in hey everyone my name is Vaneline and in hey everyone my name is Vaneline and in this video we're going to have a look at this video we're going to have a look at this video we're going to have a look at the new rules of prompting or how the the new rules of prompting or how the the new rules of prompting or how the prompting techniques have been evolving prompting techniques have been evolving prompting techniques have been evolving through the past 6 months to an year and through the past 6 months to an year and through the past 6 months to an year and we're going to get a look at the newest we're going to get a look at the newest we're going to get a look at the newest guide by OpenAI on how you should more guide by OpenAI on how you should more guide by OpenAI on how you should more effectively prompt your models of course effectively prompt your models of course effectively prompt your models of course these types of guides are very general these types of guides are very general these types of guides are very general and apply to other Frontier models such and apply to other Frontier models such and apply to other Frontier models such as Antropics Quad 3. 7 and Gemini 2. 5 Pro as Antropics Quad 3. 7 and Gemini 2. 5 Pro as Antropics Quad 3. 7 and Gemini 2. 5 Pro first we're going to go through why first we're going to go through why first we're going to go through why contexts are now much more easier to use contexts are now much more easier to use contexts are now much more easier to use and allow you to get better responses and allow you to get better responses and allow you to get better responses then uh we will have a look at how are then uh we will have a look at how are then uh we will have a look at how are newer models better at following your newer models better at following your newer models better at following your instructions then how you should add instructions then how you should add instructions then how you should add delimiters within your prompts whether delimiters within your prompts whether delimiters within your prompts whether or not you should now be using negating or not you should now be using negating or not you should now be using negating terms how to structure your prompts terms how to structure your prompts terms how to structure your prompts taking into account all of these points taking into account all of these points taking into account all of these points and then at the end I'm going to give and then at the end I'm going to give and then at the end I'm going to give you a bonus tip on how you can build you a bonus tip on how you can build you a bonus tip on how you can build better OM applications let's get started better OM applications let's get started better OM applications let's get started if you want to become better AI engineer if you want to become better AI engineer if you want to become better AI engineer and see how you can write prompts within and see how you can write prompts within and see how you can write prompts within complete AI systems you can subscribe to complete AI systems you can subscribe to complete AI systems you can subscribe to the ML expert pro get things done with the ML expert pro get things done with the ML expert pro get things done with AI boot camp here uh you will get a AI boot camp here uh you will get a AI boot camp here uh you will get a complete text tutorials on how you can complete text tutorials on how you can complete text tutorials on how you can build AI applications also we're going build AI applications also we're going build AI applications also we're going to be doing live sessions that are going to be doing live sessions that are going to be doing live sessions that are going to start on May 9th to May 11th there to start on May 9th to May 11th there to start on May 9th to May 11th there we're going to do three live sessions we're going to do three live sessions we're going to do three live sessions with lectures and then we're going to with lectures and then we're going to with lectures and then we're going to continue after each lecture with a continue after each lecture with a continue after each lecture with a lecture where we're going to do some lecture where we're going to do some lecture where we're going to do some live coding together and you will be live coding together and you will be live coding together and you will be able to ask questions to me and the able to ask questions to me and the able to ask questions to me and the whole M expert pro community so if you whole M expert pro community so if you whole M expert pro community so if you want to join and become a better AI want to join and become a better AI want to join and become a better AI engineer today go and subscribe to M engineer today go and subscribe to M engineer today go and subscribe to M Expert Pro thank you the two main Expert Pro thank you the two main Expert Pro thank you the two main resources that I've used to prepare the resources that I've used to prepare the resources that I've used to prepare the materials for this video are the materials for this video are the materials for this video are the introducing GPT4 one in the API blog introducing GPT4 one in the API blog introducing GPT4 one in the API blog post by Open AI here you can read all of post by Open AI here you can read all of post by Open AI here you can read all of their introduction along with all of the their introduction along with all of the their introduction along with all of the benchmarks that they did and the other benchmarks that they did and the other benchmarks that they did and the other one is the one is the one is the GP4. 1 prompting guide here while they're GP4. 1 prompting guide here while they're GP4. 1 prompting guide here while they're specifying specifying specifying GP41 I would say that this is pretty GP41 I would say that this is pretty GP41 I would say that this is pretty general application of the all of the general application of the all of the general application of the all of the principles to pretty much all of the principles to pretty much all of the principles to pretty much all of the available models that we have the first available models that we have the first available models that we have the first thing that have changed during the last thing that have changed during the last thing that have changed during the last year is the longer context that is year is the longer context that is year is the longer context that is available in pretty much every frontier available in pretty much every frontier available in pretty much every frontier OM right now OM right now OM right now antropic Google and antropic Google and antropic Google and OpenAI have models that are OpenAI have models that are OpenAI have models that are 200, 000 tokens or more in a context 200, 000 tokens or more in a context 200, 000 tokens or more in a context window of course I'm not speaking about window of course I'm not speaking about window of course I'm not speaking about models such as one 4 that can have a 10 models such as one 4 that can have a 10 models such as one 4 that can have a 10 million context window but nobody million context window but nobody million context window but nobody actually cares about the model those actually cares about the model those actually cares about the model those models are actually useful and from my models are actually useful and from my models are actually useful and from my prompting in the last couple of weeks of prompting in the last couple of weeks of prompting in the last couple of weeks of Gemini 2. 5 I was really impressed with Gemini 2. 5 I was really impressed with Gemini 2. 5 I was really impressed with how this model is able to handle very how this model is able to handle very how this model is able to handle very long conversations i had conversations long conversations i had conversations long conversations i had conversations where the context or the initial context where the context or the initial context where the context or the initial context was roughly 120, 000 tokens and from was roughly 120, 000 tokens and from was roughly 120, 000 tokens and from there I was continuing the conversation there I was continuing the conversation there I was continuing the conversation and the model was pretty much and the model was pretty much and the model was pretty much remembering everything and was able to remembering everything and was able to remembering everything and was able to refer to the context even further down refer to the context even further down refer to the context even further down the conversation itself within the the conversation itself within the the conversation itself within the guides from OpenAI i found this right guides from OpenAI i found this right guides from OpenAI i found this right here 1 million tokens is more than eight here 1 million tokens is more than eight here 1 million tokens is more than eight copies of the entire React code base so copies of the entire React code base so copies of the entire React code base so for example if you're using VS Code or for example if you're using VS Code or for example if you're using VS Code or cursor you'll probably be able to fit cursor you'll probably be able to fit cursor you'll probably be able to fit everything within the context along with everything within the context along with everything within the context along with your instructions for the database and your instructions for the database and your instructions for the database and etc also this allows for models to have etc also this allows for models to have etc also this allows for models to have longer chain of thought conversations longer chain of thought conversations longer chain of thought conversations within themsel so even if models are not within themsel so even if models are not within themsel so even if models are not specifically reasoning models such as R1 specifically reasoning models such as R1 specifically reasoning models such as R1 and the all models from OpenAI your and the all models from OpenAI your and the all models from OpenAI your models will be able to still use some of models will be able to still use some of models will be able to still use some of the available tokens for chain of toad the available tokens for chain of toad the available tokens for chain of toad and probably give you even better and probably give you even better and probably give you even better results also if you have tools that are results also if you have tools that are results also if you have tools that are outputting large amounts of text for outputting large amounts of text for outputting large amounts of text for example uh when you're using web search example uh when you're using web search example uh when you're using web search or maybe when you're doing some scraping or maybe when you're doing some scraping or maybe when you're doing some scraping or have very large documents that are or have very large documents that are or have very large documents that are being returned as a result of a tool being returned as a result of a tool being returned as a result of a tool execution this will be nicely added to execution this will be nicely added to execution this will be nicely added to the prompts that you have and uh I've the prompts that you have and uh I've the prompts that you have and uh I've already told you from my experience for already told you from my experience for already told you from my experience for example with Gemini 2. 5 Pro it feels example with Gemini 2. 5 Pro it feels example with Gemini 2. 5 Pro it feels like the context windows are finally like the context windows are finally like the context windows are finally being useful also here on this chart on being useful also here on this chart on being useful also here on this chart on the right you see a benchmark by openai the right you see a benchmark by openai the right you see a benchmark by openai and here they're saying that uh for and here they're saying that uh for and here they're saying that uh for example on the x-axis you have this up example on the x-axis you have this up example on the x-axis you have this up to a million tokens on the right even to a million tokens on the right even to a million tokens on the right even though there is a decrease in accuracy though there is a decrease in accuracy though there is a decrease in accuracy of the performance of the models uh of the performance of the models uh of the performance of the models uh models such as GPT4. 1 mini and models such as GPT4. 1 mini and models such as GPT4. 1 mini and GPT4. 1 are pretty good at getting GPT4. 1 are pretty good at getting GPT4. 1 are pretty good at getting accuracy in even larger context accuracy in even larger context accuracy in even larger context unfortunately they didn't compare their unfortunately they didn't compare their unfortunately they didn't compare their models to something like Gemini 2. 5 Pro models to something like Gemini 2. 5 Pro models to something like Gemini 2. 5 Pro or Quad 3. 7 one of the selling materials or Quad 3. 7 one of the selling materials or Quad 3. 7 one of the selling materials that AI gurus were essentially selling that AI gurus were essentially selling that AI gurus were essentially selling to their users were the so-called to their users were the so-called to their users were the so-called prompting libraries and while there is prompting libraries and while there is prompting libraries and while there is some value within those it is probably some value within those it is probably some value within those it is probably not as effective anymore since you uh not as effective anymore since you uh not as effective anymore since you uh probably don't have to buy prompt probably don't have to buy prompt probably don't have to buy prompt library like whatever that contains some library like whatever that contains some library like whatever that contains some magical tricks within it as long as you magical tricks within it as long as you magical tricks within it as long as you are following the structures that the are following the structures that the are following the structures that the authors of those models are actually authors of those models are actually authors of those models are actually providing now of course you would still providing now of course you would still providing now of course you would still need to write your prompts or you will need to write your prompts or you will need to write your prompts or you will need to ask the models nicely to write need to ask the models nicely to write need to ask the models nicely to write better prompts for you but you will better prompts for you but you will better prompts for you but you will still need to know how to structure them still need to know how to structure them still need to know how to structure them and uh here again by OpenAI they have and uh here again by OpenAI they have and uh here again by OpenAI they have evaluation internal benchmark that shows evaluation internal benchmark that shows evaluation internal benchmark that shows that the newer models that the newer models that the newer models 4. 1 for example right here are much 4. 1 for example right here are much 4. 1 for example right here are much better at instruction following and what better at instruction following and what better at instruction following and what does this mean you can essentially tell does this mean you can essentially tell does this mean you can essentially tell now the models to order the execution of now the models to order the execution of now the models to order the execution of the steps that you want them to do so the steps that you want them to do so the steps that you want them to do so for example for agentic applications for example for agentic applications for example for agentic applications this will be pretty useful to tell for this will be pretty useful to tell for this will be pretty useful to tell for example an agent to first start with uh example an agent to first start with uh example an agent to first start with uh reading some material then based on the reading some material then based on the reading some material then based on the reading of the material use some context reading of the material use some context reading of the material use some context from the user etc and follow this order from the user etc and follow this order from the user etc and follow this order of operations also you will be able now of operations also you will be able now of operations also you will be able now to tell the models to order the outputs to tell the models to order the outputs to tell the models to order the outputs in some order that you specify and of in some order that you specify and of in some order that you specify and of course the models are much better at course the models are much better at course the models are much better at outputting the materials into a specific outputting the materials into a specific outputting the materials into a specific format while most of the models are format while most of the models are format while most of the models are pretty good with pretty much every pretty good with pretty much every pretty good with pretty much every output format I found that models are output format I found that models are output format I found that models are particularly good with markdown JSON and particularly good with markdown JSON and particularly good with markdown JSON and XML for the last couple of months the XML for the last couple of months the XML for the last couple of months the quote models were the one that quote models were the one that quote models were the one that introduced the XML formatting within introduced the XML formatting within introduced the XML formatting within your prompts and even if you look your prompts and even if you look your prompts and even if you look throughout the system prompts within throughout the system prompts within throughout the system prompts within their models and the examples that their models and the examples that their models and the examples that they're providing pretty much every they're providing pretty much every they're providing pretty much every prompt has some sort of XML delimiters prompt has some sort of XML delimiters prompt has some sort of XML delimiters and now actually OpenAI are also and now actually OpenAI are also and now actually OpenAI are also recommending this as well they even said recommending this as well they even said recommending this as well they even said within one of the materials XML within one of the materials XML within one of the materials XML performed well in our long context performed well in our long context performed well in our long context testing and from my personal experience testing and from my personal experience testing and from my personal experience this is actually the case if I'm this is actually the case if I'm this is actually the case if I'm providing some sort of specific providing some sort of specific providing some sort of specific instructions and my prompts are getting instructions and my prompts are getting instructions and my prompts are getting larger and larger I would probably try larger and larger I would probably try larger and larger I would probably try to incorporate some XML that would say to incorporate some XML that would say to incorporate some XML that would say the limits of the elements that I want the limits of the elements that I want the limits of the elements that I want and here from the blog post by OpenAI and here from the blog post by OpenAI and here from the blog post by OpenAI they again have an example with uh XML they again have an example with uh XML they again have an example with uh XML formatting for example something that formatting for example something that formatting for example something that even has attributes to the XML type it even has attributes to the XML type it even has attributes to the XML type it has some internal structure input and has some internal structure input and has some internal structure input and output example so this is a good uh use output example so this is a good uh use output example so this is a good uh use case for XML formatting so the case for XML formatting so the case for XML formatting so the recommendation here is to start with recommendation here is to start with recommendation here is to start with markdown as long as you have simpler markdown as long as you have simpler markdown as long as you have simpler prompts and again most of the frontier prompts and again most of the frontier prompts and again most of the frontier models are very good at understanding models are very good at understanding models are very good at understanding markdown they do very well with markdown they do very well with markdown they do very well with formatting of titles numbered and formatting of titles numbered and formatting of titles numbered and ordered lists and back ticks in order to ordered lists and back ticks in order to ordered lists and back ticks in order to wrap some code within the document so wrap some code within the document so wrap some code within the document so for example you can use the titles at for example you can use the titles at for example you can use the titles at first headline H2 H3 etc i found that first headline H2 H3 etc i found that first headline H2 H3 etc i found that this works pretty well and of course you this works pretty well and of course you this works pretty well and of course you can combine some markdown with XML can combine some markdown with XML can combine some markdown with XML probably most of the models are going to probably most of the models are going to probably most of the models are going to be quite all right with that still if be quite all right with that still if be quite all right with that still if you want to get some sort of repetitive you want to get some sort of repetitive you want to get some sort of repetitive output from the models you will probably output from the models you will probably output from the models you will probably still need to break down the model still need to break down the model still need to break down the model response into different prompts so keep response into different prompts so keep response into different prompts so keep that in mind those models are nowhere that in mind those models are nowhere that in mind those models are nowhere near perfect at least as as of now that near perfect at least as as of now that near perfect at least as as of now that is and I found in my personal experience is and I found in my personal experience is and I found in my personal experience that is that when you have dumber model that is that when you have dumber model that is that when you have dumber model in my case I was prompting GPT4 mini for in my case I was prompting GPT4 mini for in my case I was prompting GPT4 mini for a while Gemini 2. 0 flash in my work and a while Gemini 2. 0 flash in my work and a while Gemini 2. 0 flash in my work and there you will probably want to add a there you will probably want to add a there you will probably want to add a bit stricter delimiters in order to bit stricter delimiters in order to bit stricter delimiters in order to provide better um outputs from models provide better um outputs from models provide better um outputs from models and in my case this was pretty much XML and in my case this was pretty much XML and in my case this was pretty much XML and it was working quite all right for and it was working quite all right for and it was working quite all right for Gemini and GPT4 mini probably GP 4. 1 Gemini and GPT4 mini probably GP 4. 1 Gemini and GPT4 mini probably GP 4. 1 Mini is going to be even better than Mini is going to be even better than Mini is going to be even better than that this one is pretty that this one is pretty that this one is pretty self-explanatory at least in the past self-explanatory at least in the past self-explanatory at least in the past you were encouraged to provide positive you were encouraged to provide positive you were encouraged to provide positive reinforcement or positive examples in reinforcement or positive examples in reinforcement or positive examples in order to get what you want but at least order to get what you want but at least order to get what you want but at least OpenAI are now saying that within their OpenAI are now saying that within their OpenAI are now saying that within their GP4. 1 models and up you can use negating GP4. 1 models and up you can use negating GP4. 1 models and up you can use negating terms this is really important when you terms this is really important when you terms this is really important when you want to tell the model something like if want to tell the model something like if want to tell the model something like if you don't know the answer please reply you don't know the answer please reply you don't know the answer please reply with I don't know and uh it this would with I don't know and uh it this would with I don't know and uh it this would be pretty helpful in so many examples be pretty helpful in so many examples be pretty helpful in so many examples for example when you're doing some for example when you're doing some for example when you're doing some extraction of information from some extraction of information from some extraction of information from some documents in those cases if the documents in those cases if the documents in those cases if the extraction is not available or the extraction is not available or the extraction is not available or the information is not available within the information is not available within the information is not available within the document you would want the model to document you would want the model to document you would want the model to reply with I don't know but I found that reply with I don't know but I found that reply with I don't know but I found that previous models were extremely likely to previous models were extremely likely to previous models were extremely likely to hallucinate in such cases hopefully hallucinate in such cases hopefully hallucinate in such cases hopefully those newer models are much better at those newer models are much better at those newer models are much better at least from what I found within the least from what I found within the least from what I found within the Gemini 2. 5 Pro this is really the case Gemini 2. 5 Pro this is really the case Gemini 2. 5 Pro this is really the case so how do you structure a prompt now so how do you structure a prompt now so how do you structure a prompt now with all the new available tools that with all the new available tools that with all the new available tools that you have and the capabilities of the you have and the capabilities of the you have and the capabilities of the models so this is a structure or example models so this is a structure or example models so this is a structure or example structure from OpenAI and this is the structure from OpenAI and this is the structure from OpenAI and this is the recommended starting point so first you recommended starting point so first you recommended starting point so first you still want to start with a role and still want to start with a role and still want to start with a role and objective and uh for example you're a objective and uh for example you're a objective and uh for example you're a customer support agent that needs to customer support agent that needs to customer support agent that needs to reply to the customers etc and then you reply to the customers etc and then you reply to the customers etc and then you continue with the instructions that you continue with the instructions that you continue with the instructions that you want to provide provide quick responses want to provide provide quick responses want to provide provide quick responses to the user etc and then you might have to the user etc and then you might have to the user etc and then you might have one or more reasoning steps use the one or more reasoning steps use the one or more reasoning steps use the company rules to etc so uh in here you company rules to etc so uh in here you company rules to etc so uh in here you will probably apply all of the steps will probably apply all of the steps will probably apply all of the steps that you want your model to follow in that you want your model to follow in that you want your model to follow in order to answer the question and this order to answer the question and this order to answer the question and this will pretty much be within some sort of will pretty much be within some sort of will pretty much be within some sort of system prompt then you will probably be system prompt then you will probably be system prompt then you will probably be happy to send some output formatting happy to send some output formatting happy to send some output formatting instructions for example here you can instructions for example here you can instructions for example here you can say I want your reply to be in JSON uh say I want your reply to be in JSON uh say I want your reply to be in JSON uh specify the schema for the JSON if you specify the schema for the JSON if you specify the schema for the JSON if you have a pedantic object or something like have a pedantic object or something like have a pedantic object or something like that also most of the models by default that also most of the models by default that also most of the models by default seems to be replying in markdown so uh seems to be replying in markdown so uh seems to be replying in markdown so uh this is another great option then you this is another great option then you this is another great option then you can continue with the examples uh here I can continue with the examples uh here I can continue with the examples uh here I have given you an example between a have given you an example between a have given you an example between a customer and a chatbot and uh here you customer and a chatbot and uh here you customer and a chatbot and uh here you can provide some edge case examples can provide some edge case examples can provide some edge case examples where you want to have specific replies where you want to have specific replies where you want to have specific replies based on the provided input and output based on the provided input and output based on the provided input and output so you can go one shot two shot etc from so you can go one shot two shot etc from so you can go one shot two shot etc from my experience if you have for example my experience if you have for example my experience if you have for example three to five H case examples that are three to five H case examples that are three to five H case examples that are somewhat bizarre or strange you should somewhat bizarre or strange you should somewhat bizarre or strange you should put them here uh but also it will be put them here uh but also it will be put them here uh but also it will be helpful to give one at least general helpful to give one at least general helpful to give one at least general example that is pretty straightforward example that is pretty straightforward example that is pretty straightforward pretty common sense but still you pretty common sense but still you pretty common sense but still you include it within the prompt right here include it within the prompt right here include it within the prompt right here uh next is the place when you are going uh next is the place when you are going uh next is the place when you are going to put your context and this is pretty to put your context and this is pretty to put your context and this is pretty much the bulk of your prompt that is uh much the bulk of your prompt that is uh much the bulk of your prompt that is uh if you have for example some information if you have for example some information if you have for example some information from a user database some information from a user database some information from a user database some information about for example customers previous about for example customers previous about for example customers previous purchases or juro tickets or issues purchases or juro tickets or issues purchases or juro tickets or issues machine information device information machine information device information machine information device information etc you should probably put the etc you should probably put the etc you should probably put the information here within the context and information here within the context and information here within the context and one thing that is really interesting one thing that is really interesting one thing that is really interesting here is that OpenAI says that you should here is that OpenAI says that you should here is that OpenAI says that you should pretty much uh repeat the instructions pretty much uh repeat the instructions pretty much uh repeat the instructions at the end since the model is going to at the end since the model is going to at the end since the model is going to be much more attentive in a sense to the be much more attentive in a sense to the be much more attentive in a sense to the final instructions and uh when you're final instructions and uh when you're final instructions and uh when you're providing those you can of course still providing those you can of course still providing those you can of course still ask it to think step by step which will ask it to think step by step which will ask it to think step by step which will unlock if you will the chain of thought unlock if you will the chain of thought unlock if you will the chain of thought reasoning that pretty much every model reasoning that pretty much every model reasoning that pretty much every model has nowadays even if the model doesn't has nowadays even if the model doesn't has nowadays even if the model doesn't have reasoning per se this will do some have reasoning per se this will do some have reasoning per se this will do some sort of minimal reasoning before giving sort of minimal reasoning before giving sort of minimal reasoning before giving you an answer and why this is very you an answer and why this is very you an answer and why this is very strange well basically you will break strange well basically you will break strange well basically you will break the prompt caching with this one which the prompt caching with this one which the prompt caching with this one which is pretty bad in practice so uh you can is pretty bad in practice so uh you can is pretty bad in practice so uh you can think about the ways that this context think about the ways that this context think about the ways that this context might change of course this will not be might change of course this will not be might change of course this will not be cached even though uh this is going to cached even though uh this is going to cached even though uh this is going to be pretty much the same every time that be pretty much the same every time that be pretty much the same every time that you're putting everything together so you're putting everything together so you're putting everything together so for example you might have these for example you might have these for example you might have these instructions that are not cached and of instructions that are not cached and of instructions that are not cached and of course if you're doing thousand or tens course if you're doing thousand or tens course if you're doing thousand or tens of thousands requests this might add up of thousands requests this might add up of thousands requests this might add up to your bills while the models are to your bills while the models are to your bills while the models are advancing at breakneck speed this is one advancing at breakneck speed this is one advancing at breakneck speed this is one very interesting evolution of the newer very interesting evolution of the newer very interesting evolution of the newer models so here you see on the person QA models so here you see on the person QA models so here you see on the person QA evaluation the accuracy of the different evaluation the accuracy of the different evaluation the accuracy of the different models and then on the next row you see models and then on the next row you see models and then on the next row you see the hallucination rate i've uh read a the hallucination rate i've uh read a the hallucination rate i've uh read a lot of people that are saying that you lot of people that are saying that you lot of people that are saying that you can essentially zero out the can essentially zero out the can essentially zero out the hallucinations of these models i don't hallucinations of these models i don't hallucinations of these models i don't find this to be the case pretty much find this to be the case pretty much find this to be the case pretty much every model that I've tried thus far I every model that I've tried thus far I every model that I've tried thus far I was able to get it to hallucinate even was able to get it to hallucinate even was able to get it to hallucinate even without uh strong or forced approaches without uh strong or forced approaches without uh strong or forced approaches to it so here you see that models such to it so here you see that models such to it so here you see that models such as 01 which is not as powerful as 03 and as 01 which is not as powerful as 03 and as 01 which is not as powerful as 03 and 04 mini based on the accuracy right here 04 mini based on the accuracy right here 04 mini based on the accuracy right here uh you also see that this model has a uh you also see that this model has a uh you also see that this model has a relatively lower rate of hallucination relatively lower rate of hallucination relatively lower rate of hallucination while while while O3 has pretty much double and then on O3 has pretty much double and then on O3 has pretty much double and then on top of that O4 mini has pretty much uh top of that O4 mini has pretty much uh top of that O4 mini has pretty much uh 50% rate of hallucination so uh these 50% rate of hallucination so uh these 50% rate of hallucination so uh these numbers you of course need to go through numbers you of course need to go through numbers you of course need to go through the benchmark but still these numbers the benchmark but still these numbers the benchmark but still these numbers tell us that the models are going to a tell us that the models are going to a tell us that the models are going to a bit higher hallucination rate compared bit higher hallucination rate compared bit higher hallucination rate compared to the previous more simple models of to the previous more simple models of to the previous more simple models of course all these models are reasoning course all these models are reasoning course all these models are reasoning models i would be very happy to see models i would be very happy to see models i would be very happy to see evolations on models that are not evolations on models that are not evolations on models that are not reasoning ones but essentially what I'm reasoning ones but essentially what I'm reasoning ones but essentially what I'm saying here is that you should not trust saying here is that you should not trust saying here is that you should not trust the responses you should be able to the responses you should be able to the responses you should be able to evaluate the models that you're using evaluate the models that you're using evaluate the models that you're using and probably expect to still get some and probably expect to still get some and probably expect to still get some sort of hallucinations even if you're sort of hallucinations even if you're sort of hallucinations even if you're having very nice evaluation sets so with having very nice evaluation sets so with having very nice evaluation sets so with tools and tips that I've shown you from tools and tips that I've shown you from tools and tips that I've shown you from the guides from OpenAI you can now try the guides from OpenAI you can now try the guides from OpenAI you can now try to write even better prompts i found to write even better prompts i found to write even better prompts i found that using the correct delimiters and that using the correct delimiters and that using the correct delimiters and the pretty much good formatting and the the pretty much good formatting and the the pretty much good formatting and the structure that they're providing you can structure that they're providing you can structure that they're providing you can pretty much get better results of course pretty much get better results of course pretty much get better results of course your outputs are not going to be your outputs are not going to be your outputs are not going to be deterministic and nobody can guarantee deterministic and nobody can guarantee deterministic and nobody can guarantee you that the models are going to provide you that the models are going to provide you that the models are going to provide good answers so keep in mind that you good answers so keep in mind that you good answers so keep in mind that you should have pretty good evaluation should have pretty good evaluation should have pretty good evaluation metrics in place thank you for watching metrics in place thank you for watching metrics in place thank you for watching guys please like share and subscribe guys please like share and subscribe guys please like share and subscribe also join the Discord channel that I'm also join the Discord channel that I'm also join the Discord channel that I'm going to link down into the description going to link down into the description going to link down into the description of this video and if you want to become of this video and if you want to become of this video and if you want to become better AI engineer go and subscribe to M better AI engineer go and subscribe to M better AI engineer go and subscribe to M Expert Pro again we're going to do live Expert Pro again we're going to do live Expert Pro again we're going to do live sessions between 9th of May to 11th sessions between 9th of May to 11th sessions between 9th of May to 11th again we'll have lectures and WAP again we'll have lectures and WAP again we'll have lectures and WAP sessions where we're going to do some sessions where we're going to do some sessions where we're going to do some live coding together and you'll be able live coding together and you'll be able live coding together and you'll be able to ask questions to me and the whole M to ask questions to me and the whole M to ask questions to me and the whole M Expert Pro community and I'll see you in Expert Pro community and I'll see you in Expert Pro community and I'll see you in the next one bye
