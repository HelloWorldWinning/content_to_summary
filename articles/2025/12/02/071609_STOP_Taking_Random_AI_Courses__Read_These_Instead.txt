Timestamp: 2025-12-02T07:16:09.553873
Title: STOP Taking Random AI Courses. Read These Instead
URL: https://youtube.com/watch?v=KPhZkvPBiMk&si=JG__s9MukNvgEkvx
Status: success
Duration: 9:54

Description:
好的，这是根据您提供的文本内容，为您精心提炼和组织的摘要。

### **核心框架：学习AI的系统性路径**

本文提供了一个从入门到精通的系统性学习框架，其核心是**“目标驱动、方法先行、实践验证、持续探索”**。它强调，成为真正的AI专家，关键在于从学术源头（即研究论文）获取知识，而非仅仅停留在表面教程。

---

### **学习AI的核心纲要**

**一、 高效阅读AI论文的方法论**

1.  **设定务实的目标 (Set Goals)**
    *   **新手入门**: 从一个特定主题的5篇核心论文开始，完成即反思，避免初期倦怠。
    *   **进阶学习**: 采用5-10篇论文的短周期迭代模式，不断深入或调整方向。

2.  **采用三步阅读法 (3-Pass Reading Strategy)**
    *   **第一步：快速筛选 (Skim & Sort)**: 花10%的时间浏览标题、摘要、图表，判断论文是否值得深入阅读，果断舍弃不相关的。
    *   **第二步：整体理解 (Understand the Big Picture)**: 先阅读**引言**和**结论**，快速掌握论文的核心目标、方法和成果。
    *   **第三步：深入核心 (Dive into the Core)**: 在理解整体框架后，再深入研究论文的核心方法和结果部分，允许跳过暂时不懂的细节，通过实践和反复阅读加深理解。

**二、 奠定AI基础的五篇关键论文**

1.  **《Attention Is All You Need》 (Transformer)**: 介绍了Transformer架构，其“自注意力机制”是所有现代大语言模型（LLM）的技术基石。
2.  **GPT-3 论文**: 证明了通过扩大模型规模可以使其产生“涌现能力”（如上下文学习），从而将行业焦点从模型微调转向了**提示工程 (Prompt Engineering)**。
3.  **合成数据 (Synthetic Data) 论文**: 探讨了使用AI模型（如GPT-4）生成数据来训练其他模型的方法，指出合成数据需与真实数据结合使用，并注意质量控制。
4.  **RAG (检索增强生成) 论文**: 提出了一种解决LLM幻觉和知识过时问题的有效方案。它通过从外部数据库检索最新信息来增强模型的回答，使其更准确、可信。
5.  **MCP (模型上下文协议) 论文**: 提出了一个标准化协议，使AI模型能够与API、数据库等外部工具交互，从而执行真实世界中的复杂、多步骤任务。**RAG让模型获取信息，MCP让模型采取行动**。

**三、 寻找更多论文的资源库**

*   **Hugging Face**: 将论文与模型、数据集和代码紧密结合，实践性强。
*   **This AI**: 按主题和时间线整理论文，适合追溯一个领域的历史演变。
*   **Deep Learning Monitor**: 追踪最新、最热门的论文，保持对前沿的敏感度。
*   **Archive Sanity**: arXiv的优化版，提供更友好的搜索和浏览体验。

---

### **核心结论**

通过掌握高效的论文阅读策略并从关键研究入手，是任何希望深入理解并应用AI技术的人从入门到精通的核心路径。

---

<Mermaid_Diagram>
graph TD
    A["从新手到AI专家 (AI Mastery Path)"];
    
    subgraph "第一步: 掌握方法论 (Methodology)"
        B["设定务实目标 (Set Goals)"];
        C["三步阅读法 (3-Pass Reading)"];
        B --> C;
        C --> C1["1. 快速筛选 (Skim)"];
        C --> C2["2. 整体理解 (Understand)"];
        C --> C3["3. 深入核心 (Deep Dive)"];
    end

    subgraph "第二步: 核心实践 (Core Practice)"
        D["阅读5大基石论文"];
        D --> D1["1. Transformer <br> (现代LLM基石)"];
        D --> D2["2. GPT-3 <br> (开启提示工程)"];
        D --> D3["3. 合成数据 <br> (解决数据瓶颈)"];
        D --> D4["4. RAG <br> (连接外部知识)"];
        D --> D5["5. MCP <br> (执行真实世界任务)"];
    end
    
    subgraph "第三步: 持续探索 (Continuous Exploration)"
        E["寻找更多论文"];
        E --> E1["Hugging Face"];
        E --> E2["This AI"];
        E --> E3["Deep Learning Monitor"];
        E --> E4["Archive Sanity"];
    end

    A -- "学习路径" --> B;
    C -- "指导" --> D;
    D -- "建立基础后" --> E;
    E -- "循环迭代" --> B;

    style A fill:#FFD700,stroke:#333,stroke-width:2px,color:#333;
    
    style B fill:#ADD8E6,stroke:#333,stroke-width:1px;
    style C fill:#ADD8E6,stroke:#333,stroke-width:1px;
    style C1 fill:#E0FFFF,stroke:#333,stroke-width:1px;
    style C2 fill:#E0FFFF,stroke:#333,stroke-width:1px;
    style C3 fill:#E0FFFF,stroke:#333,stroke-width:1px;

    style D fill:#90EE90,stroke:#333,stroke-width:1px;
    style D1 fill:#DFF0D8,stroke:#333,stroke-width:1px;
    style D2 fill:#DFF0D8,stroke:#333,stroke-width:1px;
    style D3 fill:#DFF0D8,stroke:#333,stroke-width:1px;
    style D4 fill:#DFF0D8,stroke:#333,stroke-width:1px;
    style D5 fill:#DFF0D8,stroke:#333,stroke-width:1px;

    style E fill:#FFB6C1,stroke:#333,stroke-width:1px;
    style E1 fill:#FFE4E1,stroke:#333,stroke-width:1px;
    style E2 fill:#FFE4E1,stroke:#333,stroke-width:1px;
    style E3 fill:#FFE4E1,stroke:#333,stroke-width:1px;
    style E4 fill:#FFE4E1,stroke:#333,stroke-width:1px;

    linkStyle 3 stroke:#2E8B57,stroke-width:2px,stroke-dasharray: 5 5;
    linkStyle 4 stroke:#4682B4,stroke-width:2px;
</Mermaid_Diagram>

Content:
Many people try to learn AI through, Many people try to learn AI through, tutorials. But inside the industry, the, tutorials. But inside the industry, the, tutorials. But inside the industry, the, real experts are learning from reading, real experts are learning from reading, real experts are learning from reading, papers. I learned this back in 2017 when, papers. I learned this back in 2017 when, papers. I learned this back in 2017 when, I was an engineering manager at Meta. A, I was an engineering manager at Meta. A, I was an engineering manager at Meta. A, paper came out called attention is all, paper came out called attention is all, paper came out called attention is all, you need. And this paper changed, you need. And this paper changed, you need. And this paper changed, everything. So today I'm going to walk, everything. So today I'm going to walk, everything. So today I'm going to walk, you through the five papers that will, you through the five papers that will, you through the five papers that will, give you a strong foundation, how to, give you a strong foundation, how to, give you a strong foundation, how to, read them efficiently, and where to find, read them efficiently, and where to find, read them efficiently, and where to find, more papers without getting lost. And by, more papers without getting lost. And by, more papers without getting lost. And by, the way, this video is sponsored by, the way, this video is sponsored by, the way, this video is sponsored by, Warp, but more on them later. Let's, Warp, but more on them later. Let's, Warp, but more on them later. Let's, start with how to actually read papers., start with how to actually read papers., start with how to actually read papers., Here's what most people do. They find, Here's what most people do. They find, Here's what most people do. They find, the paper and then they try to read it, the paper and then they try to read it, the paper and then they try to read it, like a novel, trying to understand every, like a novel, trying to understand every, like a novel, trying to understand every, single word in the paper. And that's not, single word in the paper. And that's not, single word in the paper. And that's not, going to work. So, let me give you five, going to work. So, let me give you five, going to work. So, let me give you five, tips that actually works when you're, tips that actually works when you're, tips that actually works when you're, reading papers. Tip one is to set goals, reading papers. Tip one is to set goals, reading papers. Tip one is to set goals, that actually make sense. So, Andrew, that actually make sense. So, Andrew, that actually make sense. So, Andrew, Yang, he's a co-founder of Corsera. He's, Yang, he's a co-founder of Corsera. He's, Yang, he's a co-founder of Corsera. He's, also famous for teaching many of the, also famous for teaching many of the, also famous for teaching many of the, best machine learning courses ever. and, best machine learning courses ever. and, best machine learning courses ever. and, he says, "If you want a basic, he says, "If you want a basic, he says, "If you want a basic, understanding of a topic, read between, understanding of a topic, read between, understanding of a topic, read between, 15 to 20 papers. And if you want deeper, 15 to 20 papers. And if you want deeper, 15 to 20 papers. And if you want deeper, expertise, read 50 to 100." And that's a, expertise, read 50 to 100." And that's a, expertise, read 50 to 100." And that's a, good goal if you're aiming for expert, good goal if you're aiming for expert, good goal if you're aiming for expert, level research type roles in a specific, level research type roles in a specific, level research type roles in a specific, area. But honestly, that's a pretty high, area. But honestly, that's a pretty high, area. But honestly, that's a pretty high, bar. And if you're just starting out and, bar. And if you're just starting out and, bar. And if you're just starting out and, you're still figuring out what you want, you're still figuring out what you want, you're still figuring out what you want, to focus on, I would start with about, to focus on, I would start with about, to focus on, I would start with about, five papers. When you finish them, five papers. When you finish them, five papers. When you finish them, celebrate because it's not easy reading, celebrate because it's not easy reading, celebrate because it's not easy reading, these papers. Then ask yourself, do I, these papers. Then ask yourself, do I, these papers. Then ask yourself, do I, want to keep going on this topic or do I, want to keep going on this topic or do I, want to keep going on this topic or do I, want to learn something else? The point, want to learn something else? The point, want to learn something else? The point, is to not burn out, use short cycles, is to not burn out, use short cycles, is to not burn out, use short cycles, five to 10 papers, reflect, repeat and, five to 10 papers, reflect, repeat and, five to 10 papers, reflect, repeat and, pivot. Pick a topic and make a list. For, pivot. Pick a topic and make a list. For, pivot. Pick a topic and make a list. For, example, if you want to learn natural, example, if you want to learn natural, example, if you want to learn natural, language processing or Asians or build a, language processing or Asians or build a, language processing or Asians or build a, better foundation in AI, then make a, better foundation in AI, then make a, better foundation in AI, then make a, list of five papers. So if your goal is, list of five papers. So if your goal is, list of five papers. So if your goal is, to get a good understanding of the, to get a good understanding of the, to get a good understanding of the, foundations, I would start with the, foundations, I would start with the, foundations, I would start with the, attention is all you need paper. This, attention is all you need paper. This, attention is all you need paper. This, paper introduced the transformers which, paper introduced the transformers which, paper introduced the transformers which, made modern large language models, made modern large language models, made modern large language models, possible and I'm going to walk through, possible and I'm going to walk through, possible and I'm going to walk through, four more like this. But first let me, four more like this. But first let me, four more like this. But first let me, give you the method that I used to read, give you the method that I used to read, give you the method that I used to read, so you don't get stuck while reading, so you don't get stuck while reading, so you don't get stuck while reading, them. Now once you have a list of papers, them. Now once you have a list of papers, them. Now once you have a list of papers, you want to skim through the titles, you want to skim through the titles, you want to skim through the titles, abstracts, figures and graphs. Spend, abstracts, figures and graphs. Spend, abstracts, figures and graphs. Spend, about 10% of your time on each paper, about 10% of your time on each paper, about 10% of your time on each paper, just to decide if this is worth a deeper, just to decide if this is worth a deeper, just to decide if this is worth a deeper, read. This is more like sorting, not, read. This is more like sorting, not, read. This is more like sorting, not, studying. If a paper seems off-topic for, studying. If a paper seems off-topic for, studying. If a paper seems off-topic for, you, set it aside. There's no guilt, you, set it aside. There's no guilt, you, set it aside. There's no guilt, involved. Find another paper that fits, involved. Find another paper that fits, involved. Find another paper that fits, better. Then for the papers that do pass, better. Then for the papers that do pass, better. Then for the papers that do pass, your skim test, do not start at page one, your skim test, do not start at page one, your skim test, do not start at page one, and grind through. Read the introduction, and grind through. Read the introduction, and grind through. Read the introduction, and the conclusion first. These sections, and the conclusion first. These sections, and the conclusion first. These sections, will tell you what the paper is trying, will tell you what the paper is trying, will tell you what the paper is trying, to do and what the authors claim they, to do and what the authors claim they, to do and what the authors claim they, achieved. Then scan related work section, achieved. Then scan related work section, achieved. Then scan related work section, to see if it fits in the field. If all, to see if it fits in the field. If all, to see if it fits in the field. If all, of that makes sense, dive into the core, of that makes sense, dive into the core, of that makes sense, dive into the core, method or the results. If that sounds, method or the results. If that sounds, method or the results. If that sounds, good too, that's when you actually start, good too, that's when you actually start, good too, that's when you actually start, reading the rest of the paper. But feel, reading the rest of the paper. But feel, reading the rest of the paper. But feel, free to skip any parts that don't really, free to skip any parts that don't really, free to skip any parts that don't really, make sense right away. Even the experts, make sense right away. Even the experts, make sense right away. Even the experts, do this when they're reading. Just move, do this when they're reading. Just move, do this when they're reading. Just move, on and come back to any section if you, on and come back to any section if you, on and come back to any section if you, need it later. And the only way to get, need it later. And the only way to get, need it later. And the only way to get, better at this is through practice. Over, better at this is through practice. Over, better at this is through practice. Over, time, you're going to learn common, time, you're going to learn common, time, you're going to learn common, patterns. You'll know where to look, patterns. You'll know where to look, patterns. You'll know where to look, first, and you'll read figures faster, first, and you'll read figures faster, first, and you'll read figures faster, understandings faster, and you'll know, understandings faster, and you'll know, understandings faster, and you'll know, which sections matter more to you. So, which sections matter more to you. So, which sections matter more to you. So, give yourself time. This is a long-term, give yourself time. This is a long-term, give yourself time. This is a long-term, investment, not a one-week sprint. Now, investment, not a one-week sprint. Now, investment, not a one-week sprint. Now, that you have the reading strategy, that you have the reading strategy, that you have the reading strategy, let's apply it. Paper number one, let's apply it. Paper number one, let's apply it. Paper number one, introduced the transformer. Here's the, introduced the transformer. Here's the, introduced the transformer. Here's the, abstract, which basically talks about, abstract, which basically talks about, abstract, which basically talks about, how before this, most language models, how before this, most language models, how before this, most language models, used recurrent neural networks, which, used recurrent neural networks, which, used recurrent neural networks, which, processed text one word at a time. The, processed text one word at a time. The, processed text one word at a time. The, transformer threw that out. Instead, it, transformer threw that out. Instead, it, transformer threw that out. Instead, it, uses something called self attention., uses something called self attention., uses something called self attention., Basically, when the model looks at any, Basically, when the model looks at any, Basically, when the model looks at any, word in a sentence, it can instantly, word in a sentence, it can instantly, word in a sentence, it can instantly, weigh the importance of all the other, weigh the importance of all the other, weigh the importance of all the other, words at the same time. This made, words at the same time. This made, words at the same time. This made, training way faster and it lets models, training way faster and it lets models, training way faster and it lets models, handle much longer, more complex, handle much longer, more complex, handle much longer, more complex, context. This paper made large language, context. This paper made large language, context. This paper made large language, models possible, which is our next, models possible, which is our next, models possible, which is our next, topic. Next is the GPT3 paper from 2020, topic. Next is the GPT3 paper from 2020, topic. Next is the GPT3 paper from 2020, which showed that if you scale up a, which showed that if you scale up a, which showed that if you scale up a, model, GPT3 for example had 175 billion, model, GPT3 for example had 175 billion, model, GPT3 for example had 175 billion, parameters, the model can develop, parameters, the model can develop, parameters, the model can develop, emergent capability, which means it, emergent capability, which means it, emergent capability, which means it, started to show new abilities that it, started to show new abilities that it, started to show new abilities that it, wasn't directly trained on. More, wasn't directly trained on. More, wasn't directly trained on. More, specifically, in context learning, which, specifically, in context learning, which, specifically, in context learning, which, means the model can perform new tasks, means the model can perform new tasks, means the model can perform new tasks, without any fine-tuning. If you just, without any fine-tuning. If you just, without any fine-tuning. If you just, give it a few examples in the prompt and, give it a few examples in the prompt and, give it a few examples in the prompt and, it figures it out translation., it figures it out translation., it figures it out translation., summarization, whatever you wanted to, summarization, whatever you wanted to, summarization, whatever you wanted to, do. It shifted from thinking about how, do. It shifted from thinking about how, do. It shifted from thinking about how, do we train a model to do everything to, do we train a model to do everything to, do we train a model to do everything to, how do we prompt a model. So the quality, how do we prompt a model. So the quality, how do we prompt a model. So the quality, of your instructions control the quality, of your instructions control the quality, of your instructions control the quality, of the output. And this is why prompt, of the output. And this is why prompt, of the output. And this is why prompt, engineering became such a big deal. And, engineering became such a big deal. And, engineering became such a big deal. And, if you've been prompting more than, if you've been prompting more than, if you've been prompting more than, coding, check out Warp. Prompt-driven, coding, check out Warp. Prompt-driven, coding, check out Warp. Prompt-driven, coding environments like Warp are going, coding environments like Warp are going, coding environments like Warp are going, to become the new way of building, to become the new way of building, to become the new way of building, productionready products with their new, productionready products with their new, productionready products with their new, release. Warp is where the IDE and CLI, release. Warp is where the IDE and CLI, release. Warp is where the IDE and CLI, finally merge into one seamless, finally merge into one seamless, finally merge into one seamless, environment for coding with AI agents., environment for coding with AI agents., environment for coding with AI agents., Here's what that means in practice. Warp, Here's what that means in practice. Warp, Here's what that means in practice. Warp, combines three things into one. You, combines three things into one. You, combines three things into one. You, start with the natural language prompt, start with the natural language prompt, start with the natural language prompt, to describe what you want to build. Warp, to describe what you want to build. Warp, to describe what you want to build. Warp, generates the code and you can jump, generates the code and you can jump, generates the code and you can jump, directly into its built-in editor for, directly into its built-in editor for, directly into its built-in editor for, review and tweaks. Then you deploy to, review and tweaks. Then you deploy to, review and tweaks. Then you deploy to, production directly from the built-in, production directly from the built-in, production directly from the built-in, terminal all in a single workflow. It, terminal all in a single workflow. It, terminal all in a single workflow. It, works across the full development life, works across the full development life, works across the full development life, cycle, manages multiple long running, cycle, manages multiple long running, cycle, manages multiple long running, agents seamlessly, and has a native, agents seamlessly, and has a native, agents seamlessly, and has a native, in-app editing so you don't have to, in-app editing so you don't have to, in-app editing so you don't have to, constantly context switch between tools., constantly context switch between tools., constantly context switch between tools., Warp is actively used by over 700,000, Warp is actively used by over 700,000, Warp is actively used by over 700,000, engineers, data scientists, and product, engineers, data scientists, and product, engineers, data scientists, and product, managers, including teams at 56% of, managers, including teams at 56% of, managers, including teams at 56% of, Fortune 500 engineering organizations., Fortune 500 engineering organizations., Fortune 500 engineering organizations., Warp is free to use and you can try, Warp is free to use and you can try, Warp is free to use and you can try, their premium features for just $1 your, their premium features for just $1 your, their premium features for just $1 your, first month with the code GAN. Link is, first month with the code GAN. Link is, first month with the code GAN. Link is, in the description. Now back to the, in the description. Now back to the, in the description. Now back to the, papers. Synthetic data is becoming a, papers. Synthetic data is becoming a, papers. Synthetic data is becoming a, gamecher for training large language, gamecher for training large language, gamecher for training large language, models because these models need huge, models because these models need huge, models because these models need huge, amounts of high quality data to learn., amounts of high quality data to learn., amounts of high quality data to learn., There's only so much data that you can, There's only so much data that you can, There's only so much data that you can, get from the real world. So this paper, get from the real world. So this paper, get from the real world. So this paper, talks about artificially generating, talks about artificially generating, talks about artificially generating, training data usually created by models, training data usually created by models, training data usually created by models, like GPT4 to help train or fine-tune, like GPT4 to help train or fine-tune, like GPT4 to help train or fine-tune, other models. According to this deep, other models. According to this deep, other models. According to this deep, mind research, there are three key, mind research, there are three key, mind research, there are three key, lessons. Synthetic data is definitely, lessons. Synthetic data is definitely, lessons. Synthetic data is definitely, needed, but it works best when combined, needed, but it works best when combined, needed, but it works best when combined, with real data. Training only on, with real data. Training only on, with real data. Training only on, synthetic data can cause quality to, synthetic data can cause quality to, synthetic data can cause quality to, degrade over time. So we need to perform, degrade over time. So we need to perform, degrade over time. So we need to perform, checks to make sure that the data is, checks to make sure that the data is, checks to make sure that the data is, accurate. Next is the RAG paper from, accurate. Next is the RAG paper from, accurate. Next is the RAG paper from, 2020. RAG stands for retrieval augmented, 2020. RAG stands for retrieval augmented, 2020. RAG stands for retrieval augmented, generation. This is a solution we have, generation. This is a solution we have, generation. This is a solution we have, today for fixing the biggest weaknesses, today for fixing the biggest weaknesses, today for fixing the biggest weaknesses, of large language models like, of large language models like, of large language models like, hallucination, outdated training data, hallucination, outdated training data, hallucination, outdated training data, and the blackbox problem where we don't, and the blackbox problem where we don't, and the blackbox problem where we don't, understand where the answer is coming, understand where the answer is coming, understand where the answer is coming, from. RA works by getting fresh relevant, from. RA works by getting fresh relevant, from. RA works by getting fresh relevant, information from an external database, information from an external database, information from an external database, before the model answers your question, before the model answers your question, before the model answers your question, so it isn't stuck with whatever it was, so it isn't stuck with whatever it was, so it isn't stuck with whatever it was, originally trained on. This survey, originally trained on. This survey, originally trained on. This survey, breaks down how rag has evolved over, breaks down how rag has evolved over, breaks down how rag has evolved over, time through three phases. Native rag is, time through three phases. Native rag is, time through three phases. Native rag is, the original version that simply, the original version that simply, the original version that simply, retrieves documents and feeds them to, retrieves documents and feeds them to, retrieves documents and feeds them to, the model. Advanced rag is more, the model. Advanced rag is more, the model. Advanced rag is more, optimized with smarter pre and post, optimized with smarter pre and post, optimized with smarter pre and post, retrieval steps. Then there's the, retrieval steps. Then there's the, retrieval steps. Then there's the, modular rag, the most customizable, modular rag, the most customizable, modular rag, the most customizable, approach where each component can be, approach where each component can be, approach where each component can be, swapped or tuned for domain specific use, swapped or tuned for domain specific use, swapped or tuned for domain specific use, cases. This technique makes answers more, cases. This technique makes answers more, cases. This technique makes answers more, accurate, up-to-date, and verifiable, accurate, up-to-date, and verifiable, accurate, up-to-date, and verifiable, which helps improve the overall, which helps improve the overall, which helps improve the overall, reliability of AI systems. This last, reliability of AI systems. This last, reliability of AI systems. This last, paper is a more recent paper about MCP, paper is a more recent paper about MCP, paper is a more recent paper about MCP, which stands for model context protocol., which stands for model context protocol., which stands for model context protocol., Before MCP, if you wanted your AI models, Before MCP, if you wanted your AI models, Before MCP, if you wanted your AI models, to interact with external tools like, to interact with external tools like, to interact with external tools like, API, databases, or enterprise software, API, databases, or enterprise software, API, databases, or enterprise software, you had to manually build custom, you had to manually build custom, you had to manually build custom, connectors for every single tool and, connectors for every single tool and, connectors for every single tool and, every single model. MCP is a layer that, every single model. MCP is a layer that, every single model. MCP is a layer that, gives them the ability to take more, gives them the ability to take more, gives them the ability to take more, complex multi-step actions. An AI could, complex multi-step actions. An AI could, complex multi-step actions. An AI could, fetch data from one system, process it, fetch data from one system, process it, fetch data from one system, process it, and send a request to another system., and send a request to another system., and send a request to another system., This is important because RAD gives, This is important because RAD gives, This is important because RAD gives, models real information and MCP lets, models real information and MCP lets, models real information and MCP lets, them take real action. So together they, them take real action. So together they, them take real action. So together they, connect AI to the real world. This is, connect AI to the real world. This is, connect AI to the real world. This is, the direction that AI is headed today., the direction that AI is headed today., the direction that AI is headed today., Okay, so you're ready to start reading., Okay, so you're ready to start reading., Okay, so you're ready to start reading., Let's say you finished the five papers, Let's say you finished the five papers, Let's say you finished the five papers, that I recommended and you want to find, that I recommended and you want to find, that I recommended and you want to find, more. Where do you go find them? Well, more. Where do you go find them? Well, more. Where do you go find them? Well, here are four sources that I like and, here are four sources that I like and, here are four sources that I like and, how to use them. First is Hugging Face., how to use them. First is Hugging Face., how to use them. First is Hugging Face., This site organizes paper with models, This site organizes paper with models, This site organizes paper with models, data sets, trending papers across many, data sets, trending papers across many, data sets, trending papers across many, areas like image generation, video, areas like image generation, video, areas like image generation, video, generations, and a bunch more. It's also, generations, and a bunch more. It's also, generations, and a bunch more. It's also, super easy to browse. This AI has a list, super easy to browse. This AI has a list, super easy to browse. This AI has a list, of papers going back to 2010. They are, of papers going back to 2010. They are, of papers going back to 2010. They are, organized by topic and by era, so it's, organized by topic and by era, so it's, organized by topic and by era, so it's, useful if you want to trace how an area, useful if you want to trace how an area, useful if you want to trace how an area, has evolved over time. And if you like, has evolved over time. And if you like, has evolved over time. And if you like, to see the story from early ideas to the, to see the story from early ideas to the, to see the story from early ideas to the, current state, you want to start here., current state, you want to start here., current state, you want to start here., Deep learning monitor shows hot and, Deep learning monitor shows hot and, Deep learning monitor shows hot and, fresh papers or you can add any topic, fresh papers or you can add any topic, fresh papers or you can add any topic, that you are interested in so you can, that you are interested in so you can, that you are interested in so you can, monitor them. You can also filter by, monitor them. You can also filter by, monitor them. You can also filter by, weeks or months because new papers come, weeks or months because new papers come, weeks or months because new papers come, out almost every day. So you can set up, out almost every day. So you can set up, out almost every day. So you can set up, monitors with keywords for areas that, monitors with keywords for areas that, monitors with keywords for areas that, you care about. So you get a small focus, you care about. So you get a small focus, you care about. So you get a small focus, stream to check archive sanity. Archive, stream to check archive sanity. Archive, stream to check archive sanity. Archive, is standard place where researchers post, is standard place where researchers post, is standard place where researchers post, papers. It's not always organized in the, papers. It's not always organized in the, papers. It's not always organized in the, most friendly way. So, it helps if you, most friendly way. So, it helps if you, most friendly way. So, it helps if you, already know what you're looking for., already know what you're looking for., already know what you're looking for., Using the search feature, you can search, Using the search feature, you can search, Using the search feature, you can search, for keywords that you're interested in., for keywords that you're interested in., for keywords that you're interested in., I also keep a short list on my website, I also keep a short list on my website, I also keep a short list on my website, with these links, plus other free, with these links, plus other free, with these links, plus other free, resources like books and newsletters., resources like books and newsletters., resources like books and newsletters., Link is in the description. Now, YouTube, Link is in the description. Now, YouTube, Link is in the description. Now, YouTube, thinks that you should watch this video, thinks that you should watch this video, thinks that you should watch this video, next. So all see
