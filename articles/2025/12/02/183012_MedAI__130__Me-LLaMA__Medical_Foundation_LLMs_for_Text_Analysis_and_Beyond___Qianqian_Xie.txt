Timestamp: 2025-12-02T18:30:12.657038
Title: MedAI #130: Me-LLaMA: Medical Foundation LLMs for Text Analysis and Beyond | Qianqian Xie
URL: https://youtube.com/watch?v=V5FZBQMGSog&si=M3nm-S2unMEBJayn
Status: success
Duration: 1:04:24

Description:
好的，这是对所提供文本内容的结构化总结、核心观点、总体框架和Mermaid概念图。

### **一、Me Lama 模型研究纲要**

**1. 背景与动机**
*   **现有模型的潜力与局限**：像ChatGPT这样的大语言模型（LLMs）在医学领域展现了巨大潜力（如辅助诊断），但其准确性仍有提升空间，且闭源特性限制了其在医疗保健领域的定制化和可访问性。
*   **开源模型的挑战**：现有的开源医学大模型在开发中面临挑战，如很少同时采用持续预训练和指令微调，且鲜有模型同时整合生物医学文献和真实世界的临床笔记数据。

**2. Me Lama 模型开发框架**
*   **核心策略**：基于 Llama 2 模型，采用“持续预训练 + 指令微调”的双阶段策略。
*   **阶段一：持续预训练（构建 Me Lama 基础模型）**
    *   **数据构成**：构建了迄今为止规模最大（1290亿个token）的医学预训练数据集，混合了三种来源：
        1.  **生物医学文献**：来自 PubMed 和 PMC 的论文摘要与全文。
        2.  **临床笔记**：来自 MIMIC 系列数据库的去标识化临床文本。
        3.  **通用领域数据**：来自 RedPajama 数据集，旨在防止模型遗忘通用知识。
    *   **目标**：向模型注入深厚的医学领域知识。
    *   **产出**：Me Lama 13B 和 70B 基础模型。

*   **阶段二：指令微调（构建 Me Lama Chat 对话模型）**
    *   **数据构成**：创建了一个包含超过25,000个高质量样本的医学指令数据集，覆盖问答、摘要、信息提取等多种任务。
    *   **目标**：提升模型的指令遵循能力和在零样本（zero-shot）场景下的对话与任务处理能力。
    *   **产出**：Me Lama Chat 13B 和 70B 对话模型。

**3. 全面评估与基准测试**
*   **评估框架**：建立了一个新的综合性评估基准，涵盖6大类核心文本分析任务和12个不同的数据集。
*   **评估设置**：
    *   **监督学习**：对 Me Lama 基础模型在各任务上进行微调，并与 Llama 2、PMC Lama 等模型对比。
    *   **零/少样本学习**：直接评估 Me Lama Chat 模型的开箱即用能力，并与 ChatGPT、GPT-4 及其他开源对话模型对比。
*   **主要发现**：
    *   **性能卓越**：Me Lama 在绝大多数任务上显著优于其他开源医学大模型。
    *   **超越商业模型**：经过任务微调后，Me Lama 在多个数据集上的表现超过了闭源的 ChatGPT 甚至 GPT-4。
    *   **高效性**：Me Lama 13B 模型在许多任务上的表现与 70B 模型相当，展现了极高的性价比和实用性。

**4. 关键洞察与挑战**
*   **零样本学习的困难**：在命名实体识别（NER）等复杂任务中，即便是GPT-4也难以完美遵循指令格式，导致自动评估困难，这揭示了提升模型指令遵循能力的重要性。
*   **数据配比的重要性**：在预训练中，合理配比（如4:1的医学与通用数据）是平衡领域专业化和通用知识保留的关键。
*   **成本效益**：指令微调相比于持续预训练，是一种成本效益极高的性能提升手段。
*   **模型局限与未来工作**：当前模型存在幻觉、上下文长度限制（4096个token）等问题。未来将探索结合检索增强生成（RAG）、人类反馈强化学习（RLHF）以及适配更新的基础模型（如Llama 3）来解决这些问题。

---

### **二、核心结论 (Core Point)**

通过结合大规模、多样化的医学数据进行持续预训练和精细化的多任务指令微调，Me Lama 模型成功构建了一个在性能上超越现有开源模型、并在特定场景下媲美甚至优于顶尖商业模型的开放、高效的医学大语言模型。

---

### **三、总体框架 (Overarching Framework)**

该研究的总体框架始于识别现有通用大语言模型在医学领域的局限性，进而提出一个双阶段的解决方案：首先，通过在包含生物医学文献、临床笔记和通用文本的混合数据集上进行持续预训练，构建具有深厚医学知识的 Me Lama 基础模型；其次，通过多任务指令微调，开发出具备强大零样本能力的 Me Lama Chat 对话模型。最终，通过一个新建的、涵盖六大类任务的综合基准，系统地评估了模型的性能，并与业界领先模型进行对比，从而验证了其有效性并指出了未来方向。

---

### **四、Mermaid 概念图**

<Mermaid_Diagram>
graph TD
    subgraph "背景与动机"
        A["现有LLM在医学领域的局限性<br>(准确性不足、闭源、缺乏领域知识)"]
    end

    subgraph "Me Lama 开发流程"
        direction LR
        subgraph "数据准备"
            D1["生物医学文献<br>(PubMed, PMC)"]
            D2["临床笔记<br>(MIMIC)"]
            D3["通用领域数据<br>(RedPajama)"]
            D_PreTrain["大规模混合预训练语料<br>(1290亿 Tokens)"]
            D_SFT["医学指令微调数据<br>(2.5万+ 样本)"]
            D1 & D2 & D3 --> D_PreTrain
        end

        subgraph "模型构建"
            M_Base["Llama 2 (13B & 70B)"]
            P_PreTrain["持续预训练<br>(Continual Pre-training)"]
            M_MeLama["Me Lama 基础模型"]
            P_SFT["指令微调<br>(Instruction Fine-tuning)"]
            M_Chat["Me Lama Chat 对话模型"]
            M_Base -- "输入" --> P_PreTrain
            D_PreTrain -- "使用数据" --> P_PreTrain
            P_PreTrain --> M_MeLama
            M_MeLama -- "输入" --> P_SFT
            D_SFT -- "使用数据" --> P_SFT
            P_SFT --> M_Chat
        end
    end

    subgraph "性能评估与验证"
        E_Benchmark["综合评估基准<br>(6大任务, 12个数据集)"]
        subgraph "评估设置"
            E_Supervised["监督学习评估"]
            E_ZeroShot["零/少样本评估"]
        end
        M_MeLama --> E_Supervised
        M_Chat --> E_ZeroShot
        E_Supervised --> E_Benchmark
        E_ZeroShot --> E_Benchmark
    end

    subgraph "结论与洞察"
        F["关键发现<br>(超越开源模型, 媲美商业模型)"]
        G["核心洞察<br>(数据配比、成本效益、零样本挑战)"]
        H["局限与未来工作<br>(幻觉、上下文长度、RAG)"]
    end

    A --> D1
    A --> D2
    A --> D3
    A --> D_SFT
    E_Benchmark --> F
    F --> G
    G --> H

    style A fill:#FFDDC1,stroke:#333,stroke-width:2px
    style M_Base fill:#B2EBF2,stroke:#333,stroke-width:1.5px
    style M_MeLama fill:#C8E6C9,stroke:#333,stroke-width:2px
    style M_Chat fill:#A5D6A7,stroke:#333,stroke-width:2px
    style D_PreTrain fill:#E1BEE7,stroke:#333,stroke-width:1.5px
    style D_SFT fill:#F8BBD0,stroke:#333,stroke-width:1.5px
    style P_PreTrain fill:#FFF9C4,stroke:#333,stroke-width:1.5px
    style P_SFT fill:#FFECB3,stroke:#333,stroke-width:1.5px
    style E_Benchmark fill:#D1C4E9,stroke:#333,stroke-width:2px
    style F fill:#C5CAE9,stroke:#333,stroke-width:2px
    style G fill:#BBDEFB,stroke:#333,stroke-width:1.5px
    style H fill:#FFCCBC,stroke:#333,stroke-width:1.5px
</Mermaid_Diagram>

Content:
okay uh hello welcome everyone to the, okay uh hello welcome everyone to the, 13th uh Med AI exchange session today, 13th uh Med AI exchange session today, 13th uh Med AI exchange session today, here we have with us Dr Chan Chan Shi, here we have with us Dr Chan Chan Shi, here we have with us Dr Chan Chan Shi, from associate an associate research, from associate an associate research, from associate an associate research, scientist from uh Yale University she is, scientist from uh Yale University she is, scientist from uh Yale University she is, at the department of biomedical, at the department of biomedical, at the department of biomedical, informatics and data science and school, informatics and data science and school, informatics and data science and school, of medicine at Yale her research, of medicine at Yale her research, of medicine at Yale her research, interests are largely in natural, interests are largely in natural, interests are largely in natural, language processing and it's application, language processing and it's application, language processing and it's application, in medicine she just received the NIH uh, in medicine she just received the NIH uh, in medicine she just received the NIH uh, nlm Pathway to Independence k99 award, nlm Pathway to Independence k99 award, nlm Pathway to Independence k99 award, and has co-authored more than 50 peer, and has co-authored more than 50 peer, and has co-authored more than 50 peer, reviewed Publications her research has, reviewed Publications her research has, reviewed Publications her research has, always has been published in leading, always has been published in leading, always has been published in leading, conferences and journals like newps ACL, conferences and journals like newps ACL, conferences and journals like newps ACL, ktd Sig IR uh KNL and coling and uh so, ktd Sig IR uh KNL and coling and uh so, ktd Sig IR uh KNL and coling and uh so, please uh welcome help us welcome her, please uh welcome help us welcome her, please uh welcome help us welcome her, and for her talk on me Lama and uh, and for her talk on me Lama and uh, and for her talk on me Lama and uh, before we start the session chanan how, before we start the session chanan how, before we start the session chanan how, would you like to have questions do you, would you like to have questions do you, would you like to have questions do you, want to have questions for the end of, want to have questions for the end of, want to have questions for the end of, the presentation or uh people can, the presentation or uh people can, the presentation or uh people can, interrupt you while you're presenting, interrupt you while you're presenting, interrupt you while you're presenting, post works for me and feel free to, post works for me and feel free to, post works for me and feel free to, interrupt me if you have any questions, interrupt me if you have any questions, interrupt me if you have any questions, or yeah just give have a question in the, or yeah just give have a question in the, or yeah just give have a question in the, end of presentation post works for so, end of presentation post works for so, end of presentation post works for so, without further Ado I will hand it over, without further Ado I will hand it over, without further Ado I will hand it over, to chanchan and let's keep this interact, to chanchan and let's keep this interact, to chanchan and let's keep this interact, session as interactive as possible thank, session as interactive as possible thank, session as interactive as possible thank, you yeah, so yeah uh hi everyone so uh I'm CH, so yeah uh hi everyone so uh I'm CH, currently an associate research, currently an associate research, currently an associate research, scientist from Department of, scientist from Department of, scientist from Department of, bioinformatics and data science y school, bioinformatics and data science y school, bioinformatics and data science y school, on medicine so thanks very much for the, on medicine so thanks very much for the, on medicine so thanks very much for the, invitation in this presentation I will, invitation in this presentation I will, invitation in this presentation I will, introduce our work milama large language, introduce our work milama large language, introduce our work milama large language, models developed specifically for, models developed specifically for, models developed specifically for, medical applications so as many of you, medical applications so as many of you, medical applications so as many of you, may know large language models have, may know large language models have, may know large language models have, shown great potential to revolutionize, shown great potential to revolutionize, shown great potential to revolutionize, the medical field these models have vast, the medical field these models have vast, the medical field these models have vast, potential to revolutionize Health scale, potential to revolutionize Health scale, potential to revolutionize Health scale, by enhancing various as aspects of, by enhancing various as aspects of, by enhancing various as aspects of, medical practice and research for, medical practice and research for, medical practice and research for, example several Studies have shown that, example several Studies have shown that, example several Studies have shown that, CH gbt passed the US medical licensing, CH gbt passed the US medical licensing, CH gbt passed the US medical licensing, exam Beyond exams studies are, exam Beyond exams studies are, exam Beyond exams studies are, continuously exploring the applications, continuously exploring the applications, continuously exploring the applications, of lar models in real world Medical, of lar models in real world Medical, of lar models in real world Medical, tasks including uh improvements in, tasks including uh improvements in, tasks including uh improvements in, clinical documentation aiding physici, clinical documentation aiding physici, clinical documentation aiding physici, Physicians with timely and accurate, Physicians with timely and accurate, Physicians with timely and accurate, notes and supporting diagnostic, notes and supporting diagnostic, notes and supporting diagnostic, processes by providing insights into, processes by providing insights into, processes by providing insights into, potential differential, potential differential, potential differential, diagnosis so let me share a particular, diagnosis so let me share a particular, diagnosis so let me share a particular, impressive example of how large Lang, impressive example of how large Lang, impressive example of how large Lang, models like chat GPT can assist in, models like chat GPT can assist in, models like chat GPT can assist in, complex diagnostic cases so as reported, complex diagnostic cases so as reported, complex diagnostic cases so as reported, there is a young boy who suffered from, there is a young boy who suffered from, there is a young boy who suffered from, clnic pen for three years over that time, clnic pen for three years over that time, clnic pen for three years over that time, he saw a total of 70 doctors but none, he saw a total of 70 doctors but none, he saw a total of 70 doctors but none, were able to point his diagnosis So, were able to point his diagnosis So, were able to point his diagnosis So, eventually his mother took a Noel, eventually his mother took a Noel, eventually his mother took a Noel, approach she provided chap gbt with all, approach she provided chap gbt with all, approach she provided chap gbt with all, of her son's medical records and, of her son's medical records and, of her son's medical records and, available information hoping it made, available information hoping it made, available information hoping it made, offer some insight remarkably chbt, offer some insight remarkably chbt, offer some insight remarkably chbt, suggested a diagnosis that had been, suggested a diagnosis that had been, suggested a diagnosis that had been, being, being, being, overlooked uh so this was later, overlooked uh so this was later, overlooked uh so this was later, confirmed by neurosurgeon so given the, confirmed by neurosurgeon so given the, confirmed by neurosurgeon so given the, giv the boy a past was to uh correct, giv the boy a past was to uh correct, giv the boy a past was to uh correct, treatments so this case underscores the, treatments so this case underscores the, treatments so this case underscores the, potential of large Lang models to, potential of large Lang models to, potential of large Lang models to, complement traditional medical approach, complement traditional medical approach, complement traditional medical approach, especially when faced with complex, especially when faced with complex, especially when faced with complex, diagnostic, diagnostic, diagnostic, challenges then this brings us to an, challenges then this brings us to an, challenges then this brings us to an, important question how do Lar models, important question how do Lar models, important question how do Lar models, eventually actually perform when faced, eventually actually perform when faced, eventually actually perform when faced, with complex diagnostic challenges so, with complex diagnostic challenges so, with complex diagnostic challenges so, several studies have evaluated the, several studies have evaluated the, several studies have evaluated the, performance of sorta models such as gp4, performance of sorta models such as gp4, performance of sorta models such as gp4, and mety using challenging diagnostic, and mety using challenging diagnostic, and mety using challenging diagnostic, tasks from sources like the new ingrant, tasks from sources like the new ingrant, tasks from sources like the new ingrant, journal on medicine case reports so Le, journal on medicine case reports so Le, journal on medicine case reports so Le, models have achieved around the 30 to 40, models have achieved around the 30 to 40, models have achieved around the 30 to 40, percentage accuracy in predicting the, percentage accuracy in predicting the, percentage accuracy in predicting the, correct diagnosis while this performance, correct diagnosis while this performance, correct diagnosis while this performance, is promising there is still significant, is promising there is still significant, is promising there is still significant, room for improvement to reach the high, room for improvement to reach the high, room for improvement to reach the high, standards required for clinical, standards required for clinical, standards required for clinical, reliability uh additionally so many of, reliability uh additionally so many of, reliability uh additionally so many of, these models are actually close Source, these models are actually close Source, these models are actually close Source, nature so limiting the customization and, nature so limiting the customization and, nature so limiting the customization and, accessibility often needed for Health, accessibility often needed for Health, accessibility often needed for Health, Care applications so the question, Care applications so the question, Care applications so the question, becomes how can we improve these models, becomes how can we improve these models, becomes how can we improve these models, to be more adaptable and reliable in, to be more adaptable and reliable in, to be more adaptable and reliable in, medical settings, medical settings, medical settings, one clear path to enhancing the, one clear path to enhancing the, one clear path to enhancing the, performance of large language models in, performance of large language models in, performance of large language models in, medical context is by integrating high, medical context is by integrating high, medical context is by integrating high, quality domain specific medical data so, quality domain specific medical data so, quality domain specific medical data so, there are several critical sources of, there are several critical sources of, there are several critical sources of, information in the medical domain such, information in the medical domain such, information in the medical domain such, as biomedical literature clinical notes, as biomedical literature clinical notes, as biomedical literature clinical notes, medic book and clinical, medic book and clinical, medic book and clinical, guidelines there are two M approach for, guidelines there are two M approach for, guidelines there are two M approach for, using medical data to improve the, using medical data to improve the, using medical data to improve the, performance of large Lang models in the, performance of large Lang models in the, performance of large Lang models in the, medical context the first approach, medical context the first approach, medical context the first approach, involves working with close Source, involves working with close Source, involves working with close Source, models this commercialize models such as, models this commercialize models such as, models this commercialize models such as, open as GPT 3.5 and gp4 uh they allow, open as GPT 3.5 and gp4 uh they allow, open as GPT 3.5 and gp4 uh they allow, for f which lets users to adapt model, for f which lets users to adapt model, for f which lets users to adapt model, using their own data so fing can enhance, using their own data so fing can enhance, using their own data so fing can enhance, model accuracy on specific tasks and the, model accuracy on specific tasks and the, model accuracy on specific tasks and the, tail responses to better suit particular, tail responses to better suit particular, tail responses to better suit particular, medical context another method with, medical context another method with, medical context another method with, close Source model is retrieval, close Source model is retrieval, close Source model is retrieval, augumentation so this approach combines, augumentation so this approach combines, augumentation so this approach combines, the model with an external database of, the model with an external database of, the model with an external database of, medical information so when a user, medical information so when a user, medical information so when a user, quaries the model it can retrieve, quaries the model it can retrieve, quaries the model it can retrieve, relevant information from the external, relevant information from the external, relevant information from the external, database to generate a more reliable, database to generate a more reliable, database to generate a more reliable, response however these closed Source, response however these closed Source, response however these closed Source, models come with some notable, models come with some notable, models come with some notable, limitations firstly layer close Source, limitations firstly layer close Source, limitations firstly layer close Source, nature restricts full access and control, nature restricts full access and control, nature restricts full access and control, over the most limiting customization for, over the most limiting customization for, over the most limiting customization for, highly specific medical needs add, highly specific medical needs add, highly specific medical needs add, additionally so privacy concerns are, additionally so privacy concerns are, additionally so privacy concerns are, significant when using these models with, significant when using these models with, significant when using these models with, sensitive metadata because data is, sensitive metadata because data is, sensitive metadata because data is, typically processed on third party, typically processed on third party, typically processed on third party, servers there is a risk of data exposure, servers there is a risk of data exposure, servers there is a risk of data exposure, or no compliance with strict, or no compliance with strict, or no compliance with strict, Healthcare privacy regulations like, Healthcare privacy regulations like, Healthcare privacy regulations like, hipop so this privacy and customization, hipop so this privacy and customization, hipop so this privacy and customization, limitations highlight the uh importance, limitations highlight the uh importance, limitations highlight the uh importance, of, of, of, exploring uh open source Alternatives so, exploring uh open source Alternatives so, exploring uh open source Alternatives so, recently they are actually great, recently they are actually great, recently they are actually great, advancements in open-source large, advancements in open-source large, advancements in open-source large, language models with the Llama series, language models with the Llama series, language models with the Llama series, model from meta AI as the main, model from meta AI as the main, model from meta AI as the main, representative these models have opened, representative these models have opened, representative these models have opened, up exciting opportunities for, up exciting opportunities for, up exciting opportunities for, customization and flexibility in the, customization and flexibility in the, customization and flexibility in the, medical domain providing a foundation, medical domain providing a foundation, medical domain providing a foundation, for Depot domain specific op, for Depot domain specific op, for Depot domain specific op, optimization so effective approach to, optimization so effective approach to, optimization so effective approach to, improve the performance of this open, improve the performance of this open, improve the performance of this open, source model include continue PR, source model include continue PR, source model include continue PR, training and funing with in instructions, training and funing with in instructions, training and funing with in instructions, in the medical domain for instance, in the medical domain for instance, in the medical domain for instance, models like code llama uh apprend on, models like code llama uh apprend on, models like code llama uh apprend on, Specialized data in this case is, Specialized data in this case is, Specialized data in this case is, actually the code data which enhances, actually the code data which enhances, actually the code data which enhances, their capabilities in understanding and, their capabilities in understanding and, their capabilities in understanding and, generating programming related cont, generating programming related cont, generating programming related cont, tanks another notable example is PMC, tanks another notable example is PMC, tanks another notable example is PMC, Lama model it continue pin and front, Lama model it continue pin and front, Lama model it continue pin and front, Lama to models on biomedical papers so, Lama to models on biomedical papers so, Lama to models on biomedical papers so, which sub substantially enhance its, which sub substantially enhance its, which sub substantially enhance its, capabilities in answering medical, capabilities in answering medical, capabilities in answering medical, questions and Performing we um, questions and Performing we um, questions and Performing we um, biomedical QA, biomedical QA, biomedical QA, benchmarks so to address the need for, benchmarks so to address the need for, benchmarks so to address the need for, specialized models in the medical domain, specialized models in the medical domain, specialized models in the medical domain, several open-source medical large, several open-source medical large, several open-source medical large, language models have been developed, language models have been developed, language models have been developed, recently notable medical large language, recently notable medical large language, recently notable medical large language, models uh developed through these, models uh developed through these, models uh developed through these, methods include medon and clinical llama, methods include medon and clinical llama, methods include medon and clinical llama, which continue P PR llama to and llama, which continue P PR llama to and llama, which continue P PR llama to and llama, models with extensive biomedical, models with extensive biomedical, models with extensive biomedical, literature or clinical notes, literature or clinical notes, literature or clinical notes, respectively additionally models like, respectively additionally models like, respectively additionally models like, met apaka chat doctor and Apper care, met apaka chat doctor and Apper care, met apaka chat doctor and Apper care, were developed by instruction fing llama, were developed by instruction fing llama, were developed by instruction fing llama, 2 and Lama models so p Lama stands out, 2 and Lama models so p Lama stands out, 2 and Lama models so p Lama stands out, as the only model using both Contin pre, as the only model using both Contin pre, as the only model using both Contin pre, training and instruction fing based on, training and instruction fing based on, training and instruction fing based on, llama models uh despite these, llama models uh despite these, llama models uh despite these, achievements made by existing models uh, achievements made by existing models uh, achievements made by existing models uh, significant challenges remain for, significant challenges remain for, significant challenges remain for, developing open- Source Foundation, developing open- Source Foundation, developing open- Source Foundation, models in the medical domain so firstly, models in the medical domain so firstly, models in the medical domain so firstly, few models except PMC Lama uh imployed, few models except PMC Lama uh imployed, few models except PMC Lama uh imployed, both Contin PR training and instruction, both Contin PR training and instruction, both Contin PR training and instruction, fing techniques probably due to the, fing techniques probably due to the, fing techniques probably due to the, expensive computational cost associated, expensive computational cost associated, expensive computational cost associated, with medical with the model training, with medical with the model training, with medical with the model training, especially for the Contin pre training, especially for the Contin pre training, especially for the Contin pre training, process and only one model the clinical, process and only one model the clinical, process and only one model the clinical, Lama model used the clinical notes from, Lama model used the clinical notes from, Lama model used the clinical notes from, electronic health record which is, electronic health record which is, electronic health record which is, crucial for real world clinical, crucial for real world clinical, crucial for real world clinical, applications as it provides context, applications as it provides context, applications as it provides context, specific information from direct patient, specific information from direct patient, specific information from direct patient, care furthermore none of existing models, care furthermore none of existing models, care furthermore none of existing models, use both biomedical literature and, use both biomedical literature and, use both biomedical literature and, clinical notes which is one of the goals, clinical notes which is one of the goals, clinical notes which is one of the goals, of our project so by combining, of our project so by combining, of our project so by combining, biomedical literature and clinical notes, biomedical literature and clinical notes, biomedical literature and clinical notes, we generated the largest biomedical, we generated the largest biomedical, we generated the largest biomedical, paining data sets compared to the, paining data sets compared to the, paining data sets compared to the, previous, previous, previous, efforts so finally most models have, efforts so finally most models have, efforts so finally most models have, focused on evaluating the question, focused on evaluating the question, focused on evaluating the question, answering tasks making it difficult to, answering tasks making it difficult to, answering tasks making it difficult to, assess the generalization ability of, assess the generalization ability of, assess the generalization ability of, those Foundation models on other key, those Foundation models on other key, those Foundation models on other key, medical tax analysis tasks such as, medical tax analysis tasks such as, medical tax analysis tasks such as, information extraction and tex, information extraction and tex, information extraction and tex, classification therefore so in this, classification therefore so in this, classification therefore so in this, project our shortterm goal is to, project our shortterm goal is to, project our shortterm goal is to, developed uh powerful medical large, developed uh powerful medical large, developed uh powerful medical large, language models which is actually the, language models which is actually the, language models which is actually the, pathway we think to achieving our, pathway we think to achieving our, pathway we think to achieving our, long-term goal that developing capable, long-term goal that developing capable, long-term goal that developing capable, AI models from for complex medical tasks, AI models from for complex medical tasks, AI models from for complex medical tasks, to address the limitations of existing, to address the limitations of existing, to address the limitations of existing, medical large language models we, medical large language models we, medical large language models we, introduce a new family of open-source, introduce a new family of open-source, introduce a new family of open-source, medical models called M Lama so this, medical models called M Lama so this, medical models called M Lama so this, family includes both Foundation models, family includes both Foundation models, family includes both Foundation models, and chat enhan versions available in, and chat enhan versions available in, and chat enhan versions available in, sizes of 13 billion and 70 billion, sizes of 13 billion and 70 billion, sizes of 13 billion and 70 billion, parameters M Lama is trained on the, parameters M Lama is trained on the, parameters M Lama is trained on the, largest and most comprehensive medical, largest and most comprehensive medical, largest and most comprehensive medical, data to dates making it one of the most, data to dates making it one of the most, data to dates making it one of the most, Lobos models in this field our, Lobos models in this field our, Lobos models in this field our, development process for Lima M Lama uh, development process for Lima M Lama uh, development process for Lima M Lama uh, includes two stage continue P training, includes two stage continue P training, includes two stage continue P training, and instruction fing based on Lama 2, and instruction fing based on Lama 2, and instruction fing based on Lama 2, incorporating a massive uh 129 billion, incorporating a massive uh 129 billion, incorporating a massive uh 129 billion, paining tokens and over, paining tokens and over, paining tokens and over, 25k instruction turning samples this, 25k instruction turning samples this, 25k instruction turning samples this, data sets are drawn from a diverse range, data sets are drawn from a diverse range, data sets are drawn from a diverse range, of sources including biomedical, of sources including biomedical, of sources including biomedical, literature clinical guidelines and, literature clinical guidelines and, literature clinical guidelines and, electronic health records so in sing the, electronic health records so in sing the, electronic health records so in sing the, model incorporates a wide range of, model incorporates a wide range of, model incorporates a wide range of, medical knowledge and the pra practical, medical knowledge and the pra practical, medical knowledge and the pra practical, guidelines to evaluate milama we, guidelines to evaluate milama we, guidelines to evaluate milama we, conducted the most comprehensive, conducted the most comprehensive, conducted the most comprehensive, assessment to date covered in six, assessment to date covered in six, assessment to date covered in six, essential text analysis tasks and using, essential text analysis tasks and using, essential text analysis tasks and using, 12 diverse data sets from both, 12 diverse data sets from both, 12 diverse data sets from both, biomedical and clinical domains the, biomedical and clinical domains the, biomedical and clinical domains the, results are promising so milama out, results are promising so milama out, results are promising so milama out, performs existing uh open soci medical, performs existing uh open soci medical, performs existing uh open soci medical, models and even gp4 in both zeros shot, models and even gp4 in both zeros shot, models and even gp4 in both zeros shot, and supervisor learning, and supervisor learning, and supervisor learning, scenarios this figle provides an, scenarios this figle provides an, scenarios this figle provides an, overview of our our studies structure, overview of our our studies structure, overview of our our studies structure, and the development process for milama, and the development process for milama, and the development process for milama, uh our study consists of three main, uh our study consists of three main, uh our study consists of three main, components including pre training, components including pre training, components including pre training, instruction fing and evaluation so, instruction fing and evaluation so, instruction fing and evaluation so, firstly we developed the milama based, firstly we developed the milama based, firstly we developed the milama based, models through Contin Pro training so we, models through Contin Pro training so we, models through Contin Pro training so we, start with Lama 2 and CH it with 129, start with Lama 2 and CH it with 129, start with Lama 2 and CH it with 129, billion tokens drawn from a mixture of, billion tokens drawn from a mixture of, billion tokens drawn from a mixture of, pring data including clinical notes, pring data including clinical notes, pring data including clinical notes, biomedical articles and general demand, biomedical articles and general demand, biomedical articles and general demand, data can I interrupt for a second I'm, data can I interrupt for a second I'm, data can I interrupt for a second I'm, I'm kind of lost here so when you say, I'm kind of lost here so when you say, I'm kind of lost here so when you say, like continual pre-training are you, like continual pre-training are you, like continual pre-training are you, updating the vocabulary or you are not, updating the vocabulary or you are not, updating the vocabulary or you are not, updating the, updating the, updating the, vocabulary uh so the Contin pre is like, vocabulary uh so the Contin pre is like, vocabulary uh so the Contin pre is like, updating the models ways but you're not, updating the models ways but you're not, updating the models ways but you're not, updating the vocabulary right like you, updating the vocabulary right like you, updating the vocabulary right like you, just take the vocabulary as it is or you, just take the vocabulary as it is or you, just take the vocabulary as it is or you, are actually updating the vocabulary, are actually updating the vocabulary, are actually updating the vocabulary, yeah we do not uh make vocabulary, yeah we do not uh make vocabulary, yeah we do not uh make vocabulary, extension actually we indeed uh make a, extension actually we indeed uh make a, extension actually we indeed uh make a, initial experiments like uh extending, initial experiments like uh extending, initial experiments like uh extending, the vocabulary by uh extending the, the vocabulary by uh extending the, the vocabulary by uh extending the, vocabulary with based on the clinical, vocabulary with based on the clinical, vocabulary with based on the clinical, notes from mimic series data set and the, notes from mimic series data set and the, notes from mimic series data set and the, uh biomedical literature uh we increase, uh biomedical literature uh we increase, uh biomedical literature uh we increase, the size of the vocabulary into double, the size of the vocabulary into double, the size of the vocabulary into double, size of the original Lama 2 models but, size of the original Lama 2 models but, size of the original Lama 2 models but, the initial results is actually not, the initial results is actually not, the initial results is actually not, quite good after extending the, quite good after extending the, quite good after extending the, vocabulary the training process is quite, vocabulary the training process is quite, vocabulary the training process is quite, unstable and uh um the prend model, unstable and uh um the prend model, unstable and uh um the prend model, doesn't perform well and even, doesn't perform well and even, doesn't perform well and even, underperformed the base the back bom, underperformed the base the back bom, underperformed the base the back bom, model L but don't you think like in in, model L but don't you think like in in, model L but don't you think like in in, addition to like updating the vocabulary, addition to like updating the vocabulary, addition to like updating the vocabulary, and adding new terms probably is better, and adding new terms probably is better, and adding new terms probably is better, to learn the vocabulary on the medical, to learn the vocabulary on the medical, to learn the vocabulary on the medical, nodes will be better because if you just, nodes will be better because if you just, nodes will be better because if you just, double up the vocabulary size maybe as, double up the vocabulary size maybe as, double up the vocabulary size maybe as, you mentioned probably the training, you mentioned probably the training, you mentioned probably the training, would be unstable oh yeah so I think, would be unstable oh yeah so I think, would be unstable oh yeah so I think, ideally intuitively updating the, ideally intuitively updating the, ideally intuitively updating the, vocabulary will be better because if we, vocabulary will be better because if we, vocabulary will be better because if we, after update the vocabulary it will be, after update the vocabulary it will be, after update the vocabulary it will be, more efficient to pass and to tokenize, more efficient to pass and to tokenize, more efficient to pass and to tokenize, the uh medical text right uh because, the uh medical text right uh because, the uh medical text right uh because, because I think that when you update the, because I think that when you update the, because I think that when you update the, vocabular actually there would be some, vocabular actually there would be some, vocabular actually there would be some, words where the model weight would be, words where the model weight would be, words where the model weight would be, stable and there you would be, stable and there you would be, stable and there you would be, introducing some new words words when, introducing some new words words when, introducing some new words words when, the model word would be unstable right, the model word would be unstable right, the model word would be unstable right, so in that case it it's kind of strange, so in that case it it's kind of strange, so in that case it it's kind of strange, to add the additional words additional, to add the additional words additional, to add the additional words additional, clinical words only rather than like, clinical words only rather than like, clinical words only rather than like, learning the vocabularies on the domain, learning the vocabularies on the domain, learning the vocabularies on the domain, itself oh yeah yeah so but I think, itself oh yeah yeah so but I think, itself oh yeah yeah so but I think, another um reason that we uh the the, another um reason that we uh the the, another um reason that we uh the the, training process is unstable because the, training process is unstable because the, training process is unstable because the, paining CPUs is not actually large, paining CPUs is not actually large, paining CPUs is not actually large, enough because for like right so why, enough because for like right so why, enough because for like right so why, don't you try to learn the vocabulary on, don't you try to learn the vocabulary on, don't you try to learn the vocabulary on, the clinical domain, the clinical domain, the clinical domain, itself uh run the yeah you mean run the, itself uh run the yeah you mean run the, itself uh run the yeah you mean run the, vocabulary is like just the extend, vocabulary is like just the extend, vocabulary is like just the extend, vocabulary no just like learning the new, vocabulary no just like learning the new, vocabulary no just like learning the new, vocabulary on the clinical domain so if, vocabulary on the clinical domain so if, vocabulary on the clinical domain so if, we uh then it totally found the clinical, we uh then it totally found the clinical, we uh then it totally found the clinical, domain so we will totally use a new, domain so we will totally use a new, domain so we will totally use a new, vocabulary right we need to uh actually, vocabulary right we need to uh actually, vocabulary right we need to uh actually, very similar to train from scratch, very similar to train from scratch, very similar to train from scratch, because after we right did you compare, because after we right did you compare, because after we right did you compare, that or, that or, that or, no uh you mean did we uh make, no uh you mean did we uh make, no uh you mean did we uh make, experiments on that yeah yeah no no okay, experiments on that yeah yeah no no okay, experiments on that yeah yeah no no okay, got it okay that would be yeah very, got it okay that would be yeah very, got it okay that would be yeah very, similar like paining from scat you need, similar like paining from scat you need, similar like paining from scat you need, to then totally uh new embeddings right, to then totally uh new embeddings right, to then totally uh new embeddings right, uh that I think requires even more uh, uh that I think requires even more uh, uh that I think requires even more uh, pining data from the medical domain uh, pining data from the medical domain uh, pining data from the medical domain uh, for example Lama 2 they use like uh T, for example Lama 2 they use like uh T, for example Lama 2 they use like uh T, gigabytes uh t t u model TB data set but, gigabytes uh t t u model TB data set but, gigabytes uh t t u model TB data set but, the current available uh pre training, the current available uh pre training, the current available uh pre training, coppers for the medic Dom actually is, coppers for the medic Dom actually is, coppers for the medic Dom actually is, quite limited so if we use a separate, quite limited so if we use a separate, quite limited so if we use a separate, vocab we may need um more pring coers to, vocab we may need um more pring coers to, vocab we may need um more pring coers to, lead to but probably vocabulary size, lead to but probably vocabulary size, lead to but probably vocabulary size, would be smaller then oh sorry probably, would be smaller then oh sorry probably, would be smaller then oh sorry probably, in that case your vocabulary size would, in that case your vocabulary size would, in that case your vocabulary size would, be smaller than right uh yeah we'll be, be smaller than right uh yeah we'll be, be smaller than right uh yeah we'll be, more small smaller they need more um, more small smaller they need more um, more small smaller they need more um, training data to train a capable, training data to train a capable, training data to train a capable, clinical large language models from the, clinical large language models from the, clinical large language models from the, scratch I think this one is very similar, scratch I think this one is very similar, scratch I think this one is very similar, to the uh gon GPT uh work which is also, to the uh gon GPT uh work which is also, to the uh gon GPT uh work which is also, from our uh co-authors from University, from our uh co-authors from University, from our uh co-authors from University, of Florida they based on GB3, of Florida they based on GB3, of Florida they based on GB3, architecture contact the uh paining from, architecture contact the uh paining from, architecture contact the uh paining from, scratch, scratch, scratch, with uh UF notes and some open source, with uh UF notes and some open source, with uh UF notes and some open source, data from yeah the, data from yeah the, data from yeah the, pile okay yeah thanks for your, question yeah so firstly we actually, question yeah so firstly we actually, developed the me L based model through, developed the me L based model through, developed the me L based model through, uh continue paining then uh this, uh continue paining then uh this, uh continue paining then uh this, extensive based on the data uh 129, extensive based on the data uh 129, extensive based on the data uh 129, billion tokens from mixture prain data, billion tokens from mixture prain data, billion tokens from mixture prain data, next we created the milama chat models, next we created the milama chat models, next we created the milama chat models, by instruction fing the milama base, by instruction fing the milama base, by instruction fing the milama base, model with uh, model with uh, model with uh, 25k tail instructions these instructions, 25k tail instructions these instructions, 25k tail instructions these instructions, um spam various medical domains, um spam various medical domains, um spam various medical domains, including medical conversations question, including medical conversations question, including medical conversations question, answering summarization and the relation, answering summarization and the relation, answering summarization and the relation, instruction this frenning process is uh, instruction this frenning process is uh, instruction this frenning process is uh, to enabling the model to respond, to enabling the model to respond, to enabling the model to respond, effectively to clinical medical and, effectively to clinical medical and, effectively to clinical medical and, conversational prompts making it more, conversational prompts making it more, conversational prompts making it more, applicable, applicable, applicable, for real world medical settings finally, for real world medical settings finally, for real world medical settings finally, we evaluated the model's performance in, we evaluated the model's performance in, we evaluated the model's performance in, different settings we assess the M Lama, different settings we assess the M Lama, different settings we assess the M Lama, base model in a supervised learning, base model in a supervised learning, base model in a supervised learning, context across six key text analysis, context across six key text analysis, context across six key text analysis, tasks for the M chat models we tested, tasks for the M chat models we tested, tasks for the M chat models we tested, the zero shot and few shot capabilities, the zero shot and few shot capabilities, the zero shot and few shot capabilities, on the uh six tasks to determine their, on the uh six tasks to determine their, on the uh six tasks to determine their, effectiveness without additional task, effectiveness without additional task, effectiveness without additional task, specific training so so for the m l base, specific training so so for the m l base, specific training so so for the m l base, model we evaluate it performance in a, model we evaluate it performance in a, model we evaluate it performance in a, supervisor learning context across six, supervisor learning context across six, supervisor learning context across six, key text analysis, key text analysis, key text analysis, tasks uh to adapt the Lama 2 backb, tasks uh to adapt the Lama 2 backb, tasks uh to adapt the Lama 2 backb, models for The Med domain so we firstly, models for The Med domain so we firstly, models for The Med domain so we firstly, developed a comprehensive mixed continue, developed a comprehensive mixed continue, developed a comprehensive mixed continue, PR training data our PR training data, PR training data our PR training data, PR training data our PR training data, set is structured with a ratio of 15 to, set is structured with a ratio of 15 to, set is structured with a ratio of 15 to, 1 to4 uh consisting of biomedical, 1 to4 uh consisting of biomedical, 1 to4 uh consisting of biomedical, literature clinical nodes and general, literature clinical nodes and general, literature clinical nodes and general, doand data specifically we Source over 3, doand data specifically we Source over 3, doand data specifically we Source over 3, million four Tex biomedical articles, million four Tex biomedical articles, million four Tex biomedical articles, from PMC aparted Center and over 50, from PMC aparted Center and over 50, from PMC aparted Center and over 50, million abstracts from pmet uh based on, million abstracts from pmet uh based on, million abstracts from pmet uh based on, the P data set uh in addition to bomed, the P data set uh in addition to bomed, the P data set uh in addition to bomed, literature we also Incorporated clinical, literature we also Incorporated clinical, literature we also Incorporated clinical, notes from Real World medical context, notes from Real World medical context, notes from Real World medical context, including the identified free text Data, including the identified free text Data, including the identified free text Data, from mimic 3 mimic 4 and mimic cxr this, from mimic 3 mimic 4 and mimic cxr this, from mimic 3 mimic 4 and mimic cxr this, notes bring in Practical clinical, notes bring in Practical clinical, notes bring in Practical clinical, insights reflecting the complex, insights reflecting the complex, insights reflecting the complex, complexity of patient care uh finally to, complexity of patient care uh finally to, complexity of patient care uh finally to, prevent the model from forgotten general, prevent the model from forgotten general, prevent the model from forgotten general, knowledge during specialization we also, knowledge during specialization we also, knowledge during specialization we also, included a subset of the red pajama data, included a subset of the red pajama data, included a subset of the red pajama data, set which is a general domain data set, set which is a general domain data set, set which is a general domain data set, that replicates Lama to's original prain, that replicates Lama to's original prain, that replicates Lama to's original prain, data this ensures the model to Mains a, data this ensures the model to Mains a, data this ensures the model to Mains a, balanced understanding and avoid, balanced understanding and avoid, balanced understanding and avoid, forgotten previous land information so, forgotten previous land information so, forgotten previous land information so, alog together this data set consist of, alog together this data set consist of, alog together this data set consist of, 129 billion tokens making the uh it's, 129 billion tokens making the uh it's, 129 billion tokens making the uh it's, the L largest and the most comprehensive, the L largest and the most comprehensive, the L largest and the most comprehensive, pre-training data set in the medical, pre-training data set in the medical, pre-training data set in the medical, domain to, domain to, domain to, date I have a question here uh since, date I have a question here uh since, date I have a question here uh since, you're extending your vocabulary by, you're extending your vocabulary by, you're extending your vocabulary by, basically twice the size right so are, basically twice the size right so are, basically twice the size right so are, you fixing the embeddings of the already, you fixing the embeddings of the already, you fixing the embeddings of the already, available tokens and then learning the, available tokens and then learning the, available tokens and then learning the, embeddings of just the new tokens during, embeddings of just the new tokens during, embeddings of just the new tokens during, all of this continual pre-training or, all of this continual pre-training or, all of this continual pre-training or, all of the embeddings would be updated, all of the embeddings would be updated, all of the embeddings would be updated, yeah so good question so all embeddings, yeah so good question so all embeddings, yeah so good question so all embeddings, actually have been, actually have been, actually have been, updated that is why you need that um, updated that is why you need that um, updated that is why you need that um, General domain data to make sure that, General domain data to make sure that, General domain data to make sure that, the new embeddings for the already, the new embeddings for the already, the new embeddings for the already, available tokens it's still reasonable, available tokens it's still reasonable, available tokens it's still reasonable, because otherwise you would have these, because otherwise you would have these, because otherwise you would have these, updated embeddings for General domain, updated embeddings for General domain, updated embeddings for General domain, tokens which has now been updated on, tokens which has now been updated on, tokens which has now been updated on, medical, medical, medical, domain um so is that the reason you have, domain um so is that the reason you have, domain um so is that the reason you have, to use this General domain data um so to, to use this General domain data um so to, to use this General domain data um so to, use the general domain data is because, use the general domain data is because, use the general domain data is because, uh previously we actually do not mix the, uh previously we actually do not mix the, uh previously we actually do not mix the, general domain data and only use the, general domain data and only use the, general domain data and only use the, biomedical literature and clinical nodes, biomedical literature and clinical nodes, biomedical literature and clinical nodes, and we found that uh we quickly test the, and we found that uh we quickly test the, and we found that uh we quickly test the, results of the continue pre-end model on, results of the continue pre-end model on, results of the continue pre-end model on, some Downstream task and compare with, some Downstream task and compare with, some Downstream task and compare with, the it's back bom model Lama 2 and found, the it's back bom model Lama 2 and found, the it's back bom model Lama 2 and found, that it has uh significantly performance, that it has uh significantly performance, that it has uh significantly performance, job in most tasks would I ask for some, job in most tasks would I ask for some, job in most tasks would I ask for some, details for of those Downstream tasks I, details for of those Downstream tasks I, details for of those Downstream tasks I, just want to understand uh when your, just want to understand uh when your, just want to understand uh when your, continually pre-trained model lost, continually pre-trained model lost, continually pre-trained model lost, performance was it on General domain, performance was it on General domain, performance was it on General domain, Downstream tasks or medical doain, Downstream tasks or medical doain, Downstream tasks or medical doain, Downstream actually on both on both okay, Downstream actually on both on both okay, Downstream actually on both on both okay, yeah on both domain on General domain, yeah on both domain on General domain, yeah on both domain on General domain, and both the biomedical domain we, and both the biomedical domain we, and both the biomedical domain we, quickly test uh some QA task one is uh, quickly test uh some QA task one is uh, quickly test uh some QA task one is uh, for the biomedical domain we use the M, for the biomedical domain we use the M, for the biomedical domain we use the M, QA and M MC QA for the general Dom we, QA and M MC QA for the general Dom we, QA and M MC QA for the general Dom we, use the MML Benchmark so both those has, use the MML Benchmark so both those has, use the MML Benchmark so both those has, like around two percentage job uh in the, like around two percentage job uh in the, like around two percentage job uh in the, performance and then we actually check, performance and then we actually check, performance and then we actually check, some previous studies like uh PMC Lama, some previous studies like uh PMC Lama, some previous studies like uh PMC Lama, yeah they found that they try to, yeah they found that they try to, yeah they found that they try to, incorporate some general doand from the, incorporate some general doand from the, incorporate some general doand from the, red pajama to uh mitigate the uh, red pajama to uh mitigate the uh, red pajama to uh mitigate the uh, knowledge forgotten issue so just for, knowledge forgotten issue so just for, knowledge forgotten issue so just for, this way oh thank you that makes sense, this way oh thank you that makes sense, this way oh thank you that makes sense, thank, thank, thank, you yeah so this extensive data coverage, you yeah so this extensive data coverage, you yeah so this extensive data coverage, provides a strong foundation for mil, provides a strong foundation for mil, provides a strong foundation for mil, Lama so enabling it to uh perform, Lama so enabling it to uh perform, Lama so enabling it to uh perform, effectively across a range of medical, effectively across a range of medical, effectively across a range of medical, applications so to Bear the our mil 13, applications so to Bear the our mil 13, applications so to Bear the our mil 13, billion and the 70 billion base model we, billion and the 70 billion base model we, billion and the 70 billion base model we, conducted the continue paining on Lama 2, conducted the continue paining on Lama 2, conducted the continue paining on Lama 2, 1 billion and 70 billion model using the, 1 billion and 70 billion model using the, 1 billion and 70 billion model using the, mixed paining data the training loss is, mixed paining data the training loss is, mixed paining data the training loss is, actually to uh predict the next token in, actually to uh predict the next token in, actually to uh predict the next token in, a sequence to maximize the likely hood, a sequence to maximize the likely hood, a sequence to maximize the likely hood, of the next token given the previous, of the next token given the previous, of the next token given the previous, context the training was executed on the, context the training was executed on the, context the training was executed on the, University of fras hyper AI superc, University of fras hyper AI superc, University of fras hyper AI superc, computer, computer, computer, utilizing 160 uh Invidia a100 GPU each, utilizing 160 uh Invidia a100 GPU each, utilizing 160 uh Invidia a100 GPU each, with 80 gigabytes of memory for, with 80 gigabytes of memory for, with 80 gigabytes of memory for, optimization we uh use the learning rate, optimization we uh use the learning rate, optimization we uh use the learning rate, of 8 e minus 6 the model is training for, of 8 e minus 6 the model is training for, of 8 e minus 6 the model is training for, one ook and it takes for around 700, one ook and it takes for around 700, one ook and it takes for around 700, hours for the training process of the 70, hours for the training process of the 70, hours for the training process of the 70, billion models and we Leverage The the, billion models and we Leverage The the, billion models and we Leverage The the, Deep speed for model paralyzation which, Deep speed for model paralyzation which, Deep speed for model paralyzation which, allowed us to uh L distribute the, allowed us to uh L distribute the, allowed us to uh L distribute the, workload across multiple, workload across multiple, workload across multiple, tribunals uh so to further enhance the, tribunals uh so to further enhance the, tribunals uh so to further enhance the, capabilities of the M Lama base models, capabilities of the M Lama base models, capabilities of the M Lama base models, especially in handling diverse medical, especially in handling diverse medical, especially in handling diverse medical, tasks without the need for task specific, tasks without the need for task specific, tasks without the need for task specific, fun turning so we developed the milama, fun turning so we developed the milama, fun turning so we developed the milama, chat models through an extensive, chat models through an extensive, chat models through an extensive, instruction F turning process for this, instruction F turning process for this, instruction F turning process for this, we uh created another medical, we uh created another medical, we uh created another medical, instruction turning data comprising, instruction turning data comprising, instruction turning data comprising, 25k high quality samples sourced from a, 25k high quality samples sourced from a, 25k high quality samples sourced from a, diverse area of data sets this data sets, diverse area of data sets this data sets, diverse area of data sets this data sets, is unique in its comprehensive coverage, is unique in its comprehensive coverage, is unique in its comprehensive coverage, of both biomedical and clinical domains, of both biomedical and clinical domains, of both biomedical and clinical domains, as shown in this table our sources, as shown in this table our sources, as shown in this table our sources, including uh biomedical literature, including uh biomedical literature, including uh biomedical literature, clinical notes clinical guidelines, clinical notes clinical guidelines, clinical notes clinical guidelines, knowledge graphs and general doand data, knowledge graphs and general doand data, knowledge graphs and general doand data, the instruction data set covers a white, the instruction data set covers a white, the instruction data set covers a white, area of tasks such as Medical, area of tasks such as Medical, area of tasks such as Medical, conversation question answering, conversation question answering, conversation question answering, summarization relation instruction uh, summarization relation instruction uh, summarization relation instruction uh, classification next sentence prediction, classification next sentence prediction, classification next sentence prediction, and keywords, and keywords, and keywords, prediction so using this beard the data, prediction so using this beard the data, prediction so using this beard the data, set uh our goal is to maximize the, set uh our goal is to maximize the, set uh our goal is to maximize the, likelihood of generating uh correct, likelihood of generating uh correct, likelihood of generating uh correct, responses given the specific medical, responses given the specific medical, responses given the specific medical, prompts this fing was performed over, prompts this fing was performed over, prompts this fing was performed over, three ooks using 8 a100 gpus with a, three ooks using 8 a100 gpus with a, three ooks using 8 a100 gpus with a, learning rate of 1 minus 5 and the the, learning rate of 1 minus 5 and the the, learning rate of 1 minus 5 and the the, weight decay of 1 minus 5 and a warm, weight decay of 1 minus 5 and a warm, weight decay of 1 minus 5 and a warm, opati of one manag to to enable a, opati of one manag to to enable a, opati of one manag to to enable a, gradual adaption and we also appli the, gradual adaption and we also appli the, gradual adaption and we also appli the, Laura the parameter efficient fing, Laura the parameter efficient fing, Laura the parameter efficient fing, technique to let the models training uh, technique to let the models training uh, technique to let the models training uh, more efficiency without requiring, more efficiency without requiring, more efficiency without requiring, extensive computation resources the, extensive computation resources the, extensive computation resources the, entire process took, entire process took, entire process took, approximately 140 hours to find the 70, approximately 140 hours to find the 70, approximately 140 hours to find the 70, bilm models so this instruction fing, bilm models so this instruction fing, bilm models so this instruction fing, process allows the model to uh handle, process allows the model to uh handle, process allows the model to uh handle, the diverse medical queries and prompts, the diverse medical queries and prompts, the diverse medical queries and prompts, in a zero shot, setting this is an example of the, setting this is an example of the, instruction Samples used for f mamaat so, instruction Samples used for f mamaat so, instruction Samples used for f mamaat so, the input is the a prompt concatenating, the input is the a prompt concatenating, the input is the a prompt concatenating, with the uh input question the response, with the uh input question the response, with the uh input question the response, is actually the gold standard answer, is actually the gold standard answer, is actually the gold standard answer, which the model aims to, which the model aims to, which the model aims to, generate so most existing studies, generate so most existing studies, generate so most existing studies, actually focus preliminarily on, actually focus preliminarily on, actually focus preliminarily on, evaluating models through the question, evaluating models through the question, evaluating models through the question, answering task in our paper we aim to, answering task in our paper we aim to, answering task in our paper we aim to, provide a more comprehensive assessment, provide a more comprehensive assessment, provide a more comprehensive assessment, by creating a new Benchmark so this new, by creating a new Benchmark so this new, by creating a new Benchmark so this new, Benchmark includes six critical text, Benchmark includes six critical text, Benchmark includes six critical text, analysis tasks including question, analysis tasks including question, analysis tasks including question, answering name entity, answering name entity, answering name entity, recognition relation instruction text, recognition relation instruction text, recognition relation instruction text, classification text summarization and, classification text summarization and, classification text summarization and, the natural language inference so this, the natural language inference so this, the natural language inference so this, tasks collectively uh cover 12 diverse, tasks collectively uh cover 12 diverse, tasks collectively uh cover 12 diverse, data sets uh sources from both, data sets uh sources from both, data sets uh sources from both, biomedical and the clinical demands so, biomedical and the clinical demands so, biomedical and the clinical demands so, can I ask you one quick question here, can I ask you one quick question here, can I ask you one quick question here, because it seem like some of these task, because it seem like some of these task, because it seem like some of these task, is actually including the data set that, is actually including the data set that, is actually including the data set that, you pre-trained on don't you think that, you pre-trained on don't you think that, you pre-trained on don't you think that, that is kind of strange that um you have, that is kind of strange that um you have, that is kind of strange that um you have, the cxr you have the pbet these are, the cxr you have the pbet these are, the cxr you have the pbet these are, these were in your pre-training data set, these were in your pre-training data set, these were in your pre-training data set, too, too, too, right uh yeah so actually the data, right uh yeah so actually the data, right uh yeah so actually the data, source is the same but the task actually, source is the same but the task actually, source is the same but the task actually, um but if you train the model on those, um but if you train the model on those, um but if you train the model on those, like pre-train the model on those data, like pre-train the model on those data, like pre-train the model on those data, sets and then you f tune then you are, sets and then you f tune then you are, sets and then you f tune then you are, also evaluating on the same data set, also evaluating on the same data set, also evaluating on the same data set, this is kind of like strange right like, this is kind of like strange right like, this is kind of like strange right like, it's kind of like we are thinking of, it's kind of like we are thinking of, it's kind of like we are thinking of, like kind like data leak, like kind like data leak, like kind like data leak, here uh yeah so for the pet the like the, here uh yeah so for the pet the like the, here uh yeah so for the pet the like the, the PM summarization task here is like, the PM summarization task here is like, the PM summarization task here is like, we input the um for Conta for four Tex, we input the um for Conta for four Tex, we input the um for Conta for four Tex, and output the uh generate the uh, and output the uh generate the uh, and output the uh generate the uh, abstract to summarize the paper right, abstract to summarize the paper right, abstract to summarize the paper right, but the model was already trained on pet, but the model was already trained on pet, but the model was already trained on pet, then mimic cxa then i2b2 challenge they, then mimic cxa then i2b2 challenge they, then mimic cxa then i2b2 challenge they, also use the mimic so so most of the the, also use the mimic so so most of the the, also use the mimic so so most of the the, challenge data set that you are using, challenge data set that you are using, challenge data set that you are using, you are already including in your in, you are already including in your in, you are already including in your in, your pre, your pre, your pre, training yeah I think uh for pm and MIM, training yeah I think uh for pm and MIM, training yeah I think uh for pm and MIM, CSI it's May all have but um our, CSI it's May all have but um our, CSI it's May all have but um our, training process is actually kind of uh, training process is actually kind of uh, training process is actually kind of uh, unsupervised process right you input the, unsupervised process right you input the, unsupervised process right you input the, uh text itself you aim to generate uh, uh text itself you aim to generate uh, uh text itself you aim to generate uh, the tokens in this sequence word by word, the tokens in this sequence word by word, the tokens in this sequence word by word, token it's not really unsupervised is, token it's not really unsupervised is, token it's not really unsupervised is, self-supervised so you are still giving, self-supervised so you are still giving, self-supervised so you are still giving, the model the whole sequence so it's not, the model the whole sequence so it's not, the model the whole sequence so it's not, completely, unsupervised yeah it's kind of self, unsupervised yeah it's kind of self, supervised right like generally the next, supervised right like generally the next, supervised right like generally the next, token right so the model is still like, token right so the model is still like, token right so the model is still like, looking at that information, looking at that information, looking at that information, right uh kind I think kind of U may have, right uh kind I think kind of U may have, right uh kind I think kind of U may have, the risk but I think it's not so uh int, the risk but I think it's not so uh int, the risk but I think it's not so uh int, intuitive that direct give them the, intuitive that direct give them the, intuitive that direct give them the, super Vision, super Vision, super Vision, information uh because the downstream, information uh because the downstream, information uh because the downstream, task here here is kind of, task here here is kind of, task here here is kind of, superv uh task you have the input paired, superv uh task you have the input paired, superv uh task you have the input paired, with the gold standard, with the gold standard, with the gold standard, answer okay okay so I I'm just curious, answer okay okay so I I'm just curious, answer okay okay so I I'm just curious, like did you publish this paper, like did you publish this paper, like did you publish this paper, somewhere or this is still in review uh, somewhere or this is still in review uh, somewhere or this is still in review uh, it's still on the review okay got it, it's still on the review okay got it, it's still on the review okay got it, yeah thank, you oh yeah so this tasks uh cover 12, you oh yeah so this tasks uh cover 12, data set from both biomedical and the, data set from both biomedical and the, data set from both biomedical and the, clinical domains as strong L table so in, clinical domains as strong L table so in, clinical domains as strong L table so in, evaluate for evaluation we use test, evaluate for evaluation we use test, evaluate for evaluation we use test, specific metric to measure the model's, specific metric to measure the model's, specific metric to measure the model's, performance like for QA task we use, performance like for QA task we use, performance like for QA task we use, accuracy and micro F1 and for EMR QA we, accuracy and micro F1 and for EMR QA we, accuracy and micro F1 and for EMR QA we, follow previous studies to use the exact, follow previous studies to use the exact, follow previous studies to use the exact, match and F1 metric uh the N was, match and F1 metric uh the N was, match and F1 metric uh the N was, evaluated with the entity level micro F1, evaluated with the entity level micro F1, evaluated with the entity level micro F1, and I task uh is use use the micro F1, and I task uh is use use the micro F1, and I task uh is use use the micro F1, score the text classification we use a, score the text classification we use a, score the text classification we use a, label wise mro F1 and accuracy metric, label wise mro F1 and accuracy metric, label wise mro F1 and accuracy metric, and text mization we use the roach and, and text mization we use the roach and, and text mization we use the roach and, bird school so thei task we also use the, bird school so thei task we also use the, bird school so thei task we also use the, accuracy and the mro F1, school we evaluated the m models and two, school we evaluated the m models and two, setting so why is zero shot and the, setting so why is zero shot and the, setting so why is zero shot and the, supervisor learning to access their, supervisor learning to access their, supervisor learning to access their, performance and the generalization, performance and the generalization, performance and the generalization, abilities across diverse medical tasks, abilities across diverse medical tasks, abilities across diverse medical tasks, compared to Baseline models in the, compared to Baseline models in the, compared to Baseline models in the, supervised learning setting we F the, supervised learning setting we F the, supervised learning setting we F the, milama 13 billion and 17 based models on, milama 13 billion and 17 based models on, milama 13 billion and 17 based models on, task specific training sets for each, task specific training sets for each, task specific training sets for each, exess data set for this setting, exess data set for this setting, exess data set for this setting, evaluates the model's adaptability and, evaluates the model's adaptability and, evaluates the model's adaptability and, performance with the labeled data for, performance with the labeled data for, performance with the labeled data for, each, each, each, task for supervised learning we use the, task for supervised learning we use the, task for supervised learning we use the, uh atom Optimizer training 43 to five, uh atom Optimizer training 43 to five, uh atom Optimizer training 43 to five, ocus depending on the data size with a, ocus depending on the data size with a, ocus depending on the data size with a, uniform lar rate of 1 minus 5 our, uniform lar rate of 1 minus 5 our, uniform lar rate of 1 minus 5 our, Baseline model including Lama 2 models, Baseline model including Lama 2 models, Baseline model including Lama 2 models, in 13 billion and 70 billion PMC Lama, in 13 billion and 70 billion PMC Lama, in 13 billion and 70 billion PMC Lama, model and the medon models so PMC Lama, model and the medon models so PMC Lama, model and the medon models so PMC Lama, 13 billion, 13 billion, 13 billion, model is a biomedical L model, model is a biomedical L model, model is a biomedical L model, continually pred on biomedical papers, continually pred on biomedical papers, continually pred on biomedical papers, and the medical books medic medican are, and the medical books medic medican are, and the medical books medic medican are, the current sorta medical large language, the current sorta medical large language, the current sorta medical large language, models based on Lama 2 7 billion and 70, models based on Lama 2 7 billion and 70, models based on Lama 2 7 billion and 70, billion uh with continue paining on a, billion uh with continue paining on a, billion uh with continue paining on a, mix of clinical guidelines medical, mix of clinical guidelines medical, mix of clinical guidelines medical, papers and abstracts the serero shot, papers and abstracts the serero shot, papers and abstracts the serero shot, evaluation test the model's ability to, evaluation test the model's ability to, evaluation test the model's ability to, generalize to new test without specific, generalize to new test without specific, generalize to new test without specific, training on them so for this we use uh, training on them so for this we use uh, training on them so for this we use uh, standardized prompts across the test, standardized prompts across the test, standardized prompts across the test, data sets and compared the milama 30 and, data sets and compared the milama 30 and, data sets and compared the milama 30 and, 70 billion chat models with both, 70 billion chat models with both, 70 billion chat models with both, commercialized and open-source, commercialized and open-source, commercialized and open-source, baselines so our Baseline including the, baselines so our Baseline including the, baselines so our Baseline including the, uh top commercial lar langage models, uh top commercial lar langage models, uh top commercial lar langage models, like chat gbd and gbd4 we also use the, like chat gbd and gbd4 we also use the, like chat gbd and gbd4 we also use the, specialized medical open source larange, specialized medical open source larange, specialized medical open source larange, models uh such as M apaka PMC Lama chat, models uh such as M apaka PMC Lama chat, models uh such as M apaka PMC Lama chat, version apak which are all in, version apak which are all in, version apak which are all in, instruction f for medical tasks uh met, instruction f for medical tasks uh met, instruction f for medical tasks uh met, apaka model were based on the Llama one, apaka model were based on the Llama one, apaka model were based on the Llama one, model specifically font for the um, model specifically font for the um, model specifically font for the um, medical domain and the PMC Lama chat, medical domain and the PMC Lama chat, medical domain and the PMC Lama chat, version is an extraction font medical, version is an extraction font medical, version is an extraction font medical, large language model based on the PMC, large language model based on the PMC, large language model based on the PMC, Lama based version the APPA KO model is, Lama based version the APPA KO model is, Lama based version the APPA KO model is, specifically her for the clinical tasks, specifically her for the clinical tasks, specifically her for the clinical tasks, based on Lama 2 13 billion by, based on Lama 2 13 billion by, based on Lama 2 13 billion by, instruction, fanty Le two figures shows the uh zero, fanty Le two figures shows the uh zero, shot performance of me Lama chat models, shot performance of me Lama chat models, shot performance of me Lama chat models, and other instruction F open source, and other instruction F open source, and other instruction F open source, large language models with chat ability, large language models with chat ability, large language models with chat ability, on to on six tasks so we compare the RO, on to on six tasks so we compare the RO, on to on six tasks so we compare the RO, a score for the summarization task the, a score for the summarization task the, a score for the summarization task the, curacy score for the QA task and F1, curacy score for the QA task and F1, curacy score for the QA task and F1, score for remaining data sets uh among, score for remaining data sets uh among, score for remaining data sets uh among, 13 billion model we can see uh mil Lama, 13 billion model we can see uh mil Lama, 13 billion model we can see uh mil Lama, 13 billion chat model outperform the, 13 billion chat model outperform the, 13 billion chat model outperform the, Lama 2 13 billion chat and the PMC Lama, Lama 2 13 billion chat and the PMC Lama, Lama 2 13 billion chat and the PMC Lama, CH and met apaka 13 billion in almost, CH and met apaka 13 billion in almost, CH and met apaka 13 billion in almost, all 12 data so milama also outperform, all 12 data so milama also outperform, all 12 data so milama also outperform, the apaka 13 billion in none of our N, the apaka 13 billion in none of our N, the apaka 13 billion in none of our N, Out of 12 data sets so a models with 70, Out of 12 data sets so a models with 70, Out of 12 data sets so a models with 70, billion, billion, billion, pomet milama 13 billion chat model or, pomet milama 13 billion chat model or, pomet milama 13 billion chat model or, consistently outperform the Lama to 13, consistently outperform the Lama to 13, consistently outperform the Lama to 13, billion chat model on 11 out of 12 dates, billion chat model on 11 out of 12 dates, billion chat model on 11 out of 12 dates, uh another observation we found is worth, uh another observation we found is worth, uh another observation we found is worth, noting is that the milama 13 billion, noting is that the milama 13 billion, noting is that the milama 13 billion, chat model actually can show comparable, chat model actually can show comparable, chat model actually can show comparable, performance with the 70 billion models, performance with the 70 billion models, performance with the 70 billion models, in most data sets uh in here is eight, in most data sets uh in here is eight, in most data sets uh in here is eight, out of 12 data sets it can achieve the, out of 12 data sets it can achieve the, out of 12 data sets it can achieve the, comparable, comparable, comparable, performance in this fig we further, performance in this fig we further, performance in this fig we further, compare the future performance uh, compare the future performance uh, compare the future performance uh, between me Lama models with M 70 billion, between me Lama models with M 70 billion, between me Lama models with M 70 billion, and the current state of art medical LGE, and the current state of art medical LGE, and the current state of art medical LGE, language, language, language, model so given medic uh it doesn't have, model so given medic uh it doesn't have, model so given medic uh it doesn't have, the instruction find turnning version so, the instruction find turnning version so, the instruction find turnning version so, uh given its limited ability to follow, uh given its limited ability to follow, uh given its limited ability to follow, instruction our performance comparison, instruction our performance comparison, instruction our performance comparison, utilize a few shot approach so we, utilize a few shot approach so we, utilize a few shot approach so we, employing a one shot method for the, employing a one shot method for the, employing a one shot method for the, summarization data set since uh its, summarization data set since uh its, summarization data set since uh its, input lens is quite extensive and we use, input lens is quite extensive and we use, input lens is quite extensive and we use, five shot method for the other data sets, five shot method for the other data sets, five shot method for the other data sets, uh we can see that, uh we can see that, uh we can see that, the uh M Lama models can consistently, the uh M Lama models can consistently, the uh M Lama models can consistently, achieved better performance in 11 out of, achieved better performance in 11 out of, achieved better performance in 11 out of, 12 data sets except for the H Med QA, 12 data sets except for the H Med QA, 12 data sets except for the H Med QA, data, set uh these two figures further compare, set uh these two figures further compare, the performance of the M Lama 13 billion, the performance of the M Lama 13 billion, the performance of the M Lama 13 billion, and the 70 billion uh base models with, and the 70 billion uh base models with, and the 70 billion uh base models with, other open source large language models, other open source large language models, other open source large language models, in the supervised learning, in the supervised learning, in the supervised learning, set so in the 13 billion we can observe, set so in the 13 billion we can observe, set so in the 13 billion we can observe, that the M 13 billi model can outperform, that the M 13 billi model can outperform, that the M 13 billi model can outperform, the similar sized Medical Foundation, the similar sized Medical Foundation, the similar sized Medical Foundation, model like PMC Lama on 11 out of 12 data, model like PMC Lama on 11 out of 12 data, model like PMC Lama on 11 out of 12 data, sets and also outperform the its back, sets and also outperform the its back, sets and also outperform the its back, bound model the general Foundation model, bound model the general Foundation model, bound model the general Foundation model, Lama 23 million on 10 out of 12 data, Lama 23 million on 10 out of 12 data, Lama 23 million on 10 out of 12 data, sets and uh yeah it's also notice that, sets and uh yeah it's also notice that, sets and uh yeah it's also notice that, similarly in the zero shot setting the, similarly in the zero shot setting the, similarly in the zero shot setting the, 13 billion model was compatitive with, 13 billion model was compatitive with, 13 billion model was compatitive with, the 70 billion models in most cases uh, the 70 billion models in most cases uh, the 70 billion models in most cases uh, in this case it's steer eight out of 12, in this case it's steer eight out of 12, in this case it's steer eight out of 12, data sets so for the 70 billion models, data sets so for the 70 billion models, data sets so for the 70 billion models, yeah mil steer achieved the best, yeah mil steer achieved the best, yeah mil steer achieved the best, performance on nine out of 12 data sets, performance on nine out of 12 data sets, performance on nine out of 12 data sets, so when compared with the Lama to uh 70, so when compared with the Lama to uh 70, so when compared with the Lama to uh 70, billion and the med 70 bilon, model in this uh figure we further, model in this uh figure we further, compares the performance of M Lama, compares the performance of M Lama, compares the performance of M Lama, models in the zero shot and the, models in the zero shot and the, models in the zero shot and the, supervisor learning setting, supervisor learning setting, supervisor learning setting, against the uh commercialized so, against the uh commercialized so, against the uh commercialized so, commercialized lar R models CH GPT and, commercialized lar R models CH GPT and, commercialized lar R models CH GPT and, gp4 which have significant larger modor, gp4 which have significant larger modor, gp4 which have significant larger modor, size uh when compared with milama uh due, size uh when compared with milama uh due, size uh when compared with milama uh due, to the privacy concerns which we cannot, to the privacy concerns which we cannot, to the privacy concerns which we cannot, transfer the clinical data sets with, transfer the clinical data sets with, transfer the clinical data sets with, patient information to chat gbd and gbd4, patient information to chat gbd and gbd4, patient information to chat gbd and gbd4, so we only conducted our comparison, so we only conducted our comparison, so we only conducted our comparison, across eight open source data sets that, across eight open source data sets that, across eight open source data sets that, are not subject to this limitation in, are not subject to this limitation in, are not subject to this limitation in, this in the zero shot setting uh we, this in the zero shot setting uh we, this in the zero shot setting uh we, observe that milama models outperform, observe that milama models outperform, observe that milama models outperform, charp on five data sets but fil shot on, charp on five data sets but fil shot on, charp on five data sets but fil shot on, seven data sets when compared with gbd4, seven data sets when compared with gbd4, seven data sets when compared with gbd4, so after task specific supervisor F, so after task specific supervisor F, so after task specific supervisor F, turning it surpassed the CH GPT on seven, turning it surpassed the CH GPT on seven, turning it surpassed the CH GPT on seven, out of eight DSS and uh outperform G4 um, out of eight DSS and uh outperform G4 um, out of eight DSS and uh outperform G4 um, five out of eight data sets so due to, five out of eight data sets so due to, five out of eight data sets so due to, the model size is significantly smaller, the model size is significantly smaller, the model size is significantly smaller, then when compare with gbd4 and CH GPT, then when compare with gbd4 and CH GPT, then when compare with gbd4 and CH GPT, so uh M Lama models uh Ste show an, so uh M Lama models uh Ste show an, so uh M Lama models uh Ste show an, impressive performance and uh ability, impressive performance and uh ability, impressive performance and uh ability, for super both supervis learning and, for super both supervis learning and, for super both supervis learning and, zero shot learning process on a b area, zero shot learning process on a b area, zero shot learning process on a b area, of medical tasks so this underscore is, of medical tasks so this underscore is, of medical tasks so this underscore is, efficiency and the potential for, efficiency and the potential for, efficiency and the potential for, handling uh different medical, handling uh different medical, handling uh different medical, tasks so this table further demonstrates, tasks so this table further demonstrates, tasks so this table further demonstrates, the zero shot performance of M Lama, the zero shot performance of M Lama, the zero shot performance of M Lama, models compared to their Lama 2 models, models compared to their Lama 2 models, models compared to their Lama 2 models, or to illustrate the impact of continue, or to illustrate the impact of continue, or to illustrate the impact of continue, paining and instruction fing we can see, paining and instruction fing we can see, paining and instruction fing we can see, that uh the continue paining actually, that uh the continue paining actually, that uh the continue paining actually, can yielded a substantial performance, can yielded a substantial performance, can yielded a substantial performance, improvements AC Ross different data sets, improvements AC Ross different data sets, improvements AC Ross different data sets, with the performance Gaines ranging from, with the performance Gaines ranging from, with the performance Gaines ranging from, point uh 0.5 percentage up to, point uh 0.5 percentage up to, point uh 0.5 percentage up to, 55 percentage for example the milama 70, 55 percentage for example the milama 70, 55 percentage for example the milama 70, billion model after continue pre, billion model after continue pre, billion model after continue pre, training shows considerable improvements, training shows considerable improvements, training shows considerable improvements, over the Baseline Lama 270 billion, over the Baseline Lama 270 billion, over the Baseline Lama 270 billion, models especially in domain specific, models especially in domain specific, models especially in domain specific, task like Med QA and P Med QA so this, task like Med QA and P Med QA so this, task like Med QA and P Med QA so this, shows the benefits of continue paining, shows the benefits of continue paining, shows the benefits of continue paining, with the medical and biomedical data can, with the medical and biomedical data can, with the medical and biomedical data can, improve the performance of the model on, improve the performance of the model on, improve the performance of the model on, the domain specific data so instruction, the domain specific data so instruction, the domain specific data so instruction, fing can further uh enhance the model's, fing can further uh enhance the model's, fing can further uh enhance the model's, zero shot capabilities with the, zero shot capabilities with the, zero shot capabilities with the, performance increases from like, performance increases from like, performance increases from like, 3.7 percentage to, 3.7 percentage to, 3.7 percentage to, 51.9 percentage for example the uh, 51.9 percentage for example the uh, 51.9 percentage for example the uh, milama 70 billion chat model uh after, milama 70 billion chat model uh after, milama 70 billion chat model uh after, the instruction, the instruction, the instruction, fing already uh show significant, fing already uh show significant, fing already uh show significant, improvements in response accuracy, improvements in response accuracy, improvements in response accuracy, compared with the base model milama 70, compared with the base model milama 70, compared with the base model milama 70, billion model so overall the results, billion model so overall the results, billion model so overall the results, here uh emphasize the importance of, here uh emphasize the importance of, here uh emphasize the importance of, continue pre- training combined with, continue pre- training combined with, continue pre- training combined with, instruction te for adapting large, instruction te for adapting large, instruction te for adapting large, langage model to complex and specialized, langage model to complex and specialized, langage model to complex and specialized, tasks in the medical, so to support the advancement of the, so to support the advancement of the, medical AI research we actually have, medical AI research we actually have, medical AI research we actually have, made the mama resources accessible uh we, made the mama resources accessible uh we, made the mama resources accessible uh we, releas the data sets and evaluation, releas the data sets and evaluation, releas the data sets and evaluation, script on the GitHub repo and the, script on the GitHub repo and the, script on the GitHub repo and the, researchers can further leverage these, researchers can further leverage these, researchers can further leverage these, resources to Benchmark and refine their, resources to Benchmark and refine their, resources to Benchmark and refine their, own models so the M Lama models with are, own models so the M Lama models with are, own models so the M Lama models with are, currently published on physio so it also, currently published on physio so it also, currently published on physio so it also, enabling further deployment by the, enabling further deployment by the, enabling further deployment by the, research, research, research, community and we uh developed an online, community and we uh developed an online, community and we uh developed an online, chat board based on the milama chat, chat board based on the milama chat, chat board based on the milama chat, models this demo shows the models, models this demo shows the models, models this demo shows the models, ability to respond to the medical, inquiries so yeah this highlighting is, inquiries so yeah this highlighting is, mil's potential in some practical, applications so this is the response for, applications so this is the response for, the question then we, the question then we, the question then we, continue have a followup, question so yeah it actually support the, question so yeah it actually support the, MTI, conversations CH can I ask uh like what, conversations CH can I ask uh like what, is kind of under the H for this uh yeah, is kind of under the H for this uh yeah, is kind of under the H for this uh yeah, chpo do you have like a retrieval based, chpo do you have like a retrieval based, chpo do you have like a retrieval based, scenario uh where it goes and searches, scenario uh where it goes and searches, scenario uh where it goes and searches, for the right context I mean sort of a, for the right context I mean sort of a, for the right context I mean sort of a, rag based approach or is it purely, rag based approach or is it purely, rag based approach or is it purely, generated yeah it's just a a demo chat, generated yeah it's just a a demo chat, generated yeah it's just a a demo chat, Bo totally based on the milama 1 billion, Bo totally based on the milama 1 billion, Bo totally based on the milama 1 billion, chat version, chat version, chat version, we just build an interface here so the, we just build an interface here so the, we just build an interface here so the, uh supporting the inference is just the, uh supporting the inference is just the, uh supporting the inference is just the, orang model without information without, orang model without information without, orang model without information without, retrieval augmentation okay yeah we, retrieval augmentation okay yeah we, retrieval augmentation okay yeah we, notice the like the hallucination and, notice the like the hallucination and, notice the like the hallucination and, the factuality issue for its responses, the factuality issue for its responses, the factuality issue for its responses, so yeah we actually are trying to use, so yeah we actually are trying to use, so yeah we actually are trying to use, the retrieve augmentation method to, the retrieve augmentation method to, the retrieve augmentation method to, further improve, further improve, further improve, it thank you that makes sense, yeah so finally uh during our model, yeah so finally uh during our model, development we actually identified, development we actually identified, development we actually identified, several uh key insights and challenges, several uh key insights and challenges, several uh key insights and challenges, particularly When comparing with zero, particularly When comparing with zero, particularly When comparing with zero, shot and supervised learning settings so, shot and supervised learning settings so, shot and supervised learning settings so, in zero shot learning setting our model, in zero shot learning setting our model, in zero shot learning setting our model, like all other medical lar models uh, like all other medical lar models uh, like all other medical lar models uh, show a relative low performance on, show a relative low performance on, show a relative low performance on, certain tasks including entity, certain tasks including entity, certain tasks including entity, recognition relation instruction and EMR, recognition relation instruction and EMR, recognition relation instruction and EMR, QA so even Advanced model like gp4, QA so even Advanced model like gp4, QA so even Advanced model like gp4, struggle with this tasks in the zero, struggle with this tasks in the zero, struggle with this tasks in the zero, shop setting so this highlights the, shop setting so this highlights the, shop setting so this highlights the, unique challenges on this tasks one, unique challenges on this tasks one, unique challenges on this tasks one, reason uh after manually checking we one, reason uh after manually checking we one, reason uh after manually checking we one, reason we found that for this low, reason we found that for this low, reason we found that for this low, performance is the model actually often, performance is the model actually often, performance is the model actually often, struggle to follow instructions, struggle to follow instructions, struggle to follow instructions, consistently they cannot generate the, consistently they cannot generate the, consistently they cannot generate the, responses in the required form at this, responses in the required form at this, responses in the required form at this, Inc consistency actually also leads to a, Inc consistency actually also leads to a, Inc consistency actually also leads to a, challenging for the evaluation process, challenging for the evaluation process, challenging for the evaluation process, since the model's tendency not to, since the model's tendency not to, since the model's tendency not to, strictly follow instruction so it may, strictly follow instruction so it may, strictly follow instruction so it may, generate answers in varying, generate answers in varying, generate answers in varying, formats um so this actually challenges, formats um so this actually challenges, formats um so this actually challenges, the use of automatic metrics as this, the use of automatic metrics as this, the use of automatic metrics as this, metric actually rely on consistent, metric actually rely on consistent, metric actually rely on consistent, formatting to pass the responses, formatting to pass the responses, formatting to pass the responses, actually uh Act ACC so when the model's, actually uh Act ACC so when the model's, actually uh Act ACC so when the model's, output deviates from the expected format, output deviates from the expected format, output deviates from the expected format, automatic matrics actually we struggle, automatic matrics actually we struggle, automatic matrics actually we struggle, to align the response with the gold, to align the response with the gold, to align the response with the gold, standard so making it very hard to, standard so making it very hard to, standard so making it very hard to, perform a fair and reliable adustment of, perform a fair and reliable adustment of, perform a fair and reliable adustment of, the performance in most tasks so moving, the performance in most tasks so moving, the performance in most tasks so moving, forward improving the zero shot, forward improving the zero shot, forward improving the zero shot, capabilities for medical tasks we are, capabilities for medical tasks we are, capabilities for medical tasks we are, likely require more research not just, likely require more research not just, likely require more research not just, only on model devel ments but also on, only on model devel ments but also on, only on model devel ments but also on, better evaluation, better evaluation, better evaluation, methods on the other hand in the, methods on the other hand in the, methods on the other hand in the, supervisor learning setting we uh so, supervisor learning setting we uh so, supervisor learning setting we uh so, much stronger results so comparing the, much stronger results so comparing the, much stronger results so comparing the, milama model with the traditional state, milama model with the traditional state, milama model with the traditional state, of art methods like based on the pan, of art methods like based on the pan, of art methods like based on the pan, language models like bir or biert or P, language models like bir or biert or P, language models like bir or biert or P, bir we found that it's consistently uh, bir we found that it's consistently uh, bir we found that it's consistently uh, demonstrated can demonstrate a, demonstrated can demonstrate a, demonstrated can demonstrate a, comparable performance if not Superior, performance uh during our model, performance uh during our model, development we also observed that the, development we also observed that the, development we also observed that the, diversity and ratio of data sources can, diversity and ratio of data sources can, diversity and ratio of data sources can, significantly impact the performance, significantly impact the performance, significantly impact the performance, across the medical and general tasks so, across the medical and general tasks so, across the medical and general tasks so, models like uh PMC Lama which Ed a, models like uh PMC Lama which Ed a, models like uh PMC Lama which Ed a, 1921 ratio of medical to General data of, 1921 ratio of medical to General data of, 1921 ratio of medical to General data of, during our evaluation we actually uh, during our evaluation we actually uh, during our evaluation we actually uh, test that it has a like, test that it has a like, test that it has a like, 2.7 percentage job in performance on, 2.7 percentage job in performance on, 2.7 percentage job in performance on, both biomedical and general domain tasks, both biomedical and general domain tasks, both biomedical and general domain tasks, uh we we guess this is made due to the, uh we we guess this is made due to the, uh we we guess this is made due to the, insufficient General domain data they, insufficient General domain data they, insufficient General domain data they, used for the training and also, used for the training and also, used for the training and also, insufficient training data for the, insufficient training data for the, insufficient training data for the, biomedical domain so similarly the medal, biomedical domain so similarly the medal, biomedical domain so similarly the medal, models we they use uh 9 to1 perc ratio, models we they use uh 9 to1 perc ratio, models we they use uh 9 to1 perc ratio, of medical to General domain uh it, of medical to General domain uh it, of medical to General domain uh it, perform well on the biomedical data but, perform well on the biomedical data but, perform well on the biomedical data but, also has a performance job in the, also has a performance job in the, also has a performance job in the, general uh domain, general uh domain, general uh domain, tasks uh like around 1 percentage, tasks uh like around 1 percentage, tasks uh like around 1 percentage, decline in the general domain task so, decline in the general domain task so, decline in the general domain task so, this also suggesting a lack of general, this also suggesting a lack of general, this also suggesting a lack of general, knowledge retention so during our, knowledge retention so during our, knowledge retention so during our, experiments our model the m GMA model, experiments our model the m GMA model, experiments our model the m GMA model, utilize a 4 to one ratio of medical to, utilize a 4 to one ratio of medical to, utilize a 4 to one ratio of medical to, General data it has both uh performance, General data it has both uh performance, General data it has both uh performance, improvements on General domain task and, improvements on General domain task and, improvements on General domain task and, the medical, the medical, the medical, task so this results actually underscore, task so this results actually underscore, task so this results actually underscore, the importance of integrating the, the importance of integrating the, the importance of integrating the, general domain data to maintain a, general domain data to maintain a, general domain data to maintain a, model's adaptability across different, model's adaptability across different, model's adaptability across different, tasks however we found that finding the, tasks however we found that finding the, tasks however we found that finding the, optimal ratio between the specialized, optimal ratio between the specialized, optimal ratio between the specialized, medical data and the General Dom data is, medical data and the General Dom data is, medical data and the General Dom data is, actually very challenging and actually, actually very challenging and actually, actually very challenging and actually, requires further empirical exploration, requires further empirical exploration, requires further empirical exploration, to maximize both the domain specific, to maximize both the domain specific, to maximize both the domain specific, accuracy and generalization ability we, accuracy and generalization ability we, accuracy and generalization ability we, think future studies can investigate, think future studies can investigate, think future studies can investigate, measures to refund this balance more, measures to refund this balance more, measures to refund this balance more, effect, effectively so our model development, effectively so our model development, also highlights the substantial, also highlights the substantial, also highlights the substantial, difference in cost and resource, difference in cost and resource, difference in cost and resource, allocation between pring and instruction, allocation between pring and instruction, allocation between pring and instruction, Fant paining is actually highly resource, Fant paining is actually highly resource, Fant paining is actually highly resource, intensive for instance the preining of, intensive for instance the preining of, intensive for instance the preining of, the Lama 70 billion model uh requires, the Lama 70 billion model uh requires, the Lama 70 billion model uh requires, around 700 hours for 10068 100 gpus per, around 700 hours for 10068 100 gpus per, around 700 hours for 10068 100 gpus per, ook so amounting up to approximately, ook so amounting up to approximately, ook so amounting up to approximately, 459k dollars the instruction fing on the, 459k dollars the instruction fing on the, 459k dollars the instruction fing on the, other hand is far more economical so, other hand is far more economical so, other hand is far more economical so, turning the model with the Specific, turning the model with the Specific, turning the model with the Specific, Instructions requires about 70 hours on, Instructions requires about 70 hours on, Instructions requires about 70 hours on, 8 a00 gpus per ook costing like, 8 a00 gpus per ook costing like, 8 a00 gpus per ook costing like, 4.4 uh, 4.4 uh, 4.4 uh, 4.49k dollar so despite the lower, 4.49k dollar so despite the lower, 4.49k dollar so despite the lower, resource demand for the 13 billion model, resource demand for the 13 billion model, resource demand for the 13 billion model, uh for for the uh for the instruction, uh for for the uh for the instruction, uh for for the uh for the instruction, turning we found that the instruction, turning we found that the instruction, turning we found that the instruction, funning itself actually can produce the, funning itself actually can produce the, funning itself actually can produce the, significant improvements, significant improvements, significant improvements, uh in the milama 13 bilon chart models, uh in the milama 13 bilon chart models, uh in the milama 13 bilon chart models, it can improve its, it can improve its, it can improve its, performance uh making the performance, performance uh making the performance, performance uh making the performance, game from 12 percentage to 45 percentage, game from 12 percentage to 45 percentage, game from 12 percentage to 45 percentage, across 11 out of 12 data sets in the, across 11 out of 12 data sets in the, across 11 out of 12 data sets in the, zero shot setting when compared with its, zero shot setting when compared with its, zero shot setting when compared with its, backb model milama 13 billion base model, backb model milama 13 billion base model, backb model milama 13 billion base model, so actually this suggest that, so actually this suggest that, so actually this suggest that, instruction Fanning is a highly cost, instruction Fanning is a highly cost, instruction Fanning is a highly cost, effective strategy for model Improvement, effective strategy for model Improvement, effective strategy for model Improvement, particularly in settings where resources, particularly in settings where resources, particularly in settings where resources, are quite, are quite, are quite, limited we are further discuss the use, limited we are further discuss the use, limited we are further discuss the use, of mil models depending on task, of mil models depending on task, of mil models depending on task, requirements and the resour availability, requirements and the resour availability, requirements and the resour availability, so the base milama models developed, so the base milama models developed, so the base milama models developed, through the paining actually contain a, through the paining actually contain a, through the paining actually contain a, wide aray of medical knowledge so these, wide aray of medical knowledge so these, wide aray of medical knowledge so these, models are best suited for supervisor, models are best suited for supervisor, models are best suited for supervisor, learning task where where Fantom with, learning task where where Fantom with, learning task where where Fantom with, label data can further specialize Le, label data can further specialize Le, label data can further specialize Le, models for specific applications the, models for specific applications the, models for specific applications the, chat versions of Milan model uh which, chat versions of Milan model uh which, chat versions of Milan model uh which, has conducted the instruction fing excel, has conducted the instruction fing excel, has conducted the instruction fing excel, in handling new tasks without prior, in handling new tasks without prior, in handling new tasks without prior, specific training so this strong, specific training so this strong, specific training so this strong, instruction following capability make, instruction following capability make, instruction following capability make, them idea for zero shot or few shot, them idea for zero shot or few shot, them idea for zero shot or few shot, settings offering flexibility in, settings offering flexibility in, settings offering flexibility in, scenarios where TX specific data is, scenarios where TX specific data is, scenarios where TX specific data is, quite limited so the choice between the, quite limited so the choice between the, quite limited so the choice between the, 70 billion and the 13 billion models uh, 70 billion and the 13 billion models uh, 70 billion and the 13 billion models uh, largely depends on resource uh, largely depends on resource uh, largely depends on resource uh, limitations so the 70 plan model offers, limitations so the 70 plan model offers, limitations so the 70 plan model offers, more advanced understanding and the, more advanced understanding and the, more advanced understanding and the, complex reasoning making it ideal for, complex reasoning making it ideal for, complex reasoning making it ideal for, some complex medical tasks however it, some complex medical tasks however it, some complex medical tasks however it, demands considerable Computing resources, demands considerable Computing resources, demands considerable Computing resources, which can limit the accessibility so the, which can limit the accessibility so the, which can limit the accessibility so the, 13 BL model by contast can strikes a, 13 BL model by contast can strikes a, 13 BL model by contast can strikes a, balance between Effectiveness and, balance between Effectiveness and, balance between Effectiveness and, efficiency so make it more practicable, efficiency so make it more practicable, efficiency so make it more practicable, choice for settings with limited, choice for settings with limited, choice for settings with limited, computation resources and our findings, computation resources and our findings, computation resources and our findings, actually as Illustrated in the, actually as Illustrated in the, actually as Illustrated in the, experimental result setting uh section, experimental result setting uh section, experimental result setting uh section, the M 13 billion can achieve compartible, the M 13 billion can achieve compartible, the M 13 billion can achieve compartible, performance to the 70 billion model, performance to the 70 billion model, performance to the 70 billion model, across most data sets so making it an, across most data sets so making it an, across most data sets so making it an, excellent option where resource, excellent option where resource, excellent option where resource, constraints are concern, constraints are concern, constraints are concern, and it's and it's very crucial to, and it's and it's very crucial to, and it's and it's very crucial to, acknowledge the limitations of the, acknowledge the limitations of the, acknowledge the limitations of the, current version of milama models so like, current version of milama models so like, current version of milama models so like, other all lar existing large Lang models, other all lar existing large Lang models, other all lar existing large Lang models, mil model can sometimes produce response, mil model can sometimes produce response, mil model can sometimes produce response, that contain factor in accuracy or, that contain factor in accuracy or, that contain factor in accuracy or, biased information so which is, biased information so which is, biased information so which is, particular problem in the medical field, particular problem in the medical field, particular problem in the medical field, so to achieve this uh future iterations, so to achieve this uh future iterations, so to achieve this uh future iterations, we are exploring is to incorporate, we are exploring is to incorporate, we are exploring is to incorporate, techniques like reinforcement learning, techniques like reinforcement learning, techniques like reinforcement learning, from Human feedback and the retrieval, from Human feedback and the retrieval, from Human feedback and the retrieval, augmentation to make the response more, augmentation to make the response more, augmentation to make the response more, reliable and the current M Lama model, reliable and the current M Lama model, reliable and the current M Lama model, actually inherit the limitations from, actually inherit the limitations from, actually inherit the limitations from, its Lama to back bomb it can only adust, its Lama to back bomb it can only adust, its Lama to back bomb it can only adust, the tax up to uh, the tax up to uh, the tax up to uh, 4,096, 4,096, 4,096, token so this restricts uh their ability, token so this restricts uh their ability, token so this restricts uh their ability, to process long medical text or detailed, to process long medical text or detailed, to process long medical text or detailed, patient, patient, patient, records uh in the future I think we we, records uh in the future I think we we, records uh in the future I think we we, think like techniques like SP local, think like techniques like SP local, think like techniques like SP local, attention or extension based on long, attention or extension based on long, attention or extension based on long, Rolla could help the model to address, Rolla could help the model to address, Rolla could help the model to address, the long context issue and also finally, the long context issue and also finally, the long context issue and also finally, given the rapidly development of, given the rapidly development of, given the rapidly development of, open-source Larry models in the general, open-source Larry models in the general, open-source Larry models in the general, domain like Lama 3.1 and Lama 3.2 so we, domain like Lama 3.1 and Lama 3.2 so we, domain like Lama 3.1 and Lama 3.2 so we, actually are committed to ongoing, actually are committed to ongoing, actually are committed to ongoing, development the Advanced Medical lar, development the Advanced Medical lar, development the Advanced Medical lar, language models based on this uh most, language models based on this uh most, language models based on this uh most, advanced back bomb model in the general, advanced back bomb model in the general, advanced back bomb model in the general, domain uh so finally I would like to uh, domain uh so finally I would like to uh, domain uh so finally I would like to uh, express my gratitude to our lab members, express my gratitude to our lab members, express my gratitude to our lab members, and the collaborators from University of, and the collaborators from University of, and the collaborators from University of, Florida who have make big contribution, Florida who have make big contribution, Florida who have make big contribution, to the milama, to the milama, to the milama, project yeah thank you for your, project yeah thank you for your, project yeah thank you for your, attention and I would be very happy to, attention and I would be very happy to, attention and I would be very happy to, answer any questions you have, answer any questions you have, answer any questions you have, here thank you so much chanan uh for, here thank you so much chanan uh for, here thank you so much chanan uh for, this really nice talk and this extensive, this really nice talk and this extensive, this really nice talk and this extensive, work that you have done and uh before we, work that you have done and uh before we, work that you have done and uh before we, proceed on to the questions we would, proceed on to the questions we would, proceed on to the questions we would, like the audience to extend a virtual, like the audience to extend a virtual, like the audience to extend a virtual, Round of Applause for our, speaker okay with that uh like let's uh, speaker okay with that uh like let's uh, open the Flor for some more questions, okay I think we have a question just uh, okay I think we have a question just uh, so Ramon do you want to, so Ramon do you want to, so Ramon do you want to, ask uh yeah I just wanted to maybe make, ask uh yeah I just wanted to maybe make, ask uh yeah I just wanted to maybe make, sure I understood um it was mentioned, sure I understood um it was mentioned, sure I understood um it was mentioned, that for one of the named entity, that for one of the named entity, that for one of the named entity, recognition tasks the model didn't, recognition tasks the model didn't, recognition tasks the model didn't, perform well because of um how the, perform well because of um how the, perform well because of um how the, outputs were being generated that they, outputs were being generated that they, outputs were being generated that they, weren't, weren't, weren't, consistent um is it possible to do some, consistent um is it possible to do some, consistent um is it possible to do some, sort of post-processing to let's say, sort of post-processing to let's say, sort of post-processing to let's say, standardize the outputs better, standardize the outputs better, standardize the outputs better, or yeah this is a very good question, or yeah this is a very good question, or yeah this is a very good question, yeah so oh what I think the, yeah so oh what I think the, yeah so oh what I think the, underperformed performance from two, underperformed performance from two, underperformed performance from two, reason firstly is because the model, reason firstly is because the model, reason firstly is because the model, itself actually cannot follow the, itself actually cannot follow the, itself actually cannot follow the, instruction to generate correct answers, instruction to generate correct answers, instruction to generate correct answers, so I think this is because the task, so I think this is because the task, so I think this is because the task, setting itself is kind of a challenge, setting itself is kind of a challenge, setting itself is kind of a challenge, not like question answering task the, not like question answering task the, not like question answering task the, question answering task we use here is, question answering task we use here is, question answering task we use here is, like we uh input the question and uh let, like we uh input the question and uh let, like we uh input the question and uh let, it make uh some choices between uh a b, it make uh some choices between uh a b, it make uh some choices between uh a b, CDE e different options and it only, CDE e different options and it only, CDE e different options and it only, reply to ABC this selections but for the, reply to ABC this selections but for the, reply to ABC this selections but for the, name ener recognition and like relation, name ener recognition and like relation, name ener recognition and like relation, extraction uh it aims to extract the uh, extraction uh it aims to extract the uh, extraction uh it aims to extract the uh, recognize the entities in the given, recognize the entities in the given, recognize the entities in the given, sentence so, sentence so, sentence so, uh our prompt setting uh in here, uh our prompt setting uh in here, uh our prompt setting uh in here, following previous studies in our lab uh, following previous studies in our lab uh, following previous studies in our lab uh, we actually let the model to extract the, we actually let the model to extract the, we actually let the model to extract the, entities by um label the TX not directly, entities by um label the TX not directly, entities by um label the TX not directly, give the entities as the answer so this, give the entities as the answer so this, give the entities as the answer so this, may uh increasing the difficulties for, may uh increasing the difficulties for, may uh increasing the difficulties for, the model to correctly understanding the, the model to correctly understanding the, the model to correctly understanding the, prompts itself and the let the model, prompts itself and the let the model, prompts itself and the let the model, cannot following the instruction to give, cannot following the instruction to give, cannot following the instruction to give, as required responses even on GPD 4 uh, as required responses even on GPD 4 uh, as required responses even on GPD 4 uh, the sorta commercialized L langage, the sorta commercialized L langage, the sorta commercialized L langage, models they cannot actually uh correct, models they cannot actually uh correct, models they cannot actually uh correct, following the instructions in most cases, following the instructions in most cases, following the instructions in most cases, yeah another issue is that once the, yeah another issue is that once the, yeah another issue is that once the, model cannot follow instruction so it, model cannot follow instruction so it, model cannot follow instruction so it, will be hard for us to correctly pass, will be hard for us to correctly pass, will be hard for us to correctly pass, the uh generated answers to compare with, the uh generated answers to compare with, the uh generated answers to compare with, the gold standard indeed yeah we do some, the gold standard indeed yeah we do some, the gold standard indeed yeah we do some, post processing um based on the, post processing um based on the, post processing um based on the, prediction results of different large, prediction results of different large, prediction results of different large, langage models but actually for, langage models but actually for, langage models but actually for, different models they can generate, different models they can generate, different models they can generate, variable uh variety for mats so we need, variable uh variety for mats so we need, variable uh variety for mats so we need, to uh up C updating the post processing, to uh up C updating the post processing, to uh up C updating the post processing, process to accurately pass, process to accurately pass, process to accurately pass, the predicted results from different, the predicted results from different, the predicted results from different, models to cover most uh, cases so uh using the post processing, cases so uh using the post processing, can indeed yeah improve the performance, can indeed yeah improve the performance, can indeed yeah improve the performance, a little bit but I think the key lies in, a little bit but I think the key lies in, a little bit but I think the key lies in, we need to um develop Advanced a better, we need to um develop Advanced a better, we need to um develop Advanced a better, prompts and improve the, prompts and improve the, prompts and improve the, models ability on instru on instruction, models ability on instru on instruction, models ability on instru on instruction, following I see thank, you any more, you any more, questions okay so chanan I have a, questions okay so chanan I have a, questions okay so chanan I have a, question so I saw that you know you have, question so I saw that you know you have, question so I saw that you know you have, the data sets that you use for training, the data sets that you use for training, the data sets that you use for training, or pre-training your model uh is, or pre-training your model uh is, or pre-training your model uh is, mostly uh I think the PMC I think the, mostly uh I think the PMC I think the, mostly uh I think the PMC I think the, biomedical literature then you have the, biomedical literature then you have the, biomedical literature then you have the, mimic data I guess the clinical notes, mimic data I guess the clinical notes, mimic data I guess the clinical notes, from mimic data and then the red pajama, from mimic data and then the red pajama, from mimic data and then the red pajama, right I guess yeah so uh so clinical, right I guess yeah so uh so clinical, right I guess yeah so uh so clinical, notes I can understand like you know, notes I can understand like you know, notes I can understand like you know, these are all uh, these are all uh, these are all uh, publicly available did you use anything, publicly available did you use anything, publicly available did you use anything, that was Private data any electronic, that was Private data any electronic, that was Private data any electronic, health records that was like private or, health records that was like private or, health records that was like private or, which was not out there in the, which was not out there in the, which was not out there in the, web because sorry uh just one so because, web because sorry uh just one so because, web because sorry uh just one so because, what kind of like all the other models, what kind of like all the other models, what kind of like all the other models, out there they are also trained on these, out there they are also trained on these, out there they are also trained on these, kind of data sets like if you look at, kind of data sets like if you look at, kind of data sets like if you look at, medp or if you look at chat GPD or, medp or if you look at chat GPD or, medp or if you look at chat GPD or, things like that so I was just curious, things like that so I was just curious, things like that so I was just curious, that was there anything that was used in, that was there anything that was used in, that was there anything that was used in, the data training that is not like you, the data training that is not like you, the data training that is not like you, know uh out there and publicly available, know uh out there and publicly available, know uh out there and publicly available, okay yeah that's a good question so uh, okay yeah that's a good question so uh, okay yeah that's a good question so uh, the current milama version didn't use, the current milama version didn't use, the current milama version didn't use, the uh private data because uh you know, the uh private data because uh you know, the uh private data because uh you know, it requires extensive computation resour, it requires extensive computation resour, it requires extensive computation resour, like over 100 gpus uh conducting Pro, like over 100 gpus uh conducting Pro, like over 100 gpus uh conducting Pro, chaining over one month so uh like at in, chaining over one month so uh like at in, chaining over one month so uh like at in, y we doesn't have the enough computation, y we doesn't have the enough computation, y we doesn't have the enough computation, resources to support uh the prain on, resources to support uh the prain on, resources to support uh the prain on, such a big model so we collaborate with, such a big model so we collaborate with, such a big model so we collaborate with, our collaborators from University of, our collaborators from University of, our collaborators from University of, Florida in front lay side they have the, Florida in front lay side they have the, Florida in front lay side they have the, uh superc computer AI uh supercomputer, uh superc computer AI uh supercomputer, uh superc computer AI uh supercomputer, clusters uh over thousand gpus there but, clusters uh over thousand gpus there but, clusters uh over thousand gpus there but, um if for example we would like to use y, um if for example we would like to use y, um if for example we would like to use y, EHR for the continue P training in, EHR for the continue P training in, EHR for the continue P training in, Florida site is uh, Florida site is uh, Florida site is uh, actually impossible that we can transfer, actually impossible that we can transfer, actually impossible that we can transfer, the large scale private EHR data so we, the large scale private EHR data so we, the large scale private EHR data so we, can only uh use uh the data from our, can only uh use uh the data from our, can only uh use uh the data from our, coll collaborators like um University of, coll collaborators like um University of, coll collaborators like um University of, Florida's notes to uh conducting the, Florida's notes to uh conducting the, Florida's notes to uh conducting the, conduct the Contin pre training uh in, conduct the Contin pre training uh in, conduct the Contin pre training uh in, the University of fridas server so in, the University of fridas server so in, the University of fridas server so in, our uh recent work yeah we actually have, our uh recent work yeah we actually have, our uh recent work yeah we actually have, based on the Lama, based on the Lama, based on the Lama, 3.1 uh base model to conduct the further, 3.1 uh base model to conduct the further, 3.1 uh base model to conduct the further, uh Contin training with this open source, uh Contin training with this open source, uh Contin training with this open source, data along with the private data from, data along with the private data from, data along with the private data from, the for side okay because yeah I think, the for side okay because yeah I think, the for side okay because yeah I think, it would be very c as of now if I look, it would be very c as of now if I look, it would be very c as of now if I look, at your mama it's kind of a model a, at your mama it's kind of a model a, at your mama it's kind of a model a, medical model which uh like which is, medical model which uh like which is, medical model which uh like which is, kind of similar if I take CH GPT and, kind of similar if I take CH GPT and, kind of similar if I take CH GPT and, just train it on this data like fine, just train it on this data like fine, just train it on this data like fine, tune it once more on this data and run, tune it once more on this data and run, tune it once more on this data and run, it I guess yes it can kind of work well, it I guess yes it can kind of work well, it I guess yes it can kind of work well, but I guess if you take M Lama and then, but I guess if you take M Lama and then, but I guess if you take M Lama and then, uh train it on some specific domain, uh train it on some specific domain, uh train it on some specific domain, specific data and then run that's where, specific data and then run that's where, specific data and then run that's where, I think the main idea like the main uh, I think the main idea like the main uh, I think the main idea like the main uh, like it the performance uh how do I say, like it the performance uh how do I say, like it the performance uh how do I say, the performance Gaines comes I guess, the performance Gaines comes I guess, the performance Gaines comes I guess, like it will probably perform better, like it will probably perform better, like it will probably perform better, than Char that was what I was like, than Char that was what I was like, than Char that was what I was like, curious about that's the reason I asked, curious about that's the reason I asked, curious about that's the reason I asked, like if you had any private data or a, like if you had any private data or a, like if you had any private data or a, smaller data set that you had that you, smaller data set that you had that you, smaller data set that you had that you, used for your uh evaluation or anything, used for your uh evaluation or anything, used for your uh evaluation or anything, of that sort instead of yeah I think, of that sort instead of yeah I think, of that sort instead of yeah I think, using private data is actually important, using private data is actually important, using private data is actually important, because uh in the latest version of the, because uh in the latest version of the, because uh in the latest version of the, Llama 3 point uh llama 3.1 we quickly, Llama 3 point uh llama 3.1 we quickly, Llama 3 point uh llama 3.1 we quickly, test its zero sh performance on some uh, test its zero sh performance on some uh, test its zero sh performance on some uh, QA tasks as we use in K 3 QA data sets, QA tasks as we use in K 3 QA data sets, QA tasks as we use in K 3 QA data sets, we found that it performance is quite, we found that it performance is quite, we found that it performance is quite, High the zero sh performance is even, High the zero sh performance is even, High the zero sh performance is even, close to the supervis Fant performance, close to the supervis Fant performance, close to the supervis Fant performance, and we guess actually in layer, and we guess actually in layer, and we guess actually in layer, already used the training data yeah, already used the training data yeah, already used the training data yeah, exactly, exactly, exactly, yeah any more, questions I question about the red PMA, questions I question about the red PMA, from the this SL so if I I just wanted, from the this SL so if I I just wanted, from the this SL so if I I just wanted, to make sure if found correctly the the, to make sure if found correctly the the, to make sure if found correctly the the, purpose that we uh use the data set for, purpose that we uh use the data set for, purpose that we uh use the data set for, red PMA is because it's it can be it, red PMA is because it's it can be it, red PMA is because it's it can be it, could it could be if the the me Rama can, could it could be if the the me Rama can, could it could be if the the me Rama can, forget the general knowledge while it is, forget the general knowledge while it is, forget the general knowledge while it is, fine-tuning oh yeah yeah so it can uh, fine-tuning oh yeah yeah so it can uh, fine-tuning oh yeah yeah so it can uh, quickly forgot is uh General domain, quickly forgot is uh General domain, quickly forgot is uh General domain, knowledge uh during the continue paining, knowledge uh during the continue paining, knowledge uh during the continue paining, especially when we use a very large, especially when we use a very large, especially when we use a very large, learning rate and the red pajama uh data, learning rate and the red pajama uh data, learning rate and the red pajama uh data, actually is kind of a replicat of the uh, actually is kind of a replicat of the uh, actually is kind of a replicat of the uh, paining data for the Lama 2 so most uh, paining data for the Lama 2 so most uh, paining data for the Lama 2 so most uh, following previous studies we choose, following previous studies we choose, following previous studies we choose, this one, this one, this one, and try to sample a small subset from it, and try to sample a small subset from it, and try to sample a small subset from it, to uh let the model to have a smoothly, to uh let the model to have a smoothly, to uh let the model to have a smoothly, training, training, training, process to like uh adapting to the, process to like uh adapting to the, process to like uh adapting to the, medical domain where people uh do not, medical domain where people uh do not, medical domain where people uh do not, forgot the uh General domain knowledge, forgot the uh General domain knowledge, forgot the uh General domain knowledge, when they learning during the, okay if there are no more questions uh I, okay if there are no more questions uh I, would again uh like to thank our speaker, would again uh like to thank our speaker, would again uh like to thank our speaker, chanchan it was for such a nice talk and, chanchan it was for such a nice talk and, chanchan it was for such a nice talk and, uh please I would like the audience to, uh please I would like the audience to, uh please I would like the audience to, extend another round of virtual, Applause okay with that thank you have a, Applause okay with that thank you have a, nice day we can end the session thank, nice day we can end the session thank, nice day we can end the session thank, you
