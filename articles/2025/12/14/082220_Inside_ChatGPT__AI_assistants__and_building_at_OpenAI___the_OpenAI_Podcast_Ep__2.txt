Timestamp: 2025-12-14T08:22:20.160974
Title: Inside ChatGPT, AI assistants, and building at OpenAI — the OpenAI Podcast Ep. 2
URL: https://youtube.com/watch?v=atXyXP3yYZ4&si=GjkwGZYfLZ8ohBdn
Status: success
Duration: 1:07:18

Description:
好的，这是根据提供的文本内容提炼和总结的核心思想。

### **核心思想纲要**

1.  **ChatGPT的诞生：从低调实验到全球现象**
    *   **命名与发布**：产品名称“ChatGPT”是在发布前一晚才从更复杂的“Chat with GPT-3.5”简化而来，最初定位为“低调的研究预览”。
    *   **意外的成功**：团队对其病毒式传播始料未及，发布初期甚至内部也对其是否足够成熟存有疑虑，这次成功印证了“与现实接触”的重要性。

2.  **OpenAI的核心产品哲学：迭代、反馈与透明**
    *   **开发模式转变**：从类似硬件开发（发布周期长、力求完美）转向软件开发模式（快速迭代、频繁更新），以尽快获得真实世界的反馈。
    *   **用户反馈的价值**：用户反馈是模型改进（如通过RLHF解决“谄媚”问题）和安全保障的核心驱动力，是与“现实”保持联系的关键。
    *   **应对偏见与争议**：通过发布模型规范等方式提高透明度，力求模型默认行为中立、不强加价值观，同时允许用户在一定范围内定制。

3.  **AI能力的演进与未来交互范式**
    *   **从即时响应到“代理智能”(Agentic AI)**：AI正从同步问答演变为异步、能独立思考和执行复杂任务的“代理”。用户愿意为更高质量的结果等待更长时间（如Deep Research功能）。
    *   **多模态的突破**：
        *   **图像生成 (Image Gen)**：凭借“一次成功”的高质量和精准的指令遵循能力，带来了超出娱乐范畴的巨大实用价值（如图表制作、家居设计）。
        *   **代码生成 (Codex)**：致力于解决从代码补全到生成完整项目等复杂任务，并认识到教会AI软件工程中的“品味”和“风格”是更深层次的挑战。
    *   **个性化与记忆**：记忆功能是未来的关键，AI将成为更懂用户的“超级助理”，这也带来了对隐私保护的新要求和相应的功能设计（如临时聊天）。

4.  **OpenAI的文化与未来人才观**
    *   **核心文化**：高度的“主人翁精神”(Agency)、强烈的好奇心和快速的适应性是团队能够快速创新的基石，鼓励员工自驱解决问题。
    *   **未来所需技能**：对个人而言，最重要的不是“提示工程”等具体技巧，而是要积极使用AI技术，并培养诸如好奇心、委派任务能力和终身学习等核心人类素-养。

5.  **AI对社会的影响与展望**
    *   **能力民主化**：AI将极大地降低专业技能门槛，使更多非专业人士能在医疗、艺术、编程等领域获得高水平的辅助和能力。
    *   **加速科学发现**：AI强大的推理能力将成为科学研究的“子程序”，有望极大加速物理、数学等基础科学的进程。

---

### **核心结论**

AI的发展核心正从单纯追求模型能力，转向通过与现实世界的持续互动和反馈，构建能够自主解决复杂问题的迭代式、代理式智能伙伴。

---

### **内容的总览框架**

该内容围绕一个核心框架展开：即通过“迭代部署”（Iterative Deployment）将AI技术快速推向真实世界，收集大规模用户反馈，再利用这些反馈来驱动**技术**（模型能力、安全性）和**产品**（交互范式、功能）的共同进化，形成一个加速的正向循环。

---

### **核心概念关系图 (Mermaid)**

<Mermaid_Diagram>
graph TD
    subgraph "核心哲学与驱动力"
        A["AI技术基础 (GPT模型)"]
        B["迭代部署与现实反馈循环"]
    end

    subgraph "产品与交互演进"
        C["ChatGPT (对话范式)"]
        D["多模态扩展 (Image Gen, Voice)"]
        E["代理智能 (Agentic AI - Codex, Deep Research)"]
    end

    subgraph "模型能力进化"
        F["性能提升 (RLHF)"]
        G["安全性与对齐 (处理偏见)"]
        H["个性化与记忆"]
    end

    subgraph "社会影响与未来"
        I["能力民主化 (医疗, 艺术)"]
        J["科学发现加速"]
        K["未来技能重塑 (好奇心, 主人翁精神)"]
    end

    A -- "驱动" --> B
    B -- "催生" --> C
    C -- "演进为" --> D
    D -- "进一步演进为" --> E

    B -- "优化" --> F
    B -- "优化" --> G
    B -- "优化" --> H

    F & G & H -- "共同作用于" --> B

    E -- "赋能" --> I
    E -- "赋能" --> J
    I & J -- "要求" --> K

    style A fill:#BDE0FE,stroke:#333,stroke-width:2px
    style B fill:#FFFACD,stroke:#FFA500,stroke-width:3px
    style C fill:#D7E3FC,stroke:#333,stroke-width:1px
    style D fill:#D7E3FC,stroke:#333,stroke-width:1px
    style E fill:#A2D2FF,stroke:#00008B,stroke-width:2px
    style F fill:#C1E1C1,stroke:#333,stroke-width:1px
    style G fill:#C1E1C1,stroke:#333,stroke-width:1px
    style H fill:#C1E1C1,stroke:#333,stroke-width:1px
    style I fill:#90EE90,stroke:#333,stroke-width:1px
    style J fill:#90EE90,stroke:#333,stroke-width:1px
    style K fill:#FFC0CB,stroke:#333,stroke-width:1px
</Mermaid_Diagram>

Content:
Hello, I'm Andrew Maine and this is the opening eye podcast. My guests today are Mark Chen, who is the Chief Research Officer at Opening Eye and Nick Turley, who is the head of chat GPT. We're going to be talking about the early viral days of chat GPT. We're going to talk about image gin, how opening eye looks at code and tools like Codex, what kind of skills they think that we might need for the future and we're going to find out how chat GPT got its totally normal name. Even half of research doesn't know what those three letters stand for. You're going to have an intelligence in your pocket that it can be your tutor, it can be your advisor, your software engineer. There's a real decision the night before, can we actually launch this thing? First off, how did Open Eye decide on that awesome name? I was going to be a chat with GPT 3.5 and we had a late night decision to simplify. Wait, wait, wait, wait. Say that again again. I was going to be chat with GPT 3.5. which rolls off the tongue even more nicely. That's, and you said that was a late night decision, meaning like weeks before you finally decided what to call it, right? Right, right, right. No, weeks before we hadn't started on the project. Oh goodness. But yeah, I think we realized that that would be hard to pronounce and came up with a great name instead. So that was the night before. Roughly, not even the day before. It was all kind of a blur at that point. I would imagine a lot of that was a blur. And I remember here, I remember being in a meeting when we talked about the low-key research preview, which like really was like we really thought like, oh, this is, because it's, it was 3.5, 3.5 was a model. It had been out for months. And from a capabilities point of view, when you just look at the e-vow, you're like, yeah, it's the same thing, but we just put the interface in here and made it so you didn't have to prompt as much. And then chat JPT comes out. And when was the first sign that this thing was blowing up? I'm curious for everyone, I was there a slightly on recollection of that, that era because it was a very confusing time. But for me, Day 1 was sort of, is the dashboard broken classic, like the logging camp, you're right. Day 2 was like, oh, weird. I guess like Japanese Reddit users discovered this thing. Maybe it's like a local phenomenon. Day 3 was like, okay, it's going viral, but it's definitely going to die off. And then by day 4, you're like, okay, it's gonna, gonna change the world. Mark, did you have any expectation about that? No, honestly, I mean, we've had so many launches, so many previews over time. And yeah, this one really was something else, the take off ramp was huge. And yeah, my parents just stopped asking me to go work through Google. Wait, so wait, wait a second. Up until chat JPT, your parents were asking like what you're doing here? Yeah, no, I mean, they just never heard of OpenAI. I think for many years, thought AGI was this pie in the sky thing and I wasn't having a serious job. So it was a real revelation for them. Yeah. What was your job title at the time? I think just member of technical staff. Member technical staff. And then that'll blow up and now you're ahead of research. I guess so, yeah. So all right. Yeah, actually on the JPT name, I think even half of research doesn't know what those three letters stand for. It's kind of funny. You know, like half of them think it's generative pre-training, half of them think it's generative pre-trained transformer. And what is it? It's the letter. Okay. All right. Yeah, those people, they don't know the name of it. Yeah. It is, it's weird how just a silly name like that, all of a sudden becomes a thing. Would you see that with like, you know, Google, Yahoo, Kleenex, things like that, Xerox? And sometimes they were, some of those were names by intention and this was really just a silly sort of name. For me, the moment that I felt like after watching the launch, watching it accelerate, I knew what was going to happen. And then what it did was when it was on South Park. And remember that when South Park made fun of the name and... That was the first time I'd watched South Park. And let's just say a while. And that episode, I still think it's magic. Yeah. And it was obviously profound to watch and see, you know, something you help make show up in pop culture. But there's the punch line in the end where it's like, oh, this was co-written by Chad GPT. I think they took that off though. I think when they did. I think in later episodes, because they used to say, I think written by like, Trey Parker and Chad GPT. And then no, it was. And then I think later, I think they may have pulled that off at some point. I don't remember like... Well, I strongly feel that you shouldn't have to give credit to it. Yeah, that wasn't... That's what I was not using. I had to give credit to Chad GPT for every aspect of my life. Well, might as well say Chad GPT maybe with Andrew. So it's easy for prep for your interviews. You know, one of my co-producers, Justin, probably uses it. I haven't asked him yet, because I'd like to think that he's handcrafting every single question that we're thinking about here. But I am sure. You say it was a bit of a blur. I'll tell you like a standout moment for me, the launch of Chad GPT was... I don't know if you remember this, but the Christmas party. And we'd had several weeks of Chad GPT out there. And Sam Altman went up and said, hey, it's been exciting to watch this, but the internet being the internet. And I think we all felt this way, it's gonna die down. Spoiler alert. It did not die down. And it just kept accelerating. What were the things you had to do internally to sort of keep this thing up and running? It's more people wanted to use it. We had quite a few constraints. And for those of you who remember, you know, I think you guys remember, Chad GPT was down all the time in the beginning. And that was, yeah. We'd said, hey, there's a research preview, no guarantees, you know, maybe it goes down. But the minute you had people loving and using this thing, that didn't feel super good. So, you know, people were certainly working around the clock to keep the site up. I remember, you know, we obviously ran out of GPUs. We ran out of database connections. We had, you know, we're getting rate limited in some of our providers. Nothing was really set up to run a product. So in the beginning, we just built this thing. We called it the fail well. And it would just tell you kind of nicely that the thing was down and made a little poem, I think was generated by GPT-3, about being down and sort of tongue in cheek. And that got us through the winter break because we did want people to have some sort of a holiday. And then when we came back, we were like, okay, this is clearly not valuable. You can't just go down all the time. And eventually we got to something that we could serve everyone. Yeah. And I think, you know, the demand really speaks to the generality of Tatch GPT, right? We had the ceases that Tatch GPT embodied what we wanted in AGI just because it was so general. And I think, you know, you're seeing that demand ramp just because people are realizing, you know, any use case that I want to give or to throw to the model it can handle. We were kind of known as the company working on AGI. And I think prior to Tatch GPT, the API was certainly the first time we had a public offering where people could go use it and do it. But then it was more for developers and stuff. And I think that as long as people were sort of thinking AGI, that seemed to be the point at which people thought these models would be useful. But we saw GPT-3, we saw that that was useful. And then we saw that we can do other things re-useful. Was everybody at OpenAI on board with Tatch GPT being useful or being ready to launch? Yeah, I don't think so. You know, even the night before, I mean, there's this very famous story at OpenAI of, you know, Iliya taking 10 cracks at the model, you know, 10 tough questions. And my recollection is maybe only on five of them, he got answers that he thought were acceptable. And so there's a real decision the night before, do we actually launch this thing? Is the world actually gonna respond to this? And I think it just speaks to when you build these models in-house, you so rapidly adapt to the capabilities. And it's hard for you to kind of put yourself in the shoes of someone who hasn't kind of been in this model training loop and see that there is real magic there. Yeah. Yeah, I think to build on that, like the Condoracy internally about, you know, is this thing good enough to launch? I think it was humbling, right? Because it's just a reminder of how wrong we all are when it comes to AI, why frequent contact with reality is so important. Could you elaborate more on that contact with reality? What does that mean? Yeah, I mean, when you think about iterative deployment, one way I like to frame it is, you know, there's no point everyone agrees where it's suddenly useful, right? And I think usefulness is this big spectrum. And so, you know, there's not one capability level or one bar that you meet and suddenly, you know, the model is useful for everyone. Were there any hard decisions about what to include or what to focus on? We were very, very principled on Chatsubitee to not balloon the scope. We were adamant to get feedback and data as quickly as we could. I'm always in slack telling you things, by the way, that didn't make it as bad as this. I remember actually there was a lot of controversy about like the UI side, for example, we didn't launch with history and then we thought we would probably want that. And, you know, guess what? That was the first request. I also think there's always the question, like, can we train an even better model? Like, you know, with two weeks more time, I'm glad we didn't because, you know, we, I think got a ton of feedback as we did. So, yeah, there was a ton of the scope discussions and, you know, the holidays were coming up. So I think we had this kind of natural forcing function for getting something out. Yeah, there's this habit of things that, if it's going to come after a certain point November, it's not going to come out like February. Yeah, there's a sort of window where things would fall on either side. Well, that would be the classic method in big tech. I think we're definitely a bit more flexible in that we should. I felt like one of the big impacts was, once people are out using it, it felt like the rate of these things improving was tremendous. I don't know if that was something that we really had in a calculus. We could certainly think about training on the larger site more data, scaling compute, but then the idea of actually having them, the signal you would get from that many people using it. Yeah, I think over time, you know, feedback really has become an integral part of how we build the product. And it's also become an integral part of safety. And so you always feel the time costs of losing out on feedback. You know, you can deliberate in a vacuum, right? Are they going to respond to this better? Are they going to respond to that better? But it's just not a substitute for just bringing it up there, right? I think our philosophy is, let the models have contact with the world. And if you need to revert something, that's fine. But I think there's really no substitute for this fast feedback. And it's become one of the big levers for how we improve model performance too. It's sort of funny. I feel like we started with shipping these models in a way that is more similar to hardware where you make like one launch very rarely and it has to be right. And you know, you're not going to update the thing and then you're going to work on the next big project. It's capital intensive and the timelines are long. And over time, and I think chat to be tea was kind of the beginning. It's looked more like software to me where you make frequent updates. And you have a kind of a constant pace that will can adopt something doesn't work. You pull it back and you sort of lower the stakes in doing that and you increase the empiricism. And of course, just operationally too, you can innovate faster in a way that is more and more in touch with what users want. Yeah, one of the examples we had of that was the model becoming too obsequious or sycophantic. Could you explain what happened there where that was where people all of a sudden say, hey, it's telling me I've got a 190 IQ and the most handsome person in the world, which I had no problem with personally, but other people did. And what was going on there? Yeah, so I think one important thing is we rely on user feedback to improve the models, right? And it's this very complicated mix of reward models, which we use in a procedure we call RLE chat, right? Using human feedback to use RLE to improve the models. Did you give me just like a brief example what that would mean? Yeah, yeah. So I think one way to think about it is when a user enjoys a conversation, they provide some positive signal. Yeah, a thumbs up, for instance. And we train the model to prefer to respond in a way that would elicit more thumbs up, right? And this may be obvious in retrospect, but stuff like that, if balanced incorrectly, can lead to the model being more sycophantic, right? You can imagine users might want that kind of, that feeling of a model saying good things about them, but I don't think it's a very good long-term outcome. And actually, when we look at kind of our response to sycophantic and the rollout that resulted there, I think there were a lot of good points about it. This was something that was flagged just by a small fraction of our power users. It wasn't something that a lot of people who generally use the models notice. And I think we really picked that out fairly early. We responded to it, I think, with the appropriate level of gravity. And yeah, I think it just shows that we really do take these issues quite seriously, and we want to intercept them very early. Yeah, it felt like there was maybe 48 hours since the model came out, and then Joanne Zhang had a response explaining exactly what happened. And I think that that's the hard part. How do you navigate that? Because the problem with social media is you were basically monetized by engagement time. You want to keep people on there longer so you can show them more ads. And certainly, the more people use chatup-ed, obviously there's a cost to open-air ideas, maybe use it once and stay around forever, but that's not practical. How do you weigh that? The idea of making people happy with what they're getting versus making the model be broadly more useful than just pleasing. I feel very lucky in this regard, because we have a product that's very utilitarian. People use it to either achieve things that they do know how to do, but don't feel like doing faster or with less effort. Or they're using it to do things that they couldn't do at all. First example is maybe writing an email that you've been dreading. Second example might be running a data analysis that you didn't actually know how to do in Excel. True story. So those are very utilitarian things. Fundamentally, as you improve, you actually spend less time on the product, right? Because ideally it takes less turns back and forth, or maybe you actually delegate to the eyes so you're not in the product at all. So for us, time spent, it's very much not the thing we optimized for. We do care about your long-term retention, because we do think that's a sign of value. If you're coming back three months later, that's what it means we did something right. But what that means is, I always show me the incentive and I'll share the outcome. We have, I think, the right fundamental incentives to build something great. That doesn't mean we'll always get it right. The sycophancy events were really, really important and good learning for us, and I'm proud of how we acted on it. But fundamentally, I think we have the right setup to build something awesome. So that brings up the challenge. I wonder how you'd navigate that is that one of the things early on when Chatsubita came out, there was like the allegations, it's woke. And people are trying to promote some sort of agenda from it. My argument always been like, you train a model on kind of on corporate speak, average news and a lot of academia, that's gonna kind of follow into that. And I remember Elon Musk was very critical about it. And then when he trained the first version of Groc, it did the same thing. And then he's like, oh yeah, when you trained it on this sort of thing and did that. And internally at OpenEye, there were discussions about how do we make the model not try to push you, not try to steer you? Could you go a little bit how you try to make that work? Yeah, so I think at its core, it's a measurement problem. And I think it's actually bad to downplay these kinds of concerns because they are very important. And we need to make sure that the model, the default behavior that you get is something that's centered that doesn't reflect bias on the political spectrum or in many other axes of bias. And at the same time, you do want to allow the user the capability to, if you wanted to talk to a reflection of something with more conservative values to be able to steer that a little bit, right? Or liberal values, right? And so I think the thing is you wanna make sure that defaults are meaningful and they're centered and that's a measurement problem. And you also want to give some flexibility, right? Within bounds to steer the model to be a persona that you wanted to talk to. I think that's right. I think in addition to neutral defaults, the ability to bring your own values to some extent, I think being transparent about the whole thing is I think really, really important. I'm not a fan of secret system messages that try to hack the model into saying or not saying something. What we've tried to do is publish our spec. So you can go look at, if you're getting certain model behavior, is that a bug? Is it violation of our own synod spec? Or is it actually in the spec, in which case, who to criticize and who to yell at? Or is it just under-specified in the spec in which case that allows us to improve it and add more specificity into that document? So by sort of publishing the rules of the AI that it's supposed to be following, I think that's an important step to have more people contribute to the conversations than just the people inside of a banana. So we're talking about like the system prompt, the part of the instruction that the model gets before the user puts the input. Well, I think it's one of that. Yeah. This is the problem is one way to steer it in the model, but it goes much deeper into that, right? Yeah, we have a very large document that outlines across a bunch of different behavior in categories how we expect the model to behave. And just to give you an example here, right? You can imagine if there's someone who comes in with just like an incorrect belief, just a factually incorrect kind of a point of view, how should the model interact with that user, right? And should it reject that point of view outright? Or should it collaborate with the user on kind of figuring out what's true together? And we take that latter point of view and I think there are a lot of very subtle decisions like this which we put a lot of time in. Yeah, that's a hard one because I think some things that you can test for and you can try to figure out how an entire culture is gonna adopt something that's challenging. Like if I was some of those convinced that the world was flat, like how much should the model push back against me? And somebody would be like, oh, I should push it back all the way, but it's okay. What if you're one religion or not another and... Yeah, it turns out rational people and well many people can disagree on how the model should behave in these instances. And you're not always gonna get it right, but you can be transparent about what approach we took. You can allow users to customize it. And I think this is our approach. I'm sure there's ways we can improve on it, but I think that being transparent in the open about how we're trying to tackle it, we can get through that. How are you thinking about, as people start to use these models more and more, regardless of whether or not that's some dial you're trying to turn, it's just the more useful it becomes, the more people wanna use it. There was a time when nobody wanted a cell phone and now we can't get away from them. And how are you thinking about relationships people are forming with their systems? Obviously, I mentioned this earlier, this is a technology you have to study. Designed in a static way to do XYZ. It's highly empirical. So as people adopt and the way that they use the product, something that we need to go understand and act on as well, I've been observing this trend with interest where I think increasing number of people, especially Gen Z and younger populations are coming to chat to be a thought partner. And I think in many cases, that's really helpful and beneficial because you've got someone to brainstorm on a relationship question. You've got someone to brainstorm on a professional question or something else, but in some cases it can be harmful as well. And I think detecting the scenarios and first and foremost, having the right model behavior is very, very important to us. So actively monitoring and in some ways, it's one of the problems we're gonna have to grapple with because with any technology that becomes ubiquitous, it's gonna be dual use. People are gonna use it for all this awesome stuff and people are gonna use it in ways that we wish they didn't. And we have some responsibility to make sure that we handle that with the appropriate gravity. I find myself having longer conversations with it. I like the memory function. I like the fact you can turn it off if you don't want. And I think about like, what's this gonna be two years from now or three years from now when it has a much longer memory, much more context with this? I like the idea to have these sort of like, memento anonymous modes too, or it's not gonna store this, but I kind of wonder how much you've been thinking about two years, three years down the road. What's that going to be like when chat GPD knows way more about you? Yeah, I mean, I think memory is just such a powerful feature. In fact, it's one of the most requested features when we talk to people externally. It's like, this is the thing I really wanna pay more for. And I think you like in it to, if you've ever kind of had a personal assistant, you know, you, No, I'm not. Well, you do need to build up a relatable work. I'm sorry guys, I'm sorry guys. But you know, it's, yeah, it's just like, it's kind of in any kind of relationship that you have with a person, right? You build up context with them over time. And I think just the more they know about you, right? The richer the relationship, the more, you know, it can also help you, right? You can work together to collaborate on tasks together. I do become self-conscious of the fact that like it knows everything about me when I'm grumpy. And I've, I've argued with it recently, by the way. That's good. Yeah. You should be able to argue with it. Yeah. You understand a lot about yourself and having a thing to argue with. And I think you spare others of that experience, which can also be beneficial. Just don't argue on math and science. You're not gonna win the, yeah, no, I think that's. Increasingly very unlikely. Yeah. Yeah, I think memory's cool. In two marks point, it's been part of our vision for a long time because, you know, we said we were gonna build a super assistant before we really knew what that meant. Chad JBT was sort of the early demonstration to that idea. But if you kind of think about, you know, real world intelligences, you know, even they are not particularly useful on their first day. And I think being able to solve that problem or begin to solve that problem has been profound. So your earlier question though, you know, it really does feel like, you know, if you fast forward a year or two, Chad JBT or things like it are gonna be your most valuable account by far. It's gonna know so much about you. And that's why I think given people ways to talk with this thing, you know, in private is very important. We make, you know, this like temp chat thing very, it's like literally on the home screen because we think it's, you know, increasingly important to let you talk about stuff sort of off the record too. So it's an interesting question. And I think privacy and AI is gonna be an interesting one for the next couple of years. I wanna switch gears, talk about another release, which again, kind of caught people by surprise and blow up was image gen. And I was here for Dolly, Dolly two. And then Dolly three came out and I thought, Dolly three, I thought was a very capable model, but it seemed like it preferred a certain kind of image and a lot of the utility and the capabilities for variable binding was sort of kind of hidden away. And then image gen was kind of just this breakthrough moment that it caught me off guard. How did you guys feel about the launch of that? Yeah, honestly, it caught me off guard too. And this really props to the research team, you know, gave in particular to a ton of work here, Kenji, many others on the team. So amazing. Did phenomenal work. And I think it really spoke to this thesis that when you get a model, just good enough that in one shot, it can generate an image that fits your prompt. That's going to create immense value. And I think we never quite had that before, right? That you just get the perfect generation, oftentimes on the first try. And I think that's something very powerful, you know, like people don't want to pick the best out of a grid. I think, yeah, you just got very good prompt following and you know, this great style transfer too, right? This ability to kind of put images as context for the models of modify and to change and the fidelity that you could do that with. I think that was really powerful for people. I think this image and experience, it was just kind of another mini-chop-2-BT moment all over again, where you know, you have kind of this, you've been staring at this for a while, you're like, yeah, it's going to be cool. I think people really like it, but you're kind of, you know, you're launching like 20 different things. And then suddenly the world is going crazy in a way that you kind of only find out by shipping. Like I remember distinctly, you know, we had like 5% of the Indian internet population try. Image Gen over the weekend. And I was like, wow, we're reaching new types of users who we wouldn't even have thought, you know, who might not have thought of using chat-2-BT. That's really cool. And to Mark's point, I think a lot of this is because there's this discontinuity where something suddenly works so well and truly the way you expected, where I think it blows people's minds, you know, and I think we're going to have those moments and other modalities to, you know, I think voice, you know, it hasn't quite passed the Turing test yet, but I think the minute it does, people are going to, I think find that immensely powerful and valuable, you know, the video is going to have its own moment where it starts meeting the expectations that users have. So I'm really excited about the future because I think there's so many of these magical moments coming that are really going to transform people's lives. And also you change sort of chat-2-BT's relevance for people because, you know, there's, I've always felt like there's text people and there's image people and like some of them are a little bit different. And now they're all using the product and discovering the value across the board. The moment when it launched, I think it kind of illustrated the problem that had been with image models before. And, you know, when Dolly came out, it was super exciting because you're like, I'm like doing pictures of space monkeys and all these sorts of things. The moment you try to do a really complex image, and that's the phrase I brought up before, which is variable binding, you start to see these things drop off. And that was when I realized, oh, there's going to be a challenge for other image systems that don't have kind of the scale and the compute of like a GPT-4 under the hood. And now was it just, was it basically that like taking like a GPT-4 scale model and say now you do images that they'd break through? Well, I think there are a lot of different parts of research that made this sister a big success, right? I think with a complicated multi-step pipeline, it's never just one thing, right? It's like very good post-training. It's very good training. And I think it's just all of that coming together, right? Variable binding definitely was one thing that we paid a lot of attention to. I think one thing about the image on launch is a launch that was very deep. I think people, they started by working on, creating anime versions themselves, but you realize when you play with it more, the infographics, they work with it. Oh yeah. You actually create charts, you create. Comic book panels. Yeah. You can mock up what your home would look like. Exactly. We could have richer in it. Different furniture. I've heard all these things from users that are completely surprising about the way that you see it. We did the podcast setup by literally taking some photos of chairs in the room and just put it in there and saying create a better setup. And it was amazing. So we've seen kind of a lot of the, again, there was a lot of the anime style images, which kind of like for some, it was just sort of the weird thing where it was just better than what we'd seen before. And I don't think anybody is ready to be really surprised by an image model in that way. I think obviously internally and externally, what were some of the things that surprised you or some of the new things you saw people doing? Yeah, I'll tell you a quick story there too, because up until the day of launch, we're trying to figure out what's the right use case to showcase, you know, like, and I think I'm so glad we ended up on kind of anime styling. It's just everyone looks good as an animated character. Yeah. That's true. I mean, it's funny, with original chat to BT, I thought it would be strictly utilitarian product and then a surprise of people use it for fun. In this case, it was sort of the opposite, whereas like, okay, this is gonna be really cool for me. And it was people are gonna like have fun with this thing. But then I was like really surprised by all the genuinely useful ways of using image gen, whether or not it's planning your home project, as I mentioned earlier, you know, of doing a construction, you wanna see what things would look like if you know, you had this remodel or this furniture or whatever, to, you're working on a slide deck for this important presentation. And you just wanna have really useful, consistent illustrations that are on topic and get it. So I really have been kind of personally surprised by the utility in this case, because I knew it would be fun. That was not a question. Yeah, I think I, you said to generate a tier list of AI companies and it opened the eye at the top. Yeah. You win model. What? Good post training. Yeah, yeah, it just happened, you know, who knew? What has been the thinking in, it's changed, because I remember originally with Dolly, the idea of like, okay, we have to be a lot of, very controlled about what it can do, what it can't do. Originally, remember we first launched, you couldn't do people, which was not a very useful model. And then finally was trying to roll back. How much of that was cultural shift? How much that was the technological ability to control for things? And how much of that was just saying we've gotta push the norms? I would say it was both cultural shift and improvement in our ability to control things. The culture shift, you know, I'm not gonna deny it. I think when I joined OpenAI, there was a lot of conservatism around, you know, what capabilities we should give to users. Maybe for good reason. The technology was really new. A lot of us were new to working on it. And, you know, if you're gonna have a bias, you know, biasing towards safety and being careful. It's not a bad, you know, in DNA to have. But I think over time we learned that there's so many positive use cases that you effectively prevent when you make arbitrary restrictions in the model. What about faces? Why not? Why can't I make any face I want? So this is a good example of a, you know, capability that's got pros and cons and you can err on one side or the other. But, you know, when we first shipped image uploads into chat GPT, we had some debates, you know, about what capabilities do you allow versus where are you conservative? And I think one debate that we had is like, do we upload, allow the upload of images with faces? Or rather, when you upload an image that contains a face, do you, you know, should we just like gray out the face? Because you avoid so many problems, right? You can make inferences about people based on their face. You could say mean things to people based on their face. And, you know, you would just take a giant shortcut on all the nearly issues if you didn't allow that. But I've always felt we need to err on the side of freedom and we need to do the hard work. And I think in this case, you know, there's so many valid ways, you know, if I want feedback on makeup or on my haircut or anything like that, I want to be able to talk to chat GPT about it. That was our valuable and benign use cases. And I would prefer to allow and then study, you know, where does that fall short? Where is that harmful? And then iterate from there, it was just taking a default stance on disallowed. And I think that's one of those ways in which our stance and posture has changed a bit over time in terms of where we set, you know, where we start. Yeah, we were very good. I think imagining worst case scenarios, what if I use these faces to evaluate hires for a company or whatever, but also it's like, hey, is this eczema? You know, like, you know, there's a lot of utility there. And honestly, I think there are certain demands of AI safety where worst case scenario thinking is very appropriate. So I think that is an important way of thinking about risk when it comes to certain forms of risks that are existential or even just very, very bad, you know, we have the preparedest framework, which helps us reason through some of those things, you know, can the AI let you make a bioweapon? It's good to think about the worst case there, because it can be really, really bad. So you kind of have to have that way of thinking in the company and you have to have certain topics where you think about safety in that way, but you can't let that kind of thinking spill over onto other domains of safety where the stakes are lower because you end up, I think, making very, very conservative decisions that block out many valuable use cases. So I think being sort of principled about different types of safety on different time horizons and with different levels of stakes is very important for us. I think I want to blunt mode sometimes and just because like right now it actually roasts you. Well, I mean, like, yeah, cause I'll ask the model, like with the voice in speech out model, be like, do I sound tired? And it's like, well, you know, I don't really wanna, you know, and I'll be like, yeah, you know, just trying to get it to be honest. You know, I think there's many cultures that would prefer a blunt or chat GPT. Yeah, very much on the radar. Yeah, just to piggyback off Nick's answer. I think it's the iterative deployment that gives us the confidence, right, to push towards user freedom. And we've had many cycles of this. We know what users can and can't do. And that gives us the confidence to launch with the restrictions that we do. One of the other capabilities, one of the other gym and native capabilities that's been very interesting has been code. And I remember early on GPT 3, we saw that all of a sudden it could spit out entire react components. And we saw that, oh, wow, there's some utility there. And then we went, we actually trained a model more specifically on code. And that led to, we had code X and we had code interpreter now, code X is somehow back. And, you know, new form, same name, but the capability to keep increasing. And we've seen code work its way first into VS code via co-pilot. And then cursor and then I win surf, which I use all the time now. What, how much pressure has there been in the code space? Because I'd say that if we ask people who made the top code model, we might get different answers. Yeah, and I think it reflects that when people talk about coding, they're talking about a lot of different things, right? I think there's coding in a specific paradigm. Like if you pull up an ID and you wanna kind of get a completion on a function, that's very different from, you know, agentic style coding, you know, you ask, you know, I want this PR. And, you know, and I think we've done a lot of focus. I'm trying to, can you pack a little bit waving by agentic coding? Yeah, yeah. So I think when you draw a distinction between more kind of real-time response models, you can think of chat to be, to first order, as you ask a prompt and then you get a response fairly, fairly quickly. And a more agentic style model where you give it a fairly complicated task, you let it work in the background. And after some amount of time, it comes back to you with what it thinks is something close to the best answer, right? And I think we see increasingly that the future will look like more of a sync kind of, you know, where you're asking it very difficult hard things. And you're letting the model think and reason and come back to you with really the best version of what it can come back with. And we see the evolution of code in that way too. I think eventually we do see a world where you'll kind of give a very high level description of what you want and the model will take time and it'll come back to you. And so I think our first launch codecs really reflects that kind of paradigm where we are giving it PRs, units of fairly heavy work that encapsulate, you know, a new feature or, you know, a big bug fix. And we want the model to spend a lot of time thinking about how to accomplish this thing rather than kind of give you a faster response. And you had your question, you know, there's coding is such a giant space. There's so many different angles at it. Kind of like talking about knowledge work or something incredibly broad, which is why I don't think there's one winner and I think there's one best thing. I think there's so many options. And I think developers are the lucky ones because they have so many choices right now. And I think that's fundamentally exciting for us too. But to Mark's point, I think this agentic paradigm has been particularly exciting for us. One framing I often use when thinking about product here is I want to build products that have the properties such that, you know, if the model gets two X better, product gets two X more useful. And I think, yeah, chat to be has been a wonderful thing because I've heard a long time. I think that was true. But I think as we look at, you know, smarter and smarter models, I think there's some limit to people's desire to talk to like a PhD student versus, you know, that they might value other attributes of the model like its personality and you know what it can actually do in the real world. But experiences like codecs, I think they create the right body such that we can drop in, you know, smarter and smarter models and it's going to be quite transformative because you get the interaction paradigm right where people can specify this task, give the model time and then get a result back. So I'm really excited where it's going to go. It's an early research preview, but just like with chat GBT, we felt like it would be beneficial to get feedback as early as possible and excited where we're going to take it. I was using SONNET a lot, which I love. I think SONNET for coding is fantastic, but with 04 Mini Medium setting in WinSurf, I found it was great. I found once I started using that, I was really happy because won the speed, everything goes like that. And I think that, and I think they're very good reasons why people like other models and I don't want to get a comparison, but I found out for me for the kinds of tasks I was using, this was the first time. I was very happy you guys put that out there because- Absolutely, yeah. And you know, we feel like there's still a lot of low-hanging fruit in code. It is a big focus for us. And I think we'll find in the near future, you'll find many more good options for the right code model tailored for your use case. Yeah, I find often if I just need a quick answer, like I don't write something in Dart, we'll just get a 4.1 and say, but yeah, something bigger. I think that's going to be the harder part is because yeah, these EVALS are some way saturated, but also everybody has their own criteria that we look at. And that's going to be kind of a question to sort of see, how are we going to adapt to all that? Right, yeah, I mean, specifically in code, right? I think there's more beyond, did it get you the right answer? With code, you know, people care about the style of the code, they care about, you know, however most it was in the comments, it cares about, you know, how much proactive work did the model do for you, right? On other functions. And so I think, you know, there's a lot to get, right? And users often have very different preferences here. Yeah, it's funny. I used to, you know, people used to ask me, you know, what domains are going to like, you know, be transformed by, you know, fastest and I used to say, you know, it's code because like similar to math and other things, it's very, very, verifiable and decibel and I think those are the domains that are particularly great to do RL on and, you know, they're therefore going to see all this, this awesome, you know, agentic stuff just suddenly work. I still think that's true, but the thing that surprised me about code is that, you know, there is still so much of an element of taste in terms of what makes good code. And there's, you know, there's a reason that, you know, people train to be a professional software engineer. It's not because their IQ gets better because they, or rather because they learn, you know, how to build software inside an organization. What does it mean to write good tests? What does it mean to write good documentation? How do you respond when someone disagrees with your code? Those are all actual elements of being a real software engineer that we're going to have to teach these models to do. So I expect progress to be fast and I still think code has a ton of nice properties that make it very ripe for the genetic products. But I do think it's very interesting to agree that, you know, the element of taste and style and real world software engineering matters. It's interesting too, because with chat GPT and the other models, you're kind of dealing with having to bridge the divide between consumer and pro. I open up chat GPT and I tell my friends like, oh yeah, cause I'll plug it into whatever code model I'm working because I can actually connect it to there. And I think about, you know, well, that's a very different use case a lot of other people, although I've shown people like how to go in and use, you know, an IDE and actually have it just write documents for you and create folders and stuff, which people don't realize, like, yeah, you can do that. You can have chat GPT actually control it and do that, which is cool. But then you think about like, okay, we've got a tab now for images. There's the codecs tab. So if I want to connect to GitHub and have it work through there, and there's a Sora into there. So it's kind of interesting to see how all of these things are coalescing into there. How do you differentiate between a consumer feature, a professional feature and maybe like an enterprise feature? Look, we build a very general purpose technology and it's going to be used by a whole range of folks. And unlike many companies, which have this kind of founding user type and then they use technology to solve that user's problems, we do start often with the technology, observe who finds value in it and then iterate for them. Now with codecs, our goal was very much to build for professional software engineers, knowing though that there's sort of a splash zone where I think a lot of other people will find value in it and we'll try to make it accessible for those people as well. There are a lot of opportunities to target non-engineers and personally really motivated to create a world where, or help build a world where anyone can make software. codecs is not that product, but you could imagine this product's existing over time. But as a general principle, it's really hard to predict exactly who the target user is until we made some of these general purpose technologies available because it gets back to the empiricism I was talking about. We just never exactly know where the value is going to last. Yeah, and I think even to dig deeper into that, assuming that you could have a person who's mostly using Tatchu BU for coding, right? But 5% of the time, they might just want to talk to the model or like, 5% of the time, they just want a really cool image, right? And so I think there are certainly archetypes of people who use the models, but in practice, we see that people want this exposure to different capabilities. Yeah. With codecs and watching the launch of that, it kind of struck me. There are some tools you see that there's a lot of excitement about because there's a lot of internal demand for that. How much are you using it internally? Are tools like that? More and more. OK. I'm really excited to just say the internal adoption. It's everything from exactly what you'd expect people using codecs to offload to tests, to we have an analyst workflow that we'll look at, logging errors and automatically flag them and slack people about it. So there's all these ways that we've actually heard some people using it as a to-do where future tasks they're hoping to do, they're starting to fire off codecs tasks. So this is the perfect type of thing that I think you can talk to internally. And I'm very excited about the leverage that engineers are going to get out of a tool like this. I think it's going to allow us to move faster with the people we have and make each engineer that we hire 10 times more productive. So in some ways, internal usage is a very good predictor of what we want to take. Yeah. I mean, we don't want to ship something to other people that we don't find value in ourselves. And I think leading up to launch. Laundry. Laundry, buddy. Laundry, buddy, the essential partner. OK, sorry. I mean, yeah, we had some power users, though, that hundreds of PRs a day that they were generating personally. So I think there are people internally finding a lot of utility from what we're building. Also, if you think about internal adoption, it's also a good reality check because people are busy adopting new tools to take some activation energy. So actually, the thing you find when you try to talk with things internally is some of their reality component of how long it takes people to actually adjust to a new workflow. And it's been humbling to watch. So I think you learn both about the technology, but you also learn about some of the adoption patterns when you're trying to get a bunch of busy people to change the way they're very good. As you build these tools, internally, people have to learn how to use them and are having to adapt. And there's a lot of question now about what kind of skills do people need in the future? What kind of skills do you look for on your teams? I've thought about this a lot. Hiring is hard, especially if you want to have a small team that is very, very good and humble and able to move fast, et cetera. And I think curiosity has been the number one thing that I've looked for. And it's actually my advice to students when they ask me, what do I do in this world where everything's changing? Because I mean, for us, there's so much that we don't know. There's a certain amount of humility you have to have about building on this technology. Because you don't know what's valuable. You don't know what's risky until you really study and go deep and try to understand. And when it comes to working with AI, which we obviously do a lot, not just in code, but in every facet of our work, it's asking the right questions that is the bottleneck, not necessarily getting the answer. So I really fundamentally believe that we need to hire people who are deeply curious about the world and what we do. I care a little bit less about their experience in AI. Mark presumably feels a bit different about that one. But for the product side, it's being curiosity that I've found the most, the best predictor of success. No, I mean, even on research, I think increasingly less, we index on you have to have a PhD in AI, right? I think this is a field that people can pick up fairly quickly. I also came into the company as a resident without much formerly AI training. And I think correlated to what Nick said, I think one important thing is for our new hires to have agency, right? Opening as a place where you're not going to get so much of a, oh, here's today you're going to do thing one, thing two, three, three. It's really about being kind of driven to find, hey, here's the problem. No one else is fixing it. I'm just going to go dive in and fix it. And also adaptability, right? It's a very fast changing environment. That's just the nature of the field right now. And you need to be able to quickly figure out what's important and pivot what you need to do. The agency thing is real. I think we often get asked for, how does Upenea keep shipping? And it feels like you're pushing something out every week or something like that. It's funny because it never feels to me. I always feel like we could be going even faster. But I think fundamentally, we just have a lot of people with agency who can ship. And that comes to pride. That comes to research. That comes to policy. Shipping can mean different things. We all do very different things at Upenea. But I think the ratio of people who can actually do things. And the lack of red tape except where it matters, you know, in a couple areas where I think red tape is very, very important. But, you know, I think that is what makes Upenea very unique. And it obviously affects the type of people who we want to hire too. I was brought into the company because I was originally given access to GPD3. And I just started showing all these use cases for it and making videos every week for it. Yeah. And I was annoying people, I'm sure. But I was just not. It was really fascinating. It was exciting. It was an exciting time. I described it to people like, you know, they, I think they built a UFO and I get to play with it. You know, and then I make it hover and like, oh, you made it hover. I'm like, well, they built it. I just pressed the button and got to do that. But that was just what I found very empowering was the fact that I, I'm self taught. I learned to code by Udemy courses and stuff and then to be a member of the engineering staff and be told, just go, just go do stuff. Nothing too critical. I didn't break anything anybody. And that's good to know that that kind of spirit is still there. And I think that is part of the reason why OpenAI is able to ship, even though, you know, it was like 150, 200 people worked on GPD4. I think people forget about that, you know, totally. And honestly, this is how, and even chat to be, this is how it came together. You know, we had a research team. They'd been working, you know, for a while on instruction following and then the successor to that and, you know, post training these models to be good at chat. But the product effort came together as a hackathon. I remember distinctly, we said, like who, who, who's excited to, you know, go build consumer products. And we had all these different people, like we had a guy from the super computing team, who, you know, was like, I'll make an iOS app. I've done that in the past life where we had a researcher who wrote some back-end code. And it was just convergence of people who were excited to do stuff. And I think the ability to do so. And I think that's how you get the next chat to be. It is, is running an organization where, where that is possible and continues to be possible at this scale. Hackathons were my favorite thing because one, being a performer in love and show and tell, but it was just neat to be able to see things that you knew were going to be a product or something later on. Because when you're playing with the technology, this advanced and all that, do you guys still do them? Yeah, absolutely. Yeah. We've had some fairly recently and they are typically. Last week. Yeah, I know. Can't say what it was about. Yeah, but it was an exciting thing. Sure. And it's how you find out what's possible. Yeah. Yeah. I'm excited to hear that. I do have a question, which is how much as it grows, again, like when when I started, I think like 150 people on the company. Now there's like 2000 and now, you know, I see a video with Sam talking to Johnny. I and how much is that going to change the character, the spirit of bringing in all this? I think all the outside expertise has been great. We've seen this great sort of run of products, but do you see it changing the culture? Well, I mean, I think probably in the right way, right? It's like, I think when we look at AI, we don't think of it as some fairly narrow thing. And we've always been kind of in thought by just the potential and all the different things you could build with AI. And yeah, to next point, right? This is why we're able to ship so quickly, these people imagine all these different possibilities. They imagine the future with AI and they try to bring it about, right? And I think these are facets of that imagination, right? It's like, what does AI look like if you imagine the AI first device, for instance? You know, when you go from 200 to 2000, you'd think a lot would change. And yeah, maybe in some ways it has, but I think people often underestimate you. The number of things that we're doing, I always feel like being at opening. I feel as much closer to being in a university where, you know, you've got this kind of common reason to being there, but everyone's doing something different and you'll sit down at dinner at lunch and you'll talk to someone and learn about their thing. And you're like, wow, that's so cool. You're doing that. And so it feels much smaller because I think of the sort of broad range of things we're doing and therefore each individual effort, whether or not that's something like chat, or something like Sora or it's an etc. Is actually staffed in a very, very conservative and lean way that continues to keep people very autonomous and make sure they have resources, etc. So I think it's partly that that has made it feel very, very similar in the good ways to when I started here. We talked a bit about one of the things you look for is curiosity and Mark said that's helpful too. If I'm somebody outside of AI, okay, if I'm 25 or I'm 50 and I'm looking at the advancement of technology and maybe have it a little bit of fear because I see copywriting is one of the things that chat GPD got great at. Writing code is great. I personally have the opinion that we will never have enough people creating code because there's more things code can do in the world than we can imagine. And even if it places the copy, my wife showed me the other day on her. Her skin block or sunblock lotion bottle showed me on her sunblock lotion bottle like some very funny copy about like the ingredients. I said, oh, this is not a place I expected to see this, but that's one of the tiny little places that all of a sudden that you can put more thought into it. That being said, I know that I'm a bit of an optimist because I see all these opportunities and places to go in there. What advice do you give people, you know, whatever point they are in life about preparing for or adapting to and being part of the future? You know, I like how Mark just looked right. Oh, no, I can go. OK, I'm jumping right now. Yeah, I think the important thing is you have to really lean into using the technology, right? And you have to see how your own capabilities can be enhanced. How you can be more productive, more effective by using the technology. I fundamentally do think that the way this is going to evolve is you still have your human experts. But what I helps the most is the people who don't have that capability at a very advanced level. Right. So if you imagine, right, like as these models get much better at health care advice, they're going to help people who don't have access to care the most, right? Imension ratio, right? It's not producing, you know, an alternative for, you know, experts or, you know, professional artists, allowing people like, me and Nick to create creative expressions, right? And so I think it's kind of rising the tide that allows people to be competent and effective at a lot of things all at once. And I think that's kind of how we're going to see a lot of these tools with people. The world's going to change a lot. And I think truly everyone has a moment with the does something that they considered sacred and human. I know a guy they got bested and heartfelt, very threatened about his achievements and code. You know, abilities. Well, that happened for me a long time. I was less than someone else. Oh, yeah. I mean, yeah, it's definitely better than me. A lot of code problem solving for sure. Yeah. Right. So I think it's deeply human to feel some level of all respect and maybe even fear. And I think to Mark's point, be actually using this thing, can demystify it. I think we all grew up or, you know, learned about the word AI in a world where I'm in something pretty different from what we have today. You've got these algorithms that, you know, try to sell you things, try to do things or you've got movies, you know, where the tech's over, etc. And like that term means so many things to different people that I'm entirely unsurprised that, you know, there's fear. So actually using the thing is I think the best way to have a grounded conversation about it. And then I think from there, the best way to prepare. I think the some degree to which you need to understand the products and keep up, sure. But I think things like prompt engineering or sort of understanding the intricacies of the AI, they're kind of not the right direction. I think sort of this fundamental human things like learning how to delegate that is incredibly important because increasingly, you know, you're going to have an intelligence in your pocket that it can be your tutor, can be your advisor, it can be your. Software engineer, it's much more about you understanding yourself and the problems you have and how someone else might help. Then a specific understanding of AI. So I think that's going to be important. Curiosity, I mentioned earlier, I think asking the right questions, you'll get you only get what you put in, right. That's important. And I think fundamentally being ready to learn new things. I think the more you understand how to pick up new topics and domains, etc. The more you're going to be prepared for a world where, you know, the nature of work is shifting much faster than it's ever shifted before. So I'm prepared that my job, you know, in product is going to look different or not exist at all. But I am looking forward to picking up something new and I think as long as you bring that perspective, you're well set up to leverage AI. I think we sometimes over index and you know, sometimes certain jobs go away because like, you know, we don't really need a lot of, you know, typewriter repair people anymore, right. And then certain kinds of coding jobs are probably going to go away. But like I said, I think there's way more opportunity for coders or people to create code, however it's done. And when you mentioned like the health field, that's one of the things I hear, we were like, oh, when, you know, when we replace everything with AI, like, well, I mean, I would be very happy having an AI diagnosed me, operate on me and probably do everything else. But I do want somebody there to talk me through the procedure and hold my hand. But also, I want people asking questions like, like, you know, every day I take a bunch of vitamins. It's just the right time of day to take it. You know, I can't bother my doctor with all these silly little questions. I really don't think you end up displacing doctors. And you'd end up displacing not going to the doctor. You end up democratizing the ability to get a second opinion. Very few people have that resource or know to take advantage of a resource like that. You end up bringing Medicare into pockets of the world where that is not readily available. And you end up helping doctors gain confidence. You know, I think I often heard from doctors that, you know, they already talked to existing colleagues to get a second opinion. In some cases, that's not possible. And I think you'd be surprised by the number of doctors that use chat to be teeth. Now, on things like medicine, there's work to make the model really, really good. And we're excited to do that work. There's also work to prove that the model is really good because I think you're not going to trust that until there's some degree of sort of legitimacy. And then there's work to explain the areas where the model might not be good because increasingly once it gets to human and then super human level performances. It's hard to frame exactly where it will fall short, which is also hard to sort of reckon with. But nonetheless, I think that opportunity is one of the things that gets me up in the morning. Education might be the other one. And I think there's a tremendous opportunity to help people. What do you think is going to surprise us the most in the next year to 18 months? How does he think it's going to be the amount of research results that are powered, even in some small way by the models that we've built. And one of the kind of quiet things that's taken the field by Storm is the ability of the models to reason. And you already see some research. I'm going to make you explain what you say reason. Yeah. So this fits into the... I want you to reason through the question as you explain. Yeah. Yeah. Think out loud. Tell us your traces. Yeah. This really fits into this agentic paradigm that we were talking about earlier. And the way that the models approach solving a problem that takes some time to solve is that it reasons through it, much like your, I might, if I give you a very complicated puzzle. I think you reason probably much better than I do. I mean, I think I... I'm flattered. Yeah. Like a complicated puzzle, right? You might think to yourself, for instance, this is a crossword puzzle, right? Like you might think through all the different alternatives and what's consistent. You know, is this row kind of consistent with that column and you're searching through a lot of alternatives. You're backtracking a lot. We're trying a lot of hypotheses. And then at the end, right, you come up with a well formed answer. And so the models are getting a lot better at that. And that's what's powering a lot of the advancements in math and science encoding. So this has reached a level where today in many research papers. People are using O3 almost as a sub routine, right? There's sub problems within the research problems are trying to solve, which are just fully automated and solved through plugging into a model like. I've seen this in several physics papers. Talk to physicists even where they're like, wow, like, I had this expression that I couldn't simplify, but O3 made headway on it. And these are coming from some of the best physicists in the country. So I think you're going to see that happen more and more and more and more. And we're going to see just acceleration in progress in fields like physics and mathematics. It's a hard one to beat because, you know, I would swap many things we do in exchange for making it true, you know, significant scientific advancement. But I think we can we can have multiple of these things. I think for me, it's the fact that any well described problem that is intelligence constrained, I think will be solved in products. And I think we're fundamentally just limited by ability to do that. So what I mean is like, you know, in companies in the enterprise, there are so many problems that are fundamentally hard. The models are not smart enough to do yet without software engineering, running data analyses, whether or not it is providing amazing customer support. There's all these problems that the models fall short at today that are very, very easy to describe and evaluate. And I think that will make tremendous progress at those. On the consumer side, these problems exist too. They're a bit harder to find just because consumers are worse at telling us exactly what they want. That's the nature of building consumer products. And it's a very worthwhile where, you know, there's many hard things we do in our personal life, whether or not it's doing taxes, whether or not it's planning a trip, whether or not it's searching for a high consideration purchase, whether or not that's a house or a car or a piece of clothes. All of those things are problems where we need just a little bit more intelligence. So I think the other thing that's going to happen next year and a half is you'll see a different form factor in a I evolve. I think chat is still incredibly useful interaction model and I don't think it's going to go away. But increasingly you're going to see more of these sort of asynchronous workflows coding is just one example, but for consumers, it might be sending this thing off to go find you the perfect pair of shoes or to go leave and plan a trip or for you know to go finish your taxes. And so I think that's going to be exciting and we're going to think of a little bit differently than just a chatbot. One of my favorite examples, both from a utility point of view capability and then UI was deep research and deep research is probably the best example we maybe have of probably a gintick sort of model use right now because it used to be you would ask for a model to tell you about a topic. And then you're going to see a data or just do a big search the internet and then it just summarize all that where deep research will go find some set of data, look at it, ask a question, then go find some new data and come back to it and keep going on. And I think the first time I used other people use it like wow this is taking a while and then you added a UI change so I can actually go away and go do something else. And then you're going to show me this is working, which was a paradigm shift and I talked to Sam here about that and Sam said that was a surprise to him was the fact that people would be willing to wait for answers. And now I've seen a new metric for models as how long a model can spend trying to solve a problem, which is a good metric if it ultimately solves it. And then it's like, is this been an update to you and how you think about these things the idea of like oh we don't just want and I guess you talk about this before about a gintick. Any idea that it's not just give me the answer it's like, take your time, get back to me. I think you know to build a super assistant you got a relaxed constraints like today you have a product that is you know entirely synchronous you have to initiate everything. It's a maximally best way to help people like you think about a real world intelligence that you might get to work with. It has to be able to go off and do things over a long period of time has to be able to be proactive. So I think there's like we're sort of in this process of relaxing a lot of the constraints on the product and on the technology to better mimic a very, very helpful entity. So do five minute tasks you know five hour tasks eventually five day tasks is like a very, very fundamental thing that I think is going to unlock a different degree of value in the product. So I've actually not been that surprised that people are willing to do that. Like I don't really want to be sitting around waiting for my coworker either. And I think if the value is there. I'd gladly be doing other stuff and come back. Yeah, and we really don't do it just because right we do it out of necessity. The model needs that time to solve the really hard coding problem or the really hard math problem and it's not going to do it with less time right you can think about this as I give you some kind of brain teaser right your quick answers probably like the intuitive wrong one. And you need that actual time to kind of work throughout the cases to like, are there any gotcha here. And I think it's that kind of stuff that ultimately makes robust agents. We've seen kind of there's like the paper of the moment where somebody comes out and says, I found a blocker and I remember there was one a month or so ago and they said models couldn't solve certain kinds of problems and it wasn't hard to figure out a prompt. You could train into a model and it could solve those kinds of problems and we had a new one that talked about how they would fail at certain kinds of problem solving ones and that was kind of quickly I think debunked by showing that you know the paper kind of had flaws in there but there are things that there might be some blockers and things are things we don't know are going to be there I think brittleness is one of things there is a point where models can only spend so much time solving a problem. We're probably the point we're only having the model. You know, maybe two systems watch each other and we have to think about how a third system stops, you know, the wait for things to break down but do you see kind of any blockers between here and where I'm getting the models they're going to be solving, you know, doing things like coming up with interesting scientific discoveries. I mean I think there are always technical innovations that we're trying to come up with right fundamentally we're in the business of producing simple research ideas that scale and the mechanics of actually getting that to scale are are difficult right it's a lot of engineering, a lot of research to kind of figure out how to kind of pre pass up a certain roadblock. I think those are always going to exist right every layer of scale gives you new challenges and new opportunities so you'll fundamentally approach is the same but we're always encountering new small challenges that we have to overcome. Just to build on that I mean the other business we're in isn't building great product with with great product with these models and I think we shouldn't underestimate the challenge and amount of discovery needed to really bring these ever intelligent models into the right environment with the not that's giving them the right sort of action space and tools, whether or not that's really being proximate to the problems that are hardest understanding those and bringing the eye there. So I think there's you know the technical answer, but I think there's also the, the, you know, real world deployment and I think that always has challenges that are like very, very hard to predict yet you know worthwhile and environmental to solve. All right last question and I'll begin it's what's your favorite user tip for chat GPT. Mine is I take a photograph of a menu and I'm like it helped me plan a meal or whatever I'm trying to like you know stick to a diet or whatever so I really want that use case but like I've been trying for wine lists and that is my eval on multimodality it's delicious work. Really, it keeps embarrassing me with like hallucinated wine recommendations and I go over it and they're like never heard of that years works, but for me that's the, that's still useful. Well I mean I maybe the line lenses to dense that was a problem. That was a problem operator was it like originally was the division models that too much dense text it just loses his placement. I love using deep research and you know when I go meet someone new when I'm going to talk to someone about AI right I just preflight topics right I think the model can do a really good job of texturizing who I am who I'm about to me and what things we might find interesting. And I think it really just helps with that whole process. I'm a voice believer I it's still got, I don't like it entirely mainstream yet because it's got it's got many little things that all had up but for me, you know half of the value of voice is actually just having someone to talk to you enforcing yourself and I find that to sometimes be very difficult to do in writing so on my way to work I'll use it to process my own thoughts. And like with some lock and I think this works most days I'll have the restructured list of two dues but the time I actually get there so voice for me it needs to be the thing that you know I both love using and want to see improve.
