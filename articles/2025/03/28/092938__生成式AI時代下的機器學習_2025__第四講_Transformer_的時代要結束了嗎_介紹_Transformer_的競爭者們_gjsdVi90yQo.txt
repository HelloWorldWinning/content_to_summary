Timestamp: 2025-03-28T09:29:38.891731
Title: 【生成式AI時代下的機器學習(2025)】第四講：Transformer 的時代要結束了嗎？介紹 Transformer 的競爭者們 gjsdVi90yQo
URL: https://youtube.com/watch?v=gjsdVi90yQo&si=SQjlm2JlsdHaTkMP
Status: success
Duration: 1:22:41

Description:
好的，这是对您提供的文本的提炼和总结，满足您的所有要求。

**核心结论：**

尽管Transformer架构在并行训练方面具有优势，但对长序列处理存在局限，因此研究者们正积极探索RNN的变体（例如Mamba），以期在效率和长程依赖建模之间取得平衡。

**总体框架：**

该讲座主要探讨了类神经网络架构，尤其是Transformer（及其核心的Self-Attention）的替代方案。讲座从CNN和ResNet等架构的设计动机入手，引出RNN和Self-Attention的对比，并深入分析了Self-Attention的优势和局限性。随后，重点介绍了Linear Attention及其变体（例如Mamba），讨论了它们如何试图结合RNN的序列处理能力和Transformer的并行训练优势。最后，提到了在更大规模模型中应用这些架构的趋势，以及它们在视觉领域的应用情况。

**概要结构：**

I. **引言：类神经网络架构的设计动机**
    *   CNN：为图像设计的，减少参数，避免过拟合。
    *   ResNet：解决深层网络训练困难的问题，优化更容易。
    *   核心思想：每种架构都有其存在的理由，是针对特定问题的优化。

II. **Self-Attention的崛起与RNN的没落**
    *   Self-Attention取代RNN的原因：训练时更容易并行化，更好地利用GPU。
    *   Self-Attention的运作方式：Q、K、V的计算，以及Attention Weight的加权求和。
    *   Self-Attention的局限性：处理长序列时计算量和内存需求大。

III. **RNN的复兴：Linear Attention及其变体**
    *   RNN训练并行化的可能性：通过展开RNN的公式，消除序列依赖，实现并行计算。
    *   Linear Attention：RNN去除reflection机制后的简化版本，本质上是Self-Attention去除softmax。
    *   Linear Attention的优势：Inference时像RNN，训练时像Transformer，结合了两者的优点。
    *   Linear Attention的局限性：记忆无法改变，缺乏softmax带来的动态调整能力。

IV. **增强Linear Attention：Reflection机制的引入**
    *   RetNet：引入Gama常数，让记忆逐渐被淡忘。
    *   Gated Retention：引入Gama T，根据情境决定记忆的保留或遗忘。
    *   更复杂的reflection方法：GT矩阵的设计，以及与规定Decenn的联系。

V. **Mamba及其他RNN变体的应用**
    *   Mamba：一种能与Transformer竞争的Linear Attention架构。
    *   Jemba和MiniMax01：基于Mamba的大型语言模型。
    *   Sena：将Linear Attention应用于图像领域的深度学习模型。
    *   Mamba Out：在图像分类任务中，去除Mamba架构可能反而更好。
    *   Fine-tuning：在现有语言模型的基础上，微调Self-Attention相关的Layer。

VI. **关于Attention的赌局**
    *   讨论：2027年Self-Attention是否仍然是最强的架构。

<Mermaid_Diagram>
```mermaid
graph LR
    subgraph RNN_Family [RNN及其变体]
        direction TB
        RNN(RNN - 传统RNN) -->|去除 Reflection 机制| LinearAttention(Linear Attention)
        LinearAttention -->|引入 Reflection 机制| RetNet(RetNet)
        RetNet -->|引入 Gated 机制| GatedRetention(Gated Retention)
        GatedRetention -->|复杂 Reflection (GT矩阵)|Mamba(Mamba)

        style RNN fill:#f9f,stroke:#333,stroke-width:2px
        style LinearAttention fill:#ccf,stroke:#333,stroke-width:2px
        style RetNet fill:#ccf,stroke:#333,stroke-width:2px
        style GatedRetention fill:#ccf,stroke:#333,stroke-width:2px
        style Mamba fill:#bbf,stroke:#333,stroke-width:2px
    end

    subgraph Transformer_Based [Transformer 及其基础组件]
        direction TB
        SelfAttention(Self-Attention)
        Transformer(Transformer)
        style SelfAttention fill:#ffc,stroke:#333,stroke-width:2px
        style Transformer fill:#ffc,stroke:#333,stroke-width:2px
    end


    Motivation(设计动机) --> CNN[CNN]
    Motivation --> ResNet[ResNet]
    style Motivation fill:#efe,stroke:#333,stroke-width:2px
    CNN -->|图像特性| Transformer_Based
    ResNet -->|优化深层网络| Transformer_Based

    Transformer_Based -->|并行训练优势/长序列局限| RNN_Family
    RNN_Family -->|长序列处理/训练挑战| Transformer_Based

    RNN_Family --> LargeModels((大规模模型应用))
    Transformer_Based --> LargeModels

    LargeModels --> 视觉应用(视觉应用，例如图像处理)

    click LargeModels "https://example.com" "详细了解大规模模型应用"
```
</Mermaid_Diagram>


Content:
好 那这一堂课开始我们要讲内握的架构那在作业室里面呢 我们会训练唇封唇封现在是一个非常广泛的类神经网路的架构除了被用在大型园模型里面以外很多其他的地方也都会用到唇封比如说产生语音也会用到唇封其今天很多时候产生图片也会用到唇封那在我们的作业里面呢是要做一个产生图片的唇封我们特别不是叫大家做语言模型因为我想语言模型已经看了太多了所以我们特别是选择用唇封来产生图片让你知道说唇封有各式各样不同的应用不是只局现在大型语言模型里面而已好 那这一堂课的主轴呢是要讲有没有哪些架构有可能可以取代唇封唇封有哪些潜在的竞争对手那讲到唇封的竞争对手最最知名的就是漫吧那漫吧又是一种蛇的名称所以只要讲到这种唇封的竞争者一个很熟悉的封面就是要画一个唇封一个变形金刚跟漫吧打起来的那我这边除了打起来以外呢它中间有一个爱心等一下就告诉你说他们其实是非常类似的东西但其实这一堂课呢不会讲到太多跟漫吧有关的东西我们主要讲的是一个系列漫吧的一群好朋友们那我们这一堂课呢我们著重的点就在于类神经网路的架构大家每次看到一个类神经网路架构的时候要记得每一个架构都有一个它存在的理由每一个架构是因为不同的理由而被设计出来的比如说CNN存 sick存在的理由是什么呢CNN存在的理由是CNN是fully connected的Feel1 network的一个废话你把原来的最 general的fully connectedfeel1 network做receptive field拿掉一些不需要的位置做perempt sharing让一些位置的参数是一样的让一些参数的数值是一样的那就是converitutional layer就是CNN所以fully connectedfeel1 network是一个更犯用的内卧架构而converitutional的Neural network是专门位影像所设计的所以CNN存在的理由是什么它是根据影像的特性减少不必要的参数减少不必要的参数什么好处可以避免overfeet让你可以用更少量的资料就成功训练你的累生经广路那CNN存在的理由假设刚才讲的那一段你没有听得很懂的话CNN存在的理由是我们过去上课的录影是讲过的那如果你刚才讲的没有听得很懂的话那再赶快去复习过去上课的录影那或者是recejo connection存在的理由是什么recejo connection存在的理由跟CNN是不一样的recejo connection存在的理由是什么呢recejo connection我们上周也有提到过我们说这是一个在韩武技之前就已经存在的内臣经广路架构本顿说2016年那个出现AFAGO是人工智慧的韩武技大爆发这样2015年recejo Nego是2015年年底就有的那它就是前韩武技实习的东西了而recejo Nego存在的理由是什么呢那个时候人们发现如果训练一个很深的内卧结果不太好横轴是训练的 iteration那种轴是测试资料上面的错误率比较56成的内臣经广路跟二十成的内臣经广路发现56成的内臣经广路是比较差的很多人很直觉地认为说是不是因为overfeating56成参数太多了训练资料有限所以overfeat在训练资料上面但实际上不是因为如果你观察训练资料的话会发现在训练资料上56成的内臣经广路也是脆的比二十成要差的所以内臣经广路深结果不好并不是overfeating的问题而是在训练的时候比较深的内卧它的optimization就比较困难你本来就比较难训练好而rec 究竟的存在是为了让更深的内卧可以被训练好所以它是为了让optimization更容易那怎么让optimization更容易呢这边是引用一篇2017年的论文如果2016年是韩舞技的话那2017年大概就是中生代诸罗记时代左右在诸罗记时代那时候就已经发现说有加跟没有加rec 究竟是在这边paper里面叫Skick connection这个arrow the surface会是非常不一样的这两张图代表了训练两个不同的内臣经广路的时候你的loss长什么样子那其实这边paper最主要想要分析的其实也不是Skick connection了它就是发明了一个videoization的方法发明了一个可视化的方法可以让你看到arrow the surface长什么样子因为实际上arrow the surface应该是存在一个高为的空间中穿梭是非常高为它想办法把那个高为空间中的变化投影到二围的平面让你可以想像一下arrow the变化loss的变化长什么样子那这篇论文就分析了一下没有rec Go connection跟由rec Go connection的loss的变化发现说没有rec Go connectionloss非常的奇趋就像是一个三水化一样所以你比较难做uptenization比较容易卡在Local Minimum或者是StatlePoint而如果用Receive Connection的话那AeroService看起来比较平坦所以Uptimization比较容易所以这边举两个例子是想告诉你如果每一种架构都有它存在的理由而它们存在的理由不一定是一样的比如说CNN跟Receive Connection它存在的理由是不一样的最后大家看到一个新的架构被提出来的时候你要想想它是为了什么而被设计出来的而那Transformer的出现是因为什么样的理由呢那这边要讲得更精缺一点我们实际上值得是在这门客里面我们实际上讨论的是SaleAttention那Transformer是一个比较大的架构那它其实有非常多不同的变形事实上2019年GPD2用的Transformer跟现在Lama用的Transformer虽然都叫Transformer中间还是有一些差别的而这些差别其实会造成他们显著的能力的差异那到时候在作业室当你自己全Transformer的时候你会真的体验一下这件事你用Lama的架构跟用GPD2的架构你会得到不一样的结果好 那我们这边讲的并不是完整的Transformer我们要讨论的Transformer的Layer里面的其中一个Layer叫做SaleAttentionLayer好 那在讨论SaleAttentionLayer之前我们要讲的是SaleAttentionLayer是怎么取代掉在SaleAttentionLayer之前大家比较常用的R&N或LSTN那你知道SaleAttention是怎么取代R&N跟LSTN之后你更能想像为什么其他的架构比如说漫吧会取代了SaleAttention那这个SaleAttention在机器学习这门客是从2019年开始引入的后来从2021年开始课程里面就没有出现R&N了但事实上这堂课呢所以告诉你R&N又回来了其实漫吧就是R&N等一下你会知道为什么漫吧就是R&N好 那这些什么R&N啊SaleAttentionLayer或漫吧他们真正想要处理的问题是什么他们要解决的问题类型是输入一个Vector的Sequence要输出另一个Vector的Sequence输入一个VectorSequence以后这一个Layer要把输的资讯做某种混合产生另外一个VectorSequence那如果在做语言模型的时候通常每一个输出都只能看左边更之前的输入所以我们在输出Y1的时候会只考虑X1的资讯输出Y2的时候会只考虑X1 X2的资讯输出Y3的时候会只考虑X1 X2 X3或合起来的资讯以此类推但在其他的应用里面不一定是这样可以把整个InputSequence合起来混合起来在产生输出的Sequence不过在等一下课程里面因为我们主要专注于讨论语言模型所以我们就假设说每一个输出的Y都只能够看到比它更左边的在它之前就已经输入的X好那我们来看看RNN是怎么解这个问题的事实上RNN比较像是一整群不同架构的代程总之这边在RNN后面加一个Style就是RNN流RNN流的这种内卧架构它的基本精神是怎么样把输入X1到XT混合起来最后输出YT呢有一个东西叫做Heathens DayHeathens Day的作用是把目前已经看到的输入全部混合起来存在Heathens Day里面输出的时候只需要根据Heathens Day就可以决定现在的输出讲得更具体一点RNN流的方法它一个广义的写法可以写成这个样子H代表的是Heathens Day里面存的资讯我们这边没有特别告诉你H它应该是一个Vector还是一个Match其实两者都是可以的H是存在Heathens Day里面的资讯在第1个时间点Heathens Day里面的资讯是怎么被得到的呢它是由前一个时间点的Heathens Day的资讯HT1跟现在的输入XT所共同决定的HT1通过一个Fa这个Fa是一个寒式它长什么样子是你自己设计的通常里面有一些参数是需要通过训练资料训练出来的HT1通过Fa再加上HT1通过Fb得到HTHT再通过Fc得到YT那如果把它视觉化的话整个运作的过程是这样首先有一个H0现在输入X1根据X1跟H0还有FaFb产生H1H1通过Fc产生Y1同样步骤就反复进行输入X2这个X2通过FbH1通过Fa加起来得到H2H2再通过Fc再产生Y2那这过程就反复继续下去输入X3产生H3再产生Y3一直到输入Xt产生Ht产生Yt那这个H0我这边没有特别告诉你说它是什么样子的东西那最早年的RNN通常H是一个项量但其实它不一定要是项量它也可以是一个很大的举证那很多人对于RNN有一个误解觉得说RNN它HiddenStay就是一个项量可以存的资讯很少这是个误解它可以不要是项量它可以是一个很大的举证那更算用的一个协法是会把这个FaFb跟FC加上一个下标小T代表说呢FaFbFC这三个含是并不是一层不变的他们是会随著时间变化的那怎么让FaFbFC随著时间变化呢那你可以让这三个含是跟X有关系跟输入的X有关系你可以让X来决定FaFbFC长什么样子那因为每一个时间点输的X都不一样那这三个含是就会随著时间变化了不同那也可以想见说这样子跟时间有关的操作可以带给RNN很好的性质比如说假设今天对一个圆模型来说它想要看完整个文章的段落以后就一望过去的东西然后有新的开始那可以怎么做呢假设这是一个圆模型它可以说假设输入了X2是一个换糖符号那Fa2就执行清除这个动作把之前存的资讯清掉或者是说假设X2是一个不重要的资讯那干脆Fb2可以采取关闭这样的行为不要让X2的资讯进入X2里面去占用宝贵的储存空间等等所以把这个FaFbFC让他们跟输入有关你就可以做更复杂的操作让这样FaBC跟输入有关这是什么呢其实这就是LSTN我不知道还有多少人知道什么是LSTNLSTN,GRU这些有Gate的RNN其实就是让FaFbFC跟时间有关也就是跟输入有关那这个图看起来蛮复杂的那就是取之过去投影片的图这个图2021年以后就再也没有出现过了这次是2021年以后第四再用这个图那我们不会再细讲总之就是告诉你说过去常用的RNN的变形就是最简单的RNN以前叫Velina的RNNN最简单的RNN其实使用度很低多数人都是用LSTN或GRULSTN或GRU就是让FaFbFC跟T有关这个T就是一个Gate那其实在LSTN里面它就是有三个Gate一个叫做InputGate它就是对应到FbT一个叫FogateGate会决定模型要不要忘记东西就是对应到FaT一个AlputGate决定什么东西要被输出来就是FCT那其实RNN流的做法跟我们第二堂课奖的AI agent怎么处理Memory也有一曲同工之妙我们在讲AI agent的Memory的时候说我们要有三个模组一个输血的模组决定什么样看到的东西要被放到记忆中一个reflection的模组对记忆中有的资讯重新整理一个读取READ的模组从记忆中读取资讯出来而RN流的这整套做法呢H就是我们的记忆B就是输血的模组决定什么东西要被放到记忆中那FC就是读取的模组决定要怎么从记忆中把资讯读出来而FA就是reflection反思的模组决定记忆中的资讯要不要有变化以上就是跟大家介绍一个RNN流最犯用的式子在Inference的时候就假设你内卫已经训练好了要使用它比如说它是一个语言模型你要用它来产生一个句子的时候RN流的做法是怎么运作的呢首先你要有一个H0那要注意一下这整个LLN这整个语言模型还可能有很多其他的Layer还可能有很多其他的部件比如说NLPfully-connected-vfovo的部件我们在这个图上只把RNN相关的部分画出来RNN也不会只有一层它也有很多层所以叫想成说当我画这个图的时候我就是把原来大家常见的全former里面的selectual attention layer换成RNNRNNN成要一个输出史的状态叫H0输入第一个符号像第一个符号是Begin of sentence这个符号然后产生X1X1让我们有H1H1让我们有Y1再送到接下来的Layer去处理最后输出一个字叫大然后做Otah regressive大变成下一个时间的输入再产生X2再产生H2再产生Y2再产生加加在变成输入产生H3产生Y3产生好同的步骤就反复一直进行下去这就是RNNInference的时候所做的事情好这个是RNN接下来我们来讲selectual attention我们来看看selectual attention相较于RNN有什么好处凭什么能够取代RNN我们来看一下selectual attentionselectual attention怎么运作的呢输入是一个 sequence X1到 XT然后现在输入的每一个Vector都层上三个不同的全部制造出三个VectorX1就制造出V1K1Q1X2就制造出V2KQQQ以此类推假设我现在要算YT的时候第T个位置对你的输出YT的时候要怎么计算呢我把第T个位置的输入产生出来的QT去跟每一个位置的K都算Enerpada算个内积我用AFA来表示这个内积后的数字内积后得到的数字那这个AFA呢现在通常叫做 attention的 weight虽然它有 weight 这个字但它并不是一个参数它并不是类神经网路的参数但我们也叫它 attention的 weight就是那这个AFA T1代表QT跟K1的Enerpada那AFA T2代表QT跟K2的Enerpada以此类推那你会做一个 softmax让这些AFA的值能合起来是0那 softmax看起来是个不齐眼的动作但等一下会告诉你 softmax其实妙用无穷然后这边的AFA呢跟每一个时间点的V呢做相成做 weight 上最后就得到Y这是 still attention做的事情那希望这个大家都已经很熟悉了那在以下的课程里面如果要简化的话我就会直接把X1到ST直接指到YT代表说YT对X1到ST做 attention然后做 weight 上最后得到YT那很多人物以为 attention这个概念是来自于 attention is all you needGoogle 9.1.1.1.1.1的paper其实不是 attention的概念其实很早就有了结果所知最早可以回溯到new road to remission还有 memory network他们是什么时候的类神经网路呢你看这边的 archive连结他们是2014年的类神经网路那个时候地球上还没有多细胞生物只有单细胞生物的时候就有 attention这样子的概念了而且这两个提出 attention的内容架构你仔细看看它的连结都是在14年10月放上 Archive前后只差几天而已在这个只有单细胞生物的时代就已经非常的卷了那把 attention用到语言模型里面也从来都不是一个新的想法比如说我们实验是在2016年的时候就尝试把 attention加到语言模型里面这个是刘大洪同学做的他做的时候还是大学生现在都不是搬毕业好多年了所以这个是一个韩舞技时代的东西那看那个时候韩舞技时代放在这个 language model里面的 attention跟现在有什么不同呢就是没有任何不同就是你看我这个那个时候只是不叫做 QKV而已notation不太一样产生一个这个叫做 query不过当时是用 K来表示他然后这个是 key这个是 value然后就做你所熟知的 attention放在练出去 model里面只是在 attention之前还会先做 OSTN才做 attention然后我们来看这个 attention的这个架构在 inference的时候在生成句子的时候是怎么生的第一个输入进来产生 X1那 X1没有更之前的东西所以他只能自己跟自己做 attention产生 Y1 产生第一个字大那大呢被作为第二个输入产生 X2可以对第一看第二个位置都做 attention产生 Y2 然后产生加然后以此类推第三个时间点输入的是加可以对 123个位置都做 attention产生 Y3 产生好这个步骤就持续继续下去持续继续下去每一次都要对前面所有的位置做 attention来比较一下 R&N 跟 attention的这样的架构在 inference 在生成的时候有什么样的不同如果是 R&N 每步的运算量都是固定的如果是 attention的话你越往后见头就化越多越往后输入的序列越长运算量就越来越大那如果你看 memory的话对 R&N 来说每一次我要产生新的输出的时候我只要记得前一个时间点的 H 是什么我就能够做运算对 attention而言我要产生 Y6的时候前面 X1 到 X5 发生了什么事通通要被记住才能产生 Y6所以 attention 也非常的好废 memory随著 input 的 sequence 越长 attention 这个架构需要的 memory 对 memory 的需求就越大讲到这边有人会说但 attention 还是要好处的你可以感觉说 R&N 它的 memory很小只有这么小块感觉应该没办法寄很多资讯吧 attention 感觉可以寄无穷长的资讯等一下的课程告诉你说这是一个对 R&N 的误解等一下告诉你说 attention 可以从无限长的资讯只是一个架效所以讲到这边你可能觉得 R&N 这个 attention就是一无四处没有什么好的地方没有什么好的地方那为什么从2017年到最近 attention 这么多么内深度学习的领域呢好 那我们来看看当年 attention is all unique这篇最知名的 attention相关的论文它是怎么说的那你从它的 title attention is all unique你其实可以发现这篇文章它并不是发明了 attention很多农场文以为 attention是2017年这篇文章发明的其实不是这篇文章它的贡献不是发明 attention而是拿掉 attention以外的东西所以才叫做 attention is all unique那时候人们觉得只有 attention应该是没有办法好好做语言模型的他们把 attention以外的东西拿掉发现还是可以好好运作大家就惊呆了那 attention is all unique这篇 paper在它 introduction 里面它就说这个 attention 的好处也就是那时候他们就把 attention混在一起当中同一个东西讲这个 attention is all unique它第一个最大的好处是它在训练的时候可以更加平行化当然他们也得到了不错的结果什么叫训练的时候可以更加平行化呢那在这堂课里面我们没有花太多时间讲类神经广入的训练所以如果你想要深入了解是怎么被训练出来的除了看2022的课程以外你可以更深入看过去的课程过去有讲过 backpapagation那是一个一小时左右的路易那还讲了另外一个更深入的怎么自动做 backpapagation 的方法也就是 computation of graph那这个也是花了一个小时间左右的讲的因为上课时间是有限的所以后来这部分就没有放在实体的课堂上但如果大家有兴趣的话还是鼓励你把这些技术深入的去了解它那其实了解训练的原理对你的帮助就是你会更清楚为什么有些内卧架构是这样设计Tanformer 它真正的设计的好处并不是什么无穷常的 memory那个就是个假象那是个和路signation它真正的好处是训练的时候可以更容易平行化好 那我试在解释为什么它训练的时候可以更容易平行化好 虽然我们没有讲怎么真的训练内卧但是训练内卧基本的步骤是这个样子的假设我们今天要做的事情就是要教一个语言模型说输入偷恳Z1到ZT1然后你要产生ZT你要怎么教语言模型这件事呢第一个步骤是先算出目前的答案算出输入Z1到ZT1的时候语言模型目前的输出是什么直到目前的输出以后再跟正确答案计算差异计算完差异你才能够更新参数所以更新参数之前第一步只要能够计算出现有的答案所以你可以想像说计算出现有答案这件事情做得越快你就可以越有效的更新参数而Tanformer 它厉害的地方就是它可以快速的计算出现有的答案这件事是怎么做的呢假设我们今天要教语言模型说一句话大家好 我是人工智慧在概念上 你做的事情可能是这样的输入其实代表其实的符号然后告诉语言模型说输入代表其实的符号那你要输出大时正确答案输入代表其实的符号跟大输出家才是正确答案输入代表其实的符号输入大输入家好 是正确的答案这个是概念上的做法但是Tanformer有一个更有效的运算方法更有效的运算方法是什么呢我们今天教模型的时候我们会有一个完整的句子我们就叫教模型说出这个完整的句子大家好 我是人工智慧实际上的操作是这个样子的你把目标你把要让语言模型说出的这个句子你把你的光truth通通向右移动一个前面放其实的符号塞给语言模型语言模型可以平行的这个语言模型背后如果用的是Transformer的话Transformer可以平行的一次输出每一个时间点所有的答案你可以一次计算所有的答案跟正确答案的差异你可以一次看这么多的差异去更新参数所以Transformer厉害的地方是如果给他一个完整的句子他可以平行的计算出在每一个时间点语言模型这个Transformer要输出的下一个头肯是什么他可以一次平行计算出Begin of Sentence他会输出哪个头肯大后面会输出哪个头肯大家后面输出哪个头肯大家好会输出哪个头肯这一切是可以透过Transformer透过Self Attention的架构被平行计算出来的如果讲得更清楚一点的话我们来看看当我们用Self Attention的Layer的时候如果我们一次给Self Attention Layer一个完整的输入给他一整个C困执的输入不是一个一个头肯产生的不是Otah Progressive的情况下Self Attention会怎么运作你输入一个完整的C困C困执里面每一个头肯都会变成一个项量我们这边用X1到X6来表示X1到X6生成的步骤可以是平行的他们彼此之间并没有任何关联所以你可以平行的把X1到X6产生出来接下来Self Attention你可以平行的产生Y1到Y6因为Y1到Y6他中间生成的过程彼此之间没有任何关联性所以Y1Y2一直到Y6这6个项量是可以平行被生成出来的平行生成出来Y1到Y6之后你就可以平行的输出每一个时间点这个语言模型要把 next token的时候要猜下一个符号的时候他会输出的头肯是什么所以Self Attention他真正的好处是给一个完整的输入可以同时产生每一个时间点语言模型预测下一个 token的 token是什么给定一个完整的输入所以平行的输出每一个时间点接下来要输出的 token是什么所以这个是 transformer 的好处好 那另外一个观点来看这个 transformer你可以看发现说Self Attention 的计算是一个非常 GPU friendly 的计算是可以大幅应用 GPU效能的计算那有关这个 GPU在做语言模型的时候扮演什么样的角色下周往秀宣注教会有更严重的颇吸那在这堂课你就先记得说只要你做的是举证运算GPU就会觉得开心你讲他就会觉得他被妥善利用了好 那 transformer 其实就是一连串的举证运算怎么说 transformer 是一连串的举证运算好 现在绿色的这一排相量是输入的 X到是输入的 X1 到 X6好 那这个每一个 X呢每一排 X 呢成一个 transformation 变成每一个时间点的 query每一个时间点的 key 每一个时间点的 value接下来呢我们要算 attention 要算 inner product那怎么算 inner product把这个代表 key 的举证做 transpose直接沉上这个由 query 所组成的举证你就得到每一个时间点两两之间的 attention所以今天这个像两两之间做 inner product其实可以看作是两个举证直接相成不过有一些 attention我们是不要的因为我们假设说只有后面的时间点可以 attend到前面的时间点所以有一些 attention我们就直接不零我们就直接把它射为零那些是我们就算计算出来也不需要的然后呢你做一个 softmax得到另外一个 attention 的 matrix把这个 attention matrix在沉上由 value vector所组成的举证就得到输出了所以从输入 X 到输出 X通通都是举证运算那假设这个步骤你没有听得很懂的话反正你就记得今天你只要弄成 attention 的样子那你的运算的过程通通都是举证运算这是一个可以让 GPU 欢喜的运算过程可以有效的利用 GPU 的效能好 那另外一边我们就来看为什么 Rnn 没办法在训练的时候有效运用 GPU 的效能今天假设在训练的时候一次给 Rnn 完整的输入会发生什么事呢进入 Rnn 这一层之前的 X 关到 X 到你可以平行运算出来但是 Rn 本身是没有办法平行运算的输入 X 关之后你才能够产生 H 关有了 H 关才能有 H2有了 H2 才能有 H3 456所以你要计算 H66 你必须把前面 H 关到 H5 计算出来之后你才能计算 H6那这是 GPU 最讨厌的状态你就记得 GPU 讨厌等待那一些加速 GPU 的方法其实就是避免 GPU 去等待尽量不断的塞事情给 GPU 做像这种前面 H5 要算完才能算 H6 这个是 GPU 最讨厌的状态因为它必须要等前面的东西算完它没有办法发挥它平行化的优势好 所以你如果要让 Y 万到 Y6 一次产生出来你得把 H 万到 H6 都算出来以后你才能一次把 Y 万到 Y6 收算出来而计算 H 万到 H6 这个步骤是不容易平行化的它是一个没有办法有效运算的步骤所以现在我们可以比较Sale Attention 跟 R&N 流的做法你就知道说如果我们看 inference 的时候我们在使用这个语言模型让它产生一个序列的时候如果Sale Attention那你的计算量跟记忆体的需求会随著序列长度增加而增加而 R&N 计算量跟记忆体的需求是固定的但是Sale Attention 的好处是在训练的时候我们容易平行化容易发挥 GPU 平行化的能力而 R&N 它的坏处是它难以平行化所以在 2017 年到最近人们选择了Sale Attention加快训练的速度但是 R&N 难以平行化后面我们加了一个问号那这堂课就是要告诉你R&N 其实在训练的时候是可以平行化的Sale Attention 在 inference 的时候遇到长的 sequence就比较不理但今天人们需要长的 sequence今天语言模型都需要处理非常长的输入比如说做IG的时候语言模型从网路上搜寻了一大堆文章作为它的输入你需要长的输入或 AI Agent 要运算好几个回合跟环境不同跟环境一直的互动那也需要很长的 sequence今天又流行多模太的模型那多模太的模型跟处理文字的模型它背后的原理其实很像你只是把语音或者是把影像表示成偷坑 sequence而已但是语音跟影像表示成偷坑 sequence它会是比文字还要长的非常多的 sequence所以我们真的需要有效的处理长 sequence的方法那现在语音模型可以处理的 sequence越来越长然后以下这张图片是来自右下角这个连结就是说在2022年刚有GPT3.5的时候它大概只能读哈利波特的一个章节但到GPT4的时候它就可以读完审密的魔法时一直读到第二步那如果是靠2.1它就可以把哈利波特第一步跟第二步审密的魔法时跟消失的密式都读完如果是GPT5它可以读两个 million的偷坑它可以读200万的偷坑它不只可以把哈利波特第一集一直读到最后一集还可以几乎把魔界三步取看完所以现在这些语音模型我们希望它可以读很长的 sequence而attention这个方法在读长 sequence的时候是不利的所以人们就开始怀念起R&N的好所以有人就问了一个灵魂的扣问R&N训练的时候真的没办法平行吗我们来看看有没有让R&N平行的可能好 那我们先把h1到ht通通都列出来看看他们长什么样子好 右边是R&N最犯用的式子那h1是h0过fa加上h1过fbh2是h1过fa加上h2过fbh3是h2过fa再加x3过fb以此类推那因为算h2的时候知道h1算h3的时候知道h2所以没有办法做平行运算但真的是这样吗我们能不能把这些式子展开来看看我们就假设fahd是一个0 matrix所以我们就不考虑这一项那h1等于fb ofh1等于h1通过fb接下来我们已经知道h1是什么了我们把h1直接塞进去把h2展开让它的输入没有h1把h1用h1通过fb替代掉所以h2就变成h1通过fb再通过fa加上h2通过fb以此类推把h2带到h3的式子里让h3的输入没有h2所以就变成h3就是h1通过fb通过fa再通过另外一个fa那这边这个h2也要通过然后这个有点复杂在stu要通过fb再通过fa然后x3要通过fb总之你可以把整个式子通通都展开你可以把每一个h1到你可以把h1到h3展开让它们彼此不要有dependence但这样真的就能够平行运算了吗看起来是不行的因为展开到最后你会发现你的式子里面有非常长的连续的寒式的呼叫比如说到ht的时候x1要先通过fb要再通过fa再通过fa再通过fa提检1再通过fa提要通过一连串的fa才能够计算出答案这个也是太dependence的这个也是会让gpu等待的因为你要前一个寒式算完才能交给下一个寒式算这个地方也是不容易平行的但你会发现这些被呼连续呼叫的寒式都是fa既然是fa造成的问题我们能不能就直接把fa拿掉呢就直接把fa拿掉我们就说不要fa了这个an我们就把reflection的部分拿掉ht等于ht1加上ht过fb然后我们就把ht1到ht通通写出来h1是h0加x1过fbht2是h1加x2过fb以此类推然后我们再把h1带进去把h2带进去把h之间的dependence拿掉那我们计算出来的结果就是h1是x1过fb那ht2是x1过fbx2过fbh3是x1过fbx2过fbx3过fb然后看起来就比刚才的式子简单多了所以今天如果要算ht的话它就是ht像相加把x1到ht分别都过fb以后加起来就是ht了那像这样子的式子有没有办法平行呢其实是有一些加速的方法比如说你可以用一个叫sgenelgren的东西把这一排数字用一个更有效的方法把它算出来但是这个部分我们今天就先不讲因为其实还有更有效更简化更能平行的做法怎么做呢好 我们先简化一下式子的运用我们先简化一下我们的符号那我们先假设ht是一个第一层以第一的举证H可以是任何东西我们今天就假设它是一个第一层以第一的举证那既然H是一个举证那fb ofx也得是一个举证他们这样才加得起来那我们把fb ofxt用大低下标题来表示好 那我们就知道说H1就是ht1H2就是ht1加ht2H3是ht1D2加到D3ht就是ht1加ht2加到dt我们现在再做一下简化我们假设fc ofht就是把ht这个举证跟一个相量Qt做相成然后Qt是输入ht因为这个Qt它是跟t有关的那它怎么来的呢它是把xt沉上个举证w2Q得到QtQt沉上memory里面的东西ht就得到最终的输出y所以y1就是D1成Q1y2就是D1成Q2加ht2成Q2y3就是D1成Q3加到D3成Q3y2就是D1成Qt加到dt成Qt其实这一切都还有更有效的简化方法什么样的简化方法呢我们把dt写成vt沉上kt的transposev跟k分别是两个相量把v这个相量沉上k这个相量的transpose就是展开变成一个举证所以一个相量沉上另外一个相量transpose你得到一个举证我们叫做dt而vt跟kt是跟xt有关的xt沉上wv得到vtxt成Wk得到kt然后dt就是vt成上k的transpose我们现在可以把所有的D通通都自换掉所以y1就是v1K1的transpose成Q1y2就是v1K1的transpose成Q2加v2K2的transpose成Q2is the yt就是v1K1的transpose成Qt加到vtKt的transpose成Qt讲到这边你有没有发现了什么这个就是attention我们已经把符号把qkv通通放到4子里面了如果你还没有议会到为什么它就是selfattention的话那我们再继续做运算刚才我们是先把v1K1算出来再成Q能不能换一下计算的顺序先算k跟q呢先计算k的transpose成Q呢k的transpose是什么k的transpose是一个像亮的transpose成另外一个像亮假设他们的dimension是一样的所以可以直接做相成那你得到的结果就会是一个scaler我把k1Qt秘密为a法t1k2transposeQt秘密为a法t2以此类推我们先把k跟q相成剩下v那你得到的就是v1成个scalera法v2成另外scalera法vt成另外一个scalera法如果你不喜欢scaler放在相量后面那就把scaler放到相量前面这个不就是对一个叫做wave vectorv1到vt的东西做 wave t 上吗这其实就是selfattention它跟原来你知道的selfattention唯一的不同只是少了selfmax那因为它少了selfmax所以它跟原来selfattention还是有点不同所以它自己有一个名字叫做linearattention好所以讲到这边我们知道了什么讲到这边我们知道说linearattention就是rnn拿掉reflection我们把rnn拿掉fa拿掉reflection就变成linearattention而linearattention就是selfattention没有selfmax就是这么神奇那我们知道linearattention就是selfattention没有selfmax以后有什么好处呢linearattention这样子的rn它就是infranc的时候你用它来产生东西的时候它像是一个rnn但训练的时候你就展开把它当transformer来全它就transformer少了selfmax它完全可以套用transformer的平行化的方法来直接加速训练的过程所以假设刚才的讲attention的时候到底怎么加速的你其实没有听得很清楚的话也没有关系你知道记得selfattention是一个可以加速的东西长得像selfattention一样类似的这个泪落架构都可以用类似的方法来加速而linearattention根本就是selfattention拿掉reflection所以selfattention能怎么展开能怎么加速linearattention就能怎么展开能怎么加速训练的时候像是一个selfattention但是infranc的时候它就像是一个rn就是这么神奇好那也许讲到这边你会觉得说哎呀这个linearattention这边有一个奇怪怪的事子dt 陈以KT的transpose感觉没有非常的直观那我现在提供给你一个直观的解释来解释linearattention为什么这样运作好那linearattention为什么这样运作呢那在解释之前我们先定义一下这整个适值的dimension那在做linearattention的时候你需要先算出一个v算出一个k算出一个q我们假设q是一个低为的相量那它是由xt沉上一个举证得道的k 也是一个低为的相量xt沉上个举证得道的v是一个dpi为的相量xt沉上个举证得道的dpi可以等于低在刚才说明里面我都是预设dpi等于低但dpi其实是可以不等于低的只要q跟k的dimension一样就好了他们就可以做inupada其实v没有必要跟q跟k的dimension一样v的dimension可以是不一样好 那我们来看这个update那个memory的适值好 ht等于ht简易加上vt 陈KT的transpose把它画出来的话就是ht等于ht简易加上vt 陈VT的transposevt 陈VT的transpose是一个举证那我们等于就是把这个资讯加到memory里面vt 陈KT的transpose到底是什么意思呢我们来想想看vt 陈KT的transpose到底是什么这个举证它的每一个colon都是vt的背数这个举证绿色的举证每一个colon都是vt前面沉上一个scallon这个scallon的数值由k里面的数值所决定第一个colon的vt就沉上K这个项量的第一个dimension第二个colon的vt就沉上K这个项量的第二个dimension以此类推然后我们会把这个举证加到ht简易我们会拿这个举证来更新ht简易好 那这个vt是什么意思这个vt就是我们预计要写入memory写入heathensday etch的资讯而k值的是什么k值的是要把这个资讯写到哪里去举例来说假设k这个项量只有第二为是1其他为渡东市领那就意味著说我们把v这个资讯写到 etch 这个memory etch 这个heathensday它是一个magic这个magic的第二个colon当然你可以写到不只一个colon你可以把它用的资讯分散到不同的colon去所以你知道v是要被写进去的资讯而k它决定了这个资讯要被写进哪里是不是觉得这个式子其实也挺时关的呢好 那怎么得到输出输出就是把 etch 沉上Qt 得到 yt那这件事情的韩医是什么呢ht 我们刚才说它每一个colon就是存了不同的资讯有k来决定什么资讯要被存到哪里而Q就是把资讯从colon 从取出来Q决定要从哪一个colon取多少资讯假设现在这个Q是01000 做第二为是1那就是从第二为从这个 etch 的第二个colon把资讯取出来这个就是linear attention那其实linear attention 从来都不是什么新的想法linear attention 是20年的时候就已经知道的东西了最早就我所知道这个linear attention的paper应该是transformers are Rrn 这边paper所提出来了从它的标题你就知道它想干嘛它就想告诉你说transformer它发现原来就是rn它们两个只差了一个sofmax而已就是这么神奇所以这是20年就已经知道的事情然后在机器学习在门课的22年我们有讲过linear attention只那个时候是从另外一个角度来切入的那个时候是从transformer一路讲过来说transformer怎么简化变成linear attention这一次是反过来讲说rn 怎么变成linear attention就像是一个transformer一样好 那既然这个linear attention是20年就已经有了这个20年这个那时候应该现代智能还没有出现路西还在东非的大草原上那个时候人类已经就知道linear attention就是rn了但为什么现在linear attention没有被用的到处都是呢因为linear attention实际上干不赢transformer是干不赢self attention的那为什么linear attention干不赢self attention不光self attention很多人的解释是这个样子他说你看rnrn它的记忆就是很小但我刚才讲过rn的记忆并不小它的记忆有多大是你自己决定的你可以开一个很大的举证让rn有很大的记忆那也许你说那我就不说它记忆太小我说它记忆是有限的毕竟这个H的大小你开多大它终究是有个上线吧所以rn的记忆是有限的那你看transformer它可以attend到最开始的偷恳要attend多长就attend多长它有无限的记忆那rn记忆有限当然是没有问题也非常的直观怎么说rn记忆有限呢然后假设第一个时间点我们的k1是10000那我们就是把v1写到第一个课了k2是000就写到就把v2写到第二个课了k3是00000那就把v3写到第三个课了那h它假设最多只有第二课了的话最多只能够存第一个不同的v然后他们彼此之间不会受到感染再有更多的v你就要你就可能会重叠起来了就要存在这个重叠的memory里面你的memory就放不下那个你存弱资讯就会彼此互相感染了所以rn它能够储存的记忆有限是非常直观的但接下来我要告诉你transformer能够存的东西也是有限的怎么说transformer能够存的东西也是有限的呢假设现在q跟k的dimension是用小低来表示如果今天我们的seconds不长seconds比如说t现在是小于等于低的状态那没有问题你完全可以设计出一组key设计出一组key让你的memory之间彼此不会冲突我可以说k1是10000k2是0100k3是0100那如果我今天要把v2取出来的话我要把第二个时间点资讯取出来的话我只要q设成0100那010000只跟第二个位置的k算是1其他位置算出来的tension的数值都是0你就可以把v2原封不动的取出来但是这是在t小于等于低的状态下如果今天seconds非常长长过低了呢低你通常就设个4096那今天我们都希望t可以产生t可以处理非常长的输入比如说10000个偷恳你总是有机会遇到t是大于低的状态如果t大于低的时候会遇到什么样的问题呢你会遇到说明明你的q跟k2它是一模一样的你的query跟第二个位置的key是一模一样的那照理说我跟第二个位置的key计算inepada算出来的tension是1但是在整个漫长的c困中存在著某一个key这个key它在tply的地方出现它跟qt做inepada的时候它算出来的inepada是大于0的那为什么会这样呢那可以回去想想看想想看现实代数里面学过的东西在低为的空间中你最多只能够找到第一个ofsagonal的vector你找不到低加一个ofsagonal的vector所以当t很长的时候你总是会出现一些key这些key跟你的query就你别要req这个地方的资讯但是这个key就是跟你的query做inepada以后大于0那你今天req出来的资讯就是不同位置的wetissue你的模型的记忆就开始错乱所以今天attention它能够存的资讯也是有上线其实LinearTension存的资讯会有上线ShiftAttention存的资讯就会有上线因为它们其实一样的东西它们并不是不一样的东西它们真的就只差了一个softmax而已所以如果LinearTension赢不过chanceformer那差就是差在少了softmax所以看起来softmax算是一个简单的运作它其实蛮重要的好那LinearTension它最大的问题是什么呢LinearTension它最大的问题就是它的记忆永远不会改变你看每一次ht检1你不会做任何处理就会被放到ht那每一个时间点你都会输入xt过fb的资讯进入ht检1里面输入以后就永远进去了它就再也不会被做出任何改变了这样会造成什么问题呢这样会造成模型永远不会以往它输进去了东西就永远输进去了它永远不会改变那softmax为什么可以做到记忆的改变呢明明存的东西应该是一样的为什么softmax有机会做到记忆的改变呢以下是我的解释这个我们想像说现在是一个比较短的seconds然后呢只有三个位置你算出来的htension的位置是0.61跟0.4那你过softmax依这个位置对引导的过完softmax的htension的位置是0.45好那我们假设呢在过softmax之前Q跟K做inapada算出数值是1的话就代表这是一件很重要的事情好今天假设这个seconds变得比较长一样有一些位置算出算出htension的位过softmax之前的htension的位置是1它是重要的事情但一件事情有多重要取决于在同个seconds里面有没有更重要的事情如果更重要的事情本来见重要的事情它就变得不重要了因为在做softmax的时候整个seconds里面所有的htension是会共同被考虑的所有的htension之间是会互相影响的本来见很重要的事情如果出现更重要的事情它就变得不重要了好本来1过完softmax在这个seconds里面是0.45但这边htension的位置算出来是1但是出现更重要的事情htension的位算出来是2它出现1它的它不完爽max以后就变成只有0.17而已所以一个事情的重要性是相对于其他事情而言的就像在那个在那个暗影军团里面本来觉得艾恩很强但是后来又出现了一个监雅你就觉得监雅比艾恩更强后来就没有人再叫艾恩出来只会叫监雅出来后来我出现了BearBear又比监雅更强所以后来都是Bear出来了就是这样没有人知道在说什么算了我看了这个我独自升级以后我有一个巨大的发现我发现原来千合以入侵事件在日本官方的记载跟韩国官方的记载是非常不一样的我现在才知道原来这个梅路艾姆它就是Bear我现在才知道原来记周岛就叫做东果陀岛红的事件之前可以入侵事件在两个国家两个猎人协会官方的记载是非常不一样的那时候我可以问一下现在我独自升级的动画演到哪里了吗有人可以告诉我吗演打一语打一王了吗刚要打一王哇那以下有爆雷不想听得就把耳朵估起来但是我觉得那个接下来巨型应该都猜得到对不对现在要去打现在要去打好不想听得就把耳朵估起来现在要去打以王了对不对好后来就是韩国猎人那边有一个叫做忘记他的名字反正是一个信吹的人就发射很多火球然后同时间日本那边那个凿底克这个捷诺凿底克也发射了群新凿就引开了马尔的注意然后那些韩国猎人就潜入了以去就看这个以后然后就把以后杀了然后以后被杀了以后过一阵子以王才出现就把那些韩国猎人通通都打爬下去为什么以后被攻击的时候以后明被攻击了很久以后有八户位八户位团灭的时候以王都没有出现呢因为以王在另外一边被尼特罗残住了尼特罗不是他打了半天然后他杀尼特罗以后尼特罗就放了一个墙位把他炸手脚都断了然后就吸收了其他户位的能量结果又复原了然后这个时候他就发现了那韩国猎人他复原了以后虽然还是很弱因为韩国猎人太弱了所以他就回来晚间韩国猎人通通都打爬了然后来这个我独自升级的主角叫什么名字啊叫做叫成正语叫成正语成正语就出现然后又把梅路艾姆干爬然后把他改一个名字叫做贝尔就这样子这解释很多事你想看如果看日本官方那边的记载梅路艾姆被炸被那个核弹炸了以后他其实没死还复原了但是跟小麦下棋下著下著抱子抱子怎么回就死掉了日本官方的解释是和服色死了但你想以王这么强被炸弹四个手脚都会复活他怎么可能会和服色就死掉了原来是被成正语杀了然后这个故事我终于得到了我终于得到了满意的答案的科习可恤好没了就这样子这种之就是这么一个故事所以我们今天就学到说千颗一个事件在日本跟韩国记载是不一样的好 那我们既然说这个Linear tension的问题就是记忆不会改变那我们不能够就让他的记忆会改变呢比如说加上reflection的机制刚才为了平行化把reflection机制丢掉了好把reflection机制加进来让今天这个Linear tension可以逐渐以往所以有一个Network叫做reportationNetwork Ratnet他做的事情就是在HT简易的前面层上一个常数向叫做Gama这个Gama的作用就非常的直观他就是让一些过去的记忆逐渐被担望但这个Gama你要设小于1通常是涉淋到一支间你让Gama是淋到一支间那你就可以让记忆逐渐被担望好 所以在Inference的时候就是HT简易到HT中间要层上Gama那Trenning的时候呢对Trenning而言这边我们实际上就不推到你可以自己回去想看看对Trenning而言没有什么不同唯一的不同只有算Attention Weight的时候每一个Attention Weight后面要再层一个Gama的四方向Ava T10到1的Attention Weight要层上Gama T简易Ava T10到I的Tension Weight要层Gama T简I每一个Attention Weight且描成一个Gama的四方向他并不会影响平行化的过程所以加一个Gama没问题但是加一个Gama显示不够的每件事情都只会被担望我们其实希望模型有些事可以劳计有些事可以担望应该是依据不同的情境决定要劳计还是要担望所以后来就有一个Rannet更进阶的版本叫做Gated RetentionGated Retention就是把Gama改成Gama T也就是让Gama可以随著时间改变那怎么决定Gama T的数值呢那你就把HT层上一个存放Gama在过一个Sigma也让他输出是零到一之间就得到Gama T这个Gama他也是内握的参数也是需要被训练出来的那所以呢这个Gated Retention就是在HT简易到HT中间层上一个Gama TGama T是学出来的每次都不一样让模型决定什么东西要被寄的什么东西要被疑望也许看到一个换糖的符号在表现出一个新的段落他就决定要疑望Gama T就趋劲云林就可以把过去的资讯清空实际上在Trending的时候你要把这整个运作的过程展开回Self Attention的样子那怎么展开回Self Attention的样子呢就是在本来的QKV之外你都要多计算一个Gama那如果是Apati简易的话就在后面层上Gama i加1Gama i加2一直到Gama T所以你就是改变了原来Attention 位的计算那也不会影响你最终训练的时候平行化的过程但是这一些reflection的方法实在都太简单了有没有更复杂的方法呢有人就想说我们设计一个举证叫做GT这个GT会跟HT简易做Element Ys 相成这个符号代表Element Ys 相成如果我们可以把HT简易跟GT做Element Ys 相成我们就可以超空HT 简易里面每一个memory要被记得还是要被你忘但是如果这个GT没有做什么特别设计的话你其实就没有办法展开回原来Tension的样子了你在训练的时候加速就会有问题那这边直接告诉大家结论如果GT可以写成ET成像ST的Tension Post一样可以把这样子的reflection写回Self Attention的样子那后来在实作上有人比过了发现说这个ET呢直接设成一个都是E的项量就好就不需要再多决定ET是什么这样的结果跟ET是一个学出来的数值比起来是不会有什么太大的差距了这个E的意思就是一个项量里面所有的数值都是E好 那到底沉上这个GT把HT对GT做Element Ys 相成代表什么样的含义呢它的物理意义是什么呢我们来看一下这个GT它是E 沉上ST的Tension Post它到底是什么样子那这个GT它每一排每一排每一个Roll就是这个ST它每一个Colors的数值都是一样的但是同个Colors里面数值的大小取决于ST里面数值的大小我们把这样子的一个举证跟H做Element Ys 相成的时候它代表了什么样的意思呢假设在第一个DimensionST的第一个Dimension是0那相成之后就代表抹去这个Colors原来的记忆如果是E就代表记住这个Colors原来的资讯那如果是一个介于0到1之间的值比如说0.1就代表减弱这个Colors原来的记忆所以你看到GT呢它就是对HT里面每一个Colors存的Information它决定要爆料还是要减弱还是要抹去所以这个GT如果你把它写成E沉上ST的话它是满有物理意义的那事实上类似的设计非常非常多只能告诉你汉牛冲动那这是引用至某一篇Paper里面的Table还就告诉你说有这么多这么多的Nable架构他们通通都可以看作是广义的Recurrent Neural Network那这个表格里面用的符号跟我刚才用的符号略略有不同了它把Heathens Day用S来表示那我是用H来表示那你可以看到说这一些Nable架构的设计通通都是Heathens Day前面做的什么事在加上点什么得到新的Heathens Day新的Heathens Day在做点什么就得到最终的输出它要代表最终的输出好在这里面我们终于荷兰发现了漫把发现了漫把讲了这么久终于出现了漫把最漫把是什么呢你看漫把它是对ST检移做了一个非常复杂的Element Y色相成至于为什么会出现这么复杂的东西我们今天就不解释那你去看原始的论文你会发现漫漫读懂的因为它是从它是从Continuous Day Space的概念开始讲起跟你们类神经网路切入的点其实不太一样所以你不太清楚为什么它这样子设计不过这是第一代的漫把而且它这样子设计之后就会导致平行化有点问题所以它必须要用一个叫做Scan 的Elgurant来加速漫把的训练它没有办法像Sale Attention一样展开成只有纯粹的举证相成那后来到了Momba 2你会发现Momba 2其实跟Gate Retention是一样的它就把原来很复杂的设计改成成一个Time Dependent的Gama而已那在这一系列的类似的研究中当然Momba是最有名的大家通通都听过Momba我想它最有名的原因就是因为它是真的能打的而且它出来的右招Momba 1是在23年的年底出现的所以是原古时代就已经有的一个设计而在那个时代确实没有人打的in Transformer那个时候只有Momba Momba在那边Paper里面大力强调它是第一个真的能赢过Transformer的这种Leanier Attention的设计所以这个图上横走是不同大小的模型所以训练的Fast不太一样它这边是做到了一个最大的是一点多B的模型那重重是Propacity是你就想成Propacity越小代表模型越好那橙色的这一条线是Transformer加加就Transformer再加了一些改进比较好的Transformer现在比较常用的一个Transformer然后Momba是紫色的这一条线然后Momba是可以在多数状况下都微幅赢过Transformer加加的然后整个Momba Paper就大力炫耀这件事所以这是第一次这种Leanier Attention的架构可以赢过真正赢过Transformer那Momba当然这个这些Leanier类似Leanier Attention架构的设计它最大的目的就是在Inference的时候要加速那Momba当然提供了非常强大的加速那这个图上这个蓝色跟绿色代表的是Momba橙色跟红色代表的是Transformer重走是每秒可以处理多少偷肯在Inference的时候大家可以发现绿色跟蓝色的Bot都远高于橙色红色的Bot所以Momba可以比Transformer在Inference的时候有更好的加速前年年底出现的时候引起了一波轰动大家都觉得这个Transformer的挑战者出现了那时候常常做的一个梗图就是有一个巨大的食人是Momba然后其他下面贵的都是Transformer我现在才照额那个图是出自我独自升级的更图那另外一个想跟大家分享的这个Leanier Attention的变形呢叫做DeltaNetDeltaNet的式子你如果诈悍之下发现它写的奇奇怪怪的HT1是IdentityMetric减掉Beta成上KT 成上KT的Transpose加BetaK 成上BT 成上BT的Transpose后面这一项你现在已经不会有觉得奇怪了吧但前面怎么会有KT 成上KT的Transpose呢到底想要搞什么到底想要做什么事情那这边我们来解释一下为什么DeltaNet是这样设计这个是最原始的Leanier Attention这是最原始的Leanier Attention然后呢DeltaNet它想要做的设计是我们现在把资讯放到Memory里面但是Memory在同样的位置它也有存一些资讯呢我们能不能够把前面的资讯把它先去除掉所以它就看说现在KT到底要放资讯到哪些KL然后把本来存在H里面那些KL的资讯把它去除掉它是怎么做的呢它先去计算一下本来KT要去放置的那些KL里面到底存了什么样的V所以它就把HT简移存上KT得到另外一个项量这边用VO的来表示VO的是什么意思呢VO的就是原来存在HT简移里面的资讯如果我今天要用KT把它写入的话原来存在HT简移里面的资讯长什么样子然后呢我们现在先剪掉VO的先把本来要存进去的资讯先把本来要存进去的位置的那些资讯先剪掉然后再把新的资讯加进去所以它就像是以往的过程我们本来要把资讯放到Memory的某个地方先把Memory清空才好放进新的资讯我们前后都成一个Beta代表说我们没有要清到非常空没有要清到0的清一定程度就好了至于要清多少程度这个Beta有一个下标K代表说它也是让Nave我自己决定的然后呢把VO用HT简移沉上KT把它直接带进去带进去以后就结束了你把这个狮子稍微整理一下你就可以得到上面这一个狮子但是这边真正想要跟大家分享的是一个脑洞大开的观点这个Delta那接下来神奇的地方是这个狮子这两个狮子前面都有Beta K后面都有KT的Trends pose把它们提出来所以DeltaNet更新HT的狮子又可以写成HT等于HT简移简调Beta K沉上HT简移成KT简调VT再沉上KT的Trends pose这边都是简单的举证运算你可能觉得没有什么神奇的但有人闹门一开说这是规定Decenn这个狮子是一个规定Decenn的狮子H就是一种神秘的参数它其实是Memory但这边我们把它当参数来看等好左边的HT是规定DecennUpdate后的参数等好右边的HT简移是Update前的参数Beta K这边应该写K吗其实应该好像应该写T彩腿不过没有关系这个不是很重要这个Beta是一个LeninRate就是规定Decenn的LeninRate而Beta后面沉的正向HT简移KT简VT沉上KT的Trends pose就是Gradient再来问题是它是谁的规定呢它是设一个Loss方向的规定这个Loss方向如果你把H当作参数对它上Gradient的话正好就会是红色底线的这一个狮子而右边这个Loss它到底代表什么含义呢右边这个Loss是把KT沉上H再减掉VT取它的弄平方再成2分11这2分之间很重要这只是微分的时候要让你结果比较好看而已HKT减掉VT的弄平方是什么意思它是希望H这个Memory我用KT当作是query从里面取资讯出来的时候取出来的结果要跟VT越接近越好所以今天我们每一个TiStack我们会update memory里面的内容怎么更新Memory的内容呢更新的方向就是要让用KT取出的资讯跟VT越接近越好所以但这件事情其实也很直观因为我们现在就是用KT跟VT把资讯放到H里面我们当然希望接下来用KT可以把VT完整的抽取出来然后这边有一个Homolation告诉我们说这个H更新的方向就是要让用KT当作query可以从H里面把VT取出来代表我们今天把VT放进去的时候我们接下来是可以用同一个KT把它取出来的那事实上原来的NineAttention这个你也可以看做规定Decenn只是看做规定Decenn以后它的LT是另外一个LT就是了那用这个方法就有人发明了Titent就是Titent这个今年1月的时候不是有一个很红的东西叫做Titent然后它的Titent告诉你说这是Learning to Memorize at testing time它标访的就是它有一个神奇的memory这个memory相关的参数是在模型一边做Inference的时候一边会改变的其实它用的就是DeltaNet的概念把原来memory存的数值当作是参数把memory update做reflection这件事情看做是做Gradient Decenn这个就是Titent Learning to Memory at test time大家有兴趣的话再去研究这篇文章这是今年年初的文章好 那像现在像这种梦把类型的NineW啊最早他们都只划了一些比较小的模型所以不知道它是不是能真的使用但是现在已经有比较大的梦把类型的模型比如说JembaJemba是背上梦把的一个Language Model它比较大的版本有52B今年年初有一个模型叫MiniMax01MiniMax01也是用了Learning to Tension它是400多B的那种大模型所以Learning to Tension真的可以用来训练巨大的语言模型也可以达到跟现在成former一样好的结果那这些Learning to Tension的概念也不只是用在文字上它也用在影像上那最近有一个影像深层的模型叫做SenaSena它标榜的就是它非常的快它模型非常的小其中它用到的一个概念就是Learning to Tension还用了Learning to Tension在Infantasy之后加快Sena这个NaveWo当然在做影像处理的时候并不是加上Learning to Tension总是好的有一篇paper叫做Mamba Out从它的开头就知道它想要讲什么这些paper标题是We really need Mamba for VisionMamba Out这边paper在开头就完了一个梗它就说Covid Brian说过What can I say?Mamba Out结束了这什么意思呢就是Covid Brian在退休的时候它讲的最后一句话就是Mamba OutMamba其实就是Covid Brian的错号它的错号就是Black Mamba我还想说为什么大家讲Mamba的时候都是放个舌呢其实感觉放Covid Brian其实是可以的总之这边paper就是告诉你它就玩了个梗告诉你说Mamba也不一定要用在影像上它说过去有很多有用影像的Sena Sensor就是Learning to Tension那一种NaveWo架构现在有很多这种影像版本的Mamba它们就故意把影像版本的Mamba里面的Mamba拿掉然后看看会不会比较好我们发现在分类的任务上Mamba Out的模型表现是比其他模型要好的所以看起来Mamba在影像分类上并没有帮助但是其实你要注意这篇文章并不是觉得Mamba不需要因为你想看这篇文章它是把类似这种Tension的架构都拿掉它真正表明的是在分类的问题上你不见得需要用到Tension这种架构因为Tension它的目标就是考虑非常大的范围在分类的问题上可能不需要考虑这么大的范围只要用CNN就足够了在同边Paper里面它其实又有说有Mamba的架构在其他影像的任务上比如说影像分割上面还是可以做的比没有这种Self-attention的架构或没有Mamba架构做的还要好的那今天另外一个流行的趋势就是因为现在那些Off the Shelf的Languagesmodel比如说Lama都非常的巨大所以如果你想要做这种利念Etension有关的研究你自己设计Nave我的Architecture你自己训练你是怎么训练都干不赢Lama的所以怎么办呢所以现在那个流行的趋势就是不要再从头做起如果你只是想要研究把Self-attention改一个样子会怎样那你何不从现有的语言模型比如说Lama开始FineTube你就把它里面的Self-attention拿掉直接换成Mamba然后再FineTube看看Mamba有没有办法发挥作用或甚至很多人会尝试说把Self-attention里面的一些参数保留下来直接加到新的设计里面这样让新的Nave需要穿的参数越少越好看看你能不能有些设计能赢过传统的语言模型那这边就引用了大量的无论文在从标题里面可以知道他们在做什么比如说的Mamba 引的Lama或是Fomerate to SSN他们就是从一个现有的Self-attention-based language model开始做起那微调中间跟Self-attention有关的Layer那可以展示说插入一些attention的变形还是有用的那最后几两页投影片呢就是其实线上有一个赌局这个赌局赌的是attention all-you-need这个赌局是说attention是从2017年开始10年之后到2017年人们是不是还会觉得attention是最强的Nayua架构那现在你验证这个语言插656天然后这个语言就是想说到2027年1月1号的时候到底Self-attention transformable-based架构是不是还是爸爸的支持这个选项的人是 Jennifer Franco他是这个哈佛的教授也是mostatic L.N.的 Chief Scientist反对这个Proposal的人是Sasharoof他是Coneure的教授也是HuggingFace的research Scientist他们就是赌应该是赌各式在公司里面的股份这样子对 有什么压抑吗然后看看最后2027年的时候到底transformer还会不会继续爸爸Sasharoof其实他也是个YouTuber其实从他的影片里面学到很多东西所以这边把他的名字放在这边也推荐给大家另外一个奖机器学习的YouTuber
