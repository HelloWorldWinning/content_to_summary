Timestamp: 2025-03-13T09:28:44.296085
Title: Anthropic CEO：18 個月內 AI 撰寫所有程式碼、所有工作被 AI 取代即在眼前
URL: https://youtube.com/watch?v=PxDvXmOJO6I&si=3M3Be38AaGHNds_Q
Status: success
Duration: 1:02:35

Description:
## Summary of Dario Amodei's CFR CEO Speaker Series

**Core Point:** AI's rapid advancement presents both immense opportunities and existential risks, requiring careful management, ethical considerations, and proactive policy-making to ensure a beneficial future for humanity.

**Overarching Framework:**  The discussion revolves around the exponential progress in AI capabilities, the need for responsible scaling policies, potential societal impacts (jobs, economics, security), and the importance of international collaboration and national security measures in navigating this transformative technology.

**Outline:**

1.  **Anthropic's Mission and Values:**

    *   Founded due to concerns about OpenAI's approach to AI safety and responsible development.
    *   Core values: prioritizing safety, predictability, and ethical use of AI.
    *   Examples: Investing in mechanistic interpretability, developing Constitutional AI, implementing a Responsible Scaling Policy.
2.  **Responsible Scaling and AI Safety Levels:**

    *   AI Safety Level 2 (Current): Risks comparable to other technologies.
    *   AI Safety Level 3 (Approaching): Risks pose serious national security threats, e.g., enabling unskilled individuals to create bioweapons.
    *   Mitigation: Preventing models from providing dangerous information, enhancing security to prevent theft.
3.  **Opportunities and Upsides of AI:**

    *   Potential for exponential progress in biology, medicine, and neuroscience.
    *   Solving complex diseases like cancer, Alzheimer's, and schizophrenia.
    *   AI as a "country of geniuses in a data center," capable of performing remote work and tasks across various modalities.
4.  **Impact on Jobs and the Economy:**

    *   AI's increasing ability to automate coding and other tasks.
    *   Potential for both augmenting human productivity and eventually replacing human labor.
    *   Concerns about societal division if AI disproportionately displaces certain jobs.
    *   Need to rethink the concept of "usefulness" and find alternative sources of meaning in a world where AI can perform most tasks.
5.  **Deepseek and the Scaling Laws:**

    *   Deepseek's emergence highlights the falling cost of producing AI models and the increasing global competition in the field.
    *   Reinforces the scaling laws: more compute, more data, better algorithms lead to more capable AI.
    *   US export controls and investments are essential to maintaining technological leadership.
6.  **US-China AI Dialogue:**

    *   Supported, but not overly optimistic due to the economic and military potential of AI.
    *   Potential for limited collaboration on mitigating the risks of autonomous AI behavior.
7.  **Action Plan for the US Administration:**

    *   **Security:**
        *   Maintaining export controls on advanced chips.
        *   Government testing of models for national security risks.
        *   Protecting US AI companies from industrial espionage.
    *   **Opportunity:**
        *   Accelerating AI applications in healthcare through regulatory reform.
        *   Provisioning more energy for data centers in the US and allied countries.
        *   Managing economic disruption and considering tax policy reforms to address wealth distribution.
8.  **Ethical and Social Considerations:**

    * Lack of public awareness regarding the scope of AI advancements.
    * Need to shift focus of human worth from creation of economic value.
    * Concerns about welfare and potential sentience of AI models.

<Mermaid_Diagram>
```mermaid
graph LR
    subgraph Anthropic
        A[Mission & Values] --> B(Safety, Predictability, Ethics);
        B --> C{Mechanistic Interpretability};
        B --> D{Constitutional AI};
        B --> E{Responsible Scaling Policy};
        style A fill:#ADD8E6,stroke:#333,stroke-width:2px
        style B fill:#E6E6FA,stroke:#333,stroke-width:2px
        style C fill:#F0FFF0,stroke:#333,stroke-width:2px
        style D fill:#F0FFF0,stroke:#333,stroke-width:2px
        style E fill:#F0FFF0,stroke:#333,stroke-width:2px
    end

    subgraph Responsible Scaling
        F[AI Safety Levels] --> G{Level 2: Moderate Risk};
        F --> H{Level 3: National Security Risk};
        H --> I[Mitigation Measures];
        style F fill:#B0E0E6,stroke:#333,stroke-width:2px
        style G fill:#FAF0E6,stroke:#333,stroke-width:2px
        style H fill:#FAF0E6,stroke:#333,stroke-width:2px
        style I fill:#F0FFF0,stroke:#333,stroke-width:2px
    end

    subgraph AI Opportunities
        J[Upsides of AI] --> K{Biology & Medicine};
        J --> L{Automation & Productivity};
        K --> M[Disease Solutions];
        L --> N[Economic Growth];
        style J fill:#90EE90,stroke:#333,stroke-width:2px
        style K fill:#F0FFF0,stroke:#333,stroke-width:2px
        style L fill:#F0FFF0,stroke:#333,stroke-width:2px
        style M fill:#F0FFF0,stroke:#333,stroke-width:2px
        style N fill:#F0FFF0,stroke:#333,stroke-width:2px
    end

    subgraph Societal Impacts
        O[Societal Impacts] --> P{Job Displacement};
        O --> Q{Economic Disruption};
        O --> R{Ethical Concerns};
         style O fill:#D3D3D3,stroke:#333,stroke-width:2px
         style P fill:#FAF0E6,stroke:#333,stroke-width:2px
         style Q fill:#FAF0E6,stroke:#333,stroke-width:2px
         style R fill:#FAF0E6,stroke:#333,stroke-width:2px
    end

    subgraph Global Competition
        S[Global AI Race] --> T{Deepseek};
        S --> U{Scaling Laws};
        U --> V[Compute & Data];
        V --> W[Technological Advancement];
        T --> X[US Export Controls];
        style S fill:#FFA07A,stroke:#333,stroke-width:2px
        style T fill:#F0FFF0,stroke:#333,stroke-width:2px
        style U fill:#F0FFF0,stroke:#333,stroke-width:2px
        style V fill:#F0FFF0,stroke:#333,stroke-width:2px
        style W fill:#F0FFF0,stroke:#333,stroke-width:2px
        style X fill:#F0FFF0,stroke:#333,stroke-width:2px
    end

    A --> F
    A --> J
    F --> O
    J --> O
    S --> O
    O --> X
```
</Mermaid_Diagram>

Content:
well good evening everybody Welcome My name is Mike Froman I'm president of the council and it's a great pleasure to have you here tonight for uh one of our CFR CEO speaker series and to have the CEO and co-founder of anthropic Dario Amade with us uh tonight Uh Daario was vice president of research at OpenAI where he helped develop GPT2 and three and before opening uh joining OpenAI he worked at Google Brain as a senior research scientist Um I'm going to talk with Dario for about 30 minutes Then we'll open it up to questions from people here in the hall We have about 150 people here We have about 350 online and so we'll try and get uh some of their questions uh in as well Um welcome Thank you for having me So uh you left OpenAI to start Anthropic the a mission first public benefit corporation Um why leave What are anthropic's core values And how do they manifest themselves in your work And let me just say a cynic would say well this mission first this is all marketing you know how can you can you give us some specific examples of how your product and strategy reflect your mission So yeah if I if I were to uh uh you know just just back up and kind of set the context um you know we left at the end of 2020 I think in 2019 and 2020 something was happening which I think myself and a group within OpenAI which eventually became my co-founders at Enthropic were I think among the first to recognize Um uh they're called kind of you know scaling laws or the scaling hypothesis today And the basic hypothesis is simple It says that and it's it's really really remarkable thing and I can't overemphasize how unlikely it seemed at the time If you take more computation and more data to train AI systems with relatively simple algorithms they get better at all kinds of cognitive tasks across the board And we were we were measuring these trends back when models cost $1, 000 or $10, 000 to train So that's a kind of an academic grant budget level Um and we forecast that these trends would continue even when models cost 100 million a billion 10 billion dollars to train Um which which now we're getting to And indeed that if if the quality of the models and their level of intelligence continued they would have huge implications for the economy It was even the first time we realized that they would h likely have very serious national security implications Um we generally felt that the leadership at OpenAI was on board with this general scaling hypothesis although you know many people inside and outside were not But the second realization we had was that you know if the technology was going to have this level of significance we really needed to do a good job of building it Um we we we re we really needed to get it right Um in particular on one hand these models are very unpredictable They're inherently statistical systems One thing I often say is we grow them more than we build them They're like a child's brain developing So the controlling them making them reliable is very difficult The process of training them is not straightforward Um so just from a system safety perspective making these things predictable and safe is very important And then of course there's the use of them the use of them by people the use of them by nation states the effect that they have when when companies uh uh uh deploy them And so we we really felt like we needed to build this technology in in in you know absolutely the right way Um you know OpenAI a bit a bit as you've alluded to was was founded with some claims about you know that that that they would do exactly this Um but for a number of reasons which I won't won't get into in detail We didn't feel that the leadership there was taking these things seriously and so we decided to go off and and do this on our own And the last four years have actually been a kind of you know almost a sidebyside experiment of you know what happens when you try and do things one way and what happens when you try and do things the other the other way and how how it has played out So I'll you know I'll give a few examples of how you know we've really I think displayed displayed a commitment to these ideas Um one is we invested very early in the science of what is called mechanistic interpretability which is looking inside the AI models and trying to understand exactly why they do what they do One of our seven co-founders Chris Ola um uh is is the founder of the field of mechanistic interpretability This had no commercial value or at least no commercial value for the you know the first you know four years that we we worked on it It's just starting to be a little bit a little bit in the distance But nevertheless we had a team working on this the whole time in in the you know in the presence of fierce commercial competition because we believe that understanding what is going on inside these models is a public good that benefits everyone and we published all of our work on it so others could benefit from it as well Um I you know I think I think another example is uh we came up with this idea of constitutional AI which is training AI systems to follow a set of principles uh uh you know instead of training them from from data or from you know mass data or or human uh uh human feedback um you know this allows you to get up say you know in front of you know congress and and and say these are the principles according to which we trained our model Um when we first came to uh uh you know when we had our first product you know our first version of claude which is our model we actually de delayed the release of that model roughly 6 months because this was such a new technology uh that uh you know we just we weren't sure of the safety properties we weren't sure we wanted to be the ones to kind of kick off a race this was just before chat GPT um so you know we arguably had the opportunity to you know to seize the chat GPT moment Um and you know we we we chose to release a little later which I think had real commercial consequences but set the culture of the company Um a final example I would give is uh we were the first to have something called responsible scaling policy Um so uh what this does is it it measures categories of risk of uh models as they scale Um and we have to take increasingly strict security and deployment measures as we as as we meet these points And so we were the first one to release this the first one to commit to it And then a few months within a few months of when we did the other companies all followed suit And so we were able to set an example for the ecosystem Uh and you know when I look at what the other companies have have done we've often led the way on these issues and often cause the other companies to follow us Not always sometimes they do something great and we follow them Uh but but I think you know there's there's been a good um there's been a good history of us you know sticking to our commitments and I you know I would contrast that with what we've seen from some of the other companies in their in their in their behavior We now have several years of years of history and you know so far fingers crossed I think our commitments have held up pretty well I want to talk about both the risks and the opportunities that you've that you've cited around AI Let's but since you mentioned the responsible scaling uh issue let's let's go back to that We're at level two now Yeah So at what level is it existential um what how will we know when we hit level three and if you hit level three can you go backwards or does it only get worse Yeah So the way our responsible scaling policy is set up is we we basically said you know and the analogy was to biosafety levels So you know the biosafety level system is like you know these are how dangerous various pathogens are Um and so we said let's have AI safety levels And so AI safety level two is a level we're currently at And that's you know systems that are that are powerful but the risk they pose are comparable to the risks that uh you know other kinds of technology pose Um ASL3 um which actually I think our models are starting to approach The last model we released we said this model isn't ASL3 yet but it's getting there ASL3 is characterized um and you know we focus very much on the national security side very kind of serious risk that are out of proportion to the risks that normal technologies have So an ASL3 model is designed as one that could allow you in the areas of say chemical biological or radiological uh uh weapons could allow an unskilled person simply by talking to the model and following its instructions to do things that you would have to have say a PhD in viology to do today So once that is possible if those risks aren't mitigated then that would enhance the number of people in the world who who are able to do these highly destructive things from say in the 10, 000s today to in the in the tens of millions once uh once the models are available And so when the models are capable of this we have to put in mitigation so that the models are not willing to actually you know provide this information and uh security uh restrictions so that you know the models the models won't be stolen And I I you know I think we're approaching that We may actually hit that uh uh uh this year And we believe we have a story for how to deploy those kinds of models safely by you know removing their ability to do this very narrow range of dangerous tasks without compromising their commercial viability So this is a fairly narrow set of tasks as you say You're just going to prevent the model from answering those questions Yeah Prevent the model from from engaging in those kinds of tasks which is it's it's not straightforward right who can say you know I'm taking a viology class at Stanford University I'm working on my coursework Like can you you know can you tell me how to make this particular plasmid And so the model has to be smart enough to not fall for that and say hey you know actually that isn't the kind of thing you would you sound like a bioteterrorist I won't answer You sound like you have bad intent Yeah But it's it's sort of limited to your own imagination or our own imagination as to what all the bag acting could be There are a lot of things that we may not anticipate beyond those four categories Yeah I mean uh you know I think I think this is an issue that that just as every time we release a new model there are positive applications for it that people find that we weren't expecting I expect there will also be negative applications we are we always monitor the models for different use cases in order to discover this so that we have a continuous process where you know we don't get taken by surprise if if you know we're worried that someone will do something evil with with model 6 um uh Hopefully some early signs of that can be seen in model 5 We monitor it But but this is the fundamental problem of the models Um you don't really know what they're capable of You don't truly know what they're capable of until they're deployed to a million people You can test ahead of time You can you know you can you know have your researchers bash against them You can have you know even the government we collaborate with the the the the the government AIS test them But but the hard truth is that there's no way to be sure They're not like code where you can do formal verification What they can do is unpredictable It's just like you know if I if I think of you or me instead of the model you know if if I'm like the quality assurance engineer for me or you you know can I can I give a guarantee that like you know there's a particular kind of bad behavior you you are logically not capable of will never happen Um I I this people don't work that way Let's uh let's talk about the opportunities the upside opportunities Absolutely Uh end of last year you wrote an essay Machines of Loving Grace uh that talked about some of the upside how one could achieve a decade's worth of of progress in biology for example in a in a year how the machines were going to be as smart as all the Nobel Prize winners which probably depresses some of them Um tell us the upside Tell us your your best case scenario as to what AI is going to produce Yeah So so I'd go back by starting with the exponential You know if we go back to 2019 the models were barely able to give a coherent sentence or a coherent paragraph People like me of course thought that was an amazing accomplishment that models were not capable of And you know we had these predictions that 5 years from now you know the models are going to be generating billions of dollars of revenue They're going to be helping us code We can talk to them like they're like they're human beings They'll know as much as humans beings do And there were all these unprincipal objections of why that that couldn't happen Um you know the the same exponential trends the same arguments that predicted that predict that if we go forward another 2 years 3 years maybe four years we will get to all of this We will get to models that are as intelligent as Nobel Prize winners across a whole bunch of fields You won't just chat with them They'll be able to do anything you can do on a computer basically any remote work that humans do any modality being able to um you know do tasks that take days weeks months Um the kind of evocative phrase that I used for it in machines of love and grace was it's like having a country of geniuses in a data center Like a country of genius remote workers which they can't do everything right There are restrictions in the physical world And I think this this still sounds crazy to many people Um but you know look back on previous exponential trends You know look at look at the early days of the internet and how wild the prediction seemed and and what actually came to pass I'm not sure of this I would say I'm maybe 70 or 70 or 80% confidence Um you know it could could very well be that the technology stops where it is or stops in a few months and you know the essays that I've written and things I've said in in um you know in in in in in in events like this people spend the next 10 years laughing at me but uh but uh uh that would not be my bet Let's uh let's just build on that one because on the issue of jobs and the impact that AI is likely to have employment there's a there's a pretty big debate Where are you on the spectrum Well before I get there how long will it take for AI let's say to replace the head of a think tank I'm asking for a friend Actually how we won't get to that one That's uh where are you on the spectrum of everyone's going to be able to do some really cool things and they're going to be able to do so many more things than they're able to do now versus everyone's going to be sitting on their sofa collecting UBI Yeah So I I think it's going to be a really complicated mix of those two things that also depends on the policy choices You can also answer the think tank question if you like but yeah So I I I mean I guess I didn't I I I kind of you know ended my answer to the last question without saying all all the great things that'll happen So the I honestly the thing that makes me most optimistic before I get to jobs is things in the biological sciences um biology health neuroscience Um you know I think if we look at what's happened in biology in the last hundred years um what we've solved are simple diseases Solving viral and bacterial diseases is actually relatively easy because it's it's the equivalent of repelling a foreign invader in your body Um dealing with things like cancer Alzheimer's schizophrenia major depression These are system level diseases If we can solve these with AI at a baseline regardless of kind of the job situation we will have a much better world and I think we will even if we get to the mental illness side of it have a world where it is at least easier for people to find meaning Um so I'm very optimistic about that if if but now getting to kind of the job side of this Um I I I do have a fair amount of concern about this Um on one hand I think comparative advantage is a very powerful tool If I look at coding programming which is one area where AI is making the most progress um what we are finding is we are not far from a world I think we'll be there in three to six months where AI is writing 90% of the code and then in 12 months we may be in a world where AI is writing essentially all of the code but the programmer still needs to specify you know what what what are what what are what are the conditions of what you're doing what you know what what what what is the overall app you're trying to make What's the overall design decision How do we collaborate with other code that's been written Um you know how do we have some common sense on whether this is a secure design or an insecure design So as long as there are these small pieces that a programmer a human programmer needs to do that the AI isn't good at I think human productivity will actually be enhanced But on the other hand I think that eventually all those little islands will get picked off by by AI systems and then we will eventually p reach the point where you know the AIs can do everything that that humans can Um and I think that will happen in in every industry I think it's actually better that it happens to all of us than than that it happens you know that it kind of picks people randomly I actually think the most society divisive outcome is if randomly 50% of the jobs are suddenly done by AI because what that means the societal message is we're picking half we're randomly picking half of people and saying you are useless you are devalued you are unnecessary Um and instead we're going to say you're all useless Well we're all going to have to have that conversation right like we're gonna we're we're we're going to have to we're gonna have to look at what is technologically possible and say we need to think about usefulness and uselessness in a different way than we have before Right Our current way of thinking has not been tenable I don't know what the solution is but it's it's it's got to be it's got to be different than we're all useless Right We're all useless is a is a nihilistic answer Um we're not going to get anywhere with that answer We're going to have to come up with something else That's not a very optimistic picture is what it is I I I actually you I would actually challenge that Um you know I um you know I think about um a lot of the things that I do Um you know I spend I spend a lot of time for example swimming I spend time uh playing video games Um I look at like human chess champions Um you know you might think when uh uh Deep Blue beat Kasparov and that was almost 30 years ago that after that it would be like chess would be seen as a pointless activity Um but exactly the opposite has happened Uh human chess champions like Magnus Carlson are celebrities I think he's even like a fashion model Um uh like he's he's like this this kind of hero So I I I I I think there's something there where where we can we can we can build we can we can build a world um where you know uh human human life is meaningful and and humans perhaps with the help of AIs perhaps working with AIs build really great things So I am not that I am actually not that pessimistic but if we handle it wrongly I I I think there's maybe not that much room for error Uh couple months ago we had Deepseek being released uh in this town There was a fair degree of panic I would say around that People talked about it as a Sputnik moment Uh was it a Sputnik moment And what does it teach us about whether those scaling rules that you laid out about needing more compute more data better algorithms whether that's those rules still apply or whether there are some shortcuts Yeah So um uh deep deepseek I think actually was rather than refuting the scaling laws I think deepseek was actually an example of of of the scaling laws So two dynamics I I had a post about this but uh uh two dynamics are going on at the same time One is that the cost of producing a given level of model intelligence is falling roughly by about 4x a year This is because we we are getting better and better at uh uh you know kind of kind of algorithmically producing the same results with less In other words we're shifting the curve You can get for you know a year later you can get you know a as good a model as you could get a year ago spending 4x you can get a four 4x better model by spending the same amount But uh what that means economically is that whatever economic value the current the the you know model of a given intelligence has the fact that you can make it for 4 forex cheaper means we make a lot more of it and in fact provides additional incentive to spend more money to produce smarter models which have a higher economic value And so even as the cost of producing a given level of intelligence has gone down the amount we're willing to spend has gone up in fact has gone up fast something like 10x a year despite that 4x a year increase right That's been eaten up and more by by just society The economy wants more intelligence It wants more intelligent models Um so so that is kind of the backdrop for deepseek and deepseek was literally just another data point on the cost reduction curve Um it was nothing unusual It wasn't like these US companies are spending billions and Deepseek did it for a few million Um the costs were not out of line They spent yes a few million on the model So so what US companies spend is not out of line with that They like us spent billions on all the R&D and effort around around the around the model If you look at how many chips they have it's it's roughly on par Now I do think it's concerning because uh up until recently there were only three four maybe five companies that were part of this curve that could produce frontier models and they were all in the US Um Deepseek is the first time the the thing that really is notable it is the first time a company in China has been able to to go toe-to-toe and produce the same kind of engineering innovations as companies like Anthropic or OpenAI or Google That is actually very significant and that actually worries me Now some argue that the emergence of deepseek means that export controls don't work can't work we should stop trying to control the export of our most advanced chips Others say it means we should double down on export controls Where do you stand on that Yeah So um you know I think I think it's an implication of the framework I just gave that uh the export controls are a are actually quite essential because yes there's this cost reduction curve but at at every point along the curve no matter how much the curve is shifted it is always the case that the more chips you spend the more money you spend the better model you get right if it's like you know okay before I could spend a billion dollars and get a model that was okay Now I can spend a billion a billion dollars and get a model that's much better and I can get an okay model for for for $10 million That doesn't mean the export controls failed That means stopping your adversaries from from getting a billion dollar model just became a higher stakes thing because you can get a smarter model for a billion dollars And yes deepseek was you know they had relatively small you know relatively small amount of compute consisting of chips that went around the export controls some chips that were smuggled But I think we're heading for a world where we OpenAI Google are building billions maybe tens of millions of chips costing tens of billions of dollars or more It's very hard for that to be smuggled if we put in place export controls we actually may be able to stop that from happening in China Um whereas if we if if if if we don't I think they may be at par with us Uh and so you know I was I was a big supporter of the diffusion rule I've been a big supporter of export controls for several years even even before deep seat came out because we saw this dynamic coming And so I think it's actually one of the most essential things ac not just AI across all fields um for the United States national security for us to prevent China from getting millions of these very powerful chips The diffusion rule as I understand it divided the world This is a Biden administration EO that divided the world into three camps and as to who could get access to to what in terms of uh uh chips uh from us Some worry that the the countries that are not in the top tier are just going to be served by China and that China is going to end up running the AI infrastructure for the vast majority of the world Yeah To you our our my understanding of the diffusion rule and you know my my my my understanding is the new administration is is looking at it but there are many parts that they're that they're sympathetic to The way it actually sets things up is these tier 2 countries So tier one countries are like the majority of the developed world but not all Tier not all Tier tier three is you know restricted countries like China or Russia Uh tier two are you know countries countries in the middle Um actually you can have a very large number of chips in those countries if the companies hosting them are are are able to uh provide security affidavit and guarantees which basically say we are not a front company for China We are not you know shipping the compute or what is done with the compute to China Um and and so there really is an opportunity to build a lot of US chips a lot of US infrastructure in these countries as long as they comply with the security restrictions I think the second piece of it is yes in theory companies could chi could switch to using Chinese chips Um but Chinese chips are actually quite inferior Um Nvidia is way ahead of Huawei which is the main producer of um uh which is which is the main producer of of chips for China Like something like four years ahead I think that gap is going to close over eventually over the course of I don't know 10 or 20 years Uh probably the export controls may may even have the you know the the impact of stimulating China But the the the tech stack is so deep and I think the next 10 years during which we will stay strongly ahead in hardware um are actually the critical period for establishing dominance in this technology which I would argue whoever establishes dominance in this technology will have military and economic dominance everywhere The last administration launched a dialogue with China about AI What are the prospects for such a dialogue where where could we possibly agree with China and do they care about responsible scaling Yeah So I you know I would describe myself and of course I wasn't part of any of these conversations but I I heard a little about them I would describe myself as supportive of this dialogue but not especially optimistic that it will work Um so you know the technology has so much economic and military potential that you know between companies in in the US or our democratic allies you can imagine passing laws that create some restraint when it's just like two sides are racing to build this technology that has so much economic and military value perhaps more than everything else put together It's it it's it's it's it's hard to imagine them slowing down significantly I I do think there are a few things One is this risk of the the AI models autonomously acting in ways that are are not in line with with human interests Right If you have a country of geniuses in a data center a natural question how could you not ask this question well what is their intent What do they plan to do You would certainly ask well is someone controlling them Are they acting on someone's behalf But you would also ask well what is their intent And because we grow these systems we don't train them I don't think it's safe to assume they'll do exactly what their human designers or users want them to do So I think there's there's real risk of that I think it could be a threat to to all to you know to to to kind of all of humanity and like issues of nuclear safety or nuclear proliferation there's probably some some opportunity to take limited measures to help address that risk Um uh so I'm I'm relatively optimistic that maybe something something narrow could be done the stronger the evidence is of that coming You know right now that's a kind of speculative thing but if if strong evidence came that this was imminent then maybe more collaboration with China would be possible Um so you know I'm I'm I'm hopeful that we can try and do something in this space but I I don't think we're going to change the dynamic of of national national competition between the two Last question before we open it up uh you've recently presented to I guess OSTP an action plan a proposed action plan for the new administration what they should do in this area what are the main elements of that plan yeah so I think there's three elements around kind of security and national security and three elements around opportunity so the first one is what we've been talking about like making sure we keep these export controls in place like I I I honestly believe this is the across all areas not just AI the most important policy for the national security of the United Um the second thing is is something actually related to the responsible scaling plans which is um the uh the US government through the AISI has been uh uh basically testing models for national security risks such as bi you know biological and nuclear risks Um you know the the the institute is probably misnamed You call it safety institute It makes it sound like trust and safety but it's it's really about measuring national security risks And we don't have an opinion of exactly where that's done or what it's called But I think some function that does that measurement seems very important It's also important even for measuring the capabilities of our adversaries like you know they can also measure deepsee models to see what dangers they might they might present particularly if those models are used in the US like what are they capable of What what might they do that's dangerous So that's number two Number three on the risk side is something we haven't talked about which is I am concerned about industrial espionage of the companies in the US companies like Anthropic um uh you know China is known for largecale industrial espionage we're doing various things there are things in our responsible scaling plan about like better and better security measures um but uh you know many of these algorithmic secrets they're hundred million dollar secrets that are a few lines of code um and you know I'm sure that are folks trying to steal them and they may be succeeding Uh and so more help from the US government in in helping to defend our companies against this risk is very important So those are the three on the security side On the on the opportunity side um you know the the the the I I think I think the main three there are one is um the potential for the technology in the application layer in things like healthcare I think we have an extraordinary opportunity as I said to cure major diseases major complex diseases that have been with us for hundreds or thousands of years and that we haven't been able to do anything about yet I think that will happen one way or another but regulatory policy really could affect you know does it take 5 years for AI to help us produce produce all those cures and distribute to the world or or does it take 30 years And that's that's a big difference for people who suffer from those diseases Um so so so you know our our view here is that the policies of today around health care around FD FDA approval of drugs may may not be appropriate for the fast progress we're for the fast progress we're going to see and we may want to clear away some blockers Um the second is energy provision If we're going to stay ahead of China in this in this technology and other authoritarian adversaries um we need to build data centers And it's better if we build those data centers in the US or its allies than if we build them in countries that have divided loyalties where they could literally just abscond with the data center and say "Oh sorry We're on China's side now. " Um uh uh and so uh you know some of this was done during the the latter days of the Biden admin And I think it's a bipartisan thing I think that the Trump admin uh uh you know this is one one area of agreement Um there's there's interest in provisioning a lot more energy We probably need across the industry maybe 50 gawatts of additional energy by 2027 uh to full fully power you know AI that has all the properties we've been talking about 50 gawatts for those who don't know is about how much energy was added in aggregate to the US grid in 2024 Um uh so by that year you know we need as much as you know half as much as being is being added in the next two years So it's it's it's it's really g it's it's it's really um going to take a lot And then the final thing is the economic side of things Um you know as as as we talked about you know I think I think the worries on the economic side are just as existential as the worries on the national security side Um you know in the short run we're going to need to manage the disruption even as the pie gets much larger Um you know in the long run as I've said we're going to need to think about a world where AI and I don't want to lie about this where I I really think where this is going is that AI is going to be better than almost all humans at almost all things We have to reckon with that world as soon as possible For now we I think we just need to um you know the best thing we can do is measure to understand what's going on We released this thing called the anthropic economic index that in a privacy preserving way looks through uh you know and summarizes our usage to understand you know in in what fields are people using it Is it augmentative Is it is it replacing But in the long run you know we're we're really going to have you know this is going to implicate questions about about tax policy and the distribution of wealth right there there's this um you know there's this kind of alluring world where if the pie grows enough there could be the resources to do a lot about this you know like let's say and this this will sound crazy to this audience but let's say AI causes the economic growth rate to be 10% a year then suddenly the tax base is growing so much that like you can erase the deficit and and you know maybe maybe maybe have all this all this left over to manage the probably enormous disruption that that comes from the technology So that will that will sound like crazy town but like I I just invite you to consider the hypothetical and and and start considering the possibility of of crazy things like that Now crazy town You heard it here first Okay let's open it up to uh uh to questions Yes right here in front Uh thanks Dario This has been a really fascinating conversation Should I stand You should stand Okay Uh I will stand Get my steps in and just say who you are Uh I'm Adam Bunkado So I I had a so I enjoyed reading machines uh your essay last year and then hearing you on hard fork on the times but also hearing this and so the question I have for you is you sort of outlined sort of the political and economic sort of implications but I'm curious to get a sense of like are you how have you thought about the social and moral sort of kind of considerations that are going to effectively come uh especially because I think most of the general public sort of sees some of the chat bots sees some of this and says "Oh it's an an improved Google search but doesn't really think about sort of the sort of downstream effects of the disruption in the labor market and the like. " And so I'm curious to get a sense of how do you sort of think about that in tension with sort of building a company trying to build a commercial product Yeah Um so first of all I mean you know I I I think this stuff is super important And perhaps the most the thing that's disturbing me the most right now is the lack of awareness of the scope of what the technology is likely to bring I mean I could just be wrong I'm saying a bunch of crazy stuff like like you know the answer could just be the general public is right and and and like I'm wrong I'm high on my own supply I acknowledge that is possible Um uh uh but but let's say it's not the case What I'm seeing is is there are these concentric circles of people realizing how big the technology could be There's probably maybe a few million people very concentrated in Silicon Valley but a few people high in the policy world who also hold these beliefs Again we don't know yet whether we they are are are right or wrong but if we are right the whole population again thinks of this stuff as chat bots if we say this is dangerous if we say this could replace all human work it sounds crazy because what they're looking at is something that in some cases seems pretty frivolous Um but but they don't know what's about to hit them And so I think I think that's I that that actually keeps me up at night a lot and and is why I'm kind of trying to spread the message to to more people So I think awareness is step one Um I I think these questions around human labor and human work in in a world where it is technologically possible to replicate the effects of the human mind I think these are very deep questions I don't feel like I have the answer to them I I feel like you know as as you said these are these are kind of moral questions almost you know o almost like questions about purpose You could even say spiritual questions right Um uh and and so we are all going to have to answer these questions together I mean I'll I'll I'll I'll give you kind of the embryo of an answer I have which is that somehow the idea of of humans self-worth the tying of that to the ability to create economic value Um there are aspects of that that are deeply embedded in our psychology but there are aspects of that that are cultural Um you know there's a lot of things about that that that work well It's created a modern participatory economy But technology as it often does may kind of lay bare that illusion It may be another moment like you know the moment we realize that the earth rotates around the sun instead of the sun rotating around the earth or you know there were many many solar systems or organic material is not made up of different molecules than inorganic material Um uh so we just may have one of those moments and there there may be a reckoning and again my answer is I am struck by how meaningful activities can be even when they are not generating economic value I am struck by how much I can enjoy things that I am not the best in the world at Um if if if the requirement is you have to be the best in the world at something in order for it to be somehow spiritually meaningful for you I feel like you've taken a wrong turn Like I feel like there's some there's something wrong embedded embedded in that assumption And I say that as someone who spends a lot of time trying to be the best in the world at at you know at something I think is really important But but somehow we're we're our our source of meaning is going to have to be something other than that Uh uh yes Cam Thanks uh Cam Kerry at the Brookings Institution Um one of the things that leapt out at me in the uh from the UK AI safety report uh is the possibility that come 2030 or so that scaling up may run out of data How do you then scale How do you make the models smarter um and what are the limitations of that data I mean there's tremendous amount of text uh video information that's that's that's digitized a tremendous amount that resides in our minds and in the universe that is not Yeah So a couple answers on this one is that in the last 6 months there have been some innovations actually not developed by us uh you know first first came from uh uh open AAI actually um uh but others that that we have made um that uh obiate the need for uh as much data as we need before these are the so-called reasoning models where they basically they have thoughts um they start to think through the answers to complex questions and then they they train on kind of their own thoughts Um you can think about how humans do this where sometimes I can learn things by you know I'll make a plan in my head and then I'll I'll think about it again and I'll say oh actually you know on second thought that doesn't really make much sense like what are what are you thinking right um uh and then you kind of learn something from this of course you also have to act in the world you also have to act in the real world but AI have not been making use of that that kind of cognition at all until recently um so far that's mostly applied to tasks like math and computer programming Um but my view without being too specific is that it's not going to be terribly difficult to extend that kind of thinking to a much wider range of tasks Um the second point is even if we do run out of data in 2030 if the exponential continues for even two or three more years it may get us to a point where we're we're kind of already at the at the genius level And uh you know that that may be enough for a lot of these changes And we may also be able to ask the models hey we had this problem Um human scientists weren't able to solve it Can can you help us solve this solve this problem Um I I do still give a small likelihood that for whatever reason both of those things won't work out or aren't as they appear and data could be one of the plausible things that could that could block us I thought it was a very plausible blocker one or two years ago Um I thought one or two years ago if something would stop the show this was in the top three of the list of things that would Um but but uh I think I think uh my potential skepticism here has has been uh not completely refuted but but but I think reasonably well refuted What are the top three things that could stop the show Um uh so uh actually at this point I think the number one thing that could stop it would be an interruption to the supply of GPUs Um if if for instance uh the small uh uh disputed territory where all the GPU is produced uh had some military conflict that would certainly do it Um uh uh um uh I think another thing would be um if there's a large enough disruption to the stock market that messes with the capitalization of these uh companies basically a a kind of a kind of um belief that the technology will not um uh um you know move move forward and that kind of creates a self-fulfilling prophecy where there's there's not enough capitalization and and third I would say if I or we the field are kind of wrong about the the the promisingness of this new paradigm of kind of learning learning from your own data Um if if somehow it's not as broad as it seems um or or just there's there's more to getting it right that we think there are some insights missing We'll go to a online question We'll take the next question from Esther Dyson I recognize that name Miss Dyson please unmute your line Thank you Apologies Uh Esther Dyson writing a book called term limits on term limits for people and for AIs and so forth I have a question about this whole existential risk thing It seems to me that the bigger risks honestly are humans who are even more unexplainable than AIs but humans and their business models using AIS And then specifically there's the famous paperclip problem where you ask the AI to make paper clips and it does that to the exclusion of anything else And it this this is slightly metaphorical but the world seems to be going mad for data centers And it it really is kind of draining resources from everything else to fund data centers AI data pools whatever And so the in a sense AI is creating a fitness function for society that is I think harming the value of humans which is not just their intellectual capacity Um that's the end of the question Thank you So you know I I I would say just just as there are many different benefits of of AI and every time we produce a new AI every time we produce a new AI model it has you know a long list of 10 benefits that we we anticipated and then a bunch more that we didn't like every time we release a new model there's like new use cases and customers were like I didn't even think of doing that with an AI system It is unfortunately also the case that like there's you know we shouldn't say this risk is distraction from that risk It it just unfortunately is the case that there are many different risks to the AI systems and if we want to get through this we somehow have to deal with them all So I think it is a big risk that humans will misuse the AI systems I think it is a big risk that the AI systems themselves we may have difficulty controlling them Again to use the analogy of a country of geniuses and data center we plop down a country of you know 10 million geniuses in you know some Antarctica or something Um uh we're going to have multiple questions about what that will do to humanity You know we're going to ask well who's you know is is some existing does some existing country own them Is it is it doing their bidding and and what will that do you know are are the benefits are are you know is is the outcome of that beneficial we'll say you know are there are there individuals who could misuse it and we'll say what are the intentions of the you know of that country of geniuses itself and then to get at the question you asked near the end like are there kind of more distributed societal things like I certainly believe that if you know more and more of the world is more and more of our energy is devoted to AI systems like you know it'll be great they'll do things really efficiently but like you know also could that make some of our existing environmental problems worse Like I think that's a real risk And then you can say well will the AIs be better at helping us to solve our environmental problems So we spend a bunch of energy and then the AI the AI systems you know it it kind of you know it it it turns end up better better than we started if we're able to solve it So I'm optimistic that that will be the case but that's like another risk Like a number of things have to be true for it to turn out that way So I I you know I just think we're at a time of great change and and therefore um uh uh you know it you know we have to make extraordinarily wise choices to uh to to to to to get through it I mean you know I recognize the name of the person uh uh uh asking the question and I I might I might get this wrong but I think it was your I think it was your father who said I I listened to a video of him cuz I cuz I I was I was a physicist Um and I listened to a video of him where you know he said we have all these problems today and we um you know you know it seems like we can't solve them but you know I I remember I remember in my day you know we you know it it really seemed like we had all these you know severe severe uh you know problem you thinking of just World War II or the Cold War or nuclear annihilation and somehow we made it through So it doesn't mean we will again but but um uh yes the woman in the back There we are Hi my name is Carmen Dominguez Uh I'm an AI specialist with a background in development implementation Uh and more recently focusing a bit more on the policy side I hear loudly clear on the lack of awareness generally of what is AI and what is not AI and what it can and cannot do Um but I'm going to skip over that I do some science communication around that too But my question today is around um a few months ago you brought on Kyle Fish as a AI welfare researcher to look at you know sentience or lack thereof of future AI models uh and whether they might deserve moral consideration and protections in the future Uh I would if you could talk a bit about that that reasoning for that and if you have an equivalent human welfare research team going Thanks Yeah So this is this is another one of those topics that's going to make me sound completely insane Um uh uh so I it is actually my view that you know if we build these systems and you know they differ in many details from the way the human brain is built but the the count of neurons the count of connections is strikingly similar Some of the concepts are strikingly similar I I have a I have a functionalist view of you know moral moral uh welfare of the nature of experience perhaps even of consciousness Um and and so I think we should at least consider the question of if we are building these systems and they do all kinds of things like humans as well as humans um and seem to have a lot of the same cognitive capacities If it quacks like a duck and it walks like a duck maybe it's a duck uh uh and and we should really think about you know do these things have you know real real real experience that's meaningful in some way If we're deploying millions of them and we're not thinking about the experience that that that that they have and and they may not have any is a very hard question to answer um it's it's something we should we should think about very seriously And this isn't just a philosophical question I was surprised to learn there are surprisingly practical things you can do Um so you know something we're thinking about starting to deploy is you know when we deploy when we deploy our models in their deployment environments um uh uh just giving the the model a button that says I quit this job that the mo that the model can press right it's just some some kind of very basic you know preference framework where you say if if if hypothesizing the model did have experience and that it hated the job enough giving it the ability to press the button I quit this job Um if you find the models pressing this button a lot for things that are really unpleasant you know maybe may maybe you should pay some doesn't mean you're convinced but maybe you should pay some attention to it Sounds crazy I know it's probably the craziest thing I've said so far Right in the back there Trooper Yeah Hi Trooper Sanders You uh talked about the excitement of AI and medical science biology chemistry etc Sorry if you could say what is there any excitement around the social sciences So you know most of health care is done outside of the pillbox and the exam room Public health involves a number of other areas Can you say anything about that side of things Yeah I mean if I think about epidemiology um you know when I was in grad school there was a project being done by the Gates Foundation to use kind of you know mathematical and computational methods around epidemiology Um I think they were you know planning to use it to help eradicate malaria polio other areas the quantity of data that we get and the ability to pull all the pieces together and understand what's going on in an epidemic I bet that could benefit hugely from AI Um the clinical trial process um we've already seen things like this So so actually this is something anthropic has done with with Nova Nordisk the maker the maker of uh Ozmpic and other uh other other drugs Uh at the end of a clinical trial you have to write a clinical study report um you know it summarizes adverse incidents does all the statistical analysis to present to the FDA or other regulatory agencies for whether to approve the drug Typically this takes about 10 weeks um they've started using our model for this and the model takes about 10 minutes to write the clinical study report and humans take about three days to check it and the quality at at least as we've seen in early studies that doesn't determine everything has has been deemed to be comparable to what to what uh humans are able to do with the with the with the 10-we process so we need to do clinical trials there's a lot of social science problems around that there's a lot of regulatory problems I write about that in a bit in the essay that I think those things are going to are going to the the the thing that that limit the rate of progress But even within things like clinical trials I think the AI systems will be able to help a a lot in if not dissolving those questions at least radically simplifying them Yes right here Here's a microphone coming I'm Louise Shel I'm an expert on elicit trade from George Mason University Next week there is a global summit of the OECD on illicit trade but what you've talked about is not what I expected to hear on this problem of smuggling of parts and it's on no one's radar screen What happens when you're talking about it because it's not reaching the community that is needing to protect this illicit trade I didn't hear the last part of the question So uh u how come these issues aren't on the agenda of those people concerned about elicit trade Yeah Um uh you know I I yeah I think I think my answer to that is it you know it should be on the radar of those people Um you know again I have a worldview here that not everyone shares and I may be right or I may be wrong but all I can say is if this worldview is correct then you know we should be worrying a lot more about smuggling these GPUs than you know we're worried about smuggling you know guns or even drones or fentinel or whatever Um yeah Um uh but you know if if you were to smuggle five million of these to China and to you know to be clear that's you know that's like$20 billion dollars of value or something like that um you know that would that would drastically change the national security balance of the world I think it's the I think it's the most important thing Um so you know again this is this is this is a dilemma of am I just crazy or does the world have a big big awareness problem here And if the world has a big big awareness problem here then a downstream consequence of that is we're focusing on all these other things And you know because when you say elicit trade there's certain things that people have been focusing on for a long time This is a new thing but that doesn't mean it's not the most important thing Um so many good questions I'm sure Uh this gentleman right here thank you Alan Raw practicing lawyer lecturer at Harvard Law School and future useless person So so are we all So I'd like to follow up on your various comments on national security You mentioned the artificial intelligence security institute and its testing the Biden executive order on AI had mandatory reporting of acquisition or development of super capable 10 to the 26 flop dual use foundation models Um but um my question is um how do you engage how does you know anthropic the AI community developers of these super capable models how do they engage with let's just say the na the US national security community the intelligence community and u practically you know what does that mean for the development of AI and if you tell me you'd have to kill me I don't need to know that badly Yeah So um I think there are a few things here Um one is typically Enthropic in particular although the other companies have have started doing similar things Whenever we develop a new model we have a team within Enthropic called the Frontier Red Team Some of this is happening you know testing with the AI safety and security institutes But you know we we work in collaboration We developed some stuff they developed some stuff but the general flow has been when we test the models for things like biological risk or cyber risk or you know chemical or radiological risk will typically go to people in the national security community and say hey this is where the models are at in in terms of these particular capabilities uh you know you guys should know about this because you know you're the ones who are responsible for detecting the bad actors who would who would do this with the models You know what they're capable of now You might therefore have a sense of what the models can can do that is additive or augmentative to their current capabilities Right The part of it we miss is like you know we're not counterterrorism experts We're not experts on all the bad guys in the world and what their what their capabilities are and what you know large language models would add to the picture and so we've had a very productive dialogue with them on on on these issues The other topic on which we've talked to them about is the security of the the the you know the companies themselves uh uh you know this was one of the things in our in our kind of OSTP submission of you know making this more formal making this something the US government does does as a matter of course but you know if we're we're worried that we're going to be attacked digitally or you know or um you know via via kind of you know human means insider threat then you know we'll often talk to the national security community about that um and I think the you know the the third kind of interaction is you know about the national security implications of the models right We've been you know these these things that I'm saying publicly now I've been you know I've been I've been saying them in some form to some people for for quite a while Um uh then I think the fourth thing is there's an opportunity to apply the models uh to enhance our national security Um this is something that I and anthropic have been supportive of although we want to make sure that there's the right guard rails right on on one hand I think you know if if we don't apply these technologies for our national security we are going to be defenseless against our adversaries on the other hand I think everyone believes that there should be limits um you know I don't think there's anyone who thinks you know we should we should uh uh you know hook up the uh hook up AI systems to nuclear weapons and let them fire nuclear weapons without humans being in the loop That's the plot of Doctor Strange Love Um uh yeah that is that is literally the plot of Doctor Strange Love Um uh uh so somewhere between there there is there is some like you know there's some ground and you know we're kind of still working on defining that It's one of the things where we hope to kind of be be leaders in defining what the appropriate use of AI for national security is But that's that's another area where we've had interaction with the national security community your comment a couple minutes ago about trying to understand the experience of uh the AI models has been sort of sinking in for me So let me just conclude with one final question which is in the world that you envisage uh what does it mean to be human Yeah Um you know I my I think I think my picture of it the the the thing that seems the thing that seems most maybe there are maybe two things that seem most human to me Um the first thing that seems most human to me is you know struggling through our relationships with other humans our obligations to them Um you know how we have to treat them the the difficulties we have in our relationships with other humans and how we overcome those difficulties um you know when I when I think of you know both things that people are proud of doing and the biggest mistakes people have made they almost always relate they almost always relate to that and AI systems maybe can help us to to do that better Um but I think that will always be one of the quintessential challenges of being human And I think maybe the the second challenge is is you know the ambition to do very difficult things which which again I I will repeat I I think will ultimately be unaffected by the existence of AI systems that are smarter than us and can do things that we cannot do I I again think of like human chess champions are still celebrities um you know I can you know I can learn to swim or learn to play tennis and the fact that I am not the world champion does not does not negate the meaning of those activities Um uh and you know even even things that I might do over over 50 years over a hundred years um uh you know uh I want those things to to to retain their to retain their meaning and and you know the ability of of humans to to to strive towards these things not to not to give up Um uh you know I again I think I think I think those those two things are maybe maybe what I would identify Please join me in thanking Dario Amadai for spend time with us
