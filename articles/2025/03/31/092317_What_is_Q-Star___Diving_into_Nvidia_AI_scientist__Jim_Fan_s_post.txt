Timestamp: 2025-03-31T09:23:17.036517
Title: What is Q-Star? (Diving into Nvidia AI scientist, Jim Fan's post)
URL: https://youtube.com/watch?v=rDXrZTMfgDk&si=qvFhX52s14wuYn_X
Status: success
Duration: 22:59

Description:
好的，这是根据您提供的英文文本提炼和总结的核心观点，并按照要求进行了结构化、识别核心结论、定义总体框架，并生成了相应的 Mermaid 概念图。

### **核心观点摘要 (Simplified Chinese)**

**1. 背景：Q*项目引发的关注**
*   最近关于 OpenAI 的 Q* (Q-star) 项目引发了大量猜测。
*   据推测，Q* 是导致 OpenAI 董事会解雇并重新聘用 Sam Altman 的“导火索”。
*   虽然具体细节未知，但两位知名 AI 研究者（Jim Fan 和 Nathan Lambert）的分析指向了相似的技术方向。

**2. Q* 的核心技术推测：超越简单语言预测**
*   **目标：** 提升大型语言模型（LLM）解决复杂问题（尤其是数学问题）的能力，使其具备更强的推理和规划能力。
*   **类比“思维快与慢”：** 当前 LLM 的“下一步预测”模式类似系统1（快速、直觉），而解决复杂问题需要系统2（慢速、审慎、逻辑）。Q* 旨在让 LLM 实现更审慎的“规划”能力 (Yann LeCun 的观点)。
*   **关键技术组合推测：**
    *   **思维树 (Tree of Thoughts, ToT)：** 相较于线性的“思维链”(Chain of Thought, CoT)，ToT 允许模型探索问题的多个可能解决路径（分支），形成一个决策树。
    *   **奖励模型 (Reward Model)：** 为了高效地在巨大的“思维树”中搜索，需要一个模型来评估不同路径（分支）的优劣，类似于 AlphaGo 中的价值网络/策略网络，指导搜索方向。
    *   **过程监督奖励模型 (Process-Supervised Reward Model, PRM)：** 与仅奖励最终结果的 ORM 不同，PRM 在推理的 *每一步* 都提供反馈（如人类评分），让模型知道哪一步是好的，哪一步错了，从而更有效地学习推理过程。OpenAI 的 "Let's verify step-by-step" 论文探讨了这一点。
    *   **强化学习 (Reinforcement Learning, RL) / Q-Learning & 自博弈 (Self-Play)：** 借鉴 AlphaGo 的成功经验，Q* 可能利用 RL，让模型通过“自博弈”产生大量合成数据进行训练。模型（生成器）产生推理步骤，奖励模型进行评估，形成一个自我改进的闭环，可能使其能力超越基于人类数据的训练极限。

**3. 潜在的“飞跃”与担忧**
*   **“永动机”式的自我提升：** 将 LLM（生成器）、思维树搜索、过程奖励模型和强化学习（自博弈）结合，可能创造出一个能不断自我提升、超越人类能力的系统。
*   **超人数学能力：** 如果 Q* 在数学推理上取得突破，可能达到超人水平。
*   **现实世界影响：**
    *   **破解加密：** 理论上，超强的数学能力可能破解当前依赖计算难度的加密算法（如 AES），对金融（加密货币）、网络安全等造成颠覆性影响。
    *   **通往 AGI 的一步？** 这种强大的自主学习和推理能力被一些人视为通往通用人工智能（AGI）的关键一步，引发了对潜在风险的担忧。

---

**核心结论 (Core Point):**

推测认为，Q* 项目通过整合思维树搜索、过程奖励模型和强化学习等先进 AI 技术，旨在赋予大型语言模型（LLM）强大的、可持续自我改进的复杂推理能力（尤其在数学领域），这可能导致其性能超越人类水平，并引发对潜在社会影响（如加密安全）和 AGI 风险的广泛关注。

---

**总体框架 (Overarching Framework):**

该内容描述了一个旨在**突破当前 LLM 能力限制的先进 AI 框架**，其核心是**整合**：
1.  **强大的生成模型 (LLM)** 作为基础推理引擎。
2.  **复杂的规划与搜索机制 (Tree of Thoughts)** 来探索解题路径。
3.  **精细化的学习反馈系统 (Process-Supervised Reward Models)** 来指导和优化推理过程中的每一步。
4.  **自主学习与迭代改进循环 (Reinforcement Learning / Self-Play)** 来产生超越人类标注数据的经验，实现能力的持续指数级增长。
这个框架的目标是实现**更深层次的逻辑推理和规划能力**，特别是在形式化和复杂的领域（如数学），并可能**催生出具有超人能力的 AI 系统**。

---

**Mermaid 概念图 (Conceptual Map):**

<Mermaid_Diagram>
graph TD
    subgraph "背景与推测 (Context & Speculation)"
        A1["OpenAI 董事会风波"] -- "触发因素 (Triggered by)" --> B["Q* 项目 (Project Q*)"];
        A2["研究者分析 (Researchers' Analysis) <br>Jim Fan, Nathan Lambert"] -- "理论推测 (Theorized about)" --> B;
        style A1 fill:#f9f,stroke:#333,stroke-width:1px,color:#333
        style A2 fill:#f9f,stroke:#333,stroke-width:1px,color:#333
        style B fill:#ffcc00,stroke:#333,stroke-width:4px,color:#000
    end

    subgraph "Q* 核心技术推测 (Speculated Core Technologies)"
        B -- "可能整合 (Potentially Integrates)" --> C{"大型语言模型 (LLM)<br>GPT-4 (Generator)"};
        B -- "可能整合 (Potentially Integrates)" --> D{"思维树 (Tree of Thoughts)<br>多路径规划/搜索"};
        B -- "可能整合 (Potentially Integrates)" --> E{"奖励模型 (Reward Model)<br>评估路径/步骤"};
        B -- "可能整合 (Potentially Integrates)" --> F{"强化学习 (Reinforcement Learning)<br>Q-Learning, 自主学习"};
        B -- "可能整合 (Potentially Integrates)" --> G{"过程监督 (Process Supervision)<br>PRM, 逐步反馈"};
        style C fill:#ccf,stroke:#333,stroke-width:2px,color:#000
        style D fill:#ccf,stroke:#333,stroke-width:2px,color:#000
        style E fill:#ccf,stroke:#333,stroke-width:2px,color:#000
        style F fill:#9cf,stroke:#333,stroke-width:2px,color:#000
        style G fill:#9cf,stroke:#333,stroke-width:2px,color:#000
    end

    subgraph "技术关联与机制 (Technical Relations & Mechanism)"
        C -- "生成推理步骤 (Generates Reasoning Steps)" --> D;
        D -- "需要评估指导 (Needs Evaluation/Guidance)" --> E;
        G -- "训练/优化 (Trains/Optimizes)" --> E;
        E -- "指导搜索 (Guides Search in)" --> D;
        F -- "通过自博弈/经验 (Via Self-Play/Experience)" --> H["生成合成数据 (Generates Synthetic Data)"];
        H -- "用于训练 (Used for Training)" --> C & E & F;
        F -- "实现自我改进循环 (Enables Self-Improvement Loop)" --> B;
    end

    subgraph "灵感来源与对比 (Inspirations & Comparisons)"
        I["AlphaGo"] -- "强化学习/自博弈/搜索 (RL/Self-Play/Search)" --> F & D & E;
        J["思维快与慢 (Thinking Fast & Slow)"] -- "类比系统2思维 (Analogy to System 2 Thinking)" --> D;
        K["思维链 (Chain of Thought)"] -- "对比/改进自 (Compared to/Improved from)" --> D;
        L["结果监督 (Output Supervision)"] -- "对比/改进自 (Compared to/Improved from)" --> G;
        style I fill:#e6ffe6,stroke:#333,stroke-width:1px,color:#333
        style J fill:#e6ffe6,stroke:#333,stroke-width:1px,color:#333
        style K fill:#e6ffe6,stroke:#333,stroke-width:1px,color:#333
        style L fill:#e6ffe6,stroke:#333,stroke-width:1px,color:#333
    end

    subgraph "目标与潜在影响 (Goals & Potential Impact)"
        B -- "目标是 (Aims for)" --> M["增强推理能力 (Enhanced Reasoning)<br>尤其数学 (Esp. Math)"];
        M -- "可能导致 (May Lead to)" --> N["超人性能 (Superhuman Performance)"];
        N -- "潜在风险/应用 (Potential Risks/Applications)" --> O["破解加密 (Breaking Encryption)<br>如 AES"];
        N -- "潜在风险/应用 (Potential Risks/Applications)" --> P["迈向 AGI 的一步? (Step towards AGI?)"];
        O -- "影响 (Impacts)" --> Q["金融安全 (Financial Security)<br>网络安全 (Cyber Security)"];
        style M fill:#b3ffb3,stroke:#333,stroke-width:2px,color:#000
        style N fill:#ffb3b3,stroke:#a00,stroke-width:2px,color:#000
        style O fill:#ff9999,stroke:#a00,stroke-width:2px,color:#000
        style P fill:#ff9999,stroke:#a00,stroke-width:2px,color:#000
        style Q fill:#ff6666,stroke:#a00,stroke-width:1px,color:#000
    end

    subgraph "结论 (Conclusion)"
        R["Q* 具体细节未知 (Exact Details Unknown)<br>但概念值得探讨 (Concepts Worth Exploring)"];
        style R fill:#eee,stroke:#333,stroke-width:1px,color:#000
        B --> R;
    end
</Mermaid_Diagram>

Content:
WEBVTT Kind: captions Language: en over the past few days there's been a over the past few days there's been a over the past few days there's been a lot of speculation about this Innovation lot of speculation about this Innovation lot of speculation about this Innovation with open AI called qar which apparently with open AI called qar which apparently with open AI called qar which apparently was the trigger for this whole boardroom was the trigger for this whole boardroom was the trigger for this whole boardroom Saga that led to the firing and then Saga that led to the firing and then Saga that led to the firing and then rehiring of Sam Alman ultimately I guess rehiring of Sam Alman ultimately I guess rehiring of Sam Alman ultimately I guess nobody really knows what qar is two of nobody really knows what qar is two of nobody really knows what qar is two of the more credible ones share lot the more credible ones share lot the more credible ones share lot similarities so this this one is by uh similarities so this this one is by uh similarities so this this one is by uh AI researcher at n Nvidia called Jim fan AI researcher at n Nvidia called Jim fan AI researcher at n Nvidia called Jim fan who did his PhD under F Lee at Stanford who did his PhD under F Lee at Stanford who did his PhD under F Lee at Stanford and then also there's another AI and then also there's another AI and then also there's another AI researcher called Nathan Lambert so I I researcher called Nathan Lambert so I I researcher called Nathan Lambert so I I think there's some you know potentially think there's some you know potentially think there's some you know potentially some links to what they're saying and some links to what they're saying and some links to what they're saying and when you read when you read when you read through their posts they're quite through their posts they're quite through their posts they're quite technical so what I wanted to do is just technical so what I wanted to do is just technical so what I wanted to do is just to break down at a at a high level to break down at a at a high level to break down at a at a high level conceptually what exactly is the you conceptually what exactly is the you conceptually what exactly is the you know Innovation if we just read through know Innovation if we just read through know Innovation if we just read through it here Nathan posted a Blog a few hours it here Nathan posted a Blog a few hours it here Nathan posted a Blog a few hours before I did and discussed very similar before I did and discussed very similar before I did and discussed very similar ideas tree of thought and process reward ideas tree of thought and process reward ideas tree of thought and process reward model and then what Nathan said was the model and then what Nathan said was the model and then what Nathan said was the C hypothesis I can stand behind from C hypothesis I can stand behind from C hypothesis I can stand behind from literature tree of thought reasoning literature tree of thought reasoning literature tree of thought reasoning something to search over process reward something to search over process reward something to search over process reward models rank all the steps of reasoning models rank all the steps of reasoning models rank all the steps of reasoning GPT 4 to score all feres of the tree Q GPT 4 to score all feres of the tree Q GPT 4 to score all feres of the tree Q learning to optimize and you can see learning to optimize and you can see learning to optimize and you can see here Jim has written a lot more detail here Jim has written a lot more detail here Jim has written a lot more detail about how he believes about how he believes about how he believes qar is um modeled on some of the key qar is um modeled on some of the key qar is um modeled on some of the key innovations that made alphao possible innovations that made alphao possible innovations that made alphao possible including things like reinforcement including things like reinforcement including things like reinforcement learning un using both a policy neuron learning un using both a policy neuron learning un using both a policy neuron Network and a value neuron Network and Network and a value neuron Network and Network and a value neuron Network and self-playing which generates a lot of self-playing which generates a lot of self-playing which generates a lot of synthetic data that enabled ago to synthetic data that enabled ago to synthetic data that enabled ago to exceed human exceed human exceed human capacity an AI can never become capacity an AI can never become capacity an AI can never become superhuman just by imitating human data alone so there's a a lot of details here alone so there's a a lot of details here alone so there's a a lot of details here and references to papers so we're going and references to papers so we're going and references to papers so we're going to go through that step by step right to go through that step by step right to go through that step by step right now so here we've got a little diagram now so here we've got a little diagram now so here we've got a little diagram with six topics which we're going to with six topics which we're going to with six topics which we're going to talk through today let's zoom in we're talk through today let's zoom in we're talk through today let's zoom in we're going to talk about using llms for math going to talk about using llms for math going to talk about using llms for math problems problems problems think Thinking Fast and Slow Chain of think Thinking Fast and Slow Chain of think Thinking Fast and Slow Chain of Thought and research reward models Thought and research reward models Thought and research reward models process versus output supervisor reward process versus output supervisor reward process versus output supervisor reward models and then finally Perpetual models and then finally Perpetual models and then finally Perpetual machine to reach superhuman capabilities machine to reach superhuman capabilities machine to reach superhuman capabilities we're going to do all of this at a very we're going to do all of this at a very we're going to do all of this at a very high level CU it's actually getting a high level CU it's actually getting a high level CU it's actually getting a bit late right here we will have some bit late right here we will have some bit late right here we will have some references to papers as you can see here references to papers as you can see here references to papers as you can see here which you can read if you want to go which you can read if you want to go which you can read if you want to go into more detail so the motivating into more detail so the motivating into more detail so the motivating problem that we're trying to solve here problem that we're trying to solve here problem that we're trying to solve here is that llm such as GPT were built as is that llm such as GPT were built as is that llm such as GPT were built as large language models to predict the large language models to predict the large language models to predict the next token from left to right and was next token from left to right and was next token from left to right and was not built to solve math problems however not built to solve math problems however not built to solve math problems however as the llms have become more and more as the llms have become more and more as the llms have become more and more powerful people have started to explore powerful people have started to explore powerful people have started to explore how we can make them more capable to how we can make them more capable to how we can make them more capable to solve complex math problems one of the solve complex math problems one of the solve complex math problems one of the techniques is this idea of Chain of techniques is this idea of Chain of techniques is this idea of Chain of Thought prompting if you just did Thought prompting if you just did Thought prompting if you just did standard input output prompting you standard input output prompting you standard input output prompting you might write a question such as the might write a question such as the might write a question such as the cafeteria had 23 apples if they used 20 cafeteria had 23 apples if they used 20 cafeteria had 23 apples if they used 20 to make lunch and bought six more how to make lunch and bought six more how to make lunch and bought six more how many apples do they many apples do they many apples do they have and it just went straight into the have and it just went straight into the have and it just went straight into the answer and said it's 27 which is wrong answer and said it's 27 which is wrong answer and said it's 27 which is wrong however if you ask ask GPT however if you ask ask GPT however if you ask ask GPT to talk through its thinking process and to talk through its thinking process and to talk through its thinking process and listing out the steps it is more likely listing out the steps it is more likely listing out the steps it is more likely to get the answer correct so in this to get the answer correct so in this to get the answer correct so in this case it outputs the cafeteria had 23 case it outputs the cafeteria had 23 case it outputs the cafeteria had 23 apples apples apples originally they used 20 to make lunch so originally they used 20 to make lunch so originally they used 20 to make lunch so they had 23 - 20 = 3 they bought six they had 23 - 20 = 3 they bought six they had 23 - 20 = 3 they bought six more apples so they have 3 + 6 = 9 the more apples so they have 3 + 6 = 9 the more apples so they have 3 + 6 = 9 the answer is 9 which is is correct answer is 9 which is is correct answer is 9 which is is correct prompting GPT to talk through the steps prompting GPT to talk through the steps prompting GPT to talk through the steps improves accuracy which takes us into improves accuracy which takes us into improves accuracy which takes us into this second bubble thinking fast and this second bubble thinking fast and this second bubble thinking fast and slow I'm sure many of you would have slow I'm sure many of you would have slow I'm sure many of you would have seen this book by Daniel canaman so this seen this book by Daniel canaman so this seen this book by Daniel canaman so this book Thinking Fast and Slow is a 2011 book Thinking Fast and Slow is a 2011 book Thinking Fast and Slow is a 2011 Popular Science book by psychologist Popular Science book by psychologist Popular Science book by psychologist Daniel canaman the book's main thesis is Daniel canaman the book's main thesis is Daniel canaman the book's main thesis is a differentiation between two modes of a differentiation between two modes of a differentiation between two modes of thought system one is fast instinctive thought system one is fast instinctive thought system one is fast instinctive and emotional system two is slower more and emotional system two is slower more and emotional system two is slower more deliberative and more logical the reason deliberative and more logical the reason deliberative and more logical the reason why I bring this up here is that why I bring this up here is that why I bring this up here is that llms the way they work is closer to llms the way they work is closer to llms the way they work is closer to system one in that is just quickly system one in that is just quickly system one in that is just quickly predicting the the next token without predicting the the next token without predicting the the next token without too much Advanced planning or too much too much Advanced planning or too much too much Advanced planning or too much thinking about what the various options thinking about what the various options thinking about what the various options are before it heads down a path however are before it heads down a path however are before it heads down a path however for something like a complex math for something like a complex math for something like a complex math problem the the thinking is that a problem the the thinking is that a problem the the thinking is that a system 2 mode of thought is more system 2 mode of thought is more system 2 mode of thought is more suitable it is slower more deliberative suitable it is slower more deliberative suitable it is slower more deliberative and more logical if if you think about and more logical if if you think about and more logical if if you think about how you might solve a complex math how you might solve a complex math how you might solve a complex math problem you might think through a few problem you might think through a few problem you might think through a few different ways of approaching it before different ways of approaching it before different ways of approaching it before you select your final answer and this is you select your final answer and this is you select your final answer and this is corroborated by Yan lon's post about qar corroborated by Yan lon's post about qar corroborated by Yan lon's post about qar so Yan Lun is met as a chief scientist so Yan Lun is met as a chief scientist so Yan Lun is met as a chief scientist so he's telling you to ignore the so he's telling you to ignore the so he's telling you to ignore the complete nonsense about qar blah blah complete nonsense about qar blah blah complete nonsense about qar blah blah blah and what he is saying is that qar blah and what he is saying is that qar blah and what he is saying is that qar is open ai's attempts at planning I've is open ai's attempts at planning I've is open ai's attempts at planning I've been advocating for deep learning been advocating for deep learning been advocating for deep learning architecture capable of planning since architecture capable of planning since architecture capable of planning since 2016 so it is about how you in a sense 2016 so it is about how you in a sense 2016 so it is about how you in a sense slow down the llm so it's not just slow down the llm so it's not just slow down the llm so it's not just rushing into spitting out the next word rushing into spitting out the next word rushing into spitting out the next word into a more deliberative way of reasoning coming to the next bubble reasoning coming to the next bubble reasoning coming to the next bubble Chain of Thought also called coot versus Chain of Thought also called coot versus Chain of Thought also called coot versus tree search the idea here and this um tree search the idea here and this um tree search the idea here and this um was discussed in a paper uh published by was discussed in a paper uh published by was discussed in a paper uh published by a number of researchers many of whom are a number of researchers many of whom are a number of researchers many of whom are with Google Deep Mind in a paper called with Google Deep Mind in a paper called with Google Deep Mind in a paper called tree of tree of tree of thoughts and the idea is that even Chain thoughts and the idea is that even Chain thoughts and the idea is that even Chain of Thought is not comprehensive enough of Thought is not comprehensive enough of Thought is not comprehensive enough you can see on this uh diagram ex you can see on this uh diagram ex you can see on this uh diagram ex extracted from the paper that you've got extracted from the paper that you've got extracted from the paper that you've got the input output kind of prompting so the input output kind of prompting so the input output kind of prompting so that's that's basically like this one that's that's basically like this one that's that's basically like this one like you ask a question and it just like you ask a question and it just like you ask a question and it just gives you the gives you the gives you the answer and then you've got this chain of answer and then you've got this chain of answer and then you've got this chain of thought prompting which is like a single thought prompting which is like a single thought prompting which is like a single reasoning path which is kind of linear reasoning path which is kind of linear reasoning path which is kind of linear and then you've got this variant on and then you've got this variant on and then you've got this variant on coot but what this paper is proposing is coot but what this paper is proposing is coot but what this paper is proposing is that instead of this linear way of that instead of this linear way of that instead of this linear way of breaking down one line of thought you breaking down one line of thought you breaking down one line of thought you would think about oh what is a sensible would think about oh what is a sensible would think about oh what is a sensible next step and maybe there are a number next step and maybe there are a number next step and maybe there are a number of different options and then here there of different options and then here there of different options and then here there there might be three and then if you there might be three and then if you there might be three and then if you went down One path then what would be went down One path then what would be went down One path then what would be the logical next step after that and the logical next step after that and the logical next step after that and then there it could start branching off then there it could start branching off then there it could start branching off into this wide tree and the idea behind into this wide tree and the idea behind into this wide tree and the idea behind tree of thoughts is that in order for tree of thoughts is that in order for tree of thoughts is that in order for llms to do well at mathematical llms to do well at mathematical llms to do well at mathematical reasoning for complex problems it should reasoning for complex problems it should reasoning for complex problems it should be able to just look ahead and look at be able to just look ahead and look at be able to just look ahead and look at different ways of solving the problem different ways of solving the problem different ways of solving the problem and and and choose in an intelligent way which is choose in an intelligent way which is choose in an intelligent way which is the best path and then output that back the best path and then output that back the best path and then output that back to the user the the problem with this kind of user the the problem with this kind of user the the problem with this kind of tree search is the size of the tree search is the size of the tree search is the size of the tree in in this example there aren't a tree in in this example there aren't a tree in in this example there aren't a lot of branches under each node so you lot of branches under each node so you lot of branches under each node so you can do a Brute Force search where you go can do a Brute Force search where you go can do a Brute Force search where you go go down every single path and then you go down every single path and then you go down every single path and then you choose the best one but in the case of choose the best one but in the case of choose the best one but in the case of games like go or in the case of an llm games like go or in the case of an llm games like go or in the case of an llm where the number of outputs at each where the number of outputs at each where the number of outputs at each step could be infinite doing a Brute step could be infinite doing a Brute step could be infinite doing a Brute Force tree search where you go down Force tree search where you go down Force tree search where you go down every node every Branch will just take every node every Branch will just take every node every Branch will just take an a ridiculous amount of time and a an a ridiculous amount of time and a an a ridiculous amount of time and a crazy amount of computational power what crazy amount of computational power what crazy amount of computational power what you want to do is to have some way of you want to do is to have some way of you want to do is to have some way of scoring the different branches so you scoring the different branches so you scoring the different branches so you can reduce the amount of work that you can reduce the amount of work that you can reduce the amount of work that you need to do to come up with the highest need to do to come up with the highest need to do to come up with the highest probability right answer that takes us probability right answer that takes us probability right answer that takes us onto the reward models reward models are onto the reward models reward models are onto the reward models reward models are essentially another neuron Network that essentially another neuron Network that essentially another neuron Network that can help you score the different options can help you score the different options can help you score the different options that you have in your tree so you can that you have in your tree so you can that you have in your tree so you can make more informed decisions about which make more informed decisions about which make more informed decisions about which branches to go down and how deep to go branches to go down and how deep to go branches to go down and how deep to go down to to increase the probability of down to to increase the probability of down to to increase the probability of getting the best answer for a given getting the best answer for a given getting the best answer for a given amount of computational power and these amount of computational power and these amount of computational power and these two screenshots here is a extract from a two screenshots here is a extract from a two screenshots here is a extract from a a YouTube video explaining how a YouTube video explaining how a YouTube video explaining how reinforcement learning works in Alpha go reinforcement learning works in Alpha go reinforcement learning works in Alpha go the channels here you can uh watch it the channels here you can uh watch it the channels here you can uh watch it for yourself it's a great video the idea for yourself it's a great video the idea for yourself it's a great video the idea here is that you are at a certain state here is that you are at a certain state here is that you are at a certain state of the game play and your next step you of the game play and your next step you of the game play and your next step you have many different options across the have many different options across the have many different options across the board because you can put your piece board because you can put your piece board because you can put your piece down at any space in this board that is down at any space in this board that is down at any space in this board that is not already occupied but here they've not already occupied but here they've not already occupied but here they've shown two shown two shown two branches and as you if you assume that branches and as you if you assume that branches and as you if you assume that you put your piece here then what are you put your piece here then what are you put your piece here then what are the next steps following on from that the next steps following on from that the next steps following on from that and then what are the next steps and then what are the next steps and then what are the next steps following on from that by going down following on from that by going down following on from that by going down each branch you can eventually have some each branch you can eventually have some each branch you can eventually have some kind of puristic to estimate your kind of puristic to estimate your kind of puristic to estimate your likelihood of winning the game down that likelihood of winning the game down that likelihood of winning the game down that path without having to go down the path without having to go down the path without having to go down the entire tree which is massive for a game entire tree which is massive for a game entire tree which is massive for a game like like like go here we've gone down like 1 2 3 4 go here we've gone down like 1 2 3 4 go here we've gone down like 1 2 3 4 five steps and that takes a certain five steps and that takes a certain five steps and that takes a certain degree of computational power but if you degree of computational power but if you degree of computational power but if you wanted to shrink that further you can wanted to shrink that further you can wanted to shrink that further you can actually only take one step and then actually only take one step and then actually only take one step and then just take the probabilities at this just take the probabilities at this just take the probabilities at this stage so really you're kind of trying to stage so really you're kind of trying to stage so really you're kind of trying to balance the accuracy of your of your balance the accuracy of your of your balance the accuracy of your of your ability to select the best path with the ability to select the best path with the ability to select the best path with the amount of searching you have to do to amount of searching you have to do to amount of searching you have to do to get there now you might ask well that's get there now you might ask well that's get there now you might ask well that's for Alpha go in in in the case of an llm for Alpha go in in in the case of an llm for Alpha go in in in the case of an llm like what is the equivalent of that and like what is the equivalent of that and like what is the equivalent of that and how might we go how might we go how might we go about training ll's with information on about training ll's with information on about training ll's with information on how good the the steps are that they're how good the the steps are that they're how good the the steps are that they're proposing well this kind of takes us to proposing well this kind of takes us to proposing well this kind of takes us to this concept of this concept of this concept of process versus output supervised reward process versus output supervised reward process versus output supervised reward models PRM versus or and to cut things models PRM versus or and to cut things models PRM versus or and to cut things short the idea behind this paper is that short the idea behind this paper is that short the idea behind this paper is that we should give feedback to the we should give feedback to the we should give feedback to the llm after it outputs every step so it llm after it outputs every step so it llm after it outputs every step so it knows how good each step is and it knows knows how good each step is and it knows knows how good each step is and it knows where it went where it went where it went wrong this uh piece came out of a paper wrong this uh piece came out of a paper wrong this uh piece came out of a paper published by open AI called let's verify published by open AI called let's verify published by open AI called let's verify step by step in May 2023 step by step in May 2023 step by step in May 2023 the fact that this paper exists also the fact that this paper exists also the fact that this paper exists also reinforces the fact that there's a lot reinforces the fact that there's a lot reinforces the fact that there's a lot of people working in within open AI on of people working in within open AI on of people working in within open AI on how to get LLS to be better at planning how to get LLS to be better at planning how to get LLS to be better at planning ahead let's take a look at this example ahead let's take a look at this example ahead let's take a look at this example a class of 30 students recently took a a class of 30 students recently took a a class of 30 students recently took a test so this is a math problem that was test so this is a math problem that was test so this is a math problem that was posed to GPT 4 if 20 students scored 80 posed to GPT 4 if 20 students scored 80 posed to GPT 4 if 20 students scored 80 points eight students scored 90 points points eight students scored 90 points points eight students scored 90 points and two students scored 100 points then and two students scored 100 points then and two students scored 100 points then what was the class average bracket the what was the class average bracket the what was the class average bracket the mean on this test and in order to solve mean on this test and in order to solve mean on this test and in order to solve this problem instead of jumping straight this problem instead of jumping straight this problem instead of jumping straight into an output the llm is encouraged to into an output the llm is encouraged to into an output the llm is encouraged to think about it step by think about it step by think about it step by step to find the class average I need to step to find the class average I need to step to find the class average I need to add up all the scores and divide by the add up all the scores and divide by the add up all the scores and divide by the number of students and where you see the number of students and where you see the number of students and where you see the color here is where a human has actually color here is where a human has actually color here is where a human has actually gone gone in gone gone in gone gone in to rate how good that step is and in to rate how good that step is and in to rate how good that step is and in this case they had only three types of this case they had only three types of this case they had only three types of scores there's positive negative or scores there's positive negative or scores there's positive negative or neutral and if it's green it means that neutral and if it's green it means that neutral and if it's green it means that it's a good step I can use a shortcut to it's a good step I can use a shortcut to it's a good step I can use a shortcut to add up the scores since they are grouped add up the scores since they are grouped add up the scores since they are grouped by value for example instead of adding by value for example instead of adding by value for example instead of adding 80 + 80 + 80 20 times I can just 80 + 80 + 80 20 times I can just 80 + 80 + 80 20 times I can just multiply 80 by 20 to get this same multiply 80 by 20 to get this same multiply 80 by 20 to get this same result similarly I can multiply 90 by 8 result similarly I can multiply 90 by 8 result similarly I can multiply 90 by 8 and 100 by 2 to get the sums of those and 100 by 2 to get the sums of those and 100 by 2 to get the sums of those groups so the total sum of the scores is groups so the total sum of the scores is groups so the total sum of the scores is 80 * 20 + 90 * 8 + 100 * 2 I can use the 80 * 20 + 90 * 8 + 100 * 2 I can use the 80 * 20 + 90 * 8 + 100 * 2 I can use the distributive property of multiplication distributive property of multiplication distributive property of multiplication to simplify this to simplify this to simplify this expression and here it is like this next expression and here it is like this next expression and here it is like this next step where it kind of messed things up step where it kind of messed things up step where it kind of messed things up up so it says it is equivalent to 80 * up so it says it is equivalent to 80 * up so it says it is equivalent to 80 * braet 20 + braet 20 + braet 20 + 1+ 90 * 8 + 1+ 90 * 8 + 1+ 90 * 8 + 100 and I'm kind of not really sure how 100 and I'm kind of not really sure how 100 and I'm kind of not really sure how it got there from this so it was scored it got there from this so it was scored it got there from this so it was scored as being red that is 80 * 21 + 720 + 100 as being red that is 80 * 21 + 720 + 100 as being red that is 80 * 21 + 720 + 100 again that's red I can do the AR again that's red I can do the AR again that's red I can do the AR arithmetic in my head or use a arithmetic in my head or use a arithmetic in my head or use a calculator 80 * 21 is uh calculator 80 * 21 is uh calculator 80 * 21 is uh 1, 680 so the total sum is 1, 680 + 720 + 1, 680 so the total sum is 1, 680 + 720 + 1, 680 so the total sum is 1, 680 + 720 + 100 which is 2, 500 so you'll see that 100 which is 2, 500 so you'll see that 100 which is 2, 500 so you'll see that even though this is not the correct even though this is not the correct even though this is not the correct answer for this it is a correct step answer for this it is a correct step answer for this it is a correct step following on from this so that's why following on from this so that's why following on from this so that's why it's in green now to find the average I it's in green now to find the average I it's in green now to find the average I need to divide 2 2, 500 by 30 blah blah need to divide 2 2, 500 by 30 blah blah need to divide 2 2, 500 by 30 blah blah blah and and so in the end it got to the blah and and so in the end it got to the blah and and so in the end it got to the answer you can see here if you think answer you can see here if you think answer you can see here if you think about this as a tree maybe this first about this as a tree maybe this first about this as a tree maybe this first step this is one option the llm can also step this is one option the llm can also step this is one option the llm can also think of maybe a number of different think of maybe a number of different think of maybe a number of different options as being its first step and then options as being its first step and then options as being its first step and then given each of those steps it can think given each of those steps it can think given each of those steps it can think of a number of different options as of a number of different options as of a number of different options as being the second step and so it can form being the second step and so it can form being the second step and so it can form a a a a a a tree and so with this reinforcement tree and so with this reinforcement tree and so with this reinforcement learning with human feedback method learning with human feedback method learning with human feedback method that's done at the process level as that's done at the process level as that's done at the process level as opposed to the output level the model opposed to the output level the model opposed to the output level the model can learn how good each of the steps are can learn how good each of the steps are can learn how good each of the steps are and when it tends to get and when it tends to get and when it tends to get wrong just to explain what is meant by wrong just to explain what is meant by wrong just to explain what is meant by output supervised reward models these output supervised reward models these output supervised reward models these are basically reward models that only are basically reward models that only are basically reward models that only give a give a give a reward if the answer is correct but for reward if the answer is correct but for reward if the answer is correct but for complicated mathematical problems llms complicated mathematical problems llms complicated mathematical problems llms get the answer wrong most of the time get the answer wrong most of the time get the answer wrong most of the time and simply by telling the and simply by telling the and simply by telling the llm that the answer is wrong actually is llm that the answer is wrong actually is llm that the answer is wrong actually is not very helpful information because it not very helpful information because it not very helpful information because it does not know where it went wrong PRM by does not know where it went wrong PRM by does not know where it went wrong PRM by doing it at each doing it at each doing it at each step it is able to really teach the llm step it is able to really teach the llm step it is able to really teach the llm where things went wrong so anyway where things went wrong so anyway where things went wrong so anyway there's a whole kind of area of res there's a whole kind of area of res there's a whole kind of area of res search around how to create these uh search around how to create these uh search around how to create these uh reward models or sometimes called value reward models or sometimes called value reward models or sometimes called value policy neuron networks to help to score policy neuron networks to help to score policy neuron networks to help to score these you know the the options that you these you know the the options that you these you know the the options that you have in the tree just a quick comment have in the tree just a quick comment have in the tree just a quick comment here about how LLS are able to crunch here about how LLS are able to crunch here about how LLS are able to crunch numbers like this like addition numbers like this like addition numbers like this like addition multiplication what's really cool about multiplication what's really cool about multiplication what's really cool about large language models is that they're large language models is that they're large language models is that they're not only able to write natural language not only able to write natural language not only able to write natural language but they have also ingested a ton of but they have also ingested a ton of but they have also ingested a ton of code so it's also able to write code and code so it's also able to write code and code so it's also able to write code and through its uh code interpretor through its uh code interpretor through its uh code interpretor functionality it's also able to execute functionality it's also able to execute functionality it's also able to execute that code so for example if it needed to that code so for example if it needed to that code so for example if it needed to crunch some numbers here it could well crunch some numbers here it could well crunch some numbers here it could well behind the scenes have built its own behind the scenes have built its own behind the scenes have built its own calculator and run it to do those calculator and run it to do those calculator and run it to do those calculations and then pass the results calculations and then pass the results calculations and then pass the results back to the large language model finally back to the large language model finally back to the large language model finally you might think okay well that all you might think okay well that all you might think okay well that all sounds like really sensible research and sounds like really sensible research and sounds like really sensible research and yen T said he's been working on it since yen T said he's been working on it since yen T said he's been working on it since 2016 what's the big deal like why why 2016 what's the big deal like why why 2016 what's the big deal like why why are people talking about this being a are people talking about this being a are people talking about this being a step to step to step to AGI and potentially having led the board AGI and potentially having led the board AGI and potentially having led the board to take some pretty drastic actions well to take some pretty drastic actions well to take some pretty drastic actions well I I think this is where the the leap I I think this is where the the leap I I think this is where the the leap comes comes comes in this approach is called reinforcement in this approach is called reinforcement in this approach is called reinforcement learning also refer referred to as Q learning also refer referred to as Q learning also refer referred to as Q learning this is the method that Alpha learning this is the method that Alpha learning this is the method that Alpha go used to train itself what's really go used to train itself what's really go used to train itself what's really interesting about Alpha go is that it interesting about Alpha go is that it interesting about Alpha go is that it was able to beat humans in playing the was able to beat humans in playing the was able to beat humans in playing the game go because it didn't just game go because it didn't just game go because it didn't just use historical gameplay data by humans use historical gameplay data by humans use historical gameplay data by humans to learn what it did was to become its to learn what it did was to become its to learn what it did was to become its own teacher a neuron network is trained own teacher a neuron network is trained own teacher a neuron network is trained to predict alphago's own move selections to predict alphago's own move selections to predict alphago's own move selections and also the winner of alphago's games and also the winner of alphago's games and also the winner of alphago's games this neuron Network improves the this neuron Network improves the this neuron Network improves the strength of treesearch resulting in strength of treesearch resulting in strength of treesearch resulting in higher quality move selection and higher quality move selection and higher quality move selection and stronger self-play in the next iteration stronger self-play in the next iteration stronger self-play in the next iteration and if we go down here so much progress and if we go down here so much progress and if we go down here so much progress towards AI has been made using towards AI has been made using towards AI has been made using supervised Learning Systems however it's supervised Learning Systems however it's supervised Learning Systems however it's expert data is often expensive expert data is often expensive expert data is often expensive unreliable simply unavailable and when unreliable simply unavailable and when unreliable simply unavailable and when it is available it imposes a SE feeling it is available it imposes a SE feeling it is available it imposes a SE feeling on the performance of systems trained in on the performance of systems trained in on the performance of systems trained in this manner in contrast reinforcement this manner in contrast reinforcement this manner in contrast reinforcement learning systems are trained from their learning systems are trained from their learning systems are trained from their own experience in principle allowing own experience in principle allowing own experience in principle allowing them to exceed human capabilities and to them to exceed human capabilities and to them to exceed human capabilities and to operate in domains where human expertise operate in domains where human expertise operate in domains where human expertise is lacking recently there has been rapid is lacking recently there has been rapid is lacking recently there has been rapid progress towards this goal using deep progress towards this goal using deep progress towards this goal using deep neuron networks trained by reinforcement neuron networks trained by reinforcement neuron networks trained by reinforcement learning these systems systems have learning these systems systems have learning these systems systems have outperformed humans in computer games outperformed humans in computer games outperformed humans in computer games blah blah blah so it is this idea that blah blah blah so it is this idea that blah blah blah so it is this idea that that if you can have a generator that that if you can have a generator that that if you can have a generator that also has a reward model that scores also has a reward model that scores also has a reward model that scores itself it can become like a Perpetual itself it can become like a Perpetual itself it can become like a Perpetual machine that makes itself better and machine that makes itself better and machine that makes itself better and better and better beyond the realm of better and better beyond the realm of better and better beyond the realm of human capability and just a very simple human capability and just a very simple human capability and just a very simple diagram here this is something suggested diagram here this is something suggested diagram here this is something suggested by uh Nathan here you've got the by uh Nathan here you've got the by uh Nathan here you've got the generator llm which is essentially your generator llm which is essentially your generator llm which is essentially your GPT 4 that's creating the different GPT 4 that's creating the different GPT 4 that's creating the different options in the tree and then you've got options in the tree and then you've got options in the tree and then you've got some kind of reward neuronet that tell some kind of reward neuronet that tell some kind of reward neuronet that tell informs the choices of which branch to informs the choices of which branch to informs the choices of which branch to go down with the tree without having to go down with the tree without having to go down with the tree without having to go through like a Brute Force search and go through like a Brute Force search and go through like a Brute Force search and through this it's able to kind of through this it's able to kind of through this it's able to kind of self-play or generate experiences all by self-play or generate experiences all by self-play or generate experiences all by itself at a rate that humans cannot aim itself at a rate that humans cannot aim itself at a rate that humans cannot aim to match through this they can create a to match through this they can create a to match through this they can create a lot of synthetic data AKA synthetic lot of synthetic data AKA synthetic lot of synthetic data AKA synthetic experiences and get much better at at experiences and get much better at at experiences and get much better at at math than humans can math than humans can math than humans can comprehend in the reuter's release it comprehend in the reuter's release it comprehend in the reuter's release it was suggested that one of the board's was suggested that one of the board's was suggested that one of the board's concerns about this qar Innovation is concerns about this qar Innovation is concerns about this qar Innovation is that it's able to break an encryption that it's able to break an encryption that it's able to break an encryption method called method called method called aes1 192 these encryption systems are aes1 192 these encryption systems are aes1 192 these encryption systems are designed to work because a certain kind designed to work because a certain kind designed to work because a certain kind of math problem is too difficult for of math problem is too difficult for of math problem is too difficult for humans to solve in a finite amount of humans to solve in a finite amount of humans to solve in a finite amount of time but if through these reinforcement time but if through these reinforcement time but if through these reinforcement working techniques where our AI models working techniques where our AI models working techniques where our AI models can exceed the mathematical capability can exceed the mathematical capability can exceed the mathematical capability of humans then perhaps these fundamental of humans then perhaps these fundamental of humans then perhaps these fundamental assumptions that underpin our encryption assumptions that underpin our encryption assumptions that underpin our encryption methods methods methods maybe broken the ramifications of that maybe broken the ramifications of that maybe broken the ramifications of that would be tremendous for example would be tremendous for example would be tremendous for example cryptocurrencies are essentially based cryptocurrencies are essentially based cryptocurrencies are essentially based on cryptography and this kind of on cryptography and this kind of on cryptography and this kind of technology so that could be broken all technology so that could be broken all technology so that could be broken all of the things that we think are private of the things that we think are private of the things that we think are private being passed over the Internet could be being passed over the Internet could be being passed over the Internet could be broken none of us really know what qstar broken none of us really know what qstar broken none of us really know what qstar means at least we've learned something means at least we've learned something means at least we've learned something new by going through gy fans Twitter new by going through gy fans Twitter new by going through gy fans Twitter post
