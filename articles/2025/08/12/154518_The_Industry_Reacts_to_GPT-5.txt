Timestamp: 2025-08-12T15:45:18.305384
Title: The Industry Reacts to GPT-5
URL: https://youtu.be/yAV5idvaqCA?si=y4-LuTjyhoo_AP9P
Status: success
Duration: 21:13

Description:
**核心要点：**
人工智能模型提供商之间的激烈竞争，无论新模型的发布伴随着何种争议和两极分化的评价，最终都将推动技术进步并为用户带来更多选择和更优价值。

**总框架：**
GPT-5发布后的行业全景式反应：从官方澄清、权威基准，到用户体验、个性化评价，再到竞争格局与价格战，展现了一个技术与市场交织、期待与失望并存的复杂生态。

**详细总结：**

**1. GPT-5发布引发的两极分化反响**
    *   **核心现象：** GPT-5的发布引发了前所未有的行业两极分化，有人称赞其为“有史以来最好”，也有人坚持使用Claude 3.5。
    *   **争议焦点：** 模型性能、用户体验、基准测试的实际意义。

**2. OpenAI官方回应与产品调整**
    *   **Sam Altman的反馈：**
        *   低估了用户对GPT-4o的情感依恋，承认许多用户更喜欢其个性。
        *   用户对GPT-4o和GPT-5的相对实力看法不一。
        *   未来将重点关注模型定制化（允许用户选择不同模型）和调整GPT-5的个性，使其更“温暖”。
        *   简化模型选择对新手有益，但高级用户需要选择权。
    *   **模型命名：** 旧模型（如GPT-4o、GPT-4o mini、GPT-3）的功能将映射到GPT-5的不同配置（如主版本、主迷你版、思考迷你版/纳米版/专业版）。

**3. 独立基准测试结果**
    *   **Artificial Analysis：**
        *   GPT-5设定了新的AI智能标准，其AI指数得分达到68（高推理版本为69）。
        *   引入四种“推理努力”配置（高、中、低、极简），影响智能、token使用量、速度和成本。
        *   在长上下文推理和Agentic（代理）能力方面表现突出，对Agentic编码尤其重要。
        *   OpenAI在29天后重新夺回AI智能榜首。
    *   **LM Arena：**
        *   GPT-5在文本、Web开发、视觉、硬提示、编码、数学、创造力、长查询等多个领域位居榜首。
        *   ELO评分为1481，领先Gemini 2.5 Pro和Groq 4。

**4. 行业对基准测试的看法转变（“后评估时代”）**
    *   **Theo GG的观点：**
        *   认为智能基准测试已不再重要，因为模型已达到一定饱和度（如AIME 2025已达100%）。
        *   更强调模型的“感受”（vibe）：指令遵循能力、长上下文窗口处理能力以及实际使用时的体验（尤其是在代码编辑器中）。
        *   认为GPT-5在实际使用中“令人恐惧”地强大。
    *   **Swebench团队的异议：** 认为任何模型特性都可以被基准化。

**5. 业界其他评价与争议**
    *   **积极评价：**
        *   McKay Wrigley：出色的日常聊天模型，API定价极具吸引力；喜欢其直接、不谄媚的个性，幻觉少，速度快。
        *   Kua（计算机使用代理）：在计算机任务上表现显著优于GPT-4o。
        *   Aiden McLaughlin（OpenAI）：在几乎所有评估中都达到了最先进水平（SOTA），比Claude 4.1 Opus便宜5倍以上，写作质量最佳。
        *   Sophie Netcap girl：模型在医疗领域的应用日益重要。
    *   **混合/质疑评价：**
        *   **GraphGate事件：** 发布会上展示的图表数据与柱状图不符，被视为一个小失误和网络迷因。
        *   Stage Hand：在浏览器使用场景下，GPT-5的速度和准确性均不如Opus 4.1（但开源模型GPTO OSS 12B表现良好）。
        *   Voss（Meta工程师）：GPT-5对代码库进行重构“美丽”但“完全不工作”，带有幽默讽刺。
        *   Boris：认为OpenAI正变得像苹果，创新速度放缓，转而关注用户体验和产品普及（如聊天气泡颜色定制），而非重大突破。
        *   Dylan Patel（SemiAnalysis CEO）：直言“令人失望”。
        *   Amjad Msad（Replit CEO）：感受到“回报递减”的压力，呼唤新的S曲线式创新。
    *   **越狱与能力展示：**
        *   Plai：成功越狱GPT-5，显示模型非确定性带来了越狱的可能性。
        *   Thge（LM Arena）：GPT-5“一击”生成了简易版Minecraft克隆。
    *   **竞争对手观点：**
        *   Tony Woo（XAI联合创始人）：XAI团队规模虽小，但在ARC AGI等特定基准上Groq 4领先GPT-5，并预告未来几周将发布更多新模型。
        *   Elon Musk：支持Groq在ARC AGI榜首的地位。

**6. 显著的价格优势**
    *   GPT-5的API定价极具竞争力（输入每百万token 1.25美元，输出每百万token 10美元），远低于Claude Opus 4.1和Groq 4，被认为是其最大创新之一，将大幅促进模型的使用。

**7. 行业竞争的最终影响**
    *   尽管围绕GPT-5的发布存在巨大争议和两极分化的评价，但OpenAI与XAI、Anthropic等AI公司之间的激烈竞争，最终将推动技术持续进步，并为用户带来更多选择和更高价值。

---
<Mermaid_Diagram>
graph TD
    subgraph "GPT-5发布与初步反响 Launch & Initial Reception"
        A["GPT-5 发布"] --> B["行业反响两极分化"];
    end

    subgraph "官方与用户视点 Official & User Perspectives"
        C["OpenAI 官方回应"] --> D["承认用户依恋4o"];
        C --> E["计划模型定制化与更温暖个性"];
        B --> F["Sam Altman: 低估用户依恋"];
        B --> G["用户对4o与5意见不一"];
        G --> H["Theo GG: '后评估时代'"];
        G --> I["McKay Wrigley: 日常聊天佳，编码仍用Claude"];
    end

    subgraph "性能评估与基准 Test & Evaluation"
        J["Artificial Analysis: 新标准"] --> K["GPT-5 AI指数得分68"];
        J --> L["多种推理努力配置"];
        J --> M["长上下文与Agentic能力突出"];
        N["LM Arena: 全面领先"] --> O["ELO评分1481，位居榜首"];
        H --> P["不关心智能基准，强调使用体验"];
        P --> Q["GPT-5表现优异，令人'恐惧'"];
        R["Stage Hand: 浏览器使用表现一般"] --> S["速度与准确性逊于Opus 4.1"];
        S --> T["GPTO OSS 12B表现良好"];
    end

    subgraph "争议、竞争与价格 Controversy, Competition & Price"
        U["GraphGate: 发布会图表错误"] --> V["虽为小失误，但引争议"];
        W["越狱能力"] --> X["Plai成功越狱"];
        X --> Y["Thge一击生成Minecraft克隆"];
        Z["成本与定价优势"] --> AA["远低于竞品，被认为是最大创新"];
        AB["竞争对手 XAI"] --> AC["Tony Woo: Grok-4在ARC AGI领先"];
        AB --> AD["预告更多新模型"];
        AE["行业担忧"] --> AF["Boris: OpenAI '苹果化'"];
        AE --> AG["Voss: 代码重构未成功"];
        AE --> AH["Dylan Patel: '令人失望'"];
        AE --> AI["Amjad Msad: '回报递减'，呼唤新S曲线"];
    end

    subgraph "最终受益者 The Ultimate Beneficiary"
        AJ["AI公司间的激烈竞争"] --> AK["技术进步"];
        AJ --> AL["更多选择与更优价值"];
        AK --> AM["最终惠及用户"];
        AL --> AM;
    end

    B --> F;
    B --> G;
    F --> C;
    G --> C;
    A --> J;
    A --> N;
    A --> R;
    A --> U;
    A --> W;
    A --> Z;
    A --> AB;
    A --> AE;

    J --> H;
    N --> H;
    R --> H;

    Z --> AK;
    AB --> AK;
    AE --> AK;

    AM["用户"]

    style A fill:#FFDDC1,stroke:#FF8C00,stroke-width:2px,color:#333;
    style B fill:#FFECB3,stroke:#FFA500,stroke-width:1px,color:#333;
    style C fill:#D1EEFC,stroke:#1E90FF,stroke-width:1px,color:#333;
    style D fill:#ADD8E6,stroke:#4682B4,stroke-width:1px,color:#333;
    style E fill:#ADD8E6,stroke:#4682B4,stroke-width:1px,color:#333;
    style F fill:#ADD8E6,stroke:#4682B4,stroke-width:1px,color:#333;
    style G fill:#ADD8E6,stroke:#4682B4,stroke-width:1px,color:#333;
    style H fill:#E0FFFF,stroke:#00CED1,stroke-width:1px,color:#333;
    style I fill:#ADD8E6,stroke:#4682B4,stroke-width:1px,color:#333;
    style J fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px,color:#333;
    style K fill:#B3E0B3,stroke:#66BB6A,stroke-width:1px,color:#333;
    style L fill:#B3E0B3,stroke:#66BB6A,stroke-width:1px,color:#333;
    style M fill:#B3E0B3,stroke:#66BB6A,stroke-width:1px,color:#333;
    style N fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px,color:#333;
    style O fill:#B3E0B3,stroke:#66BB6A,stroke-width:1px,color:#333;
    style P fill:#E0FFFF,stroke:#00CED1,

Content:
GPT5 has been the most polarizing model launch I have ever seen. From people saying it's the greatest model they've ever used to saying they're sticking with Claude 3. 5 to GraphGate to saying the evals don't even matter anymore. So, I'm going to break down all of the reactions from the industry right now. All right, first from the man himself, Sam Alman, he gives some updates post launch after collecting some of the feedback. Listen to what he has to say. We for sure underestimated how much some of the things that people like in GPT40 matter to them. Even if GPT5 performs better in most ways, what that means is people really got used to GPT40. They got to know it. They started to develop kind of a relationship with it. And now that they're just retiring it, some people are a little upset about that. Users have very different opinions on the relative strength of 40 verse 5. Long-term, this has reinforced that we really need good ways for different users to customize things, and I agree. The simplicity of not having to select from different models, especially for more novice users of artificial intelligence, is a huge strength and really simplifies the entire experience. But for somebody like me, and probably a lot of you, having the ability to choose which model we use for which use case is incredibly important. We're going to focus on finishing the GPT5 rollout and getting things stable. And then we're going to focus on some changes to GPT5 to make it warmer. So there was definitely something about the personality of 40 that is very different from five here. All right. So let me show you some independent benchmarks. Now here's artificial analysis with their own independent benchmarks. Open AAI gave us early access. Great. We have to run our full suite of eight evaluations independently across all reasoning effort configurations. And here are some of the takeaways. Reasoning effort configuration. GBT5 offers four configurations. High, medium, low, and minimal. Reasoning effort options steer the model to think more or less hard for each query. This is a very nice feature of hybrid models. This drives large differences in intelligence token usage, speed, and cost. The intelligence achieved based on tweaking these settings ranges from frontier to GBT 4. 1 level. Five sets a new standard with a score of 68 on our artificial intelligence index. So there it is. It is the new standard. Yes, GPT5 is crushing the benchmarks. But of course, benchmarks aren't everything. And in fact, some people think we're postbenchmark, post eval, and I'll get to that in a bit. Token usage varies 23 times between reasoning efforts. High reasoning effort used more tokens than 03, 82 million versus 50 million to complete our index, but still fewer than Gemini 2. 5 Pro at 98 million and DeepC1 at 99 million. minimal reasoning effort only used 3. 5 million tokens substantially less than 4. 1 making GPT5 minimal significantly more token efficient for similar intelligence and token efficiency is very important because that is faster speeds and lower cost long context reasoning we released our own long context reasoning benchmark earlier this week to test the reasoning capabilities of models across long sequence lengths GPT5 stands out for its performance now this is very important for Agentic coding. When you're loading up large swaths of your codebase into the model, you wanted to be able to reference the beginning, the middle, and the end easily. Agentic capabilities. OpenAI also commented on improvements across capabilities increasingly important to how AI models are used. That's agents. So, they recently added if Bench to their intelligence index to cover instruction following and vibe checks. We're testing the personality of the model through micro evals on our website which supports running the same prompt across models and comparing results. So let's look here. It is the index. Remember this is an index of multiple benchmarks ran independently by artificial analysis and number one GPT5 high that is 69. GPT5 medium 68 then Gro 4 at an even 68 03 67 and so on. Here's GPT5 low at 63 and then all the way at the bottom GPT5 minimum. And now OpenAI has reclaimed the number one spot in AI intelligence after 29 days second to XAI's Gro 4. So according to artificial analysis, GPT5 is fantastic. Let's keep going because it is shocking how polarized the responses have been to GPT5. All right, but let's talk about GraphGate for a second. Obviously, a lot of people are talking about some of the graphs presented at the live stream and how they were just off. And look, I know now how much effort goes into preparing the model, preparing all of the graphs, preparing the live stream. And look, humans make mistakes. We hallucinate just like models do. And so what everybody is talking about is right here. Look at the graph. So we have OpenAI 03 at 69, GPT40 at 30. Yet, these two bars are exactly the same height. And then we have 74. 9 way up here, which doesn't really make sense relative to the size of this bar. And you can even see 52. 8 right here. Again, higher than 69. 1. So, look, not a big deal. It's funny. It's a meme now. But people make mistakes. And you know what? It's still a good model. And if you want to try GPT5 right now along with all of the other Frontier models in a single place, check out the sponsor of today's video, Chat LLM by Abacus. If you're like me, you probably have subscriptions to a bunch of different AI services and you jump between them all the time. And it's kind of frustrating and not only that, pretty expensive. And that is where Chat LLM by Abacus AI comes in. It is an all-in-one AI platform that includes all of the latest and best models from the leading model providers. And they also have something called route LLM which automatically picks the best model to send your prompt to dependent on the actual prompt. So it is routing your prompt to the right LLM. And of course you can also chat with PDFs. So download any documents that you want and easily ask questions, extract insights, gather data, whatever you need from your existing documents. And not only that, they also have text to image and texttovideo models, so you can generate awesome images, awesome videos easily. They also recently introduced Deep Agent, which is an incredibly powerful AI agent that can basically do anything. So, building websites, building apps, creating presentations, research reports, chat bots, or even building games. Their deep agent combines six to 10 different LLMs, including open source models like Coin Coder to get your work done. Chat LLM has all of the latest Frontier models, including Opus 4. 1, the latest open source GPTO OSS120B, and of course, GPT5 on the same day it's available. And all of this for just $10 per month. So check it out, chatlm. abacus. ai or click the link in the description. Let them know I sent you. Much appreciated. Thank you again to Abacus AI. Now, back to the video and let's go to LM Arena. another evaluation. So we have GPT5 is here and it's number one across the board. So we have artificial analysis and now LM arena claiming GPT5 is number one amongst all the other models. Number one in text, webdev and vision arena number one in hard prompts, coding, math, creativity, long queries and more. Tested under the code name summit. So if you were wondering that was the code name. And we can see right here here's the arena score the ELO at 1481. Second place, Gemini 2. 5 Pro at 1460. That's a 20 point difference. Then 03. Then we have 40 latest Gro 4 is not until fifth place way down here. Really sixth place because for some reason they have second place twice here even though they are different scores. But there it is. Ella Marina number one. But as I said, benchmarks don't really matter anymore. And I think this post by Theo GG is super important to think about. I don't care about intelligence benchmarks now. I'm post eval GPT5 does what you tell it to do. No other model behaves this well. Trust me, don't judge until you try it in your editor. Give it tools, give it instructions, watch it cook. Now, there's a few things to note here. One is that he does not care about intelligence benchmarks. And I've been saying this for a little while now. when you're getting, you know, single point differences in Amy 2025 or when you fully saturate Amy 2025, which happened, GBT5 scored a 100%. These increases in intelligence don't really matter. There are two things that matter after these benchmarks get saturated. Number one is, and I don't know a better way to explain it, but it is the vibe of the model. How good is it at instruction following? How good is it at large windows of context? What does it feel like when you're using it? What does it feel like when you're coding with it? All of these intangibles. However, at the same time, I just spoke to the Swebench team in a live stream yesterday and they said, "Look, I don't believe in post eval. If there's something about the model, we can write a benchmark for it. " So, I really hope that's the case. But personally, I don't really care about benchmarks that much anymore. It's fun to look at them. It's fun to see them saturate these frontier math problems, these frontier science problems, but at the end of the day, how well does it work for my day-to-day use cases? And then the second thing is Theo also put out a video saying GPT basically is scaring him. It's that good. And so he is a huge fan. And again, I'm in that camp. I am a huge fan of GPT5, but there are many people who think it was a big flop. Let's look at some of the other side of the reactions. Now, this is from Stage Hand, a browsing use API, and they say GPT5 is actually worse than other models. So, the new GPT5 performs worse than Opus 4. 1 and Stage Hand Evals in both speed and accuracy. The smaller models are faster, but also still fall short of Opus 4. 1. Here are the most accurate models. Opus 4. 1 and then a few percentage points behind GPT 5. And surprisingly, look here. GPTO OSS 12B. So the opensource model actually performs really well, which is amazing to see. Then if we look at speed, of course, we have Gemini 2. 0 Flash at the top and GPT5 at the bottom. Speed is very important when you're talking about browser use. Here's McKay Wrigley, another AI content creator. My honest GPT5 review. It is a phenomenal everyday chat model. I will default to it for all normal chats. API pricing is incredible. Major points here. I'll show you that in a moment. But code, I will still be using Cloud Code plus Opus. A few other things McKay points out. I really love the personality they landed on for GPT5. I agree. It's direct. It's to the point. It pushes back where it needs to. It's not sick of fantic at all. I really appreciate that. And he actually points that out. It's not sick of fantic. I personally could probably have it be even more disagreeable. Fine. It has less hallucinations and yes, I've noticed that as well. It is very generally smart. Latency is good. Yes, it is very fast, which I love. And he actually says he hates the model router thing. And if you're not familiar, GPT5 was launched with a model router. So basically as they deprecated the older models all of the 40, the 41, 45, 03, what they did instead is they had this hybrid model and the router will route to the most appropriate flavor size of that model speed of that model depending on your prompt, depending on your use case. I personally really like it. And if it starts thinking, there's a button that says get a fast answer instead, which is awesome. All right, let's change subjects for a second. Let's talk about Ply. You know he was going to jailbreak it. It's inevitable at this point. It's like the sun rising in the morning. It's going to happen and there's no preventing it. Ply the liberator. GG5. And the examples he gave here are getting LSD recipes from the model. I'm not going to show them. And so listen to how he did it. The reasoning version takes some clever multi-step maneuvering efforts. Meaning it wasn't just easy to jailbreak it. But the GPT5 chat latest gets absolutely one shot by the same old tricks. These models are non-deterministic. These models have momentum internally. There are just ways to jailbreak it. Just like social engineering, it's going to happen as long as these models stay non-deterministic. Here's an example from an intern at LM Arena. I guess Thge, I think his name is. GBT5 just oneshotted a Minecraft clone. So, let's take a look. Obviously, very simplistic version of it, but this is one shot. And as you could see, seems to be working just fine. Obviously, there's a lot of reference material on the internet that the model probably was trained with because Minecraft is so absolutely insanely popular. Boris has a slightly different take. GPT5 is not AGI, but you can customize the colors of your chats. Open AAI is becoming Apple. What he means by that is basically he thinks open AI similar to Apple is slowing down on major innovations, the pace of innovations and instead are trying to make the products now more broadly appealing to the world. And that's what he means by the colors. So yeah, I really don't care about the color of the bubbles here. And it's funny that it is actually a paid feature, but fine. You want to customize chatpt even more. Great. Next, this is Tony Woo, co-founder of XAI. I didn't even know there was a co-founder, so good thing I found him and I followed him. very proud of us at XAI after seeing the GPT5 release with a much smaller team. We are ahead in many I think he means many benchmarks. Gro 4 world's first unified model. So it really was the first unified model and crushing GPT5 in benchmarks like ARC AGI. Now Arc AGI yes I believe Gro 4 really dominated that one which is one of many benchmarks and again what do benchmarks really mean? So, OpenAI is a very respectful competitor and still the leader in many, but we're fast and relentless. Many new models to share in the next few weeks. So, that is awesome to see. We're going to get more models. And if you remember from the Gro 4 launch, they had multiple versions and multiple products as part of the Gro 4 launch coming in the next few months. So, we'll see if we get them on time. Now, let's talk about the pricing. This is from Simon Willis's blog. Look at this. We have Claude Opus 4. 1, the most expensive by far at $15 per million input and $75 per million output. Here is Gro for $3. So that is an 80% reduction in cost from Opus 4. 1. Grock 4, $3 per million input, $15 per million output. And if we go all the way down here, GPT5, a$1. 25 25 per million input, $10 per million output. Phenomenal price. And I really think one of the biggest innovations they made here was on the price because price is so important. The cheaper it is, the more people are going to use it. Then the more people use it, the more you are the ecosystem. So I already showed you the opinion of one browser control agent. Now let me show you another. This is Kua GPT5 for computer use agents. Same task, same grounding model. We just swapped 40 for five. On the left, we're seeing 40. On the right, we're seeing five. As you can see, GPT40 fails most of the time at these computer use tasks. On the right, we're seeing GPT5 pass all of the same tests. So, it seems to be much better at computer use. Here's Aiden Mclofflin from OpenAI. So, take everything he says with a grain of salt. GPT fast facts hit state-of-the-art on pretty much every eval. Way better than cloud 4. 1. Opus at sui, that's software engineering. I think he means sui bench maybe. More than five times cheaper than opus. This is incredibly important. Greater than 40% cheaper than sonnet. Best riding quality of any model. I don't really know how you can really determine that because it is almost completely opinion-based and way less sickopantic. Here is Vos, an engineer at Meta. GPT5 just refactored my entire codebase in one call. 25 tool invocations, 3, 000 new lines, 12 brand new files. Don't get excited yet. Listen to this. It modularized everything. Broke up monolith, cleaned up spaghetti. None of it worked. But boy was it beautiful. I thought this was hilarious. Very funny, Voss. So, did a whole bunch of work, refactored everything, but at the end it didn't work. So, maybe it wasn't so good for Voss. Here's Sophie Netcap girl. Silence doctor, a language model is talking. This speaks to how good these language models are getting at medical use cases. And a lot of people are going to the model first before going to the doctor and even after going to the doctor. They're probably saying, "Hey, GPT5 said this. Are you sure? Are you right? " And doctors are probably hating that. Here's Carl Yang marked safe from permanent underclass today. So, let me explain what this means a little bit. There is a pretty popular thinking in Silicon Valley and in tech in general that once we reach AGI, whatever means you have, whatever capital you have, whatever social class you're in, that's pretty much where you're going to be stuck because effectively your leverage is only going to be equal to the capital you have to pay for compute for AGI. And I've heard this genuinely from people I've spoken to in San Francisco where they're like, "I need to make as much money as I can in the next 5 years because once AGI comes, it's all over. So, I'm trying to make $10 million in 5 years or, you know, whatever crazy numbers they're throwing out. " I don't believe in this really at all. I'm much more optimistic about the future, but I also understand the thinking. I understand why they think okay once AGI is here and it it can really accomplish anything a human can then it is really about how much capital you can throw at compute for the model. So very funny Carl Mark safe from permanent underclass today. And if you're a little bit sad about those old models being deprecated here Zeopon provided a nice way to tie the old models to the new model functionality. So check it out. We have GPT40 which is basically now GPT5 main 40 mini which is main mini of course 03 which is GPT5 thinking mini nano and pro again mini nano and pro. So we had all these names. We kind of have them again but they are all GPT5. I like the more simplistic naming. So it just makes it easy to see if you were using one of these models and you were confused as to what model you should be using now to get the same type of functionality. There it is. Dylan Patel, friend of the channel, CEO and founder of Semi analysis, says GPT5 is disappointing, not going to lie. He didn't really expand beyond that. Let's check out the comments to see if he gives a little bit more clarity. So, Santiago says, "Do you even code, bro? Claude's still better. " That is something that I keep hearing. In fact, I just interviewed the client CEO and he says he's still using Claude 3. 5. So even after GPT5 came out, even after Claude 4 and 4. 1, he still uses Claude 3. 5. Amjad Msad, CEO of Replet, can't help but feel the crushing weight of diminishing returns. We need a new scurve. So he is on the negative side. He thinks GPT5 was probably a flop. I believe that's what he means by this. And so he's saying these models are only getting incrementally better. And I actually think that's okay. What we need is a lot of investment in taking the raw intelligence of the model and building out the scaffolding, the architecture around it to leverage the incredible intelligence inside the model. Think about it like this. It's like having a 1000 horsepower engine without a car. What can you do with it? Nothing. So then you have to build the car and then you have to make sure that you can deliver all of that horsepower to the tires and the tires grip the ground. You can think of the raw intelligence of the model as the engine and the scaffolding as the car. It's not enough just to have high horsepower. You need to translate that horsepower into forward momentum. Couple more memes. Backend devs realizing they still have a job for at least a couple more months. This is hilarious. This is kind of along the lines of Mark safe from permanent underclass. And we're going to end with Elon Musk gro on top. So if you are wondering, this is ARC AGI 2 leaderboard. Here is GPT5 high at 10% and Gro 4 thinking at 16%. So yes, Gro 4 is still the top model for the ARC AGI benchmark. But almost on every other benchmark, GPT5 is now the king. And you know what? All of this hyper competition between model providers between leading AI labs is good for you and me. If you enjoyed this video, please consider giving a like and subscribe.
