Timestamp: 2025-08-31T18:33:46.755750
Title: [大模型面试] MoE vs Dense的优点 LLM不可能三角 主流LLM为何选用MoE架构 Bw66rOg_65c
URL: https://youtube.com/watch?v=Bw66rOg_65c&si=Gouln-OCAvfJe31i
Status: success
Duration: 27:27

Description:
### MoE模型：大规模AI模型的高效扩展之路

**核心观点：**
MoE模型通过解耦模型参数量和实际计算量，实现更高效、更大规模的模型训练，同时在性能上超越同等计算量的Dense模型，是未来大模型发展的重要方向，但其训练稳定性仍是一个核心挑战。

**Overarching Framework (整体框架):**
MoE模型的基本概念、提出动机、架构细节、性能优势、常见改进及面临的挑战。

---

**一、什么是MoE模型 (What is MoE Model?)**

*   **定义：** MoE模型是一种“稀疏模型”（Sparse Model），与传统“稠密模型”（Dense Model）相对。
*   **核心机制：** 它将传统FFN层替换为多个被称为“专家”（Expert，即小型MLP）的结构。对于每个输入，一个“路由选择器”（Router）会选择并仅激活其中Top-K个专家进行计算，其余专家不参与。
*   **发展历程：** Google（Switch Transformer, ST-MoE）在2017年已开展工作，DeepSeek等公司近期进一步推动了MoE模型的普及和规模化训练。

**二、MoE模型的提出动机 (Why MoE Model?)**

1.  **克服Dense模型的“不可能三角”瓶颈：**
    *   **背景：** 大模型性能提升依赖于“Scaling Law”（扩展定律），即增加模型大小、数据量和计算量。
    *   **问题：** Dense模型在扩展时，模型性能、计算成本和模型尺寸之间存在“不可能三角”制约，无法同时兼顾。增大模型尺寸虽提升性能，但计算成本（训练与推理）会剧烈增加。
    *   **MoE解决方案：** MoE模型通过**解耦总参数量和实际计算量**来突破瓶颈。它允许模型拥有巨大的总参数量以提升性能，同时通过只激活部分专家来控制实际计算量和成本。
        *   **总参数量**（Total Parameters） vs. **实际激活参数**（Activated Parameters）。
2.  **知识专业化：**
    *   FFN层被认为是存储模型知识的地方。MoE将FFN细分为多个专家，每个专家可以学习和存储特定领域的专业知识，使得模型内部知识结构更专业化、精细化。
    *   通过更大的总参数量存储更多知识，而不增加每次计算的成本。
3.  **引入稀疏归纳偏置（Sparse Inductive Bias）：**
    *   类似于CNNs引入卷积偏置以捕捉数据中的稀疏模式，MoE通过人为设计引入稀疏性，帮助模型更高效地从数据中学习低维模式，从而提高学习效率和性能。

**三、MoE模型的架构细节 (Architecture Details)**

*   **Router机制：**
    *   Router本身是一个FFN，后接Softmax层，计算每个输入进入每个专家的概率分布。
    *   **Top-K参数：** 决定每个输入激活多少个专家（例如，Top-2）。
    *   **工作流程：** 输入 -> Router -> 概率分布 -> 选Top-K专家 -> 计算 -> 结果加权聚合 -> Add & Norm层 -> 最终输出。
*   **多层MoE结构：** MoE模型并非只有一层专家，而是由多个MoE层堆叠而成，每个层都有其独立的专家组，不同层专家间的激活模式共同形成对知识的综合处理。

**四、MoE模型的性能优势 (Performance Benefits)**

1.  **更优的模型性能：** 在相同的训练计算量（Training Flops）下，MoE模型通常比同等计算量的Dense模型表现更好，甚至在相同激活参数量下也能超越Dense模型。
2.  **高训练效率：**
    *   在达到相同性能目标时，MoE模型所需的训练时间更短（例如快2倍）。
    *   消耗的计算量（Flops）更少（例如少3倍）。
    *   “样本效率”（Sample Efficient）更高，学习速度更快。
3.  **易于扩展 (ScaleUp)：** MoE架构更容易扩展到万亿级别的参数量，这对于构建更大、更强大的大模型至关重要。

**五、MoE模型的常见改进 (Common Enhancements)**

1.  **DeepSeek MoE的改进：**
    *   **细粒度专家（Fine-Grained Expert）：** 将少量大专家拆分为更多小专家。实验表明专家数量存在“甜蜜点”（

Content:
今天来给大家录一个主题也就是mexture of expertsmoe models这是面试当中经常被问到的一个话题moe model我认为是最近被deep seekwe3这个模特给带火了但其实moe model早在2017年时候Google就做了很多这方面的工作比较有名的就是Google的switch transformer还有随后的STmoemoe虽然提出的比较早但真正能把moe这类的模特做大并且能够训练出来我觉得Google还有deep seek做出了非常重要的贡献moe这个话题可以分为几个部分第一就是moe这类模型它的架构第二个就是moe的训练第三个可能就是moe在inference的时候一些病情的策略那么今天这个视频主要集中在给大家讲讲moe model的架构方面第一个问题就是什么是moe model那我们用这篇配合的图来给大家解释一下其实moe model是一个sbus model它是相对于我们常说的dance model而言的dance model也就是我们早期常见的存former的架构简单的可以把早期的lm架构给理解为有两个部分第一个就是腾胜的部分第二个就是这个fn的部分那所谓的dance model也就是这里的fn是一个dance的mlp架构那如果是moe也就是sbus架构就是把这里的fn换成了很多mlp在这里称之为expert那这里有一个区别就是对于输入这里有一个rauter来控制这个输入会进入到哪些expert里面进行计算因为这里在实际计算当中只会挑部分的expert进行击活也就是这里所看到的expert1expert3和expert6那么其他的expert这里都没有参与计算所以这类的model是一个sbus model等会我们再具体再来讨论一下sbus的moe model会不会比dance model更好好在哪里我们再来通过这个图看一下moe架构类伏的细节这个图展示的就是一个moe架构对于这四个输入分配的机制下面这里就是前面所提到的attention layer那这里就是把fn换成的moe layer如果我们先看这一个输入的话这个输入进入到一个rauter之后它会通过一个fn也就是这个rauter本身就是一个fn再加上一个softmax计算出每个expert所蹲进的分配的properability然后这里还有一个重要的超参也就是topk选多少个expert这里k.2也就是选两个所以在计算了softmax之后就会有一个properability distribution因为这里k.2所以就选properability最大的那两个expert那这个里面就是0.7对于的是第一个expert然后是这里的0.2对于的就是第三个expert这样这个头肯就被分配到了这两个expert上面最后再做一个accregation也就是加合然后再通过add and the nomination layer得到最终的输出那对于第二个input也是类似的经过这个rauter之后因为这个输入不一样了所以这个输入就被分配到了第一个和第三个expert那对于第三个输入就被分配到了第二个和第四个expert那这个输入就被分配到了第三个和第四个expert所以从概念上来理解moe的话就是把之前一个dance的ffn layer换成了mo里面的sbusmoe layer我们讨论完了第一个问题什么是moe之后我们讨论第二个问题为什么大家要提出moe这样的架构或者说提出moe这种架构它背后的motivation是什么这就是我经常说的知气然还要知气所以然知道what还要知道why所以我们来讨论一下这个why的问题为了回答这个问题我们首先得从sgaming law开始说起现在的大元模型的发展基本上已经证明sgaming law是有效的简单的说就是从三个方面将模型给sgaming up这三个方面分别就是模型大小也就是参数量训练数据还有计算量当我们从这三个方面把模型sgaming up之后这个模型的性能也会随之提升所以领域类现在都是基本严用这个slu 去开发新的模型但是对于淡死这一类的模型我们在严用sgaming law的时候就发现遇到了瓶景这个瓶景我用一个词来解释叫不可能三角什么叫不可能三角呢就是这里有三个因素互相制约对于淡死模型来说我们不可能三者兼顾这三个因素分别是模型的性能也就是performance其次就是计算的成本第三个就是模式的size也就是我们sgaming law想继续增大的我们来看一下为什么淡死模式会遇到impossible 传媒这个问题如果继续严用sgaming law的思想我们必然要在这个纹度上面增加淡死模式的大小但同时因为它是一个淡死模式所以你增加了参数量之后虽然performance也跟着上去了但是它的cost也会几句的增加这个cost包括训练也包括推理时候的cost所以这就出现了不可能三角不可能对于淡死模型来说同时改善这三个伪度所以这个时候就提出了moe这样的一个思想moe模型本质是把模型的大小也就是它的参数量和计算量的关系进行了结偶这里结偶的意思就是我们可以把模型的参数做得很大很多但是能够控制住模型的计算量使得计算量不会像淡死模糊一样随着这个参数增加而几句的增加因为moe模糊它只击火部分的experts所以它存在两种参数一种就叫做total parameters另外一种叫做实际的击火参数真正模型的计算量是跟这个击火参数相关的跟总参数量没有直接的必然关系所以再回到我们这个不可能三角上面这样我们就能把模型的参数量变大从而使得模型有更好的performance但同时因为有击火参数的存在所以这个cost我们也能很好的控制住所以moe价格从不可能三角这个角度来说就比淡死模糊要好这是第一个解释第二个解释就是一般来说我们认为现在的大元模型Lolage也就是知识的部分是存在于ff这样的价格里面的也就是知识其实是存在于ff参数里面的当我们把f分分成一个个小的experts之后我们其实是引入了所谓的专家的概念这样不同的小的专家当然也是ff分了对应的就是不同的知识这样使得模型类部就有一些专业化的区分可能这个experts对应的是某一类的专业知识这个experts可能对应的就是另外一类的专业知识比如说某个专家可能就对边程类的知识比较擅长那可能另外一个专家就对艺术或者是稳资类的比较擅长同时因为moe的模型可以做得更大使得这个整体moe的layer可以有更多的参数量所以它可以存主下更多的知识相比较淡丝摸到里面的f分而言但又因为它每次计算的时候只击火少量的experts所以这种知识的增加并不会导致计算量的增加第三个解释是我自己个人的一个理解我觉得moe比淡丝摸得好很重要的一个关键点就出在spos这个点上面如果大家对syn对的模型比较了解的话syn模型本质上就是在模型价格里面引入了一个spos的性质让模型通过convolution的layer去形造数据里面存在的那些spos pattern那spos这个哲学思想我觉得能贯穿整个moe's能力甚至是dplanning的发展在早期moe's能力的时候很多的时候就是通过人为的设计故意的在模型当中去引入这种spos的性质我觉得这种spos本质就是从过数据当中去找一些d围的 pattern就类似于现心带数里面的swed所以回到moe这个价格上面来说我认为这种moe的设计其实就是类似于syn里面的convolution的设计人为的加入了一些prire这个prire就是通过人类的鲜艳知识知道这个数据里面或者是信心里面存在一些spos pattern从而去设计这种spos的加构去捕捉信息当中的这些node dimension pattern这样做的一个好处就会使你的能力feat.c提高这就是为什么syn model在图像上面它的性能要好于比如说mrp尤其是当你的数据量比较小的时候这就是因为syn model引入了这种indulctiv bias所以我觉得moe的加构从本上来说也就是人为设计的一种加构引入了这种spos的indulctiv bias使得它的学习效率更高了所以在同样数据量的情况下moe加构要好于dance加构当然这是我个人的一些理解大家可以拿去进行批判式的思考我们再来看一下这个图通过这个图我想给大家强调一个知识点这个图展示的是训练好的moe模型里面experts的机火的情况很做标展示的这些就是experts的ID通过标这里展示的是dome and specialization的百分比然后这里有几个不同的domegithub,archive,vikipid,boost,c4这里我想给大家强调的一个点就是这里其实是有不同的layer的layer0,layer7和layer15因为我们常说moe model里面有多少个expert很可能会给大家产生一个错觉就是这个model里面只有那么些expert但其实不是的每一个layer都有对应的比如说这里可能是64xperts所以整个模型是有很多个这样moelayer叠加起来的所以它不存在某一个expert的学习或者是代表某一类的知识而是不同层的expert一个综合体我们就来具体看一下这什么意思比如说我们看archive这上面可能是一些科研相关的paper你看这个的几火就不是某一个特定expert几火比如说在layer0可能是这个expert到layer7之后就变成这些experts到layer15又变成这些experts所以这个experts几火是横跨不同层的很多expert的一个综合体对于这一个模型来说它不存在某一个或者某几个expert在archive这样的培论上面几火所以大家要理解这个layer的存在回到了what外的问题那剩下就是号的问题amoe这样的架构到底必但是架构好在哪里那我们就来看一下这篇paper来自于alentinstitute这里我想稍微解释一下为什么讲面试题还要给大家误paper因为我觉得现在知识更新实在是太快了很多的知识你是没法在教科书甚至是课堂上面获得的大部分面试当中问到的那些问题很可能就来自于最新的paper所以这也是为什么我想带着大家看看这些原始的paper我知道网上现在有一些讲解面试题提供那种巴古的回答是直接让大家背送我觉得这种方式其实是一种很糟糕的学习方式你可能记住了答案但很快也忘记了而且在面试当中如果你遇到像我这样的面试官不超过三个问题我就能知道你到底是真懂了还是通过突击背松巴古面试题如果你是后者的话那给我印象简回是非常的糟糕所以我建议大家还是知气然并且知气所谓然从根本上去理解这些知识点的源头当然我也知道大家不可能花时间去读这些paper所以这也是我为什么想作为一个英口的目的我想帮大家把这些问题进行加工这样解释大家的时间我们先来看一下这个图这个图比较的是在相同的Trining Flops下Dance Dream的EyeCrisseyDance Dream的任务就有上面展示的这些具体我就不给大家解释了这里比较了三个模型两个是Dance Model一个是1B 一个是7B另外一个就是一个Moe Model总参数量是7B机火参数量是1B所以机火参数就跟1B的Dance Model可以进行比较总参数量7B就可以跟这个7B的Dance Model进行比较这个红色的线在所有的Banch Mark上面都是比另外两个模型要好的这就表明Moe Model在总参数量一样的情况下也就是跟这个Dance 7B的进行比较Moe Model是要好的哪怕机火参数是一样的比如说两个都是1B的模型Moe模型仍然是1BDance模型要好的我们再来看一下这个图Moe比Dance Model另外一个好处就是它的训练时间要少很多并且它的Flops也要少很多这里比较的是一个Moe Model和一个Dance Model他们的机火参数是一样的也这个Dance Model有1.3个Binion的参数这个Moe Model也有1.3个Binion的机火参数但是总参数量是6.9个Binion上面这一行展示的是消耗相同Tocons或者是相同Flops情况下所对应的TrinityLoss我来认识Loss和HelasWag这个Banch Mark上面的性能下面这一行展示是在同样TrinityTime的情况下上面只有三个指标的性能我们就先看TrinityTime可以看到这个红线在TrinityLoss上面是比Dance模型这个蓝线要低一些同时BadationLoss也是要低一些换句话说也就是相同的TrinityTimeMoe Model可以达到更好的Trinity和BadationLoss这里展示的就是Moe Model达到同样的性能训练起来所消耗的时间是更少Moe Model要比Dance模型快两倍大家不要小看两倍当训练一个Moe Model并且参讯很大的时候能结少50%的训练时间这已经是一个巨大的结盛了在看上面的FloxMoe Model要比对应的Dance Model少三倍的Flox所以从这个角度来说Moe Model是比Dance Model更加高效的一个网络架构我们再来看一下Google的这篇Switch Transformers这是Moe里面非常经典的一篇配合Google把Moe这种架构给ScaleUp做到了Trilling Parameters这个图和展示的就是Spaus Model也就是Moe它的一个Scaling Law这一类意就表示Expert数目1R4一直到256表示就是有多少个Expert很做标就是模型的残数这个图想表示的就是这种Moe的架构它是可以被ScaleUp的所以这Expert数量的增多模型残数也在不断的增大同时Taslox在不断的减小也就是性能在不断的提升然后这里做者跟Dance Model也就是这里的T5Base进行了比较上面这些都是Moe Model但是有不同的Expert的数目可以看到在相同的TrillingStab下面这些Moe模型是要源号于Dance Model的Google的做者在这篇配方里面想要传达的一个重要的结论就是Moe Model比Dance Model是更加的Sumple Efficient在同样的训练的Token情况下这种Moe模型学习是更快的因此这样的模型更容易的ScaleUp这也呼应了我在前面讲外的时候给大家解释了为什么现在要提出Moe这样的架构Google在这里也做了一个比较把Moe架构和Dance架构进行了直接的比较这里主要比较的是两个方面一个是TrillingStabs一个是TrillingTime也就是实际的训练时间可以看到不论是从TrillingStabs还是TrillingTime来讲这种Moe的架构都要比这两个Dance架构要更加的Efficient的一些因为Moe架构是更加的Sumple Efficient所以实际训练起来所消耗的时间是更短在这里有25倍到7倍的加速这个是相当客观的所以Google的这篇paper这里问了一个问题给定一个Trilling的Duration和Combination of Budget我们是选择训练一个Dance model还是选择训练一个Sparse model根据这些结果我相信大家一期目中已经有了答案如果我们训练的Budget是很定的话我们当然会选一个Sparse model因为训练起来效率更高我们逐完了Moe的基本架构再来看一下Moe里面的一些常见的改进这个图来自于Dipsick MoeDipsick Moe就引入了两个非常重要的改进第一个叫Fine Green的Expert简单的说就是把之前的比较大的Expert做得更小但是更多比如说之前是N个大Expert现在就做成二N个小Expert另外一个比较重要的改进就是引入了SharedExpert这个想法简单的说就是在模型里面有一些Expert是不经过Router选择的也就是说所有的输入都会经过Expert因此叫做SharedExpert那剩下的Expert是要经过Router选择的那这些改进到底有没有好处呢我们来看一下这个图比较的就是不同Expert的数目从832到64Moe架构的性能注意这里在增加Expert的数目的时候同时也保证总的参数量不变还有计算的Cost也不变所以每个Expert相对应该都变小了那横作标这里展示的是TokenS也就是Flops动作标是Polformance可以看到在相同的Flops情况下32也就是这个潜绿色的线是要好于8也就是这个深紫色线的也就是说当我们把Expert从8增加到32的时候模型的性能是有一定提升的但是如果将Expert数目增加到64的时候这个提升就非常的小了可以看到这个潜绿色的线基本跟红色线已经是重合了虽然在MIMI U这个数据几上面64Gasper 6A比32个要好一点这个结果就说明Expert数目并不是越多越好它有一个Sweetpoint在这个实验里面就是32当Expert数目够多的时候这个性能的提升就不太明显了我觉得这个用我前面的Low-Dermansion SuperSity那个很容易理解你可以简单的理解为这里是在找一个Low-Dermansion能够将模型所学校的指示进行降为如果大家对Sweet比较熟悉的话MISOED去恢复一个图片只选8个违度的话可能图片恢复的效果并不是特别好当你选32个违度的时候图片能够被恢复得很好当你继续增加这个违度的时候比如说到64的时候可能恢复出来的都只是一些噪音了也就是说很可能你的图像信息就在一个约等于32的低尾空间里面所以类似的我们也可以把MIS放到我刚才的思考框架里面去理解那么可能在它这组实验里面模型所学的Low-Lidge可能就已经能够应蛇到一个32围的Expert的Low-Dermansion SuperSity我们接着再来看一下这个图这个图展示的是有没有SharedExpert模型的性能有多大的差别红色的是32个没有SharedExpert蓝色的是31个Raulty的Expert加上一个SharedExpert从它这组实验来看似乎加入一个SharedExpert这个模型性能并没有太大的改善甚至还略为的差了一些比如说ValentationLoss这个率显甚至还在这个红线上面一点点所以这跟Dipsig MOU源式的陪伴里面的结论是有一些想法的这片Paper做着Augula的一个点就是这种SharedExpert其实人为注入了一个鲜眼知识那其实如果不设SharedExpert让模型自己去学哪些Expert对应的是CominLow-Lidge这些学习应该让模型自己去完成而不是由人从价格上来设定所以这片Paper做着认为这个SharedExpert可能作用不大另外MOU里面还有一个比较重要的知识点就是这个头肯的分配问题也就是Routing Strategies有两种常见的头肯分配方式一种叫TokenChoice一种叫ExpertChoice这个背后的Motivation是因为常见的TokenChoice也就是我们前面所说的那种方式我们按照Token来分配这个Router是针对每一个头肯如果这个地方是Top2的话那么对于第一个头肯V从中选两个Expert出来比如说这里选的是Expert1和Expert3对于Token2的话这个Router选择的是Expert3和Expert4那这里就会有一个问题也就是MOU里面最常见的Low Balance问题有可能这些Token都会集中的分配到某些Expert上面然后有一些Expert可能没有分配这样的话这些没有分配的Expert可能训练的就不充分也就失去了MOU这个价格他本身的意义所以这个时候就有人提出那我们可以不从Token的角度来分配而是从Expert的角度来分配所以这里的TopK就是指我从Expert的角度来选TopK出来这样的话因为每个Expert都会选TopK所以对于Expert来说他分配到的Token数目是一致的所以这就有Balance的Workload那具体怎么做呢也非常简单只不过把选头肯的这种方式给倒过来对于Expert1我还是通过这个Router选4个头肯那这里选的是为Love2Starty那对于Expert2这里应该是写错了应该是2选的是We Love Quad Library因为站在Expert的角度选所以这些Token有可能会被重复的选到大家可以看到通过这种Expert choice的方式每个Expert收到的都是4个头肯所以就不存在Token choice里面所遇到的 load balance的问题那这个Expert choice这个方法也有一些问题那第一个问题就是Token抓平的问题大家注意到为Love2Starty是第一个Expert选为Love Quad Library是第二个Expert选那这些没有选到的Token比如说印则这两个头肯没有被选到那么他们就会被抓不掉这就是Token抓平这种Token抓平可能会影响到模型的性能因为他们没有经过计算那第二个比较大的问题就是在春年的时候你有这样完整的句子进行选择但是在influenance时候因为模型是一个Oteroic Glass所以你没法看到后面的这些Token所以这个方法在influenance时候使用会有很大的问题那常用的方法就是在这个roder里面再加入一个MLP的小的网络通过这个小的网络对于每一个influenance时候看到的Token进行预测这个Token会不会在这个TokenK里面如果会的话就用这种方式分配Token那因为这个MLP比较小所以在春年的时候很容易就能够训练出来这种Expert Choice和Token Choice这两种Token分配方式哪一种更好横作标是Flops也就是Token动作标是Performance红色的是Token Choice浅绿色是Expert Choice可以看到这个Token Choice的性能是源好于Expert Choice性能的从这个Valutation loss的客户上面还有春年Loss的客户上面都能看到这样类似的结论这也是为什么在现在的Moei model里面Token Choice仍然是首选当然也会有很多其他的比如说像Low the Balance Loss去客户Token Choice里面Low the Balance的问题那大家可能会好奇如果Moei有这么多有点的话为什么直到最近Moei这个架构才活起来那答案就是这一类的Spot Model它训练起来是比较困难的会存在所谓的Trinity Instability问题这边配合里面就有一个图展示了这种Spot Model也就是Moei训练中存在的问题那左图这个就是一个安stable的Trinity Run也就是不稳定的训练可以看到这个Trinity Loss训练到最后的时候直接就崩掉了所以如何能够稳定的训练一个Moei model这是一个十分有挑战的问题这里面也有很多的技术细节和支持点那我想在单独的视频里面给大家讲解关于Moei架构这方面的支持我就给大家录完了希望这些支持能够帮助到大家对大家的面试有用另外也想宣传一下我跟几个小伙伴打造的一个网站网站的地址是Statis.winStatis就是学习但是付出win就是WN 圣逆的意思这个网站主要是给大家刷课学习的然后我会把我的大模型面试系列做一个课程放到网站上面所以我网级我录制的关于大模型面试的视频然后每个视频我还设计了一些Quiz看完视频之后可以做一下这些练习体验检验你的支持是否学得扎实目前这个网站还是免费的注册登录就可以开始学习如果大家感兴趣的话请抓紧时间注册在我没有改变主义收费之前欢迎大家试用也欢迎大家给我提议检最后注大家GOODLUCK拿到好的欧帅
