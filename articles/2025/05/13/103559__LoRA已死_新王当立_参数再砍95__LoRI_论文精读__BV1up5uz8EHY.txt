Timestamp: 2025-05-13T10:35:59.590287
Title: “LoRA已死，新王当立”参数再砍95%！LoRI【论文精读】 BV1up5uz8EHY
URL: https://b23.tv/myyTZMD
Status: success
Duration: 18:13

Description:
好的，我将根据您的要求，对提供的文本进行提炼、总结，并生成相应的概要和概念图。

**核心结论：** Low-Rank Adaptation (LoRA) 的改进版 Row-Rink 通过减少可训练参数和降低多任务场景下的参数干扰，为大模型的参数高效微调提供了一种有效的方法。

**总体框架：**

1.  **Row-Rink 简介：**
    *   Row-Raw 的改进版，解决了 Row-Raw 在多任务场景下的不足。
    *   减少参数干扰，降低可训练参数，提升模型性能。
2.  **Row-Rink 的优势与劣势：**
    *   **优势：** 减少可训练参数，降低成本；减少多任务场景下的参数干扰；在持续学习中保持灾难性遗忘问题的缓解。
    *   **劣势：** 需要额外的计算资源进行吸收编码；对超参数选择敏感；在某些任务中可能逊于全量微调。
3.  **Row-Rink 的核心思想：**
    *   在 Row-Raw 的基础上进行改进，实现多任务场景下的高效、快速微调。
    *   将投影矩阵 A 固定为随机投影，不再更新。
    *   对矩阵 B 引入任务特定的吸收编码，只保留与特定任务相关的参数进行更新。
4.  **Row-Rink 的关键技术环节：**
    *   冻结随机投影矩阵 A，避免重复更新，节省资源。
    *   通过计算选举的预刺来确定吸收编码，保留与任务最相关的矩阵 B 参数。
    *   设计不同的任务适配器，在表示空间中保持正交性，减少干扰。
    *   在持续学习中，通过两阶段训练（安全对齐阶段和任务适应阶段）缓解灾难性遗忘问题。
5.  **Row-Rink 的公式和实验：**
    *   五个关键公式描述了权重更新、吸收编码计算、拼接方法和线性合并方法。
    *   实验在自然语言理解、数学推理、代码生成和安全对齐等任务上进行，与全量微调、LoRA 等方法进行对比。
    *   实验结果表明，Row-Rink 在单任务和多任务场景下均表现出色，尤其是在参数量较少的情况下。
6.  **结论与未来展望：**
    *   Row-Rink 在减少可训练参数和降低多任务场景下的参数干扰方面取得了显著成果。
    *   未来将探索结构化的吸收模式，并将 Row-Rink 的核心设计扩展到扩散模型或多模态模型中。

<Mermaid_Diagram>
    graph LR
    subgraph Row-Rink 概述
        A[Row-Rink简介] --> B(解决Row-Raw多任务问题);
        A --> C(减少参数干扰);
        A --> D(降低可训练参数);
        A --> E(提升模型性能);
    end

    subgraph 优势与劣势
        F[优势与劣势] --> G{优势: 降低成本, 减少干扰, 缓解遗忘};
        F --> H{劣势: 额外计算, 超参数敏感, 某些任务略逊};
    end

    subgraph 核心思想
        I[核心思想] --> J(Row-Raw改进, 多任务高效微调);
        I --> K{冻结矩阵A, 随机投影};
        I --> L{矩阵B, 任务特定吸收编码};
    end

    subgraph 关键技术
        M[关键技术] --> N(冻结矩阵A, 节省资源);
        M --> O(吸收编码, 保留关键参数);
        M --> P(任务适配器, 正交性减少干扰);
        M --> Q(持续学习, 两阶段缓解遗忘);
    end

    subgraph 公式与实验
        R[公式与实验] --> S{关键公式};
        R --> T{多任务对比实验};
        R --> U{结果优秀};
    end

    subgraph 结论与展望
        V[结论与展望] --> W(成果显著, 高效微调);
        V --> X(探索结构化吸收);
        V --> Y(扩展到多模态模型);
    end

    B --> C & D & E
    G --> H
    J --> K & L
    N --> O & P & Q
    S --> T & U

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#f9f,stroke:#333,stroke-width:2px
    style I fill:#f9f,stroke:#333,stroke-width:2px
    style M fill:#f9f,stroke:#333,stroke-width:2px
    style R fill:#f9f,stroke:#333,stroke-width:2px
    style V fill:#f9f,stroke:#333,stroke-width:2px

    style G fill:#ccf,stroke:#333,stroke-width:1px
    style H fill:#fcc,stroke:#333,stroke-width:1px
    style K fill:#ccf,stroke:#333,stroke-width:1px
    style L fill:#ccf,stroke:#333,stroke-width:1px
    style N fill:#ccf,stroke:#333,stroke-width:1px
    style O fill:#ccf,stroke:#333,stroke-width:1px
    style P fill:#ccf,stroke:#333,stroke-width:1px
    style Q fill:#ccf,stroke:#333,stroke-width:1px
    style S fill:#ccf,stroke:#333,stroke-width:1px
    style T fill:#ccf,stroke:#333,stroke-width:1px
    style U fill:#ccf,stroke:#333,stroke-width:1px
    style W fill:#ccf,stroke:#333,stroke-width:1px
    style X fill:#ccf,stroke:#333,stroke-width:1px
    style Y fill:#ccf,stroke:#333,stroke-width:1px

</Mermaid_Diagram>


Content:
你知道马斯克的过团队最近在偷偷裁员吗?不过被裁的不是工程师,而是AI模型的参数这项能够让AI巨头省下90%算力成本的秘密武器就长在新华大学最新的论文,Low-E里面消参数如灭霸响子保留5%的关键神经人,性能不降法生在微调霸主,Low-Raw的基础上再减95%的参数本期硬和解读,Carlovy如何改写模型训练法则让参数效率重速AI生态欢迎来到六博士的民主会我们每周都将至少有一篇高子安的论文精度发布出来相信在一系列的论文学系以后你,就能成为人工智能领域的专家Low-Raw的基础今天,给大家介绍一篇关于大园模型搞笑微调的研究它叫Row-ERow-E,Row-Raw就是Row-Raw-Rink Adaptation的改进版Row-Raw-Rink Adaptation又减成Row-Raw它是离补了Row-Raw在多任务场景下的不足Row-Raw-Rink只剩下减少这款夸任务的干扰同时降低可训练的参数最终达到提升模型性能的目的我们将会从以下五个部分展开今天的讲解我们首先会对Row-Raw-Rink的游略式进行分析让大家对于这个方法有一个比较全面的认识接著是Row-Raw-Rink的核心思想我们来看它的创新之处是在哪里在有我们就会深入到它的方法部分看它是怎么样提出三个部分的创新在之后就是实验分析的部分这个部分作者就验证Row-Rink的这个有效性以及有略性最后论文的结论以及未来工作的涨挖作为结尾总结了Row-Raw-Rink的贡献并探索了它后面的发展方向我们总结了三个优势、三个劣势首先Row-Raw-Rink它的优势是非常明显的它相比于Row-Raw的话最多是可以减少95%的可训练参数这对于大规模的这种模型的微调以及部署是能够大幅度的去降低成本的另外Row-Rink能够有效的减少多任务场景下的参数干扰来保证模型在多任务的学习里面的性能另外非常重要的一点是Row-Raw-Rink在持续学习当中的还解的载浪性以外的问题这对于模型的长期发展是非常重要的但是Row-Rink它也不是说只有优势没有劣势它也有劣势的在使用这种吸收沿码较准的时候它就需要额外的计算资源增加了预出里的事件另外Row-Raw-Rink对于超餐的选择也是非常敏感的不恰当或是不合理的超餐就导致性能的下降在某些任务里面它也有可能是烧讯于这种全量的微调但就总体而言它Row-Rink的优势还是非常明显的也是为高效微调提供了全型的失路Row-R的核心思想是在Row-R的技术上去做了改进实现多任务场景下面的高效、长速微调具体来说就是Row-Rink通过将投影剧称A固定为随机投影不再对它进行更新然后对于举证B就引入任务的特定吸收编码对它进行吸收化处理只保留了跟特定任务密切相关的仓书进行更新这个策略不仅能够降低可训练仓书的数量还有效的减少不同任务之间的仓书干扰使得模型在多任务的学习里面能够更好的发挥提升整体的性能这种创新的方法还为解决多任务场景下仓书干扰的问题提供了一种新的失路接下来我们聚焦在这个洞结地自投影以及吸收编码这个是Row-Rink的重要的技术环节投影举证A会被洞结并固定成随机的高十分多的出事化这样做的话就避免对举证A反复的更新从了解审了大量的类存空间以及计算资源最终也不会影响到模型的性能接下来就是吸收编码的教训过程首先要利用这个教训的数据级对举证B去进行更新然后来计算选举的预刺来确定吸收编码最终只保留那些跟任务最相关的比举证的仓书进行后续的训练更新透过这种方式Row-Rink能够精准的控制长数更新的范围提高模型的训练教律跟效果这个是文章的公式仪公式仪它就展示了这个全中更新的一个具体数学形式这个Delta-T它表示的是任务T的全中更新的举证它是由低制举证A和低证举证B相成得到的右边这个是原文的Fig-EA这个Fig-EA这个图就非常直观的展示在不同的吸收比率底下仅仅冻结A举证的模型使得一个表现那我们可以看到即便是在90%的吸收比率下面这个模型它的性能依然是比较稳定的说明冻结举证A的可行性减少是配器和运动的干扰是Row-Rink方法的一个重要的部分Row-Rink方法通过巧妙的设计使得不同的任务的设计器在表示空间里面进入正角这样就让每个任务就像是分配了一个独立的通道鼻子之间不会产生过多的干扰具体来讲在Poparty一里面它从理论上去证明在高为的水基投影的情况下不同任务的Row-Rink的设计之间的类迹、进式式领的也就是他们是正角的在实际的操作里面作者提出了两种合并的方法一种是合并的拼接另外一种是现行的拼接我们来看一下经过这一系列的这种设计Fig-1B里面就直观去展示了Row-Rink以及前人的方法的一个比较可以看得到Row-Rink在Math跟Code的两个问题上面它都是有表领先的优势尤其是在Code上面相对于前人在MLU这个上面虽然它略讯于前人的方法但是差距并不大接下来我们来关注一下持续学习中的吸溯性作用在Row里面持续学习中的吸溯性的利用是一个重要的方面为了在持续学习里面要记保持模型的安全对企Row-Rink设计了一个两阶段的训练阶段首先是安全对企的阶段那现在是在安全数据上训练一个安全的侏配器那竟然逆模型的安全的行为准则然后是任务适应的阶段那机器之前较准好的任务的特定也麻了反正对每个下轮任务进行微调得到既安全又能适应特定任务的这个支配器那在这里的话吸溯也麻它就发挥了非常重要的作用它通过了隔离参数的更新使得了安全对企的阶段跟任务适应的阶段更新的参数它不肯回复互相干扰那有效的缓解了这个宰拉性疑问的问题这个Torfig 移吸它就杂志的从安全任务到其他任务的持续学习的一个效果那我们可以看到Row-Rink它是一扬保留著在安全性能的那就是它有一个非常好的一个效果它的前人的一些方法就出现了明显的宰拉性疑问的这种现象那这种吸溯性的利用的这个方法为持续续吸中的支持保留了很好的一个思路接下来我们来看看Row-Rink的各项的方法细节像这个Torf那Torf A就展示了Row-Rink的整体的结构其中这个抖音局证A它就被冻结成随机的高十分部那举证B就通过吸溯也麻轻轻更新那这种设计不仅可以减少可训练的参数的数量还通过了吸溯性控制的参数的更新范围那Fig-RB呢也就是Torf A展示了Row-Rink再次陪企的合并里面的一个优势通过保持不同的任务的伺服器的正交性显著了减少了合并式的这种干扰那这个Torf A就展示了Row-Rink再持续学习里面的应用透过这个两阶段的训练过程的研码吸出研码的这个使用就实现了安全对企和任务适应的同时有效的缓解了宰拉性疑问的问题那接下来我们来聊聊这个Row-Rink的关键公式那一共有五个公式那公式一就是我们前面提到的更新举证调查T就是第一次举成A跟B的成期那公式二它就描述了吸溯研码的计算方法那通过全局的预制保留了最大幅度的参数来实现这个B举证的吸溯化公式三四就描述了拼接的方法就讲的是将多个SPEC A跟B的举证拼接以后进行加全平均来实现任务失佩器的合币那公式五的话就是掌握了这个现性的合币方法那分别就是对A举证跟B举证进行这个加全球会以后再相成得到合币的这个社会器那实现的部分是比较全面的首先数据级的部分分别是四个分别是这个治疗语言理解数学推理大马生成安全对企那对比的这个机线方法就有这个全量微调Lolong跟Dolong那还做了这个超长方面的一个探索就像学习率还有这个Banche等等那接下来我们来看一下这个单任务性的首先这个PableE它展示的是在这个自然女N的这个方面的一个对比那可以看到了在那马生的这个情况下这个RuviD呢它是取得了最好的效果那在多个的这个数据上面它的表现呢都是最有的那它的性能都是最有的那这个Ruvi的S也是非常的亮眼那它的这个参数相比小于传统的Rolong或是Dolong它的这个参数量只有他们的20分之一那损失的性能并不大那Misro的7B的话呢这个情况下呢那实际上它的这个成绩一样是非常亮眼的虽然这个RuviD的话呢并没有取得最好的成绩当然也接近了跟这个前面这个它是刺右就差了0.4那我觉得更亮眼的还是这个RuviS那这个它还是用了这个20分之一的这个更新的这个参数它就达到了这个相差不多的这个性能所以这个还是非常厉害的那Table呢它分别是展示了在Math数学然后代码这个Human Evaluate以及Safety安全方面的几个领域的一个验证那我们可以看到RuviD跟RuviS的话呢它基本上是展具的最有跟刺右的主要的这个数据那其中的话呢在用Lama 3 杠8B的时候RuviD的这个优势会更大性能上的这个优势那在Meshore 7B的话呢就是RuviS的优势呢看了会更大那我们可以看得到啊这个RuviS在用这么少的参数的根金的情况下也能取得这么高的成绩啊确实是非常厉害的实际上在真实的场景里面呢这种多任务的合并的情况的大模型呢应该才是更加的常用的那现在Table3它就展示了在自然语言数学理解还有代码声层以及安全对企四个方面去做了评估我们可以看到RuviD呢它占据了多数的最优这个加速的还有刺右就下面这个代画线的多数的这个最优跟刺右的这个成绩都蓝过在这个RuviD的这是个上面那所以看可以看得出啊这个RuviD这个Ruvi的这个设计它的一个有效性那当然这个RuviS其实也不差因为它的这个优势的话呢甚至在于用比较少的这个差数呢去获得一个比较好的一个成绩那在持续学习这个方面呢RuviD呢那更是有很大的一个优势的表现那首先呢我们来看这三个的SyncTask还有两个的Multitask他们不论是Safety也好自然也好或者说是这个代码或者是数学也好那他们的几个这个Tformance的水平呢都是比较在同一个线上而且是水平比较高的那像我们看了这个Multitask的Roa的话呢那它就差的比较多了就可能只有其他人的大概是一半那我们可以看到完了因为这个MultitaskRoa第一呢它的这个更新的这个参数相对会更多一些那所以的话呢那在后续呢这个在Multitask的话呢它会比这个Ruvi的S呢是要稍微性能要好一点点那基本上呢我们可以看SyncTask的这个RuviD呢它其实不像上下跟那个Multitask那多数情况它是领先的当然领先的其实也并不多那所以也可以看得到这个Multitask的这个Ruvi实际上是非常Powerful的那因为呢它不仅仅是要一个任务它是关注的是多任务那它如果出来的表现都跟这个SyncTask的不像上下的话说明它的这个结果也非常强的这个卢邦信那当然呢在这个安全方面的话呢这个更新参数更少的RuviS是更有优势的那它的提升的这个幅度是比较大的我们可以看到这里有史将近3的点那当然这个提升不多这个有2个点那所以呢各有千秋那但是总结下来就是这个Ruvi它就是非常好的我们来看到双实验的Figus A的这个部分那4A的话呢它讲的是这个较准的这个步骤跟Aqueous的一个关系那分别的验证的数学以及代码的两个方面那我们可以看得到那这个总体上呢就是你的这个教验的步骤越多它呈现的就是一个正相关的一个趋势当然那它这个过程它并不是陷阱它就有点像是这种会有一点取著它就不一定说你增加了它就一定是变好它总体的趋势是变好的像到一千的话它就会总体上来看会比600之前的都要好这个步骤那Figus A的B呢它就是讲这个C书的一个比率还有这个LayerIndus的一个关系那我们可以看到Up and Down那其实就是这个粹前相的这个部分那就是两个口谊那我们可以看到了Up and Down它比这个KV这个KV是注意力机制方面的类由它这个C书的比率是要更低的其中Up and Down就会比到那个更低那这个C书比率它越高的话这个参数就越少那越低的话就参数越多那很明显那它这个表明就是说首先Up and Down这个参数它是比到总体上来看是要更多的2月升的话就那个Layer的Indus 越大的话那它的这个参数也是越多的那这个也是给这个一个可解释性那一般来讲那模型肯定是立得有参数那就参数它才能学习那它主要是集中在这个LayerIndus比较大的然后是在Up and Down的这些位置里面在这个KV前相的这个部分里面那Figus A的视线呢它看所的是这个研码的这个Guerna-Layer也就是它的力度跟这个性能之前的关系那从这个C的这个图来看就是当它选择那个研码的范围越大的话那它性能是要会越好的那这个摸抖极的话那实际上就是整个摸抖里面去选一个圆码那你越往下面的话那就是到Layer到Mesh X那它的这个范围就越来越小了那再有这个Figus A的视线呢就是讲这个合并权重的一个效果那这个合并权重的话我们可以看得出来实际上有好有坏比如说像在0.4的时候除了Code那它的这个效果都会很不错但是呢在Code上面的这个退化就比较严重了所以它相对来讲不要均衡的是这个位置等0.3的时候总体的一个效果是比较好的那再讲完这个消息宴以后就到结论跟FutureWare的部分那Loby在减少可训练差数和降低端物场景下的差数刚有方面取得非常显出的成果那为大模型的这个差数的高效微调提供了一种有效的绝发案那FutureWare主要就是集中在探索结构化的吸收模式比如像简直啊或者是快吸收等等那另外一个呢是江陆瑜的这个核心设计要拓展到这个扩散模型或者是4月以原的这种多模太的顶誉的这些模型来满足这个多模太应用的一个需求那我们今天云主会呢到这里就结束了如果对理由所起发了感谢三年加班住如果理由什么想法欢迎留言我们下期节目不见不散
