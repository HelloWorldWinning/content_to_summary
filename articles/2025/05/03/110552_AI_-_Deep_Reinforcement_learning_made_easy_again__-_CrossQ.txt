Timestamp: 2025-05-03T11:05:52.431506
Title: AI - Deep Reinforcement learning made easy again! - CrossQ
URL: https://youtube.com/watch?v=UpACdOz6ntw&si=P6XstylsFoB54OmJ
Status: success
Duration: 46:58

Description:
好的，这是对提供文本的总结，重点在于核心思想和清晰的结构，并附上美人鱼图：

**总结：**

**核心结论：** 通过正确应用批量归一化（Batch Normalization）并移除目标网络，可以显著提高深度强化学习的样本效率并降低计算成本。

**总体框架：** 本研究旨在通过改进训练过程本身来加速深度强化学习，而不是仅仅增加训练量。

**I. 引言**

*   深度强化学习（DRL）的目标是学习一个策略，以最大化累积折扣奖励。
*   提高样本效率至关重要，尤其是在真实机器人等昂贵环境中。
*   计算效率（例如，墙钟时间或梯度步数）同样重要。
*   更新数据比率（UTD）是每次环境交互的更新次数。

**II. 当前技术水平的局限性**

*   当前技术通过增加UTD来提高样本效率。
*   基于模型的策略优化和诸如RedQ、DrQ和带有重置的SAC等无模型方法都属于这一类。
*   高UTD会增加计算成本和算法复杂度。

**III. 批量归一化（Batch Normalization）**

*   批量归一化是一种深度学习技术，可以加速训练并改善损失状况。
*   它标准化了层之间的激活，从而可以使用更大的学习率并改善梯度流。
*   批量归一化通过估计批量的均值和标准差来工作，然后应用变换。
*   推理时，使用训练期间计算的运行平均值。

**IV. 交叉 Q（CrossQ）：解决批量归一化的挑战**

*   直接在DRL中应用批量归一化通常会失败，因为行为策略导致状态和动作分布不匹配。
*   **解决方案：** 将状态和动作批量连接在一起进行前向传递，以确保一致的批量统计信息。
*   交叉 Q 删除了目标网络，从而简化了算法并提高了效率。
*   更宽的层可以进一步提高性能。

**V. 实验结果**

*   交叉 Q 在样本效率方面优于当前的技术水平。
*   交叉 Q 的计算效率更高（梯度步数减少 95%）。
*   交叉 Q 的 Q 函数偏差可能高于 RedQ，但仍然表现良好。
*   实验表明，批量归一化是性能提升的主要驱动力。

**VI. 未来方向**

*   在真实机器人上测试交叉 Q。
*   将交叉 Q 扩展到更大规模的任务，例如视觉强化学习。

<Mermaid_Diagram>
    graph LR
    subgraph DRL Framework
        A[Deep Reinforcement Learning] --> B(Sample Efficiency);
        A --> C(Computational Cost);
        A --> D(Algorithm Complexity);
    end

    subgraph Current State of Art
        E[High Update-to-Data Ratio (UTD)] --> F(Model-Based Optimization);
        E --> G(RedQ, DrQ, SAC with Resets);
        F --> H{Increased Sample Efficiency};
        G --> H;
        H --> I[High Computational Cost & Complexity];
    end

    subgraph Batch Normalization & CrossQ
        J[Batch Normalization (BatchNorm)] --> K(Accelerates Training);
        K --> L{Improved Loss Landscape};
        L --> M(Larger Learning Rates);
        L --> N(Better Gradient Flow);
        O[CrossQ Methodology] --> P{Concatenate State & Action Batches};
        P --> Q(BatchNorm with Consistent Statistics);
        O --> R{Remove Target Networks};
        O --> S{Wider Layers};
        Q --> T{State-of-the-Art Sample Efficiency};
        R --> T;
        S --> T;
        T --> U(Reduced Computational Cost);
        U -- Empirical Result --> V{Main Performance Driver: BatchNorm};
    end

    subgraph Future Directions
        W[Real-World Testing (Robotics)] --> X(Real-Time Performance);
        Y[Scaling Up (Visual RL)] --> Z(Larger Task Performance);
    end

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style J fill:#ccf,stroke:#333,stroke-width:2px
    style O fill:#afa,stroke:#333,stroke-width:2px
    style T fill:#faa,stroke:#333,stroke-width:2px
    style W fill:#aac,stroke:#333,stroke-width:2px
    style Y fill:#aac,stroke:#333,stroke-width:2px
</Mermaid_Diagram>


Content:
WEBVTT Kind: captions Language: en uh welcome back everybody to another uh welcome back everybody to another uh welcome back everybody to another episode uh with meti with a fantastic episode uh with meti with a fantastic episode uh with meti with a fantastic researcher today we have two amazing researcher today we have two amazing researcher today we have two amazing researchers uh Daniel and Adia who are researchers uh Daniel and Adia who are researchers uh Daniel and Adia who are going to present for us cross Q which is going to present for us cross Q which is going to present for us cross Q which is a top five% Spotlight at the coming eye a top five% Spotlight at the coming eye a top five% Spotlight at the coming eye clear conference So Daniel off to clear conference So Daniel off to clear conference So Daniel off to you well thank you very much uh for the you well thank you very much uh for the you well thank you very much uh for the nice introduction hon and also thanks nice introduction hon and also thanks nice introduction hon and also thanks for the invitation um today ad and I for the invitation um today ad and I for the invitation um today ad and I present joint work which is going to be present joint work which is going to be present joint work which is going to be at I Clea at I Clea at I Clea 2024 um it's called cross Q batch 2024 um it's called cross Q batch 2024 um it's called cross Q batch normalization and deep reinforcement normalization and deep reinforcement normalization and deep reinforcement learning for greater sample efficiency learning for greater sample efficiency learning for greater sample efficiency and simplicity all right let's jump and simplicity all right let's jump and simplicity all right let's jump right into it right into it right into it so what we want to show you in this so what we want to show you in this so what we want to show you in this presentation is that you can accelerate presentation is that you can accelerate presentation is that you can accelerate deep reinforcement learning methods um deep reinforcement learning methods um deep reinforcement learning methods um with old tricks uh as from from regular with old tricks uh as from from regular with old tricks uh as from from regular deep learning and more specifically what deep learning and more specifically what deep learning and more specifically what we do is we take uh an off policy OC we do is we take uh an off policy OC we do is we take uh an off policy OC critic method like sack we delete the critic method like sack we delete the critic method like sack we delete the target networks and we add batch target networks and we add batch target networks and we add batch normalization um and then what we end up normalization um and then what we end up normalization um and then what we end up with is uh state-ofthe-art sample with is uh state-ofthe-art sample with is uh state-ofthe-art sample efficiency um at uh reduced vastly efficiency um at uh reduced vastly efficiency um at uh reduced vastly reduced computational cost and you can reduced computational cost and you can reduced computational cost and you can already see that on the right hand side already see that on the right hand side already see that on the right hand side these are aggregated performances over these are aggregated performances over these are aggregated performances over six moku environments um and uh on the six moku environments um and uh on the six moku environments um and uh on the top you see the sample efficiency where top you see the sample efficiency where top you see the sample efficiency where cross Q outperforms the current state of cross Q outperforms the current state of cross Q outperforms the current state of thee art baselines and on the bottom uh thee art baselines and on the bottom uh thee art baselines and on the bottom uh you can see the computational you can see the computational you can see the computational efficiency uh which is where cross Q efficiency uh which is where cross Q efficiency uh which is where cross Q really shines so sorry yes uh just a really shines so sorry yes uh just a really shines so sorry yes uh just a question so so on the x-axis you're question so so on the x-axis you're question so so on the x-axis you're drawing percentage of environment steps drawing percentage of environment steps drawing percentage of environment steps right so yes what is the total number of right so yes what is the total number of right so yes what is the total number of environment steps uh so that is a 300K environment steps uh so that is a 300K environment steps uh so that is a 300K environment step so it is basically what environment step so it is basically what environment step so it is basically what the Bas lines used um and then on the Y the Bas lines used um and then on the Y the Bas lines used um and then on the Y AIS we have the iqm normalized returns AIS we have the iqm normalized returns AIS we have the iqm normalized returns um but we're going to get more into um but we're going to get more into um but we're going to get more into detail this like more as an overview of detail this like more as an overview of detail this like more as an overview of uh kind of what you're going to expect uh kind of what you're going to expect uh kind of what you're going to expect okay okay all right um cool right so for okay okay all right um cool right so for okay okay all right um cool right so for the purpose of this talk let's uh back the purpose of this talk let's uh back the purpose of this talk let's uh back it up a little bit and quickly introduce it up a little bit and quickly introduce it up a little bit and quickly introduce the reinforcement learning problem and the reinforcement learning problem and the reinforcement learning problem and um here we have an agent um interacting um here we have an agent um interacting um here we have an agent um interacting with an environment by sending actions a with an environment by sending actions a with an environment by sending actions a to this environment and then in return to this environment and then in return to this environment and then in return it gets a new state and reward from this it gets a new state and reward from this it gets a new state and reward from this environment and the goal of this agent environment and the goal of this agent environment and the goal of this agent is to maximize the cumulative discounted is to maximize the cumulative discounted is to maximize the cumulative discounted reward which It Gets By by interacting reward which It Gets By by interacting reward which It Gets By by interacting with the environment um with the environment um with the environment um and the the idea is kind of to learn to and the the idea is kind of to learn to and the the idea is kind of to learn to learn a policy which maximizes this um learn a policy which maximizes this um learn a policy which maximizes this um and this is depicted in this loop on the and this is depicted in this loop on the and this is depicted in this loop on the right hand side which is the update right hand side which is the update right hand side which is the update Loop so what is very important and what Loop so what is very important and what Loop so what is very important and what a lot of research focuses on is to a lot of research focuses on is to a lot of research focuses on is to increase the sample efficiency of these increase the sample efficiency of these increase the sample efficiency of these methods because usually these methods methods because usually these methods methods because usually these methods need a lot of those interactions with need a lot of those interactions with need a lot of those interactions with the environment and especially in the the environment and especially in the the environment and especially in the case where we don't just have any case where we don't just have any case where we don't just have any simulated environment but we have for simulated environment but we have for simulated environment but we have for example a real robot um that needs to example a real robot um that needs to example a real robot um that needs to interact with with with the physical interact with with with the physical interact with with with the physical environment um samples are really environment um samples are really environment um samples are really expensive and that is both because the expensive and that is both because the expensive and that is both because the Real Environment Works in real time um Real Environment Works in real time um Real Environment Works in real time um so it is kind of slow to generate these so it is kind of slow to generate these so it is kind of slow to generate these samples samples samples um and on the other hand for example we um and on the other hand for example we um and on the other hand for example we also have problems like the hardware also have problems like the hardware also have problems like the hardware wears down over time and we just cannot wears down over time and we just cannot wears down over time and we just cannot afford to to have millions and millions afford to to have millions and millions afford to to have millions and millions of of of interactions uh but one thing that is interactions uh but one thing that is interactions uh but one thing that is probably equally important is the time probably equally important is the time probably equally important is the time spent over here in this on this right spent over here in this on this right spent over here in this on this right hand side which is kind of the update hand side which is kind of the update hand side which is kind of the update procedure um if we're on a real system procedure um if we're on a real system procedure um if we're on a real system for example we also might have real time for example we also might have real time for example we also might have real time constraints and we just don't have time constraints and we just don't have time constraints and we just don't have time to wait forever um for uh SGD to to give to wait forever um for uh SGD to to give to wait forever um for uh SGD to to give us our new policy and we will refer to us our new policy and we will refer to us our new policy and we will refer to this as the computational efficiency of this as the computational efficiency of this as the computational efficiency of such an algorithm and we might measure such an algorithm and we might measure such an algorithm and we might measure this for example in wall clock time or this for example in wall clock time or this for example in wall clock time or uh the number of gradient steps uh uh an uh the number of gradient steps uh uh an uh the number of gradient steps uh uh an algorithm is algorithm is algorithm is doing one important definition that doing one important definition that doing one important definition that we're going to need in in in the rest of we're going to need in in in the rest of we're going to need in in in the rest of this talk is the update to data ratio this talk is the update to data ratio this talk is the update to data ratio which is simply the ratio between the which is simply the ratio between the which is simply the ratio between the number of updates you do per environment number of updates you do per environment number of updates you do per environment step all right and that brings us step all right and that brings us step all right and that brings us actually right next to the next slide so actually right next to the next slide so actually right next to the next slide so which is about the question what does which is about the question what does which is about the question what does current state-of-the-art do and it is current state-of-the-art do and it is current state-of-the-art do and it is they really increase these update to they really increase these update to they really increase these update to data ratios to have very high update to data ratios to have very high update to data ratios to have very high update to data ratios and there's a number of data ratios and there's a number of data ratios and there's a number of examples so classically we have examples so classically we have examples so classically we have modelbased policy optimization as a as a modelbased policy optimization as a as a modelbased policy optimization as a as a model mod based algorithm where you model mod based algorithm where you model mod based algorithm where you train a model and then you generate train a model and then you generate train a model and then you generate synthetic data so you can do more policy synthetic data so you can do more policy synthetic data so you can do more policy updates per real environment updates per real environment updates per real environment interactions and then more recently on interactions and then more recently on interactions and then more recently on the model fre side we have algorithms the model fre side we have algorithms the model fre side we have algorithms like Red Q draw Q or sack with like Red Q draw Q or sack with like Red Q draw Q or sack with resets um which do higher update data resets um which do higher update data resets um which do higher update data ratios and take different measures to be ratios and take different measures to be ratios and take different measures to be able to do able to do able to do so and arguably high utds are great in so and arguably high utds are great in so and arguably high utds are great in the sense that they boost sample the sense that they boost sample the sense that they boost sample efficiency but but they definitely come efficiency but but they definitely come efficiency but but they definitely come at the cost of uh very high at the cost of uh very high at the cost of uh very high computational cost um and they add or computational cost um and they add or computational cost um and they add or usually add um the problem of more usually add um the problem of more usually add um the problem of more algorithmic complexity so you need for algorithmic complexity so you need for algorithmic complexity so you need for example regularization tricks in the in example regularization tricks in the in example regularization tricks in the in the case of Red Q and draw Q you need to the case of Red Q and draw Q you need to the case of Red Q and draw Q you need to account for the Q function bias and that account for the Q function bias and that account for the Q function bias and that just makes the overall algorithm more just makes the overall algorithm more just makes the overall algorithm more complicated so the question that we want complicated so the question that we want complicated so the question that we want to ask is to ask is to ask is um could we accelerate the training um could we accelerate the training um could we accelerate the training itself instead of just training more um itself instead of just training more um itself instead of just training more um to be in a sense more like uh the soft to be in a sense more like uh the soft to be in a sense more like uh the soft actor critic which uses um a low update actor critic which uses um a low update actor critic which uses um a low update to data ratio so it does one environment to data ratio so it does one environment to data ratio so it does one environment interaction and then one interaction and then one interaction and then one training all right and to tell you more training all right and to tell you more training all right and to tell you more about that we're actually going to about that we're actually going to about that we're actually going to quickly switch to adicha and in case you quickly switch to adicha and in case you quickly switch to adicha and in case you have any questions about this you can have any questions about this you can have any questions about this you can ask this well switch so okay um so in case nobody has any so okay um so in case nobody has any so okay um so in case nobody has any questions I'm going to questions I'm going to questions I'm going to proceed um all right so as Daniel said proceed um all right so as Daniel said proceed um all right so as Daniel said we want to now look into really we want to now look into really we want to now look into really accelerating uh training instead so accelerating uh training instead so accelerating uh training instead so instead of uh specifically trying to instead of uh specifically trying to instead of uh specifically trying to understand you know Q bias medication understand you know Q bias medication understand you know Q bias medication and and such things we are going to look and and such things we are going to look and and such things we are going to look into ideas from Deep into ideas from Deep into ideas from Deep learning architectural ideas uh because learning architectural ideas uh because learning architectural ideas uh because we actually are aware of some of these we actually are aware of some of these we actually are aware of some of these things and that uh specifically here is things and that uh specifically here is things and that uh specifically here is going to be batch going to be batch going to be batch normalization so batch is pretty old by normalization so batch is pretty old by normalization so batch is pretty old by now uh for some sense of old it came out now uh for some sense of old it came out now uh for some sense of old it came out in 2015 and they found that uh if you in 2015 and they found that uh if you in 2015 and they found that uh if you use batn uh in your deep networks like use batn uh in your deep networks like use batn uh in your deep networks like just stuff between your layers then uh just stuff between your layers then uh just stuff between your layers then uh you're going to be able to descend the you're going to be able to descend the you're going to be able to descend the loss landscape really fast it loss landscape really fast it loss landscape really fast it accelerates training um you can get to accelerates training um you can get to accelerates training um you can get to the same low losses uh and order of the same low losses uh and order of the same low losses uh and order of magnitude faster and why bom helps in magnitude faster and why bom helps in magnitude faster and why bom helps in this way it's it's still a bit of a this way it's it's still a bit of a this way it's it's still a bit of a mystery uh there's been many papers mystery uh there's been many papers mystery uh there's been many papers about it and nothing's very conclusive about it and nothing's very conclusive about it and nothing's very conclusive yet but the general idea is that it yet but the general idea is that it yet but the general idea is that it improves the conditioning of your improves the conditioning of your improves the conditioning of your network and this will uh somehow let you network and this will uh somehow let you network and this will uh somehow let you use bigger bigger learning rates and use bigger bigger learning rates and use bigger bigger learning rates and just make the gradients flow just make the gradients flow just make the gradients flow better so what is bnom exactly let's better so what is bnom exactly let's better so what is bnom exactly let's unpack this unpack this unpack this so uh let's say that you got some neural so uh let's say that you got some neural so uh let's say that you got some neural network uh you got some layers let's say network uh you got some layers let's say network uh you got some layers let's say they are fully connected layers uh could they are fully connected layers uh could they are fully connected layers uh could be anything really but let's say they're be anything really but let's say they're be anything really but let's say they're FC layers and you have some FC layers and you have some FC layers and you have some nonlinearities on top of them so you can nonlinearities on top of them so you can nonlinearities on top of them so you can augment this network by stuffing in augment this network by stuffing in augment this network by stuffing in these batch Nom layers so now let's blow these batch Nom layers so now let's blow these batch Nom layers so now let's blow this up what's inside one of these this up what's inside one of these this up what's inside one of these bional bional bional layers uh so the way this layer works is layers uh so the way this layer works is layers uh so the way this layer works is uh you got an incoming batch of samples uh you got an incoming batch of samples uh you got an incoming batch of samples these samples are feature vectors uh one these samples are feature vectors uh one these samples are feature vectors uh one for every data point and let's say that for every data point and let's say that for every data point and let's say that this feature Vector has a bunch of this feature Vector has a bunch of this feature Vector has a bunch of different entries inside of it uh like different entries inside of it uh like different entries inside of it uh like red green blue and red green blue and red green blue and purple um now these entries inside the purple um now these entries inside the purple um now these entries inside the feature vectors can be differently feature vectors can be differently feature vectors can be differently distributed uh like some can have very distributed uh like some can have very distributed uh like some can have very high variance and some can be pretty high variance and some can be pretty high variance and some can be pretty narrow and peaked and and they have narrow and peaked and and they have narrow and peaked and and they have different average values they're different average values they're different average values they're definitely distributed being the definitely distributed being the definitely distributed being the point um and the batom authors argue point um and the batom authors argue point um and the batom authors argue that uh these different diverse that uh these different diverse that uh these different diverse distributions and also the way they distributions and also the way they distributions and also the way they change quick question in this in this uh change quick question in this in this uh change quick question in this in this uh picture each column corresponds to the picture each column corresponds to the picture each column corresponds to the data point yeah each column is one data data point yeah each column is one data data point yeah each column is one data point yes that's a feature Vector yeah point yes that's a feature Vector yeah point yes that's a feature Vector yeah yeah I got yeah I got yeah I got you um right so so you send in these you um right so so you send in these you um right so so you send in these batches of of data points uh which are batches of of data points uh which are batches of of data points uh which are these different feature these different feature these different feature distributions and what bom does is it distributions and what bom does is it distributions and what bom does is it estimates the batch uh moments so it's estimates the batch uh moments so it's estimates the batch uh moments so it's going to estimate the empirical mean of going to estimate the empirical mean of going to estimate the empirical mean of the batch it's going to subtract that the batch it's going to subtract that the batch it's going to subtract that from these samples then it's going to from these samples then it's going to from these samples then it's going to divided by the estimated standard divided by the estimated standard divided by the estimated standard deviation and then it's going to do two deviation and then it's going to do two deviation and then it's going to do two quick operations which are oh by the way quick operations which are oh by the way quick operations which are oh by the way so upon doing this uh the expectation is so upon doing this uh the expectation is so upon doing this uh the expectation is that you somewhat standardize your data that you somewhat standardize your data that you somewhat standardize your data of course like your data is not really of course like your data is not really of course like your data is not really gaussin distributed but if it were gaussin distributed but if it were gaussin distributed but if it were gaussin distributed then the result of gaussin distributed then the result of gaussin distributed then the result of this transformation would be that you this transformation would be that you this transformation would be that you get uh something resembling a unit get uh something resembling a unit get uh something resembling a unit gum um in any case this is a very nice gum um in any case this is a very nice gum um in any case this is a very nice uh data pre-processing step in pretty uh data pre-processing step in pretty uh data pre-processing step in pretty much any regression pipeline so so much any regression pipeline so so much any regression pipeline so so Bashon just bakes it in and then you've Bashon just bakes it in and then you've Bashon just bakes it in and then you've got these two extra learned uh parameter got these two extra learned uh parameter got these two extra learned uh parameter vectors so these are not like dense vectors so these are not like dense vectors so these are not like dense layers these are just like flat single layers these are just like flat single layers these are just like flat single vectors that you element wise multiply vectors that you element wise multiply vectors that you element wise multiply and and then add this is uh to restore and and then add this is uh to restore and and then add this is uh to restore the representational capacity of pom the representational capacity of pom the representational capacity of pom because if you very aggressively because if you very aggressively because if you very aggressively standardize your data like this it standardize your data like this it standardize your data like this it somehow kills how much kills the somehow kills how much kills the somehow kills how much kills the diversity of functions you can learn so diversity of functions you can learn so diversity of functions you can learn so that's why they include these two extra that's why they include these two extra that's why they include these two extra vectors so that's the learnable scale vectors so that's the learnable scale vectors so that's the learnable scale and and and shift and once you apply these shift and once you apply these shift and once you apply these Transformations uh you get the so-called Transformations uh you get the so-called Transformations uh you get the so-called normalized vectors of course it's not normalized vectors of course it's not normalized vectors of course it's not really normalized right they're actually really normalized right they're actually really normalized right they're actually denormalized afterwards in a more denormalized afterwards in a more denormalized afterwards in a more controlled manner with gamma and beta controlled manner with gamma and beta controlled manner with gamma and beta but that's what they call them um but that's what they call them um but that's what they call them um important caveat to important caveat to important caveat to remember um when you're doing inference remember um when you're doing inference remember um when you're doing inference you don't necessarily have a batch to you don't necessarily have a batch to you don't necessarily have a batch to feed into your network you probably have feed into your network you probably have feed into your network you probably have single data single data single data points um or you could have differently points um or you could have differently points um or you could have differently sized batches depending on what you want sized batches depending on what you want sized batches depending on what you want to do so then uh you don't actually use to do so then uh you don't actually use to do so then uh you don't actually use the batch estimates uh of these mean and the batch estimates uh of these mean and the batch estimates uh of these mean and variances you instead use a running variances you instead use a running variances you instead use a running average that you've been silently average that you've been silently average that you've been silently Computing during test time to uh Computing during test time to uh Computing during test time to uh standardize your data um sorry you've standardize your data um sorry you've standardize your data um sorry you've been Computing it at training time and been Computing it at training time and been Computing it at training time and you use it at test time so so that's how you use it at test time so so that's how you use it at test time so so that's how baton Works uh but uh now we would like baton Works uh but uh now we would like baton Works uh but uh now we would like to use batom in DL specifically in in to use batom in DL specifically in in to use batom in DL specifically in in off policy TD which is where you know off policy TD which is where you know off policy TD which is where you know the most benefits of of sample the most benefits of of sample the most benefits of of sample efficiency gains lie but there are some efficiency gains lie but there are some efficiency gains lie but there are some issues uh and we not the first ones to issues uh and we not the first ones to issues uh and we not the first ones to have had this have had this have had this idea but it turns out that the internet idea but it turns out that the internet idea but it turns out that the internet is uh littered with the graves of is uh littered with the graves of is uh littered with the graves of batchnorm in d parl implementations uh batchnorm in d parl implementations uh batchnorm in d parl implementations uh people have found that it just doesn't people have found that it just doesn't people have found that it just doesn't work uh usually when people insert batch work uh usually when people insert batch work uh usually when people insert batch Nom in like your Q functions uh they Nom in like your Q functions uh they Nom in like your Q functions uh they find that it simply fails to train or find that it simply fails to train or find that it simply fails to train or the Q networks stge like the Q values the Q networks stge like the Q values the Q networks stge like the Q values just blow up to infinity or something just blow up to infinity or something just blow up to infinity or something like this uh and by now in all these like this uh and by now in all these like this uh and by now in all these years the general recommendation or years the general recommendation or years the general recommendation or wisdom is that don't use batch Nom if wisdom is that don't use batch Nom if wisdom is that don't use batch Nom if you can avoid it in fact it never has you can avoid it in fact it never has you can avoid it in fact it never has seemed to work so people always avoid seemed to work so people always avoid seemed to work so people always avoid it um let's now try and understand why it um let's now try and understand why it um let's now try and understand why it doesn't seem to work for people so it doesn't seem to work for people so it doesn't seem to work for people so what happens in TD learning you sample what happens in TD learning you sample what happens in TD learning you sample some uh off policy transitions from your some uh off policy transitions from your some uh off policy transitions from your replay buffer you sample them in patches replay buffer you sample them in patches replay buffer you sample them in patches right so you got States the actions you right so you got States the actions you right so you got States the actions you took at those States took at those States took at those States historically uh the rewards that you got historically uh the rewards that you got historically uh the rewards that you got in those Transitions and the next states in those Transitions and the next states in those Transitions and the next states that you got right after and then you uh that you got right after and then you uh that you got right after and then you uh you would take your currently trained you would take your currently trained you would take your currently trained policy whatever iterate you have during policy whatever iterate you have during policy whatever iterate you have during the agent training process and you the agent training process and you the agent training process and you sample sample sample actions for the next States according to actions for the next States according to actions for the next States according to that policy so so your sampled batch is that policy so so your sampled batch is that policy so so your sampled batch is of policy data but your sample actions of policy data but your sample actions of policy data but your sample actions are according to your current are according to your current are according to your current policy and now you're going to do two policy and now you're going to do two policy and now you're going to do two forward passes forward passes forward passes first you're going to feed s comma a first you're going to feed s comma a first you're going to feed s comma a into your Q Network you're going to into your Q Network you're going to into your Q Network you're going to generate Q generate Q generate Q predictions then you're going to predictions then you're going to predictions then you're going to generate Q predictions for the uh next generate Q predictions for the uh next generate Q predictions for the uh next time steps pair of s comma time steps pair of s comma time steps pair of s comma a and finally you're going to compute a and finally you're going to compute a and finally you're going to compute the TD loss uh which is this and where the TD loss uh which is this and where the TD loss uh which is this and where gamma is the discount gamma is the discount gamma is the discount Factor now this is uh probably the Factor now this is uh probably the Factor now this is uh probably the standard implementation that most of standard implementation that most of standard implementation that most of these uh uh actor critics these uh uh actor critics these uh uh actor critics uh training methods have the actor uh training methods have the actor uh training methods have the actor training step is separate but we don't training step is separate but we don't training step is separate but we don't show that here it's not so show that here it's not so show that here it's not so relevant but there's something here that relevant but there's something here that relevant but there's something here that can really cause uh issues for batn and can really cause uh issues for batn and can really cause uh issues for batn and in this paper we identify that so if you in this paper we identify that so if you in this paper we identify that so if you look closely at the two forward passes look closely at the two forward passes look closely at the two forward passes you'll see that the state uh batches you'll see that the state uh batches you'll see that the state uh batches they have to be identically distributed they have to be identically distributed they have to be identically distributed right it doesn't matter uh what St and right it doesn't matter uh what St and right it doesn't matter uh what St and St +1 exactly are the point is that uh St +1 exactly are the point is that uh St +1 exactly are the point is that uh they're just uh pretty much the same set they're just uh pretty much the same set they're just uh pretty much the same set just offset by one time step and they just offset by one time step and they just offset by one time step and they have the same distribution you can have the same distribution you can have the same distribution you can verify this empirically of course but verify this empirically of course but verify this empirically of course but what's different is this these guys they what's different is this these guys they what's different is this these guys they have different distributions because at have different distributions because at have different distributions because at comes from your replay buffer but A+ one comes from your replay buffer but A+ one comes from your replay buffer but A+ one comes from your currently trained policy comes from your currently trained policy comes from your currently trained policy so these batches have different so these batches have different so these batches have different statistics and we can look at this statistics and we can look at this statistics and we can look at this empirically so here's uh an agent that empirically so here's uh an agent that empirically so here's uh an agent that was trained over many many time steps was trained over many many time steps was trained over many many time steps the standard training length that we the standard training length that we the standard training length that we have in this paper and we're going to have in this paper and we're going to have in this paper and we're going to take snapshots of the action take snapshots of the action take snapshots of the action distributions uh over time and on the distributions uh over time and on the distributions uh over time and on the y-axis you see one um coordinate of Your y-axis you see one um coordinate of Your y-axis you see one um coordinate of Your Action space and on the x-axis is Action space and on the x-axis is Action space and on the x-axis is another coordinate these are just like another coordinate these are just like another coordinate these are just like arbitrarily sampled dimensions of Your arbitrarily sampled dimensions of Your arbitrarily sampled dimensions of Your Action space just to get some 2D Action space just to get some 2D Action space just to get some 2D plot and you see that uh plot and you see that uh plot and you see that uh the the distribution of actions from the the distribution of actions from the the distribution of actions from your replay buffer is is very different your replay buffer is is very different your replay buffer is is very different that's blue it's very different from the that's blue it's very different from the that's blue it's very different from the actions that your policy wants to actions that your policy wants to actions that your policy wants to propose and it's persistently different propose and it's persistently different propose and it's persistently different throughout the process of throughout the process of throughout the process of training and this can be lethal for training and this can be lethal for training and this can be lethal for bchn so you can imagine that when you do bchn so you can imagine that when you do bchn so you can imagine that when you do the first forward pass it's going to the first forward pass it's going to the first forward pass it's going to compute some batch statistics and then compute some batch statistics and then compute some batch statistics and then when you do the second forward pass it's when you do the second forward pass it's when you do the second forward pass it's going to take the blue bad statistics going to take the blue bad statistics going to take the blue bad statistics and and try to normalize The Red Badge and and try to normalize The Red Badge and and try to normalize The Red Badge with those statistics and because all of with those statistics and because all of with those statistics and because all of the features in the intermediate layers the features in the intermediate layers the features in the intermediate layers of a q function are kind of influenced of a q function are kind of influenced of a q function are kind of influenced by a t+ by a t+ by a t+ one those are also going to be wrongly one those are also going to be wrongly one those are also going to be wrongly normalized and this is a normalized and this is a normalized and this is a huge ad may may I ask quick question you huge ad may may I ask quick question you huge ad may may I ask quick question you said that with the states this St and St said that with the states this St and St said that with the states this St and St plus1 they identically plus1 they identically plus1 they identically distributed but why if we know that St distributed but why if we know that St distributed but why if we know that St plus1 distribution is dep it depends on plus1 distribution is dep it depends on plus1 distribution is dep it depends on St and at MH why can we see that they St and at MH why can we see that they St and at MH why can we see that they identically distributed yeah that's a identically distributed yeah that's a identically distributed yeah that's a good question so if you got a very very good question so if you got a very very good question so if you got a very very tiny um um episode length let's say tiny um um episode length let's say tiny um um episode length let's say let's say you good just two time steps let's say you good just two time steps let's say you good just two time steps then of course the starts and the ends then of course the starts and the ends then of course the starts and the ends have to be different distributor right have to be different distributor right have to be different distributor right so in that case they're going to be so in that case they're going to be so in that case they're going to be different but in practice for the kinds different but in practice for the kinds different but in practice for the kinds of problems that we care about uh here of problems that we care about uh here of problems that we care about uh here uh like these continuous control uh like these continuous control uh like these continuous control problems with like a thousand length problems with like a thousand length problems with like a thousand length episodes right uh there you just episodes right uh there you just episodes right uh there you just arbitrarily uh sample any St and then arbitrarily uh sample any St and then arbitrarily uh sample any St and then the next uh state is st+ one the next uh state is st+ one the next uh state is st+ one and also at the next time step you're and also at the next time step you're and also at the next time step you're you're going to commit St plus one and you're going to commit St plus one and you're going to commit St plus one and then whatever actions there are and then then whatever actions there are and then then whatever actions there are and then St plus two to the replay buffer so in St plus two to the replay buffer so in St plus two to the replay buffer so in this sense they're almost identically this sense they're almost identically this sense they're almost identically distributed sure the start State distributed sure the start State distributed sure the start State distribution can be slightly different distribution can be slightly different distribution can be slightly different uh for all of your episodes but like uh for all of your episodes but like uh for all of your episodes but like afterwards uh they need to be nearly the afterwards uh they need to be nearly the afterwards uh they need to be nearly the same I see got you thank you MH all same I see got you thank you MH all same I see got you thank you MH all right so so so as I said this these right so so so as I said this these right so so so as I said this these moments are different and this moments are different and this moments are different and this is sorry to interrupt but can you repeat is sorry to interrupt but can you repeat is sorry to interrupt but can you repeat the explanation again I I still don't the explanation again I I still don't the explanation again I I still don't think I understood that uh about the think I understood that uh about the think I understood that uh about the independent independent independent distribution uh about s yeah s and St distribution uh about s yeah s and St distribution uh about s yeah s and St plus1 mhm okay so so let's say you got a plus1 mhm okay so so let's say you got a plus1 mhm okay so so let's say you got a very very long episode and all your very very long episode and all your very very long episode and all your episodes are pretty long episodes are pretty long episodes are pretty long then uh in in this limit uh the way you then uh in in this limit uh the way you then uh in in this limit uh the way you commit transitions this s comma a comma commit transitions this s comma a comma commit transitions this s comma a comma S Prime transitions to your replay S Prime transitions to your replay S Prime transitions to your replay buffer is that you're going to take buffer is that you're going to take buffer is that you're going to take every time step you're going to commit every time step you're going to commit every time step you're going to commit that transition to your replay buffer that transition to your replay buffer that transition to your replay buffer and then you're going to move on to the and then you're going to move on to the and then you're going to move on to the next time step like as the agent acts next time step like as the agent acts next time step like as the agent acts and then you're going to commit that and then you're going to commit that and then you're going to commit that transition to your replay transition to your replay transition to your replay buffer so everything that you committed buffer so everything that you committed buffer so everything that you committed into into into St in your replay buffer is also going St in your replay buffer is also going St in your replay buffer is also going be committed into st+ one be committed into st+ one be committed into st+ one except for the very very first time step except for the very very first time step except for the very very first time step in your let's say thousand L in your let's say thousand L in your let's say thousand L episode I see okay okay I think I episode I see okay okay I think I episode I see okay okay I think I understand a bit more yeah cool yeah understand a bit more yeah cool yeah understand a bit more yeah cool yeah okay great um right uh and uh this is okay great um right uh and uh this is okay great um right uh and uh this is why we regard them as being nearly why we regard them as being nearly why we regard them as being nearly identical and and if you actually plot identical and and if you actually plot identical and and if you actually plot out these distributions for them they do out these distributions for them they do out these distributions for them they do look identical like it's barely a look identical like it's barely a look identical like it's barely a rounding rounding rounding error so so we have uh so so by the way error so so we have uh so so by the way error so so we have uh so so by the way this is a problem and we see that this is a problem and we see that this is a problem and we see that everybody who tried to use batom in everybody who tried to use batom in everybody who tried to use batom in whatever implementations that we saw whatever implementations that we saw whatever implementations that we saw online uh seemed to not be aware of this online uh seemed to not be aware of this online uh seemed to not be aware of this distinction that the fact that these uh distinction that the fact that these uh distinction that the fact that these uh a batches have different distributions a batches have different distributions a batches have different distributions so they're using batch from so they're using batch from so they're using batch from WR so we now need a way to correct it WR so we now need a way to correct it WR so we now need a way to correct it which is what we do uh it turns out that which is what we do uh it turns out that which is what we do uh it turns out that this fix is just absurdly simple so so this fix is just absurdly simple so so this fix is just absurdly simple so so look on the left here uh you have these look on the left here uh you have these look on the left here uh you have these two forward passes and then you generate two forward passes and then you generate two forward passes and then you generate the loss right so the tweak that we the loss right so the tweak that we the loss right so the tweak that we suggest is the suggest is the suggest is the following just following just following just concatenate the St and St plus1 patches concatenate the St and St plus1 patches concatenate the St and St plus1 patches and the action batches together into one and the action batches together into one and the action batches together into one big batch like double-sized batch and big batch like double-sized batch and big batch like double-sized batch and just send it through your Q Network in a just send it through your Q Network in a just send it through your Q Network in a single forward single forward single forward pass and then split uh the out of of pass and then split uh the out of of pass and then split uh the out of of this big batch back into into QT and QT this big batch back into into QT and QT this big batch back into into QT and QT +1 over +1 over +1 over here um so why do we do this uh we do here um so why do we do this uh we do here um so why do we do this uh we do this because now from the perspective this because now from the perspective this because now from the perspective the point of view of the batch Norm the point of view of the batch Norm the point of view of the batch Norm layers in your layers in your layers in your network all the inputs like every network all the inputs like every network all the inputs like every element of a batch that it sees belongs element of a batch that it sees belongs element of a batch that it sees belongs to one unified mixture to one unified mixture to one unified mixture distribution uh that contains samples distribution uh that contains samples distribution uh that contains samples both from at and and the at Prime uh at both from at and and the at Prime uh at both from at and and the at Prime uh at plus one batch uh and and when you have plus one batch uh and and when you have plus one batch uh and and when you have this kind of a wider mixer distribution this kind of a wider mixer distribution this kind of a wider mixer distribution you're going to use consistent moments you're going to use consistent moments you're going to use consistent moments uh that are going to at least the means uh that are going to at least the means uh that are going to at least the means are going to look like the the are going to look like the the are going to look like the the arithmetic mean of the two means of both arithmetic mean of the two means of both arithmetic mean of the two means of both batches uh and we use this mixed mean batches uh and we use this mixed mean batches uh and we use this mixed mean these mixed uh normalization moments and these mixed uh normalization moments and these mixed uh normalization moments and therefore there's never a mismatch therefore there's never a mismatch therefore there's never a mismatch now uh and by the way we find that this now uh and by the way we find that this now uh and by the way we find that this works very effectively this was a very works very effectively this was a very works very effectively this was a very very simple heck very simple heck very simple heck um and do you know and do you know if I um and do you know and do you know if I um and do you know and do you know if I will conate not just two consecutive but will conate not just two consecutive but will conate not just two consecutive but longer consecutive sequence will that longer consecutive sequence will that longer consecutive sequence will that trick perform trick perform trick perform better H uh in what case would you have better H uh in what case would you have better H uh in what case would you have a longer consecutive sequence of forward passes um uh if for example I don't know passes um uh if for example I don't know passes um uh if for example I don't know that uh if you want to that uh if you want to that uh if you want to have have have some if you want to take into account uh some if you want to take into account uh some if you want to take into account uh uh the the longer um the longer effect uh the the longer um the longer effect uh the the longer um the longer effect of uh uh of your actions like like a of uh uh of your actions like like a of uh uh of your actions like like a long long Horizon type of effect of your long long Horizon type of effect of your long long Horizon type of effect of your actions um I think in a real it's actions um I think in a real it's actions um I think in a real it's there's a special name for these actions there's a special name for these actions there's a special name for these actions where you kind of like uh take the where you kind of like uh take the where you kind of like uh take the effect not just the immediate action but effect not just the immediate action but effect not just the immediate action but like a sequence of actions okay I think like a sequence of actions okay I think like a sequence of actions okay I think you're referring to the end step returns you're referring to the end step returns you're referring to the end step returns right yeah yeah yeah okay okay um so right yeah yeah yeah okay okay um so right yeah yeah yeah okay okay um so again I think this uh here will this again I think this uh here will this again I think this uh here will this should depend uh very much on the on the should depend uh very much on the on the should depend uh very much on the on the precise way in which you you do your Q precise way in which you you do your Q precise way in which you you do your Q predictions there are many ways to do Q predictions there are many ways to do Q predictions there are many ways to do Q predictions like in dqn for example they predictions like in dqn for example they predictions like in dqn for example they don't even feed the actions in they have don't even feed the actions in they have don't even feed the actions in they have different action heads so so I don't different action heads so so I don't different action heads so so I don't know like what exact way of predicting know like what exact way of predicting know like what exact way of predicting uh Q values you would do in in a uh Q values you would do in in a uh Q values you would do in in a specific endep implementation but if it specific endep implementation but if it specific endep implementation but if it were done in this manner uh where you were done in this manner uh where you were done in this manner uh where you just feed in like s and a for a specific just feed in like s and a for a specific just feed in like s and a for a specific time Step at the same time then I would time Step at the same time then I would time Step at the same time then I would say that yes uh if you're doing this for say that yes uh if you're doing this for say that yes uh if you're doing this for like two or three or four time steps like two or three or four time steps like two or three or four time steps then uh absolutely uh you should then uh absolutely uh you should then uh absolutely uh you should concatenate them all concatenate them all concatenate them all together got you thank you together got you thank you together got you thank you yeah uh okay so so we find that this yeah uh okay so so we find that this yeah uh okay so so we find that this works uh surprisingly well I mean works uh surprisingly well I mean works uh surprisingly well I mean there's not much surprising about it there's not much surprising about it there's not much surprising about it it's just uh we we fix the inconsistency it's just uh we we fix the inconsistency it's just uh we we fix the inconsistency of moments problem and then it it works of moments problem and then it it works of moments problem and then it it works uh but there's some there's something we uh but there's some there's something we uh but there's some there's something we just changed here that's that's more just changed here that's that's more just changed here that's that's more than just concatenation and it's this than just concatenation and it's this than just concatenation and it's this you see in in modern deep rltd you see in in modern deep rltd you see in in modern deep rltd learning uh we have something called a learning uh we have something called a learning uh we have something called a Target Network it it's a copy of your Target Network it it's a copy of your Target Network it it's a copy of your live Q Network so these parameters Theta live Q Network so these parameters Theta live Q Network so these parameters Theta dot are gone now we just have one Q dot are gone now we just have one Q dot are gone now we just have one Q Network so let's unpack this what's Network so let's unpack this what's Network so let's unpack this what's going on here going on here going on here uh so the target network is you can uh so the target network is you can uh so the target network is you can think of it as uh a kind of think of it as uh a kind of think of it as uh a kind of slower time wearing copy or version of slower time wearing copy or version of slower time wearing copy or version of your live Q your live Q your live Q network uh and this is here to is going network uh and this is here to is going network uh and this is here to is going to be something less than one so this is to be something less than one so this is to be something less than one so this is like a poak average or or a moving like a poak average or or a moving like a poak average or or a moving average or a low pass average or a low pass average or a low pass filter and the way this has an effect on filter and the way this has an effect on filter and the way this has an effect on your TD learning is that you're going to your TD learning is that you're going to your TD learning is that you're going to use this slower Q Network to predict use this slower Q Network to predict use this slower Q Network to predict your regression targets um your uh qt+ your regression targets um your uh qt+ your regression targets um your uh qt+ one which has the effect of somehow one which has the effect of somehow one which has the effect of somehow slowing down the bootstrapping the slowing down the bootstrapping the slowing down the bootstrapping the credit assignment backwards in time that credit assignment backwards in time that credit assignment backwards in time that is characteristic of of value based is characteristic of of value based is characteristic of of value based methods in in reinforcement methods in in reinforcement methods in in reinforcement learning and this can be widely regarded learning and this can be widely regarded learning and this can be widely regarded as a as a nasty thing or a necessary as a as a nasty thing or a necessary as a as a nasty thing or a necessary evil uh because like why do you want to evil uh because like why do you want to evil uh because like why do you want to slow down your learning slow down your learning slow down your learning uh and many have tght uh and many have tght uh and many have tght this however it's been really crucial this however it's been really crucial this however it's been really crucial for deepl stability in fact ever since for deepl stability in fact ever since for deepl stability in fact ever since deepl became really deep uh which is deepl became really deep uh which is deepl became really deep uh which is 2015 in in dqn Target networks have been 2015 in in dqn Target networks have been 2015 in in dqn Target networks have been a part of it so Target networks first a part of it so Target networks first a part of it so Target networks first came out with dqn uh and and the version came out with dqn uh and and the version came out with dqn uh and and the version of Target networks they use was a of Target networks they use was a of Target networks they use was a periodic snapshot of your live Network periodic snapshot of your live Network periodic snapshot of your live Network every 500 5, 000 steps or so or probably every 500 5, 000 steps or so or probably every 500 5, 000 steps or so or probably more uh and the year after came out ddpg more uh and the year after came out ddpg more uh and the year after came out ddpg which is for continuous control uh and which is for continuous control uh and which is for continuous control uh and that version used the polyc average the that version used the polyc average the that version used the polyc average the kind that we are showing on the left kind that we are showing on the left kind that we are showing on the left side side side here um but surprise surprise in this here um but surprise surprise in this here um but surprise surprise in this paper we somehow find out and there's paper we somehow find out and there's paper we somehow find out and there's more in in the appendix if you want to more in in the appendix if you want to more in in the appendix if you want to see that it turns out you don't need see that it turns out you don't need see that it turns out you don't need Target networks if you Sim simply Target networks if you Sim simply Target networks if you Sim simply somehow use normalized activations or somehow use normalized activations or somehow use normalized activations or bounded activation functions and and bounded activation functions and and bounded activation functions and and guess what everybody is using relu guess what everybody is using relu guess what everybody is using relu unbounded activations um and it's unbounded activations um and it's unbounded activations um and it's blowing up for them without Target net blowing up for them without Target net blowing up for them without Target net just use bounded activations or use just use bounded activations or use just use bounded activations or use feature feature feature normalizers and guess what patom is a normalizers and guess what patom is a normalizers and guess what patom is a feature normalizer and it also makes feature normalizer and it also makes feature normalizer and it also makes Target networks Target networks Target networks unnecessary uh so in in conclusion what unnecessary uh so in in conclusion what unnecessary uh so in in conclusion what what we then do is to turn sack into what we then do is to turn sack into what we then do is to turn sack into cross Q we make just three simple cross Q we make just three simple cross Q we make just three simple changes first we use batch Nom uh we changes first we use batch Nom uh we changes first we use batch Nom uh we just add batch Nom between all the just add batch Nom between all the just add batch Nom between all the layers and we use it the right way or or layers and we use it the right way or or layers and we use it the right way or or or the same Safe Way the way we showed or the same Safe Way the way we showed or the same Safe Way the way we showed and this just uh shows the same kind of and this just uh shows the same kind of and this just uh shows the same kind of sample efficiency boost as people see in sample efficiency boost as people see in sample efficiency boost as people see in supervised learning we delete Target supervised learning we delete Target supervised learning we delete Target Nets this simplifies the algorithm and Nets this simplifies the algorithm and Nets this simplifies the algorithm and by the way also makes it U easier to use by the way also makes it U easier to use by the way also makes it U easier to use batch batch batch Nom in your in your Q networks because Nom in your in your Q networks because Nom in your in your Q networks because with Target Nets you have to worry about with Target Nets you have to worry about with Target Nets you have to worry about this extra running average of your this extra running average of your this extra running average of your parameters and bnom has its own running parameters and bnom has its own running parameters and bnom has its own running averages and and the the combination of averages and and the the combination of averages and and the the combination of these two is just uh really complicated these two is just uh really complicated these two is just uh really complicated to even think about to even think about to even think about um and lastly super minor change we just um and lastly super minor change we just um and lastly super minor change we just use wider layers in the Q Network this use wider layers in the Q Network this use wider layers in the Q Network this raises the performance ceiling that we raises the performance ceiling that we raises the performance ceiling that we can achieve and now uh Daniel is going can achieve and now uh Daniel is going can achieve and now uh Daniel is going to show you some of the experimental to show you some of the experimental to show you some of the experimental results that we got lots of plots and results that we got lots of plots and results that we got lots of plots and and fun things so if you have any and fun things so if you have any and fun things so if you have any questions you can uh ask right away yeah questions you can uh ask right away yeah questions you can uh ask right away yeah I I I yeah this is very very interesting I I I yeah this is very very interesting I I I yeah this is very very interesting actually so so so what you've done is actually so so so what you've done is actually so so so what you've done is you you actually simplified the the r you you actually simplified the the r you you actually simplified the the r deepl problem as back to or deepl deepl problem as back to or deepl deepl problem as back to or deepl solution right that's solution right that's solution right that's right yeah so is there any theoretical right yeah so is there any theoretical right yeah so is there any theoretical grounding behind those grounding behind those grounding behind those additional uh well tricks that that additional uh well tricks that that additional uh well tricks that that you've you've looked into like because I you've you've looked into like because I you've you've looked into like because I noted that you were mentioning this POC noted that you were mentioning this POC noted that you were mentioning this POC averaging as a low pass filter in a way averaging as a low pass filter in a way averaging as a low pass filter in a way right is there a way we can connect you right is there a way we can connect you right is there a way we can connect you know some of the theories from the two know some of the theories from the two know some of the theories from the two fields to try to understand this fields to try to understand this fields to try to understand this determinantal effect of you know slowing determinantal effect of you know slowing determinantal effect of you know slowing down learning uh more than just by down learning uh more than just by down learning uh more than just by noticing it empirically so to say h uh noticing it empirically so to say h uh noticing it empirically so to say h uh so we actually have some extra so we actually have some extra so we actually have some extra experiments uh where we uh show that experiments uh where we uh show that experiments uh where we uh show that Target Nets actually do slow down Target Nets actually do slow down Target Nets actually do slow down learning and and we disentangle this learning and and we disentangle this learning and and we disentangle this slowing down effect from the presence of slowing down effect from the presence of slowing down effect from the presence of batom um and this is purely empirical batom um and this is purely empirical batom um and this is purely empirical theoretically uh we don't have I would theoretically uh we don't have I would theoretically uh we don't have I would say we have no clue by this say we have no clue by this say we have no clue by this folks yeah yeah no no I mean but this is folks yeah yeah no no I mean but this is folks yeah yeah no no I mean but this is this is amazing stuff yeah thank you this is amazing stuff yeah thank you this is amazing stuff yeah thank you yeah we also have a slide for this in a yeah we also have a slide for this in a yeah we also have a slide for this in a in a minute I in a minute I in a minute I think think think yeah all right can you guys see my yeah all right can you guys see my yeah all right can you guys see my screen yes all right okay screen yes all right okay screen yes all right okay um one quick thing before we get to the um one quick thing before we get to the um one quick thing before we get to the experiments um is as ad mentioned it experiments um is as ad mentioned it experiments um is as ad mentioned it makes the algorithm really simple so makes the algorithm really simple so makes the algorithm really simple so here we actually just have a um example here we actually just have a um example here we actually just have a um example of a very minimal Jack's implementation of a very minimal Jack's implementation of a very minimal Jack's implementation of the critic loss of the critic loss of the critic loss and uh I kind of want to quickly note and uh I kind of want to quickly note and uh I kind of want to quickly note the main changes so basically here you the main changes so basically here you the main changes so basically here you now just have one single uh Q Network um now just have one single uh Q Network um now just have one single uh Q Network um just below that you do the concatenated just below that you do the concatenated just below that you do the concatenated forward passes we've seen um it's also forward passes we've seen um it's also forward passes we've seen um it's also pretty easy one thing you then need to pretty easy one thing you then need to pretty easy one thing you then need to do of course is split up uh the BET into do of course is split up uh the BET into do of course is split up uh the BET into into the original batches again and uh into the original batches again and uh into the original batches again and uh then you need to put a stop gradient on then you need to put a stop gradient on then you need to put a stop gradient on the next few values because of course the next few values because of course the next few values because of course yeah you want to use them as yeah you want to use them as yeah you want to use them as targets um and that's it and the nice targets um and that's it and the nice targets um and that's it and the nice thing is uh you don't see all of the thing is uh you don't see all of the thing is uh you don't see all of the things that you can remove namely the things that you can remove namely the things that you can remove namely the target networks and the polyc averaging target networks and the polyc averaging target networks and the polyc averaging and so on so yeah all right without and so on so yeah all right without and so on so yeah all right without further Ado Let's uh oh sorry and it's further Ado Let's uh oh sorry and it's further Ado Let's uh oh sorry and it's not exclusive to uh sack but you can not exclusive to uh sack but you can not exclusive to uh sack but you can actually do this with td3 or any other actually do this with td3 or any other actually do this with td3 or any other off policy uh uh TD learning method um off policy uh uh TD learning method um off policy uh uh TD learning method um and why why why and why why why and why why why Jacks why Jack um I just love Jacks to Jacks why Jack um I just love Jacks to Jacks why Jack um I just love Jacks to be honest yeah I I'm a fan and it just I honest yeah I I'm a fan and it just I honest yeah I I'm a fan and it just I don't know we just I we also had an don't know we just I we also had an don't know we just I we also had an implementation in pytorch in the implementation in pytorch in the implementation in pytorch in the beginning when when we were thinking beginning when when we were thinking beginning when when we were thinking about which framework we're going to do about which framework we're going to do about which framework we're going to do our experiments in and it we just have our experiments in and it we just have our experiments in and it we just have much uh faster run times um for well much uh faster run times um for well much uh faster run times um for well across the board of course also for the across the board of course also for the across the board of course also for the the for the baselines but uh for the for the baselines but uh for the for the baselines but uh for everything so it just was faster for us everything so it just was faster for us everything so it just was faster for us to do all of these experiments which to do all of these experiments which to do all of these experiments which work quite a lot um as yeah we're work quite a lot um as yeah we're work quite a lot um as yeah we're probably going to see in a sec just probably going to see in a sec just probably going to see in a sec just overall here because yeah we evaluated overall here because yeah we evaluated overall here because yeah we evaluated everything on 10 seeds each to be as as everything on 10 seeds each to be as as everything on 10 seeds each to be as as rigid as possible um and so on yeah and rigid as possible um and so on yeah and rigid as possible um and so on yeah and okay here you basically on this slide okay here you basically on this slide okay here you basically on this slide see the sample efficiency and I've see the sample efficiency and I've see the sample efficiency and I've hinted at this in the very first slide hinted at this in the very first slide hinted at this in the very first slide um already with this aggregated curve um already with this aggregated curve um already with this aggregated curve here you see it in a per environment here you see it in a per environment here you see it in a per environment setting um and we can see that again setting um and we can see that again setting um and we can see that again cross Q either matches or surpasses the cross Q either matches or surpasses the cross Q either matches or surpasses the current state-ofthe-art um which is draw current state-ofthe-art um which is draw current state-ofthe-art um which is draw Q or Red Q and that with only 5% of the Q or Red Q and that with only 5% of the Q or Red Q and that with only 5% of the gradient steps because we don't have the gradient steps because we don't have the gradient steps because we don't have the High utds um it also translates to DM High utds um it also translates to DM High utds um it also translates to DM control environments for example and control environments for example and control environments for example and many more and we do have DM control in many more and we do have DM control in many more and we do have DM control in our appendix um I didn't include them our appendix um I didn't include them our appendix um I didn't include them here in the slides uh but yeah just as a here in the slides uh but yeah just as a here in the slides uh but yeah just as a side note and side note and side note and um you can also see that we have um you can also see that we have um you can also see that we have a version of cross Q based on td3 just a version of cross Q based on td3 just a version of cross Q based on td3 just to really show the point which which we to really show the point which which we to really show the point which which we always said you can combine it with always said you can combine it with always said you can combine it with other methods as well and we see the other methods as well and we see the other methods as well and we see the same kind of performance increases there same kind of performance increases there same kind of performance increases there um which is super nice um yeah and if um which is super nice um yeah and if um which is super nice um yeah and if you again aggregate them in the bottom you again aggregate them in the bottom you again aggregate them in the bottom right you'll see that also um uh with right you'll see that also um uh with right you'll see that also um uh with this iqm Norm return which is this iqm Norm return which is this iqm Norm return which is statistically more valid than the statistically more valid than the statistically more valid than the individual curves you see that we're um individual curves you see that we're um individual curves you see that we're um nicely sitting above the baselines all nicely sitting above the baselines all nicely sitting above the baselines all right this becomes even more dramatic if right this becomes even more dramatic if right this becomes even more dramatic if we now plot this in terms of the we now plot this in terms of the we now plot this in terms of the computational efficiency um as we said computational efficiency um as we said computational efficiency um as we said 5% of the gradient steps um which transl 5% of the gradient steps um which transl 5% of the gradient steps um which transl slates into roughly four times faster slates into roughly four times faster slates into roughly four times faster wall clock wall clock wall clock um everything is um everything is um everything is jitted so coming back to Jack so that jitted so coming back to Jack so that jitted so coming back to Jack so that makes it nice um and one side note is makes it nice um and one side note is makes it nice um and one side note is we're also actually being very nice to we're also actually being very nice to we're also actually being very nice to the baselines as um if we would have the baselines as um if we would have the baselines as um if we would have taken the original Red Q implementation taken the original Red Q implementation taken the original Red Q implementation they do all of the critic updates in a they do all of the critic updates in a they do all of the critic updates in a loop but then we would have probably loop but then we would have probably loop but then we would have probably been a 100 times faster not just four been a 100 times faster not just four been a 100 times faster not just four times faster um because here we actually times faster um because here we actually times faster um because here we actually parallelize over all of uh the critic parallelize over all of uh the critic parallelize over all of uh the critic updates so over the entire ensemble um updates so over the entire ensemble um updates so over the entire ensemble um all right all right all right cool one important thing that uh Red Q cool one important thing that uh Red Q cool one important thing that uh Red Q and dra Q really look at um is the Q and dra Q really look at um is the Q and dra Q really look at um is the Q bias of these methods and the Q bias is bias of these methods and the Q bias is bias of these methods and the Q bias is basically the difference between what basically the difference between what basically the difference between what your Q function says and the empirical your Q function says and the empirical your Q function says and the empirical return you actually get from the return you actually get from the return you actually get from the environment and and the common wisdom environment and and the common wisdom environment and and the common wisdom from those papers being that uh it's from those papers being that uh it's from those papers being that uh it's important to reduce this Q function bias important to reduce this Q function bias important to reduce this Q function bias in order to improve the learning in order to improve the learning in order to improve the learning performance um because if you um just performance um because if you um just performance um because if you um just take a like offthe shelf vanilla soft take a like offthe shelf vanilla soft take a like offthe shelf vanilla soft ector critic and you increase the ector critic and you increase the ector critic and you increase the updated data ratio uh the Q function updated data ratio uh the Q function updated data ratio uh the Q function bias will usually just bias will usually just bias will usually just explode um and we've done those explode um and we've done those explode um and we've done those experiments we've put cross Q in there experiments we've put cross Q in there experiments we've put cross Q in there because uh we we thought well maybe um because uh we we thought well maybe um because uh we we thought well maybe um cross Q implicitly also reduces the Q cross Q implicitly also reduces the Q cross Q implicitly also reduces the Q function bias but in reality as you can function bias but in reality as you can function bias but in reality as you can see in those plots um it's a little bit see in those plots um it's a little bit see in those plots um it's a little bit more more more complicated uh on the left hand side complicated uh on the left hand side complicated uh on the left hand side we've picked just randomly picked two we've picked just randomly picked two we've picked just randomly picked two environments and we have all of the environments and we have all of the environments and we have all of the other ones in the appendix appendix as other ones in the appendix appendix as other ones in the appendix appendix as well and you can kind of see Red Q has well and you can kind of see Red Q has well and you can kind of see Red Q has the lowest Q function bias it's exactly the lowest Q function bias it's exactly the lowest Q function bias it's exactly doing what it's designed for um um but doing what it's designed for um um but doing what it's designed for um um but cross Q matches uh in in those examples cross Q matches uh in in those examples cross Q matches uh in in those examples here matches um uh the performance here matches um uh the performance here matches um uh the performance um despite having a higher or sometimes um despite having a higher or sometimes um despite having a higher or sometimes even much higher Q function even much higher Q function even much higher Q function bias um so yeah the story is not that bias um so yeah the story is not that bias um so yeah the story is not that clear in in regards to clear in in regards to clear in in regards to this then the other thing that ad this then the other thing that ad this then the other thing that ad already hinted at kind of is uh the already hinted at kind of is uh the already hinted at kind of is uh the effect of disentangling this effect of effect of disentangling this effect of effect of disentangling this effect of Target networks and the batch Target networks and the batch Target networks and the batch normalization that we add and um the normalization that we add and um the normalization that we add and um the question is really how much does uh the question is really how much does uh the question is really how much does uh the target Network slow down training so target Network slow down training so target Network slow down training so what proportion of the speed up comes what proportion of the speed up comes what proportion of the speed up comes from removing the target Network and from removing the target Network and from removing the target Network and What proportion comes from actually What proportion comes from actually What proportion comes from actually adding the batch normalization um in adding the batch normalization um in adding the batch normalization um in there and we have a nice experimental there and we have a nice experimental there and we have a nice experimental setup where we use 10h activations which setup where we use 10h activations which setup where we use 10h activations which are those bounded activations where are those bounded activations where are those bounded activations where where we found that using those we can where we found that using those we can where we found that using those we can actually already train without Target actually already train without Target actually already train without Target networks um they permit us to train networks um they permit us to train networks um they permit us to train stably um and we have a little bit more stably um and we have a little bit more stably um and we have a little bit more details of that in the paper but as we details of that in the paper but as we details of that in the paper but as we see and this allows us to kind of run see and this allows us to kind of run see and this allows us to kind of run all of the combinations So Soft dector all of the combinations So Soft dector all of the combinations So Soft dector critic soft dector critic without Target critic soft dector critic without Target critic soft dector critic without Target networks then just with batch networks then just with batch networks then just with batch normalization and then as we say the normalization and then as we say the normalization and then as we say the cross Q style where we remove Target cross Q style where we remove Target cross Q style where we remove Target networks and networks and networks and atomization and there are not much atomization and there are not much atomization and there are not much surprises here um we see that removing surprises here um we see that removing surprises here um we see that removing Target networks uh usually slightly Target networks uh usually slightly Target networks uh usually slightly helps but it's not helps but it's not helps but it's not dramatic dramatic dramatic um just using batch normalization so um just using batch normalization so um just using batch normalization so keeping the Target netk in there and keeping the Target netk in there and keeping the Target netk in there and adding batch normalization usually fails adding batch normalization usually fails adding batch normalization usually fails and that is because of um this and that is because of um this and that is because of um this distribution mismatch uh we talked about distribution mismatch uh we talked about distribution mismatch uh we talked about earlier and we get the best results by earlier and we get the best results by earlier and we get the best results by using normalization without Target using normalization without Target using normalization without Target networks so here we networks so here we networks so here we really attribute the um effect in in in really attribute the um effect in in in really attribute the um effect in in in getting those much much better um sample getting those much much better um sample getting those much much better um sample efficiency numbers to using bch Norm in efficiency numbers to using bch Norm in efficiency numbers to using bch Norm in the correct way which is accounting for the correct way which is accounting for the correct way which is accounting for this distribution this distribution this distribution mismatch all right and that already mismatch all right and that already mismatch all right and that already brings me to the conclusion so um we've brings me to the conclusion so um we've brings me to the conclusion so um we've seen that batch seen that batch seen that batch normalization can accelerate deep normalization can accelerate deep normalization can accelerate deep reinforcement learning quite quite reinforcement learning quite quite reinforcement learning quite quite drastically um and we've put all of this drastically um and we've put all of this drastically um and we've put all of this into one method one algorithm we call it into one method one algorithm we call it into one method one algorithm we call it cross Q it's essentially an off policy cross Q it's essentially an off policy cross Q it's essentially an off policy TD method um plus batch normalization TD method um plus batch normalization TD method um plus batch normalization and without the target networks and what and without the target networks and what and without the target networks and what you get is state-of-the-art sample you get is state-of-the-art sample you get is state-of-the-art sample efficiency at really reduced efficiency at really reduced efficiency at really reduced computational cost compared to the computational cost compared to the computational cost compared to the current State ofthe current State ofthe current State ofthe art and uh as a bit of an Outlook in art and uh as a bit of an Outlook in art and uh as a bit of an Outlook in future work we now really want to go future work we now really want to go future work we now really want to go into the real world uh in real time on into the real world uh in real time on into the real world uh in real time on the real robot and do kind of this sort the real robot and do kind of this sort the real robot and do kind of this sort of yeah learning in the real world which of yeah learning in the real world which of yeah learning in the real world which where where most of this motivation where where most of this motivation where where most of this motivation actually came from initially also um and actually came from initially also um and actually came from initially also um and something else that we want to also look something else that we want to also look something else that we want to also look at is can we even scale this up even at is can we even scale this up even at is can we even scale this up even more and uh go to make it faster and go more and uh go to make it faster and go more and uh go to make it faster and go to for example larger tasks like visual to for example larger tasks like visual to for example larger tasks like visual RL tasks and the RL tasks and the RL tasks and the like all right and with that um I want like all right and with that um I want like all right and with that um I want to thank you and I guess we can take to thank you and I guess we can take to thank you and I guess we can take some questions thank you thank you so some questions thank you thank you so some questions thank you thank you so much this this has been amazing I mean much this this has been amazing I mean much this this has been amazing I mean fantastic work you guys um thank you uh fantastic work you guys um thank you uh fantastic work you guys um thank you uh maybe I could start first by just some maybe I could start first by just some maybe I could start first by just some questions so so I know Yan would like to questions so so I know Yan would like to questions so so I know Yan would like to see this on the real robot and uh um and see this on the real robot and uh um and see this on the real robot and uh um and I mean your sample complexity is still I mean your sample complexity is still I mean your sample complexity is still kind of high you know like a to the kind of high you know like a to the kind of high you know like a to the power 5 or e to the power six or power 5 or e to the power six or power 5 or e to the power six or something right and what do you think something right and what do you think something right and what do you think are the main gaps you know to get this are the main gaps you know to get this are the main gaps you know to get this actually operational on a you know real system so that's a good question um so system so that's a good question um so system so that's a good question um so it's something in the order of times e it's something in the order of times e it's something in the order of times e to the yeah e to the five yeah uh and we to the yeah e to the five yeah uh and we to the yeah e to the five yeah uh and we think that uh if you select the right think that uh if you select the right think that uh if you select the right kind of time kind of time kind of time discretization this is uh probably going discretization this is uh probably going discretization this is uh probably going to be to be to be uh I think with a very reasonable uh I think with a very reasonable uh I think with a very reasonable discretization this should be about 15 discretization this should be about 15 discretization this should be about 15 to 20 minutes of learning time on a real to 20 minutes of learning time on a real to 20 minutes of learning time on a real robot um so if you place in the right robot um so if you place in the right robot um so if you place in the right safety safety safety guards because you know methods like sa guards because you know methods like sa guards because you know methods like sa love to trash your robot about if you love to trash your robot about if you love to trash your robot about if you place in the right safety guards and and place in the right safety guards and and place in the right safety guards and and construct the task well I think you construct the task well I think you construct the task well I think you should be able to see interesting should be able to see interesting should be able to see interesting policies emerge uh within this time and policies emerge uh within this time and policies emerge uh within this time and it depends entirely on the task of it depends entirely on the task of it depends entirely on the task of course right uh but one of our uh uh course right uh but one of our uh uh course right uh but one of our uh uh methods that we compare against draw to methods that we compare against draw to methods that we compare against draw to one of the state-ofthe-art methods it's one of the state-ofthe-art methods it's one of the state-ofthe-art methods it's been uh tried out on on on learning been uh tried out on on on learning been uh tried out on on on learning quadri ped Locomotion from scratch in in quadri ped Locomotion from scratch in in quadri ped Locomotion from scratch in in between 10 to 20 minutes by cir lein between 10 to 20 minutes by cir lein between 10 to 20 minutes by cir lein group and they found that this works group and they found that this works group and they found that this works right so and as we showed uh cross Q right so and as we showed uh cross Q right so and as we showed uh cross Q appears to be comparable or faster than appears to be comparable or faster than appears to be comparable or faster than that in like less compute time so we that in like less compute time so we that in like less compute time so we expect that expect that expect that this should be an okay thing to try out this should be an okay thing to try out this should be an okay thing to try out on whe repots I see okay interesting um on whe repots I see okay interesting um on whe repots I see okay interesting um yeah I mean amazing um and uh one other yeah I mean amazing um and uh one other yeah I mean amazing um and uh one other question so have you guys assessed this question so have you guys assessed this question so have you guys assessed this on top of a modelbased RL setup uh on top of a modelbased RL setup uh on top of a modelbased RL setup uh because I mean like if we think about because I mean like if we think about because I mean like if we think about something like I don't know Dina Q for something like I don't know Dina Q for something like I don't know Dina Q for example or or things related to that example or or things related to that example or or things related to that wouldn't this even further boost your wouldn't this even further boost your wouldn't this even further boost your sample sample sample efficiency um it's a yeah it's a great efficiency um it's a yeah it's a great efficiency um it's a yeah it's a great point I mean this is something which point I mean this is something which point I mean this is something which we're actually looking into right now so we're actually looking into right now so we're actually looking into right now so no we' have not um assessed this fully no we' have not um assessed this fully no we' have not um assessed this fully but I mean in a way this kind of falls but I mean in a way this kind of falls but I mean in a way this kind of falls under the scaling up points I would say under the scaling up points I would say under the scaling up points I would say which I said for future work yeah so which I said for future work yeah so which I said for future work yeah so yeah we definitely have this in yeah we definitely have this in yeah we definitely have this in mind yeah that that sounds amazing yeah mind yeah that that sounds amazing yeah mind yeah that that sounds amazing yeah I mean I also want to see if anybody in I mean I also want to see if anybody in I mean I also want to see if anybody in the uh team has any questions as well um the uh team has any questions as well um the uh team has any questions as well um anyone has any questions from the tee side hey thanks for the talk I have a side hey thanks for the talk I have a side hey thanks for the talk I have a question question question regarding regarding regarding the the statistics in the batch the the statistics in the batch the the statistics in the batch normalization I was wondering have you normalization I was wondering have you normalization I was wondering have you tried instead of having one mixture for tried instead of having one mixture for tried instead of having one mixture for 80 and 80+ one have you tried 80 and 80+ one have you tried 80 and 80+ one have you tried maintaining two different statistics maintaining two different statistics maintaining two different statistics like one for 80 one for 80+ one and do like one for 80 one for 80+ one and do like one for 80 one for 80+ one and do you think it would work um so that's a good question and we um so that's a good question and we um so that's a good question and we don't have this in the paper of don't have this in the paper of don't have this in the paper of course but uh we've tried exotic things course but uh we've tried exotic things course but uh we've tried exotic things like this we've tried uh using like like this we've tried uh using like like this we've tried uh using like manually specifying different mixing manually specifying different mixing manually specifying different mixing coefficients for these two different coefficients for these two different coefficients for these two different distributions and it seems that yeah as distributions and it seems that yeah as distributions and it seems that yeah as long as you keep it consistent as long long as you keep it consistent as long long as you keep it consistent as long as you have exactly one combination and as you have exactly one combination and as you have exactly one combination and not this flip-flopping between the two not this flip-flopping between the two not this flip-flopping between the two uh this works uh this does help uh of uh this works uh this does help uh of uh this works uh this does help uh of course that's going to require you course that's going to require you course that's going to require you to hand program the the mixing to hand program the the mixing to hand program the the mixing yourself the easiest way is to just send yourself the easiest way is to just send yourself the easiest way is to just send both things together in the same forward both things together in the same forward both things together in the same forward pass in which case it's like 50/50 mix pass in which case it's like 50/50 mix pass in which case it's like 50/50 mix and it's it's super easy for you could you maintain maintain two you could you maintain maintain two you could you maintain maintain two different statistics we will mixing do yeah good question um so maintaining do yeah good question um so maintaining do yeah good question um so maintaining two different statistics would entail two different statistics would entail two different statistics would entail that you normalize the batch that comes that you normalize the batch that comes that you normalize the batch that comes with a and the batch that comes with a with a and the batch that comes with a with a and the batch that comes with a prime differently with their own prime differently with their own prime differently with their own specific statistics right yeah yeah H uh specific statistics right yeah yeah H uh specific statistics right yeah yeah H uh so I believe that uh I mean we could be so I believe that uh I mean we could be so I believe that uh I mean we could be wrong but uh this does not seem uh wrong but uh this does not seem uh wrong but uh this does not seem uh theoretically very sound to me because theoretically very sound to me because theoretically very sound to me because uh you got a network that perceives data uh you got a network that perceives data uh you got a network that perceives data according that's distributed a certain according that's distributed a certain according that's distributed a certain way and if you now normalize the way and if you now normalize the way and if you now normalize the incoming stuff incoming stuff incoming stuff differently um I believe that the differently um I believe that the differently um I believe that the predictions are going to be wrong and predictions are going to be wrong and predictions are going to be wrong and and this and this and this is this wrongness of of predictions due is this wrongness of of predictions due is this wrongness of of predictions due to mismatched moments is is exactly the to mismatched moments is is exactly the to mismatched moments is is exactly the thing that we thing that we thing that we implicate uh in this paper uh with implicate uh in this paper uh with implicate uh in this paper uh with regards to training stability so so I regards to training stability so so I regards to training stability so so I believe it would not work but I mean are believe it would not work but I mean are believe it would not work but I mean are there more interesting combinations or there more interesting combinations or there more interesting combinations or or or or or different ways to normalize or or or or different ways to normalize or or or or different ways to normalize that could be tried sure I I think that could be tried sure I I think that could be tried sure I I think that's that's an open that's that's an open that's that's an open question yes thank question yes thank question yes thank you right um James you have a question I you right um James you have a question I you right um James you have a question I I saw you unmuting yourself and then I saw you unmuting yourself and then I saw you unmuting yourself and then muting yourself do you have any muting yourself do you have any muting yourself do you have any questions you want to share now sorry I'm just having some share now sorry I'm just having some share now sorry I'm just having some computer problems I thought it was a computer problems I thought it was a computer problems I thought it was a really interesting talk really interesting talk really interesting talk though okay cool no problem right um though okay cool no problem right um though okay cool no problem right um this sounds this sounds amazing and and this sounds this sounds amazing and and this sounds this sounds amazing and and one last part from me so well actually one last part from me so well actually one last part from me so well actually two more questions One technical one two more questions One technical one two more questions One technical one less technical so um when you talk about less technical so um when you talk about less technical so um when you talk about going to Vision based RL right like going to Vision based RL right like going to Vision based RL right like doing a vision based control is this doing a vision based control is this doing a vision based control is this method a plug in play or do we need to method a plug in play or do we need to method a plug in play or do we need to add some more tricks to add some more tricks to add some more tricks to it it it H uh okay that's a good question um I H uh okay that's a good question um I H uh okay that's a good question um I think it can be used in a plug andplay think it can be used in a plug andplay think it can be used in a plug andplay manner in the same way that people use manner in the same way that people use manner in the same way that people use soft actor critic in a plug-and playay soft actor critic in a plug-and playay soft actor critic in a plug-and playay manner with with vision based RL right manner with with vision based RL right manner with with vision based RL right you replace your MLP with a convet and you replace your MLP with a convet and you replace your MLP with a convet and then you see what happens and batch Nom then you see what happens and batch Nom then you see what happens and batch Nom has been shown to accelerate convenant has been shown to accelerate convenant has been shown to accelerate convenant training that that's actually the main training that that's actually the main training that that's actually the main uh class of networks in which it's been uh class of networks in which it's been uh class of networks in which it's been used so we we hope to see similar things used so we we hope to see similar things used so we we hope to see similar things happen there too yeah I see okay cool happen there too yeah I see okay cool happen there too yeah I see okay cool right so we're asking everybody who's right so we're asking everybody who's right so we're asking everybody who's coming to us you know to give us coming to us you know to give us coming to us you know to give us feedback um about their experience in feedback um about their experience in feedback um about their experience in the review process right because we know the review process right because we know the review process right because we know that everybody is complaining about the that everybody is complaining about the that everybody is complaining about the review process and uh what we would like review process and uh what we would like review process and uh what we would like to to know is how was your experience to to know is how was your experience to to know is how was your experience with I clear in this case how was the with I clear in this case how was the with I clear in this case how was the review process was it good bad any ideas review process was it good bad any ideas review process was it good bad any ideas to improve it um so okay uh I think I mean and I it um so okay uh I think I mean and I it um so okay uh I think I mean and I did you can also say something about did you can also say something about did you can also say something about that as well from my experiment that as well from my experiment that as well from my experiment experience uh stressful as experience uh stressful as experience uh stressful as always but um I think we we were kind of always but um I think we we were kind of always but um I think we we were kind of Lucky in the sense that we had um uh Lucky in the sense that we had um uh Lucky in the sense that we had um uh well we had a lot of questions um which well we had a lot of questions um which well we had a lot of questions um which helped us improve the paper over the helped us improve the paper over the helped us improve the paper over the review period also gave us a lot of work review period also gave us a lot of work review period also gave us a lot of work and a lot of arguing to do um but uh and a lot of arguing to do um but uh and a lot of arguing to do um but uh overall it was nice to have relatively overall it was nice to have relatively overall it was nice to have relatively um responsive uh reviewers that also um responsive uh reviewers that also um responsive uh reviewers that also provided us with high quality reviews um provided us with high quality reviews um provided us with high quality reviews um I know that yeah for some of our I know that yeah for some of our I know that yeah for some of our colleagues that also wasn't the case so colleagues that also wasn't the case so colleagues that also wasn't the case so uh in that sense yeah we were we were uh in that sense yeah we were we were uh in that sense yeah we were we were happy that rather we had some work to do happy that rather we had some work to do happy that rather we had some work to do than uh just silence basically yeah yeah than uh just silence basically yeah yeah than uh just silence basically yeah yeah I would say that we had uh fairly I would say that we had uh fairly I would say that we had uh fairly thorough and responsive reviewers so thorough and responsive reviewers so thorough and responsive reviewers so before the rebuttal period closed before the rebuttal period closed before the rebuttal period closed and by the way we were we were not very and by the way we were we were not very and by the way we were we were not very early in submitting the rebuttal uh early in submitting the rebuttal uh early in submitting the rebuttal uh before the rebuttal period closed uh before the rebuttal period closed uh before the rebuttal period closed uh everybody like all the reviewers made everybody like all the reviewers made everybody like all the reviewers made sure to give us a response uh which was sure to give us a response uh which was sure to give us a response uh which was really nice of them awesome yeah and is really nice of them awesome yeah and is really nice of them awesome yeah and is your uh method open sourced can we play your uh method open sourced can we play your uh method open sourced can we play with it is it somewhere we can use it or with it is it somewhere we can use it or with it is it somewhere we can use it or you're planning to open source it or we you're planning to open source it or we you're planning to open source it or we working on it any day now hopefully working on it any day now hopefully working on it any day now hopefully hopefully in a couple of days awesome hopefully in a couple of days awesome hopefully in a couple of days awesome right so so this is a spotlight in I right so so this is a spotlight in I right so so this is a spotlight in I clear right that's what I understand clear right that's what I understand clear right that's what I understand awesome guys okay so yeah with that I awesome guys okay so yeah with that I awesome guys okay so yeah with that I want to thank you guys again uh both want to thank you guys again uh both want to thank you guys again uh both Daniel Anda um yeah this was fantastic Daniel Anda um yeah this was fantastic Daniel Anda um yeah this was fantastic work and uh congrats again and I'm work and uh congrats again and I'm work and uh congrats again and I'm looking forward to uh using your method looking forward to uh using your method looking forward to uh using your method so thank you for coming and wish you all so thank you for coming and wish you all so thank you for coming and wish you all the best in your eye CLE conference and the best in your eye CLE conference and the best in your eye CLE conference and so on so thanks all for following us and so on so thanks all for following us and so on so thanks all for following us and we see in the next episode all right we see in the next episode all right we see in the next episode all right cheers
