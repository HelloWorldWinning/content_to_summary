Timestamp: 2026-01-29T07:50:28.741031
Title: 2026最全整合！一口气学完Informer、Time-LLM、ARIMA、Pandas、LSTM、图时序预测等六大时间序列预测算法及实战！ p01 Informer时间序列预测原理解读（一） BV1eb65BcEcg
URL: https://b23.tv/DjthxrW
Status: success
Duration: 52:02

Description:
**1. 核心观点**
Informer模型通过引入稀疏注意力机制、自注意力蒸馏和生成式解码器，显著提升了Transformer在长序列时间序列预测任务上的效率和准确性。

**2. 整体框架**
Informer模型以Transformer架构为基础，针对长序列时间序列预测的特定挑战进行了多项优化，包括注意力计算效率、编码器层间信息处理以及解码器输出模式，以实现高效准确的长序列预测。

**3. 摘要**

*   **引言：Informer 模型背景**
    *   Informer 是一款AI领域最佳论文（Best Paper），专注于时间序列预测，特别是长序列预测问题。
    *   它基于Transformer架构，并对其进行了关键优化，以提高效率和性能。
    *   强调时间序列预测在股票、机器人动作预测、气温、疫情发展、工业生产、电商流量预测等领域的广泛应用。

*   **传统时间序列预测面临的挑战**
    *   **长序列预测困难：**
        *   传统算法（如Facebook Prophet、ARIMA）主要擅长短序列预测，对长序列（如预测未来30天、半年甚至一年）效果不佳。
        *   预测值作为下一时刻的输入会累积误差，导致序列越长误差越大。
    *   **RNN/LSTM 模型的局限性：**
        *   **串行计算：** RNN/LSTM 的逐时刻计算本质上是串行的，导致处理长序列时效率极低。
        *   **梯度问题：** 序列越长，模型训练越困难（梯度消失/爆炸）。
        *   实验表明，随着预测序列长度增加，计算速度下降，损失（MSE）显著上升。
    *   **传统Transformer模型的局限性（在长序列预测方面）**
        *   **自注意力机制的计算复杂度：** 传统Self-Attention的复杂度为 O(N^2)，N为序列长度。当N很大时，计算量巨大，效率低下。
        *   **冗余注意力计算：** 实验发现，在Q-K点积的注意力矩阵中，大部分得分接近于0，表明大量计算是冗余的，只有少数Q与K对具有显著的注意力得分。
        *   **解码器的自回归（Auto-Regressive）特性：** 传统Transformer解码器逐个生成输出，即预测下一个时刻需要依赖前一个时刻的预测结果，导致长序列生成时效率低，无法并行。
        *   **编码器层间效率：** 传统编码器各层输入输出形状相同，重复计算可能导致资源浪费。

*   **Informer 模型的核心创新**
    *   **1. ProbSparse Self-Attention（稀疏自注意力机制）**
        *   **动机：** 解决传统自注意力O(N^2)复杂度及冗余计算问题。
        *   **原理：** 基于观察到大部分Q-K对的注意力得分接近0，Informer不再对所有Q进行计算。它通过衡量每个Q与“均匀分布”的差异来识别“活跃”（即信息量大、与K有显著关系的）Q。
        *   **实施：** 只选择少数“活跃”的Q与所有K计算注意力，并对K也进行采样。
        *   **效果：** 将计算复杂度从O(N^2)显著降低至O(L log L)。
    *   **2. Self-Attention Distilling（自注意力蒸馏）**
        *   **动机：** 解决传统Transformer编码器层间处理效率低的问题。
        *   **原理：** 在编码器中，每经过一个自注意力层和FFN层后，通过蒸馏操作（例如池化或卷积）减少特征维度或序列长度。
        *   **效果：** 使得更深的层处理更紧凑的信息，提升了长序列处理的效率，减少了内存占用。
    *   **3. Generative Style Decoder（生成式解码器）**
        *   **动机：** 解决传统Transformer解码器的自回归模式导致的长序列生成效率低问题。
        *   **原理：** Informer解码器通过一次前向传播并行生成整个目标序列。它利用一个特殊的掩码机制，确保在生成当前时刻的输出时，只能访问到历史信息，而不能“偷看”未来信息。初始输入时，目标序列被填充为0（除了起始token），其余通过Attention机制并行计算。
        *   **效果：** 实现了长序列的并行生成，显著提高了预测效率。

*   **Informer的优势**
    *   **高效率：** 显著降低了长序列预测的计算复杂度。
    *   **高准确性：** 在长序列预测任务上表现出高精度。
    *   **多变量输出能力：** 能够同时预测多个时间序列变量（如气温、湿度、降水量、风速），适应更多复杂任务。
    *   **广泛适用性：** 作为Transformer架构的优化，具有广泛的适用性。

<Mermaid_Diagram>
graph LR
    subgraph "Informer模型"
        A["Informer模型"]
    end

    subgraph "核心任务"
        B["长序列时间序列预测"]
    end

    subgraph "基础架构"
        C["Transformer架构"]
    end

    subgraph "面临的挑战"
        P1["传统时间序列模型局限性"] --> P1_1["短序列擅长，长序列差"]
        P1_1 --> P1_2["预测误差累积"]
        P2["RNN/LSTM模型局限性"] --> P2_1["串行计算效率低"]
        P2_1 --> P2_2["梯度问题，训练困难"]
        P3["传统Transformer局限性"] --> P3_1["O(N^2)自注意力复杂度"]
        P3_1 --> P3_2["大量冗余注意力计算"]
        P3_1 --> P3_3["解码器自回归输出慢"]
        P3_1 --> P3_4["编码器层间处理效率低"]
    end

    subgraph "Informer核心创新"
        I1["ProbSparse Self-Attention\n(稀疏自注意力机制)"] --> I1_1["解决 O(N^2)复杂度"]
        I1 --> I1_2["筛选\"活跃\"Q查询"]
        I1 --> I1_3["对K进行采样"]
        I1_1 --> I1_4["降低冗余计算至O(L log L)"]

        I2["Self-Attention Distilling\n(自注意力蒸馏)"] --> I2_1["解决 编码器层间效率低"]
        I2 --> I2_2["逐层减少特征/序列长度"]

        I3["Generative Style Decoder\n(生成式解码器)"] --> I3_1["解决 解码器自回归"]
        I3 --> I3_2["并行一次性生成整个序列"]
    end

    subgraph "Informer优势"
        S1["高效率"]
        S2["高准确性"]
        S3["多变量输出能力"]
        S4["广泛适用性"]
    end

    A -- "专注于" --> B;
    A -- "基于并优化" --> C;

    C -- "在长序列预测中面临" --> P3;
    B -- "传统方法难以解决" --> P1;
    B -- "传统方法难以解决" --> P2;

    A -- "通过引入" --> I1;
    A -- "通过引入" --> I2;
    A -- "通过引入" --> I3;

    I1_1 --> S1;
    I1_4 --> S1;
    I2_1 --> S1;
    I3_2 --> S1;
    A -- "实现" --> S2;
    A -- "支持" --> S3;
    A -- "体现" --> S4;

    style A fill:#8A2BE2,stroke:#333,stroke-width:2px,color:#fff;
    style B fill:#2E8B57,stroke:#333,stroke-width:2px,color:#fff;
    style C fill:#BA55D3,stroke:#333,stroke-width:1px,color:#333;

    style P1 fill:#FFC0CB,stroke:#333,stroke-width:1px,color:#333;
    style P1_1 fill:#FFD700,stroke:#333,stroke-width:1px,color:#333;
    style P1_2 fill:#FFD700,stroke:#333,stroke-width:1px,color:#333;

    style P2 fill:#FFC0CB,stroke:#333,stroke-width:1px,color:#333;
    style P2_1 fill:#FFD700,stroke:#333,stroke-width:1px,color:#333;
    style P2_2 fill:#FFD700,stroke:#333,stroke-width:1px,color:#333;

    style P3 fill:#FFC0CB,stroke:#333,stroke-width:1px,color:#333;
    style P3_1 fill:#FF4500,stroke:#333,stroke-width:1px,color:#fff;
    style P3_2 fill:#FF4500,stroke:#333,stroke-width:1px,color:#fff;
    style P3_3 fill:#FF4500,stroke:#333,stroke-width:1px,color:#fff;
    style P3_4 fill:#FF4500,stroke:#333,stroke-width:1px,color:#fff;

    style I1 fill:#4682B4,stroke:#333,stroke-width:1px,color:#fff;
    style I1_1 fill:#87CEFA,stroke:#333,stroke-width:1px,color:#333;
    style I1_2 fill:#87CEFA,stroke:#333,stroke-width:1px,color:#333;
    style I1_3 fill:#87CEFA,stroke:#333,stroke-width:1px,color:#333;
    style I1_4 fill:#87CEFA,stroke:#333,stroke-width:1px,color:#333;

    style I2 fill:#4682B4,stroke:#333,stroke-width:1px,color:#fff;
    style I2_1 fill:#87CEFA,stroke:#333,stroke-width:1px,color:#333;
    style I2_2 fill:#87CEFA,stroke:#333,stroke-width:1px,color:#333;

    style I3 fill:#4682B4,stroke:#333,stroke-width:1px,color:#fff;
    style I3_1 fill:#87CEFA,stroke:#333,stroke-width:1px,color:#333;
    style I3_2 fill:#87CEFA,stroke:#333,stroke-width:1px,color:#333;

    style S1 fill:#90EE90,stroke:#333,stroke-width:1px,color:#333;
    style S2 fill:#90EE90,stroke:#333,stroke-width:1px,color:#333;
    style S3 fill:#90EE90,stroke:#333,stroke-width:1px,color:#333;
    style S4 fill:#90EE90,stroke:#333,stroke-width:1px,color:#333;
</Mermaid_Diagram>

Content:
本课程公号是3115秒将系统解读informer时间训练预测算法原理看不完可以收藏点个关注不迷路咱这样 现在要介绍一下我们今天的主题我们今天的主题是一个新的领域新的内容可能之前我们都没想过我们之前在科科当中其实讲得更多的是那个叫视觉地的算法是不是然后我们今天不是三起三起的时候咱们更新了一个挺大的一个系列就是先停火的叫什么叫那个穿的former这个是我们前一次直播我记著咱们直播里可能讲了四次课我们可能咱们这个三起其中有四次课都是跟穿的former相关的我们回回一样最开始我们就讲那个叫Burth最原始的穿的former然后我们讲VIT就是视觉模型怎么去用穿的former然后我们讲第一题怎么做检测的然后讲一个词问穿的former是不是我记著咱们讲了四次课要今天我们要讲的实验讯息令虽然说咱这个要作为任务实验讯息令但是我们这个整体框架来做的时候还是用什么还是会继续穿的former的一个整体框架所以说今天还是需要大家有一些这个穿的former的基础的如果说有一些是新来的同学如果对那个穿的former还不太属性的才是建议大家就是先看一下我们那个路段当中就是我们直播的就是直播课直播课的一个路行里边我们有讲了几次穿的former咱们可以回去了把那个补起来然后再看我们今天这个课可能现在来说就会更轻松一点然后先跟大家讨论一个事就是关于这个时间讯息令大家都用哪些算法或者说时间讯息令在大家的一些像不当中有没有用上过我估计应该是有的因为这个其实是挺常见的一个领域挺常见的一个领域在咱们整个的AI当中可能是P&U发觉一些我觉得更多的是做一些书句发觉任务你看我就是不列一些图以后有跟大家简单来去做一做我觉得我们要是不是那都是做得比较早的了这个都是做得比较早做得比较早的算法我觉得我们行一直不要经典因为它那个精度还是不错的行了 我看这也差不多了咱们就直问主题吧来说一说咱们这个实验学习该怎么玩一会儿呢 我会大家说一说我会今天可能没时间给大家讲这个代码然后咱们这样按照我们以往的一个经验就是直播说咱们讲这个算法支点然后我抽扣买那个圆码给大家录一遍圆码来说不难圆码来说不难但是有解定文挺著好的有解定文挺著好的到时候我会大家咱们来去好好闹一闹等你去怎么做的行了 来 终于上到三十了行 咱开始吧好 这也刻咱们来说一说关于时间训练预测我们该怎么去做首先大家可以看到我们这段请个名字叫做Efoamer大家很会想这个Efoamer它跟我们实验区训练预测有什么关系吗其实这样我们今天有大家的一个选才咱们是讲这么一篇论文它是AI当中一个Bas的Paper就是一个最佳论文所以说我们今天也大家讲的算法它第一个指导意和学习意义以及后续要大家做实际项目的时候作用用都是非常有参考价值的因为毕竟有AI当中给了它一个Bas的Paper最佳论文的一个角肯定说人界论文人界的方法有学习有工程实验的一个价值那好了 再讲咱们这个实际算法就再说我们这个Efoamer之前我们来想一想就实验训练大家哪些个长久哪些个拥有一当中我们见到过呀首先我们先看第一个第一个什么股票这个领域是不是关于这个股票预测大家可能会第一个感觉这样老师啊 你说咱们实验训练预测股票我怎么感觉不太靠谱呢因为我每次学了好有关习但反正跟股票相关的最终预测出来结果都是不太准的我觉得这个事正常的就是你能预测准那就斩了 肯定是有问题的为大家解释解释为什么会有这样的一个问题咱先回到这个机器本质我们说机器学习它是占一件事本身我们的数据是服从一些分布服从一些规律的但是我说接下来我们希望咱们这模型来学一学咱们数据当中呢我希望咱们这个模型来学一学咱们这个数据当中它第一个分布找什么样子输入和输出这也理演戏是什么那好了 我们去年好有模型但是呢到我们应有模型的时候大家想一想训练模型的时候咱们用的是历史的数据但是呢我们应用模型的时候可能预测未来预测一个新的数据这个首先想一件事我历史数据的分布历史数据的规律一定跟新的数据的分布新的数据的规律是一样的吗在股票这个市场我们可能说是不一定是不是规则在变人在变行情在变国家政策在变一些突发事件都有可能验证的结果这些是模型所没见过的所以说股票这个市场虽然说我们也是实验训练预测问题但是大家是不是看得本质了在本质当中它的数据非常不稳定规则始终处一个变化的缺点当中那比如现在咱们国家发布一个双显政策那这些较于股票新东方这一系列那是不是全废了那你就这个模型称想到吗它能提前知道我们国家发布战友政策吗这就很难了 是吧所以说股票这个市场大家可以拙请来去考虑无论咱们的实验学习算白有多么牛我觉得在这个领域都是很难做用的因为一旦数据规律改变数据分布改变我们模型在学起来在用起来这个事就没那么容易了然后右边我们有一些接手币机器人动作的预测机器人我们当前的一个动作要推测一下接下来他要去做什么人体一个姿态的估计人体的行为识别这些是不是说咱们之前是我们有客人当中是讲过这个行为识别的我们说这个行为识别是机什么去做的它不是机用美针图像而是机什么而是机用我们的一个视频前后针的一个关系那这是不是也是一个实验学习练机用我们当前理安学习的动作预测他下一个动作干什么我看他现在要要回拳了我打过来了我预测他下一个下一个时刻的动作要打到眼睛上要打蒙的这是不是一个实验学习的预测还有什么气温的还有我们的一些调度问题的还有现在比如说疫情的一个发展比如说咱们说国外吧国外的一个疫情每天的一个确诊人数是有多少的这是不是也是一个实验学习练问题好了这里给大家列了一些都是实验学习练习相关的然后我在家说一说就是我这边在实验学习相关的当中做过的一些跟实验学习相关的因为我给企业做培训比较多我就给大家说一说跟企业合作的一些课题我挑选中演了一说第一个是什么第一个比如说这样就是说我们在去生产加工一些顶价的过程当中我们想去预测一下那比如说我们这个流水线G666线当前每一时刻它的一个原材料的一个消耗预测它下一时间原材料的一个消耗这不也是先学习练问题吗据现在例如说句一个时刻表每时每刻我们原材料的变化预测下一时刻原材料我们需要多少来这是一件取决预测是不是商家当中比如说像这个淘宝或是某多多它是不是说也要给预测一下它的一个流量然后好来跟我们经验补货这些其实在他们后台一些卖家的看板当中人间的很多软件都已经集成好了有试验学习预测据你当前你的一些指标你的走势预测接下来你的每一天流量大概是有多少让你提前有准备是吧好了这一下我们都可以叫做时间序列问题那行了给大家解释解释我们时间序列都用去做什么然后接下来我们来看你说我们现在试验学习的预测的算法有多少种呢其实非常多这个领域其实我们的算法是非常庞大的咱们来看一看这个领域算法当中可能遇到的一些小的问题来我们看前两图先低个图吧我们说这是低个图低个图它说这样他说我们做时间序列我想预测准那我可能预测的未来是一个短时的一个趋势什么叫短时的趋势呢会搭绘例子比如说我今天今天我基于咱们历史0到30天的一个数据不能说这个天比如说现在我说基于还是天马还是基于咱们0到20天的一个数据0到20天的一个数据比如说每天的一个气温预测接来10天接下来10天的一个气温大家注意一下我们现在这个时间序列预测并不是说我现在拿这个0到20天的历史数据我只预测这个21天不是的我要得到的并不是一个输出的只是下一个时刻底下我要得到什么下一个时刻的一个序列的包括什么流现在我说我得到了就是前20天的一个气温的情况接来呢我要预测21天22天23一直点点23一直点点我说到30预测接来一个序列这个学业当中每一天的温度是多少这一个是我们现在要考虑的一个问题它并不是只是预测下一个时刻点而是要预测我们接下来一段的一个序列要做这样一个事传统算是当中或者说大家你们可能之前用过的一些算法或者是我们参考委员论文可能有小小问题什么问题很多算法都是去短序列去测的什么就短序列我现在知道前20天的情况我预测21天22天我甚至预测23预测未来3个时间点到这件事好做但是我说换一换你别预测未来3个时间点你预测未来30个时间点的这件事还容易蒙这件事就不容易了所以说创造算是当中做的很多都是居于什么居于我们前10个的序列预测未来的短10的一个情况这是第二坨我们说预测的都是为了短10的情况在短10的情况中它可能预测很准但是一旦你想预测一个长序列像说的未来10天未来30天甚至未来100天的时候这件事就不太好做了而且很多问题当中大家想一想我们很多问题它是为了长序列问题当我们说我们要去做这个时间去电比如说这个原始乐文它说诈栏件事它说它们可能预测那个就是电力好运未不太不太清楚搭一诈栏件事比如说就举例子比如说变压器的用电量它会基于用户比如说未来的未来半年用户的一个用电的一个变化情况我们要调解这个变压器的电压大家想一想你看它的需求是什么我首先也知道用户未来半年的一个情况我好知道也就是知道用户半年之类情况我好继续用户这半年大概的情况我好提前去修改我们变压器这个电压那是不是说我们得继续未来的一个精准的长序列现在我才能做一些干预才能结合业务去做一些任务吧所以说现在很多任务当中都是去这个长序也去预测的预测未来半年预测未来一年这都不是一个短的时间点而是非常长的一个序列是吧但是在这个比较长的一个序列当中大家想想一下我们说这件事就很难预测得很准大家可能说为什么预测不准呢为大家举例子比如说现在现在咱们按那种传统一些实验窗户的方法我说咱们先有几天数据吧就五天一二三四五就比如说咱每天的一个气温我说据每天咱们天气的一个湿度然后它的一个风速然后它的一个降水量降雨量等等然后我说接下来DUJ只标区测每天的温度那比如现在我以为有这五天的一个数据以及五千预测的结果接下来我说预测第六天当咱们想做第六天说大家说还是不说我基于咱们这个前五天的去预测这个第六天的那接下来要讲问答第七天怎么办第七天怎么去做的第七天它是不是说第七天你说它只跟前五天有关系吗只跟我们实际有的数据有关系吗我们现在实际有数据只有一二三四五只有这么五天的数据我说咱们要预测第六天这没问题但是你要讲继续预测第七天呢你还拿这个一二三四五去预测第七天吗那比如说还有第八第九第十第十一你还拿一二三四五去测吗可能来说不是了为什么因为比如说跟这个七相关谁跟七可能关于最大并不是一二三四五而是这个六是不是那八呢可能是六七九呢可能是七八十呢可能是八九是不是所以说我们说这样就是每一个时刻它的一个结果肯定跟它上一时刻是最最相关的跟它越往前的可能就没那么相关了那此时大家想想一下我要预测六七八九十甚至以后更长的一个训练我不能说预测每一个时间点都拿原来的这只有这么五天的吧那大家可能会问老师那预测第七天可以怎么办我们说预测第七天它也这样我现在拿到手这样有窗口咱刚才不是把第六天的预测出来了吧你把第六天预测出来之后我说第六天的那就当作手已支条件本我拿二三四五六预测七可以吧接下来呢我说拿三四五六七预测八拿四五六七八预测九还有大家可能说这也是好像挺好咱可以这么去做但是有一个问题大家想一想当我们把这个六七八当作已支条件的时候咱们来想一个问题它是一个真实质貌六七八是一个真实质貌它不是它是我们预测的质要在我们预测值的基础上再去预测下一刻时间的一个质这件事大家想想一下本来我们就不拿一个真实的实际的完全全正确的一个数字去做下一个质现在拿的都是我们猜的东西再去猜下一个所以大家可想是质如果这个需要越长比如说往下长再长到这个十二十三的时候假设说我现在要预测这个十三我预测这个十三的时候我可能拿八九十十一十二了我们现在拿这个我数据有一个是真实的吗没有一个是真实的全都是我预测出来的那我的预测值再去预测我下一个时刻的预测值所以说大家可能可能发现一件事长时间的一个虚烈我们想预测这一个是不来说就有一些难度了因为我们还有考虑我们预测结果再用预测结果去做下一个时刻的质这件事就变得非常非常难了所以说我们可以总结一下现在很多传统算是当中都不敢去做这个长虚烈的一个预测因为刚刚给大家说了你的虚烈越长这件事肯定是越难的但是很多实际战武实际业务当中还真就是需要我们预测一个较长的一个范围较大的一个虚烈所以说今天给大家说的我们这个与风嘛我们的背景是什么我不以大家去读这个论文让直接按照我的理解同时给大家去说这条论文的背景就是说人家研究了一个电力方面设备电力设备它说我们就有短虚烈没法做一个调控我必须实验知道用户可能未来半年未来一年这个情况我猜好事先做这个调控所以说论文的人就背景是什么我们的一个长虚烈与测问题要解决这样一件事所以后大家想一想如果我们有一些长虚烈预测问题的时候可以怎么办来我们就可以来套咱今天给大家讲的这个无法了这是咱们可以套用的一个方法然后右边这个图就是我们长虚的预测预测未来较长的一个虚烈而且我们希望就要把它预测准这个是咱们第一个出发点然后当我们再去说这个算法的时候其实大家想一想我们用哪些个经典的方法也可以做实验学业预测呢这有大家说两个这两个绝对是大家用的最多最多的方法了第一个是Facebook的一个工具包这个工具包大可能都听过挺经典的一个包是不是最主要跟你们什么我估计大家用它是因为它是比较方便的很轻松就是掉包我会大家最喜欢看的件事就是你不要写代码我甚至我一行代表都不用写不能说一行吧我不想写超过一百行代码我就要解决一个实际的任务好这个Facebook的工具包能帮助你解决这样一件事然后我简单跟大家说一说这个Facebook这个包它是主要预测这个趋势的我先跟大家说一用长久以后大家有认为时候可以拙起来考虑我们要套用的是哪一个方法当前这个包它适合预测的这些趋势什么叫做一个趋势LQ的例子比如现在咱们说我们那个股票就咱们这个今天这个股票跌浪起伏的就这种感觉无物跌浪跌浪起伏的好然后咱们用这个Facebook的这个包它可能预测出来什么它可能预测结会这样的就是这块预测在长了这块预测在D这块预测长这有DM又长又DM又长又DM又长又DM它能把这个趋势给你预测的很好就是一些突变点的一个信息一些变化的信息它能够掌握到这是人间这个算法它的一个本质设计的时候就让这个算法去捕捉各种各样的趋势各种序列变化的趋势所以说Facebook这个包它是适合做一些趋势的预测就是趋势变化比较多的时候可以适合用这个包但是一般我们说在效果可能没有那么准就是你说它某一个点它是要上升还是下降这块它预测也准但是每一个点实际值它可能偏差是比较多的然后接下来对比这个如何模型这个如何模型它是这样它虽然说它跟Facebook那个包上相反的Facebook那个包我们说它是能掌握这个趋势是吧然后我这边有这个包它可能说我这个趋势做的不是特别好但是可能我每一个点会做的较精确一些每一个点会做较精确一些但是有时候趋势我是预测不对的这就是两个极端了大家可以著急来惊考虑但是今天这两个都不是我们的主题因为它也都怎么样它俩都设立到一个大问题不适合做常序列而且还不太适合做多标签的一个输出大家可能说老师什么叫做一个多标签的输出比如说我现在有这样一个任务我们在预测去练的时候我并不光要预测比如说刚刚我们说了嘛有这个1235然后我去预测6大家觉得就是预测下一个时刻点它的一个气温版我说咱不光做一个预测气温我一口气的预测多个我说计预测咱的一个气温然后我说又预测它的一个湿度然后又预测咱们的一个降水量然后还有再预测一个风速可不可以是都可以的咱们今天要讲这个info mode它不仅是可以做一个常序列还可以做还可以做一个多标签像当于大家可以自定一任务了你的任务都可以往任务当中去套但最关键是什么最关键还是说咱们今天讲这个info mode它在常序列的文艺当中做得比较好而且效率是比较高的说明今天要专业大家说的我们的主题然后再咱们正式切入到主题的过程当中咱来想以前的时候就是咱们想我们要令传统算了改进什么东西以前是大家想我们说做这个湿焉序列我问大家咱们能第一个想要算了什么或者说大家用过的算了什么在我很多课程当中其实给提过但反正我们要做湿焉序列我觉得大家肯定能先想到的一个算法叫rstm是不是但是这个算法没有什么问题适合做湿焉序列有很多小伙伴就拿这个就是它其实是个r结构然后加了一些就是控制单元有很多小伙伴拿这个rtm做各种样的预测但是现在呢有一个问题咱们来看一看我把这个rtm完全给大家简单画一画它来这样的这是第一个位置然后这是第二个位置然后这是第三个位置这是第四个位置然后这是每一时刻的一个输入这是x1 x2 x3 x4然后它也看就是它这一课我们说rtm它最大问题是啥你看我们说它底是一个串显的过程先算x1x1会把它中间接过创给x2x2呢来说技考率x0中间接过又考率x2本身的一个输入要建设创了x3x3呢就再一接收再一做但是必须有个顺序是不是先把x1的特动体就好然后把特动串x2然后再把x2的特动体要串x3所有大家也发现在我们这个rtm中它既然是一个串显的那速度怎么样串显我们的速度你说它根本跟咱们的序列的长度成正比你的序列长度越大你需要一次算的时候需要先把前面都算把它藏算最后一个这要效率怎么样呢肯定会非常非常低咱们的效率很慢是吧这是一个问题还有什么问题呢这个模型收点欠难该想比如这款我说第二点我说第二点完之后这款有xn吧比如这个n等于1000吧等于1000行不行然后接著我们说反正算了一个反压川波这款我们都有很多组全中参数的我们说这些全中参数打扮是不是要反向川波主层做计算然后往往他下面传也是如此的当我们在反压川波做计算的时候如果你得到区别场的越长你说这个模型可以让你选择越容易越难那肯定是越难所以说现在xn吧大家看一看这是论文当中给出了一个效果论文两说这件事当我们用xn的时候我们的x则就是说你要预测的一个长度你要预测的一个长度是越长的你看越长的越长预测长度如果说它是一个越大的时候这款是这样就是1224不是12241224 48 96192 428这是我们要预测区别的一个长度如果你要预测区别长就是越大的咱们现在看第一个第一个就是红色线红色线是什么红色线就是我们的那个红色线就是我们的一个计算的一个速度了计算速度我们的一个效率会怎么样咱们的效率会越低应该效率会越低这表示我们计算的一个效率然后再看我们这个蓝色的咱们这个蓝色表什么意思这个蓝色表述我们的一个损失你的那个区别长度越大咱们的MSC就是我的预测值和真实的师傅是不要接近的如果越不接近损失了越大你看它的损失怎么样它的损失会越来越大是不是所以在这两我们可以大家可以发现这块有个轨点轨点啥意思呢就是说这样轨点之后要大幅下降了所以说接来轨点一般是我们可以选择的一些比较合适的值看看多少是一个先来说不要接近于这个48的不要接近于这个48的所以说一般的时候我们说用传统算法会用这个ITM在做学业预测的时候如果你要预测未来较长比如说大于20个大于30的时候它效果会变得非常非常差好啦跟大家解释解释咱们这个串串串串串会有这样一些小问题咱们今天讲这个与FORMER它是怎么去做的呢其实这件事我们想就是现在我们要预测实验讯率其实跟其他算法像模型都是比较类似的首先要瞄干什么咱们就说建立好我们的输入和输出之间的一个关系不就完事了吗建模建模不就是说把输入后的输出还专门价个钱把它能点到一块怎么样合适能就捕捉它们之间的一个关系咱们就怎么去做了所以说大家看这个名字有FORMER其实它是套的什么套的就是一个串串串的一个架构跟我们那个串串串的FORMER的E-Code和D-Code是非常非常类似的直播说在演任记录上做了一些优化做了优化让效率能够更高一些所以正影大家现有一个基本的认识我们这个Infomer咱的核心思想就是啥就是串串的FORMER在串串的FORMER当中我们要去在做进一步的优化大家可能会问老师为什么串的FORMER呢也我们说咱们现在在做计算的时候串的FORMER它是有一个叫注意力机设是不是然后更好的关注于我们全局的信息而且这个全中向能更好的告诉下一个时间点的一个预测的结果吧应该跟前面哪一步是比较相关的这些太能帮我们实验出来而且又是一个并行的架构再有什么再有就是它是现在最火的一个模型了所以说咱们这些论文也是套用的是串串的FORMER这个架构来我们来看一看稍微给大家回个回顾就是咱们这个经典的串串的FORMER架构是怎么去做的首先有我们这个输入比如说输入我们就说有咱们这么三个单词吧H1H2H3接下来我说算这样一件事我说接下来我要去重构每一个输入我要重构每一个输入重构每一个输的时候咱们是怎么做的我这样是不是说比如说要重构第一个词我重构第一个词的时候我说我看一看第一个词自己跟自己的关系自己跟其他词的关系我是不是都要算一下假设说咱们这儿通过内机如果大家忘了或者是之前没有听过咱们这个QKV还穿得或者是什么的先看一看我们基础内容进入另外一张锤有讲这个串串的FORMER的在这一块上当我们得到得分之之后我们说我做一个SOTMAX给它应证成为概率因为它立完之后比如说0.96是不是说我0.96被的H1加上0.02被的H2再加上0.02被的H3就等于我最终重构的一个相量了这个就是串串的FORMER当中我们基本的一个组成就是咱们苏串QKV可以干什么来可以构建我们最终每一个特征该怎么样做重构好这里大家也是老师我们每一个特征咱们实际在重构的时候怎么去做的稍微的回顾回顾咱们串的FORMER是要干什么的然后其实现在只说我们一个ECODE等一会儿呢我也大家讲一讲就是在咱们这边FORMER当中它要输出的时候它不仅是这个ECODEECODE只是帮我们去组合这个特征它说说的时候更重要的还要去参考那个DCODE的一个架构来说优势在我们这个实验讯息的当中它优势是这样了其实我觉得最大优势是什么咱就先跑开其他的不是吧最大优势就是说它比较万能你啥模型啥算法咱说都能往这个串串的FORMER当中去套而且现在现在越来越简单了在那个就是如果大家自己拦写代码在那个PETAL当中直接直接有MUTE HIGHTH的TENSHER这个模块现在人也内置进去了你甚至都可以直接逃用我估计以后川普就是那个PETALSHER他更新的时候可能会加入一些更简单模块真正的实现咱就一行代码掉用了而且它并行的加这处于机制还是挺好但是我们先要来说一说问题因为咱们要讲这边论文它更多的是解决了一下川普FORMER当中所设计到的有一些小问题来我们来想一想第一个问题是什么如果咱们这个时间需业太长比如说比如说我们的输入需列我说基于咱们的输入需列是一个96的这是我输入需列96预测它未来48个预测未来48个大家想想一下如果你的输入需列比较大我为什么说咱们输入需列一般比较大呢因为我们希望多体控一些信息所以输入需列相当于比较大当咱们输入需列比较大的时候输赞一件事对于需列担任每一个点每一个点如果有N个点每一个点都要计算它跟其他点的一个关系这个复杂度它是不是一个N方的先来说跟我们的需列长度成正比很多时候我们需列长度很大只有导致我们就在效率是比较低这是第一个事第二事儿可能大家底扣得有忘了以后为大家强力要一遍什么叫底扣的在我们底扣的时候就是大家想一想传统川普FORMER怎么说出的传统方川方这样就以扣的这以扣的就不画了底扣当中也有答画一画底扣当中比如说要输出的是未来48个说要输出未来48个点预测结果正常咱说说先输出第一个我写T吧正常先输第一个结果然后D个就是得等一个值然后我说再把第一个结果传给第二个然后第二个呢再输出D2字可结果要再把第二结果传给第三个然后第三个传第四个第四个一段传也就是说实际咱们在底扣的输出会过程当中我们并不是一次输出的48个而是先输出第一个然后看一看第一个是什么再输出第二个再看第二个什么再输出第三个再看第三什么输出第四个只有导致一个问题因为我们现在做出一个长序列的输出你长序列的一个输出每一个都要去等那这个东西他肯定不是一个实实的了那作为相当说又不要慢是吧所以说在这篇论文当中人家还对那个以扣的架构稍微做了一个更感我们说比如说像就是现在我想咱们预测未来的就居下数据预测未来48个点这次是8个点它是同时生成的一口切直接省了出来了大家能说你每一个点不需要看一看不用第二点不需要知道第一个点性心吗需要知道的但是呢我们可以在传统风险当中直接去实现直接通过注意机制的方法就能把实验出来并不用像传统的传统方面一样第一个及第二个第二及第三个这样做效率太低了而且大家想一想我们在之前就是咱们有一个专门讲传统风险的课在那个传统风险当中提到一个叫DTR的DTR是当时我们做那个目标检测的时候做目标检测我们是不是说人家那个DTR里面的一个就是捷马槽人家的扣的也是一口切输出所有物体所在的一个位置所以说今天咱们讲这个info我觉得跟DTR其实挺像的都是一口切输出所有的结果而且不用去再一个去等了这是它的一个优势就是效率是比较高的把底扣的这一块优匀大家说底扣的这一块人家改一改直接一口切要当输出我们所有的预测值好了右边是我结了一下就是这个论文当日图但是大家可能现在去看这个图片难度一会儿呢咱这样一会儿呢我们先说就是咱们整理下有当转会设计到的几个小细节咱先把这些细节支点率明白了然后呢咱再把它合成到一块这件事就好解决了整个论文当中其实它就是微少三个点来进行分析的为大家说一说乃三个点第一个点是了第一个点就是说现在我们做这个self-tensh效率太低了为什么效率低呢如果需要长度为N那我们要计算N方因为是我们K就Q1K1Q1K2Q1KNQ2K1Q2K2Q2KN是不是每一个Q都需要跟所有的N做计算那这样变成这个N方的一个伙达度那Tensh的竟然效率像是太低了论文当中提出来一种新的如果Tensh就要方式但是虽然说是新的但是就是换汤不换药吧它第一个本质是普遍的直播说加上一些小翘门加上小翘门一会儿呢为大家解释这些翘门怎么去做的第二呢就是说我们这个DCODEDCODE像我们也说的它不用不是一个月输出了要一次输出所有结果这是第二问题咱来看看说完第三个问题第三个问题是这样就是大家想一想我们传统的传统方式当中咱们做一个Code是不是有好多草我做了一层Suffer Tensh我说再做一个Suffer Tensh再做一个Suffer Tensh每一次做的时候咱们的一个输入和输出的大小是不是完全相同的输入和输入规格完全一模一样了就把一件事重复做了N次就是每次输入的输出不一样但是他们呢他们的形状和格式完全都是一样的就是数值不同但这回就有两问题你说现在我们这个常训练咱们做一遍Suffer Tensh都挺废究的你还要做多次那是不是每一次消毒都挺低的伦当著他说能不能把这个效率低的事解决呢其实可以的大家可以看到它这个画图的样子你以当这是D层这是第二层明显这个第二层它的一个体界要比D层的体积怎么样要小一圈是吧所以说就是在我们设计Eco.在我们设计编碟器的时候咱们想一想咱们一样能让我们这个输入来说每一层咱们省著参数每一层我们计算效率要更高一些这是要解决的三个问题所以说就是通编论文只需要大家了解这三个点你会为它据说就算也做什么只要大家了解这三个点了后面的任务相对来说就是会比较容易了然后我们先看一个现象这个现象是论文作者在大量实验当中都发现一个事它作为实验主要是继续时间虚列的来我们看一看它是这么说的就是先看左边这个图左边这个图是这样你看这段有Q有Q是不是就大概这么小就是现在咱们输入学长度48我们的学长度48就这样咱们先这里吧有48个Patch或48个Token点而点我这一共是48个然后每一个是不...它这里有Q1K1这有Q2这有Q2这有Q3这有Q3是不是每一个Patch每一个Token它都会有自己对应的一个Q和一个Q然后大家看这个图这个图当中你看Ki48个这个Q也只48个这样呢它们形成一个48成48曲阵局阵当中的每一个纸它用了一个就是用热度图来表示了热度图就是颜色越偏这个黄色它是这块属于这块纸越大颜色越偏黑色表这个只是越小的然后问大家只小待会儿什么这表示它内机内机如果越小给大家举个例子两项亮这是一个Q想亮吧这是一个Q想亮如果这两项是这样的时候它内机位多少内机位底二在这也就是黑色的它是没什么关系的如果一个Q它是这样一个Q是这样就是说两个好的跟一个人式的连贴点和贴上去了此时呢它两个的一个内机很大你要内机很大的时候我们得到的要QQ它的一个权重式也会比较大就是偏这种颜色的但是我可大家来看一看就是在我们整个的一个就整个的这个曲阵当中我问问大家是黑色的多还是一些黄色的多黄色就代表有注意机制它们的关系特别强关于特别紧密黑色就是说这个Q和这个Q这两就比如这两吧这它两之间或者它两之间如果是黑色的表示它们今天关于怎么样它们之间就是说是没有关系张一件事那我们来观察观察呢其实作者发言一件事还说我们在这个QQ形成了一个内机举人当中其实大部分都是黑色的黑色表是什么黑色就表示咱们这件事白玩了我们希望通过这个Q和Q的内机好算出来当前我们的输入当中每个派迟跟其他的一个派迟之间就其他的输去其他的一个输入点当中的特装我看看它们这样没有点线我们希望它们都有点线的并且这个点线要非常鲜明有大有小这样才是最好的特装来就去问得比较开但是现在呢我们来观察一下由于我们这样吐当中给出来都是黑色黑色是我说很多比如说这款我这个拿黑色画这个Q和Q和Q3以算接近领了代表它没什么关系Q和Q和Q和Q和Q3也接近领了都没什么关系那你说要输相当于在这个Q和Q这个内机操作有白玩了等于领上它也有没关系没关系不用代表在杀信心没有得到嘛所以说现在论文字发现一件事你看右边它这件事给总结到一个吐当中这个吐呢我们这个横座表横座表就是会有多少就是会有多少个就这么说了签出了纵索则壹把纵索表了纵索表就是他们的一个关系的得分值关系的一个得分值然后横座表就有多少个你看关系得分值比较高的就正常用一个颜色高亮出来的有关系的样本数量多少极少数不需要极少数只占了其中壹小部分但是大部分你看这块这么多的样本大部分强颁下这壹些Q和Q之间的关系怎么样大部分强颁下这个SIMPOS就是我们的Q和Q他们之间的一个关系大部分他们的关系都结尽于0了这是发现了一个规矩我们都要Q和Q之间有好多都是没什么关系的但是优势也少部分他们之间还是有关系的那所以大家想一想我们对于这个任务来说是不是只需要我们找那壹小部分就行了只需要找得有关心就行了因为没关系的那壹些他对特征起不了啥作用全职都为00成就要数都尽运0了没有任何意义向他白玩式吧所以说这个作者在他在做的时候发言这样一个就是这样之后他就要想既然大部分的Q和Q之间是没什么关系的那你说我为了出于咱们计算效率的一个考虑没关系那壹些在还有标要吗我们还有标关心他们数就不关心了实关于谁有特点的有性格的他们这样关系是不要强烈的但是大家也想一个问题我们该怎么样去计算呢怎么去计算比如说当前这个Q和Q怎么去算壹算哪些Q和Q之间他的关系强烈哪些Q和Q之间他关于不强烈我说把那些关系强烈的瞻选出来这件事来怎么去做呢接下来这个论文作者在大量片幅当中都要去强调我们该怎么样能得到壹些好的Q注意接下来我们要重点放大重点我们要放Q当中Q里边有些的Q就像比如这里比如这些个Q这些Q我们说的什么样就是没什么没什么用的为啥他跟那个Q壹成完之后那些算结局顶结局顶而东西像那壹白发是不是所以接下来为壹掌什么找这些个位置所对的Q他们是经历旺盛的他们是真正找特征的能找这个关系的所以接下来我们要善远善选哪些啥哪些个Q他是有用的哪些个Q是没用的这里咱们先拿Q为他举例子壹货我规他马上可能会问为什么是在Q当去找的来壹货我规大家举的例子说咱们在Q当去找有用的我们要去完成壹间什么事然后来看他壹直没用大家看他有没有什么问题先壹些规矩来看他有没有什么问题我喝了我水然后带给他还要替他摸咱们这有聊同有待解解读我们单字有什么意思来看他有没有什么问题这个风呐啥我说有大家属于现代的看壹靠我今天把他那个给他连结给他发壹下多换两分钟换两分钟写上了然后就给他讲我把给他有连结给他先发壹去他也参考给他壹本来这是这个 inform 它是 a a 一个 by 色 paper论文 论文里边写了很多数学就是我看来论文论文写了太多数学了 它是拿这个概率的就是拿概率的方式给大家解释的我觉得公司写算是复杂了其实这个事呢没那么复杂 但是公式啊略显复杂了一些大家也看一看 而且这是有圆码的圆码到时候会充满大家去路的 圆码会充满大家去路的用起来挺简单的而且这个这个这个代码我觉得有一点挺好的 为大家看一看我看这块它购件数据的话正呢购件数据这块直接给一个东西正再来看这个它这个什么Customer Data啥意思我跟大家加上Customer什么意思啊当时看这个圆码的时候我觉得哎我去我说这个这个做者真的就是挺够意思的跟我们写写好什么东西啊叫做一个就是若大家自定义就是也不能叫客户吧就是你说你讲自定义自定义数据结束啊想跑你自己任务的时候我们该参考时候无版去读这个数据参考时候无版去读这个数据这个确证使人家做得比较好的一点啊很多就是很多那个一些算法啊这些个圆码实现都不管你自己说一咋办就读人家数据完事了这块呢给你整个Customer Data现在自己数据咱们怎么读这块我参考那给参考这个人家去写我觉得这个挺好的刚才有点说到了就是QE可以当作有一些怎么样有一些他们全之非税领的全之非领的我们叫什么我们就说他没用没用呢我们就说他是偷懒不干活的但是这里要搭想你说偷懒不干活的一般来说是有Q导致的还有是有K导致的咱们想一想我们说穿的方法咱们就要方式这样有我们的一个QQE比如说他去问跟K1K2K3K5之间K6他们这样的一个关系Q2再问一遍跟他们关系Q3再问一遍他们跟关系也就这里比如现在咱为拿其中一个Q我就我拿不同颜色吧比如拿这个拿红色这个呢我说他是这种Q我是这样他是Q几呢我说他是随便有Q吧他就是Q6随便有Q然后接下来有在哪个Q比如这个Q我看完他是可能是Q26吧就是随便我求有两个Q这个是Q26咱一共呢不是学习的长度为48嘛所以说一共有Q1到Q48但是现在呢我是随便关它两个关这两个它俩其实效果挺极短的咱们来看大图啊右边又要当作这个红色的比如这个Q6Q6大家可以看一看他们的一个注力机制或者他们算出来的一个SouthMess完之后啊我们这个得分指是怎么样哎是一个DA桶起服务的感觉是不是哎什么地方起伏啊就你的爱恨情愁表现得非常明显就代表比如一个公司同时之间哎我就比较比较急的比较说我谁个用你啊我谁喜欢那个人啊这就是这个Q6哎他比较出来的就是这个爱恨情愁跟谁关系好你看突出出来了给谁关系不好给他折下去了是不是明显的一个趋势但是大家来看呢这个Q26Q26就跟我似的谁也不敢得罪哎我说咱俩关系也行哎跟你关系好哎那个在差不多哪去咱们都是挺好的家人我跟谁也放不上机眼跟谁也放不上咱俩地的这就是这个Q26没有什么特点没有什么色彩跟谁都一样哎那大家想一想他跟谁都一样是不是比较评问的过程应该他说比较评问没有跟谁特别差的也没有跟谁特别好的谁是惹不起谁头都不起的感觉那你说对于这两个Q来说呀咱更喜欢哪一个问大家就是说要你选哪一个咱不是说找这项目呢啊就是说咱们现在要找能干活的特征我们说既然是找特征我肯定是说要找好的特征找有关系的特征谁能找有关系特征Q6是不是这个关系疼体现出来但是你看这个Q26就像这个均匀分布式的我这点权重与路均粘吧一共48个我说好在那儿权重比如说一处长48每一个都是48分之一他就完事了最初现在怎么发现了在我们的宽瑞当中其实是有一些有问题的有很多的宽瑞他都要干什么他都在去烂于冲树因为大家看一看我们上面呢画这个可愣的结果是不是要常委结构这种常委分布导致了什么导致了在我们任务当中有很多Q都在去冲树那你说冲树的这种这种机器就是进乎于军营粉部的Q你说他有意义有价值吗我觉得好有价值一点用都没有他只是说把这个特定都匆区分配一下没有什么太大意义那你说怎么办他说既然咱们要不然这样咱们说能不能做一个筛选把这些有用的Q咱找出来他去算这个注意机制没有的这些Q我说咱就不要了就不用他们了呗他们算之后也是个平均分那平均分这个事我在算一遍干啥但直接平均分比外事了嘛所以说现在大家注意一下以方法这些论刀券它是这样一件事我们在做这个长训练的时候由于以Code而Dcode的都需要做我们的Self-Tension都需要做自主义机制但是由于我们的书乳输入区列都比较长如如现在你要把每一个Q跟每个Q再做计算咱们效率太低了所以说呢连这些论刀券要做低点事就是采药对谁做采药对我的Q做采药在做的Q当中我先找出来哪些个Q是这样积极的哪些Q是好的它是有实义价值的我说你拿出来上显那些歌我说跟均匀分布的试道就是跟下一种均匀分布的感觉我说我就不用了你就拉倒吧就不用你了均匀分布就来说没什么用我希望咱们把这个iPhone清朝表现的明显一些的好这是我们要去做一件事但是怎么做呢怎么做啊论刀券练了一个最佛达的公式它是从概率学上去推导的但是我觉得说没必要说这么复杂我只要大家解释解释就是人家实际在代码当中实际的做法它是这样它是我们偷懒的这些Q就不太有Q就像是均匀分布的均匀分布我们说在这个概率上我们可以很容易表示就均匀分布的要感觉然后你们压的特点你有我有钱都有只要有一些明显的它是不是差异会比较大那时候接着咱算战有事吧我们做现在这块这个字写错计算其语计算其语均匀分布的要差异对于让我们现在每个Q我算一算比如现在我说这个Q我跟让它跟谁算让它跟均匀分布许算均匀分布长啥样来我画一个我拿来设画吧均匀分布长这个样子啊就是这就是一个均匀分布然后我说现在对每个Q比如这这个Q6这还有个Q26我这用这个Q6去算一算它跟均匀分布的一个差异你们现在看有这个差异挺大是不是Q26再跟这个均匀分布算一个差异明显它差异就很小是吧那也就是现在我们可以去算一算咱们现在这里有的每一个Q跟均匀分布的一个差异咱们该说了为什么要算跟均匀分布的差异因为发现了就那些比较偷懒的Q比较没有的Q它就跟均匀分布长得挺像的我们通过这个差异来描述一下这个Q它到底是不是一个机极的到底有没有起到作用这个就是我们现在咱们的第个定的方法去计算每一个Q它跟均匀分布的一个差异大家注意一下为什么我们要选均匀分布因为这个是通过实验大家发现了一个结论很多的Q因为这个常委分布嘛导致了很多Q好多类似均匀分布的那正好我说那我就跟你算了本跟你结均匀分布再好算一算咱的关系是什么样如果说我们的一个关系是比较大的我就说咱们的Q是比较积极的如果关系如果咱差异是比较小我就说这个Q也没什么有用也没什么用这样一件事好咱们来看就是它起先前名字就是Properation它的方法机会大姐姐上就在圆码上怎么去做的就是顿文当状这个公式说得及其复杂但是带码当做的确实有及其简单所以在家提个小就有时候大家看那个论文你说这个公式我都快上提案了我都感觉我要学天文式的但是很多时候在带码当时实现就是非常非常简单的就是就是这样我这也大家说咱们这也大家说带码当中是怎么实现的咱们来举个实践的例子来看一看这个例子就是卷码当中连这么做的我就是挺容易的一件事但是愿意绕带你跟着这个思路首先呢我们说这样咱们的输入区列长度是96就是我输入当中有X1X2X3一直到X496输入是96的然后为了说就正常咱们是96个96个派尘然后它的key跟那个key都里分别做计算所以说反而度是96平方但是现在我们说怎么去提到这个效率有时候先做第一件事咱们现在key当中做材料大家说老师你是不是说错了我们刚才不是说有些Q它是不太好的吗你应该现在Q当中做材料找出来Q当中好的你怎么在key当做材料来来来来咱们先旅于旅是这样一件事我们说现在你看这个分布我们画出来的比如说Q6这个分布这边大家注意听这边可能会少月照还有这个Q26咱们的夜夜军然后怎么去做的你看这个Q6和这个Q26我们这个分布怎么画出来的是不是咱们这个任务当时48个是我这一块我说这是跟第1个的然后这是跟第2个的这是跟第3个的这是跟第4个的第2个我说这是跟第48个的我要把当前这个Q跟每一个Q的结果都算出来我说它这个分布咱们画出来是不是那下面呢也是如此它还比跟第1个的跟第2个的跟第3个第2个一直要跟这个第48个的每一个我说都想算出来咱们知道它在分布上什么样子但是现在大家想一想你说有必要把这48个都算一算吗我说这些个比较好的Q它可能跟大部分K效果来说都还不错呢都能体验这个差异我每一票都算一遍吧尤其是下有那些比较难的或者说不干活的Q不干活的Q你说我用跟48都算一遍吗我跟其中20个算一遍我就大概只要它傻样了就比如说大家单单大家评价一个人你评价这个人的时候你看它身边的朋友长什么样子一般我们这么说嘛叫物以类聚针一群分嘛你来看它身边跑不到时候你用把它身边所有朋友都看一遍吗我是我挑一些吧我挑在身边朋友当中选择20个而且是随机选20个是不是就可以了所以这里我们要干什么比如说在这个就是那个路人公事然后写贼复杂其实就是做了一个材料在这个Key当中我们现在有48个我这位置不用48
