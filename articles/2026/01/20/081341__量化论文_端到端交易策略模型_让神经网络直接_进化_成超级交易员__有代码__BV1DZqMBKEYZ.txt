Timestamp: 2026-01-20T08:13:41.065338
Title: 【量化论文】端到端交易策略模型：让神经网络直接'进化'成超级交易员!(有代码) BV1DZqMBKEYZ
URL: https://b23.tv/THWpOqO
Status: success
Duration: 7:10

Description:
好的，这是根据您提供的文本内容提炼的核心思想摘要。

### **核心思想摘要**

1.  **核心思想：直接学习 (Direct Learning)**
    *   **目标**：抛弃传统交易策略中选股、仓位控制、风险控制等多个独立环节。
    *   **方法**：使用单一的深度学习模型，实现从输入特征到最终交易决策的“端到端”学习，直接输出一个完整的、最优的交易策略。

2.  **模型框架与输出**
    *   **模型结构**：对具体的深度学习模型（如Transformer, RNN, MLP等）没有限制，具有普适性。
    *   **输出层设计**：模型的最后一层输出一个 `N+1` 维的向量。
        *   `N`：代表股票的数量。每个维度对应一只股票的仓位权重，正值为做多，负值为做空。
        *   `+1`：代表现金的仓位。通过控制现金比例，天然地集成了风险控制。
    *   **激活函数**：使用Softmax等函数，将输出值映射为代表投资组合权重的比例。

3.  **关键创新：收益导向的损失函数**
    *   **设计理念**：训练模型的关键在于损失函数的设计。该方法的核心是创建一个直接与投资组合的未来收益挂钩的损失函数，从而引导模型朝着收益最大化的方向进行优化。
    *   **主要版本**：
        *   **L1-L4**：基于“加权收益率变化”构建，并引入了不同的归一化方法（如最大值归一化、Lp范数归一化等）。
        *   **L5 (PRC)**：一个特别的版本，直接使用“价格差”替代“收益率差”。实验证明此版本效果最佳，因为它保留了原始价格信息，让深度学习模型能捕捉到收益率计算中可能丢失的模式。
    *   **平滑改进 (Smooth)**：由于原始损失函数中的符号函数（Sign）在零点不可导，作者使用`tanh`函数进行替换，使其变得平滑可导，更有利于模型训练。

4.  **实验结果与发现**
    *   **数据集**：基于30年的标普500数据进行回测。
    *   **关键结论**：
        *   在多种模型和损失函数组合中，启用现金管理（Hold）和平滑（Smooth）策略通常能获得更好结果。
        *   L3（Lp范数归一化）和L5（价格差）版本的损失函数表现突出。
        *   **L5 (PRC) 版本的策略在21-23年的回测中取得了超过50%的最高收益率**，甚至优于几种强化学习方法。

---

### **核心结论 (一句话)**

通过设计一个直接以投资组合收益为优化目标的损失函数，可以端到端地训练一个深度学习模型，使其直接输出包含选股、仓位和风控的完整交易策略。

---

### **总体框架 (Overarching Framework)**

该框架提出了一种端到端的交易策略学习方法。它始于输入市场特征数据，将其喂给一个结构可变的深度学习模型。该模型的输出层被特殊设计为N+1维，直接代表N支股票和现金的投资组合权重。训练的核心是一个创新的“收益导向损失函数”，它将模型的输出（投资组合权重）与下一期的实际市场收益进行比较，计算出以收益为导向的损失。通过反向传播算法，这个损失被用来不断优化模型参数，最终目标是让模型学会如何调整投资组合权重以最大化未来的投资回报。整个过程形成了一个闭环，最终产出一个可以直接在市场中执行交易决策的、高度整合的策略模型。

---

### **核心概念图 (Mermaid Conceptual Map)**

<Mermaid_Diagram>
graph TD
    subgraph "输入与模型 (Input & Model)"
        A["市场特征数据"] --> B{"深度学习模型 (结构无关)"};
        B --> C["输出层 (N+1维)"];
        C -- "生成" --> D["投资组合权重 (V) \n (做多/做空/现金)"];
    end

    subgraph "训练核心：收益导向的优化 (Training Core: Return-Oriented Optimization)"
        F["实际市场数据 \n (收益率/价格)"] -- "市场反馈" --> E{"收益导向的损失函数"};
        D -- "模型预测" --> E;
        subgraph "损失函数变体"
            G["L1-L4: 基于收益率"]
            H["L5(PRC): 基于价格差"]
            I["平滑改进 (Smooth)"]
        end
        E --- G & H & I
        E -- "计算损失" --> J["反向传播优化"];
        J -- "更新模型参数" --> B;
    end
    
    J --> K["最终策略模型 \n (端到端决策)"];

    style A fill:#ADD8E6,stroke:#333,stroke-width:1.5px
    style B fill:#F9F7D8,stroke:#333,stroke-width:2px
    style C fill:#FFFFE0,stroke:#333,stroke-width:1px
    style D fill:#98FB98,stroke:#333,stroke-width:1.5px
    style E fill:#FFB6C1,stroke:#333,stroke-width:2.5px
    style F fill:#ADD8E6,stroke:#333,stroke-width:1.5px
    style J fill:#D3D3D3,stroke:#333,stroke-width:1.5px
    style K fill:#B19CD9,stroke:#333,stroke-width:2.5px
    style G fill:#FFE4E1,stroke:#333,stroke-width:1px
    style H fill:#FFE4E1,stroke:#333,stroke-width:1px
    style I fill:#FFE4E1,stroke:#333,stroke-width:1px
</Mermaid_Diagram>

Content:
今天给大家介绍一篇思路心引的文章我们的交易策略通常包括悬骨,则是仓位控制,风险控制之损等等,包含多个环节而本篇文章提出能不能就用深度学习模型一套模型就完成所有的动作直接学习出能够产生最有策略的模型这就是标题当中的Directly Learning直接学习通过收益导向的损失韩数,直接学习出交易策略这张图是文章的核心,左边是一个神经网路模型不限制具体模型,可以是Transform,可以是Arn,也可以是赶制器只是在最后的输出层要求输出的围度是N加1N是骨票的数量加1代表后的就是持有现金鸡火含数是10所有输出的取值范围会在服役到1之间现在我们展示思考一下总共N加1为的输出N是骨票的数量输出可以用来控制每个骨票的持商比例输出的符号,大雨灵是做多,小雨灵是做空N加1当中的加1代表持有现金输出用来控制持有现金的比例也就解决了封控的问题如果得到这个模型就可以实现整个交易策略关键是怎么让这个模型进行训练这就是文章提出的收益导向的算是含速算是含速的输入包括V,Ritun和SignV是基于输出层输出层的结果计算得到的权重Ritun是实际的收益率Sign是输出的符号,符1和1作者实现了若干版本的算是含速第一个版本在扩号当中的前半部分是加权收益差Vi是骨票的权重两个Ritun相检是收益率变化Sign是多空方向最这里的Vi这个i是取之范围是1-n不包括最后那个后的后半部分H表示只有现金这里的小E是一个人微设定的开关当E为0的时候相当于忽略后的对于策略来说是希望扩号内的值越大越好通过前面加一个符号来转为求最小化成为的算是含速这是第二个版本的算是含速将Ritun部分做了规矣化处以一个Max这是第三个版本它使用AR范畴做规矣化然后第四个版本呢是也是做规矣化不过分母的不再是简单的处以Max而是一个加权求和值第五个版本很特别前面都是基于收益率变化可以看到两个Ritun相检第五个版本是居于价格值的变化这里就不是某一个算是含速而是将前面几个版本当中的Ritun相检的Ritun差替换为价格差前面讲的几个版本的算是含速其实是存在一个问题因为引入了SineSine的含速只是1和1在0处是不可倒的作者对此进行了改进它使用Tan和Han数来代替Sine也就是把前面所有的版本的损失含速的Sine都替换成Tan文章当中将其称为Smooth平滑在后面的实验中也做了对比从这个图上可以看到Tan和Han数的曲线是贴近Sine但是在0处是可倒的来看一下文章的实验数据是使用30年的标捕500的股票构造了8个特征用最后一年作为测试级测试级的前一年是验证级在之前的28年都是训练级试验中对比了多种模型包括DefomeTime,Crossformer,Autofomeer以及多层感知器这些模型是怎么使用呢所以就到这回到这张图前面是这个模型模型的最后再加一层只要加一层N加1却保输出的维度是N加1使得损失含速可以工作也就是说明这个结构可以失配多种模型看一下回测结果这是2023年一年的回测收益率说明一下表结构顶部的列分为后的和No后的表示损失含速是否包含后的效也就是用前面提到的人为设定的那个小异进行控制1到L4是四种版本的损失含速每一行代表一种具体的模型NS和S代表是否进行平滑从表上可以看到有两种模型这两种模型在L3的收益率最高也都启用了平滑也都启用了后的看另一个测试结果这里使用的是21年22年23年的回测结果都是用L3自损版本的自损含速同时也和几种强化学习的方法进行的对比从图上可以看到本文的方法实现了最高的收益率这三年最高的收益率超过50%有一点很有意思实现最高收益率的是使用PRC的版本也就是前文提到的版本5用价格差作者给的解释是价格差没有经过与踪进行平滑生肚学习模型能够学到特别的形型也就是说100块长到110块和一块钱长到1块一毛所包含的形型不一样生肚学习模型能够捕捉到这个形型这个很可惜文章的测试结果当中没有给出下一步质和最大回测的数值不过文章给出了代码大家可以自信负现我早时间也做一期负现的视频最后做一个总结这篇文章给的思路是通过设计损失含速来引导模型输出策略这其实是一个梦幻以求的方式只要模型足够强大速距堆进去直接训练出有效的策略如果在结合上大圆模型一起训练可能这就是未来的方向好了这次就到这里谢谢大家观看欢迎评论去讨论交流
